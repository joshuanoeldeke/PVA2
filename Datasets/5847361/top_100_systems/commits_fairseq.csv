commit_index,author_name,author_email,date,commit_hash,commit_link,commit_message,files_changed,are_we_interested,has_test_file,unittest_in_code,unittest_in_added_diffs,unittest_in_removed_diffs,pytest_in_code,pytest_in_added_diffs,pytest_in_removed_diffs,Mig: assert,Mig: fixture,Mig: import,Mig: skip,Mig: failure,Mig: testcase,Mig: add Param,tags,u_added_count_testcase_subclass,u_added_count_self_assert,u_added_count_setup,u_added_count_setupClass,u_added_count_teardown,u_added_count_teardownClass,u_added_count_unittest_skip,u_added_count_unittest_self_skip,u_added_count_unittest_expected_failure,u_added_count_unittest_mock_pattern,u_added_count_import_unittest,u_removed_count_testcase_subclass,u_removed_count_self_assert,u_removed_count_setup,u_removed_count_setupClass,u_removed_count_teardown,u_removed_count_teardownClass,u_removed_count_unittest_skip,u_removed_count_unittest_self_skip,u_removed_count_unittest_expected_failure,u_removed_count_unittest_mock_pattern,u_removed_count_import_unittest,p_added_count_native_assert,p_added_count_pytest_raises,p_added_count_simple_skip,p_added_count_mark_skip,p_added_count_pytest_expected_failure,p_added_count_fixture,p_added_count_usefixtures,p_added_count_parametrize,p_added_count_generic_mark,p_added_count_generic_pytest,p_added_count_monkeypatch,p_added_count_pytest_mock,p_added_count_import_pytest,p_removed_count_native_assert,p_removed_count_pytest_raises,p_removed_count_simple_skip,p_removed_count_mark_skip,p_removed_count_pytest_expected_failure,p_removed_count_fixture,p_removed_count_usefixtures,p_removed_count_parametrize,p_removed_count_generic_mark,p_removed_count_generic_pytest,p_removed_count_monkeypatch,p_removed_count_pytest_mock,p_removed_count_import_pytest,u_added_matches_testcase_subclass,u_added_matches_self_assert,u_added_matches_setup,u_added_matches_setupClass,u_added_matches_teardown,u_added_matches_teardownClass,u_added_matches_unittest_skip,u_added_matches_unittest_self_skip,u_added_matches_unittest_expected_failure,u_added_matches_unittest_mock_pattern,u_added_matches_import_unittest,u_removed_matches_testcase_subclass,u_removed_matches_self_assert,u_removed_matches_setup,u_removed_matches_setupClass,u_removed_matches_teardown,u_removed_matches_teardownClass,u_removed_matches_unittest_skip,u_removed_matches_unittest_self_skip,u_removed_matches_unittest_expected_failure,u_removed_matches_unittest_mock_pattern,u_removed_matches_import_unittest,p_added_matches_native_assert,p_added_matches_pytest_raises,p_added_matches_simple_skip,p_added_matches_mark_skip,p_added_matches_pytest_expected_failure,p_added_matches_fixture,p_added_matches_usefixtures,p_added_matches_parametrize,p_added_matches_generic_mark,p_added_matches_generic_pytest,p_added_matches_monkeypatch,p_added_matches_pytest_mock,p_added_matches_import_pytest,p_removed_matches_native_assert,p_removed_matches_pytest_raises,p_removed_matches_simple_skip,p_removed_matches_mark_skip,p_removed_matches_pytest_expected_failure,p_removed_matches_fixture,p_removed_matches_usefixtures,p_removed_matches_parametrize,p_removed_matches_generic_mark,p_removed_matches_generic_pytest,p_removed_matches_monkeypatch,p_removed_matches_pytest_mock,p_removed_matches_import_pytest
0,Sergey Edunov,edunov@fb.com,2017-09-14 17:22:43-07:00,e734b0fa58fcf02ded15c236289b3bd61c4cffdf,https://github.com/pytorch/fairseq/commit/e734b0fa58fcf02ded15c236289b3bd61c4cffdf,Initial commit,46,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,3,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestLabelSmoothing(unittest.TestCase):'],"[('True', '(gradcheck('), ('True', '(gradcheck(lambda x, y: criterion.apply(x, y, 0.1, None, weights), (input, target)))'), ('True', '(gradcheck(lambda x, y: criterion.apply(x, y, 0.1, None, None), (input, target)))')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1,Sergey Edunov,edunov@fb.com,2017-09-15 11:40:28-07:00,a15acdb062952633be7e3048f473416e1e73bbbb,https://github.com/pytorch/fairseq/commit/a15acdb062952633be7e3048f473416e1e73bbbb,Architecture settings and readme updates,7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
2,Sergey Edunov,edunov@fb.com,2017-09-18 10:26:01-07:00,c6de2190f80633333d7ec07ac9cebc54c59585b6,https://github.com/pytorch/fairseq/commit/c6de2190f80633333d7ec07ac9cebc54c59585b6,More fixes,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
3,Sergey Edunov,edunov@fb.com,2017-09-18 12:53:13-07:00,a8bc4d0a6f82a9a4199deb6d97b0753f106ad62a,https://github.com/pytorch/fairseq/commit/a8bc4d0a6f82a9a4199deb6d97b0753f106ad62a,Small fix,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
4,Myle Ott,myleott@fb.com,2017-09-19 12:42:48-04:00,a615533788c1842483a9708787db0d73902dc1ec,https://github.com/pytorch/fairseq/commit/a615533788c1842483a9708787db0d73902dc1ec,"Better training support when GPUs are in ""exclusive mode""",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
5,Sergey Edunov,edunov@fb.com,2017-09-24 11:16:47-07:00,2d3161daa8dece8b9f593acdb19520c856d7c524,https://github.com/pytorch/fairseq/commit/2d3161daa8dece8b9f593acdb19520c856d7c524,"Issue #2, Checking size attribute of dst when dst is None",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
6,Myle Ott,myleott@fb.com,2017-09-26 11:05:51-07:00,03c4a71698ad1f64f08f83196987f655b05ef181,https://github.com/pytorch/fairseq/commit/03c4a71698ad1f64f08f83196987f655b05ef181,Fix generation when vocabulary is small relative to beam size (fixes #7),5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
7,Myle Ott,myleott@fb.com,2017-09-28 09:32:10-07:00,4593ebfaf9cd88934cb46063744bf0fbec48382d,https://github.com/pytorch/fairseq/commit/4593ebfaf9cd88934cb46063744bf0fbec48382d,Fix handling of partially-empty initial batch (#11),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
8,Myle Ott,myleott@fb.com,2017-09-27 19:33:32-07:00,2ad588856238aadd27120078810c742c490c4aac,https://github.com/pytorch/fairseq/commit/2ad588856238aadd27120078810c742c490c4aac,Refactor PaddingCollater,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
9,Myle Ott,myleott@fb.com,2017-10-11 10:05:29-04:00,7aba60843f0d79033b1da1746481d5f870923ec0,https://github.com/pytorch/fairseq/commit/7aba60843f0d79033b1da1746481d5f870923ec0,Update progress_bar to be more robust to changes in tqdm (#21),3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
10,Sam Gross,colesbury@gmail.com,2017-10-11 10:14:19-04:00,ae0c05d920661cc1d66f2db9c2a8073092e7df0f,https://github.com/pytorch/fairseq/commit/ae0c05d920661cc1d66f2db9c2a8073092e7df0f,Fix call ordering to ATen addmm and sum (#22),2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,6,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestConvTBC(unittest.TestCase):'],"[('AlmostEqual', '(output_tbc.data.transpose(0, 1).transpose(1, 2), output1d.data)'), ('AlmostEqual', '(conv_tbc.weight.grad.data.transpose(0, 2), conv1d.weight.grad.data)'), ('AlmostEqual', '(conv_tbc.bias.grad.data, conv1d.bias.grad.data)'), ('AlmostEqual', '(input_tbc.grad.data.transpose(0, 1).transpose(1, 2), input1d.grad.data)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
11,Sergey Edunov,edunov@fb.com,2017-10-11 16:10:09-07:00,a8260d52d66740a501ae345692924715086d5e65,https://github.com/pytorch/fairseq/commit/a8260d52d66740a501ae345692924715086d5e65,BPE transformation for IWSLT,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
12,Sergey Edunov,edunov@fb.com,2017-10-11 16:11:28-07:00,8f058ea0fb41b5694e2c3130d7e917ae8489cf40,https://github.com/pytorch/fairseq/commit/8f058ea0fb41b5694e2c3130d7e917ae8489cf40,"Don't generate during training, add --quiet to generate.py",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
13,Sergey Edunov,edunov@fb.com,2017-10-11 16:12:59-07:00,3f9b9838c309e2688110e0fd6f6477aa64aee375,https://github.com/pytorch/fairseq/commit/3f9b9838c309e2688110e0fd6f6477aa64aee375,Ignore invalid sentences in test and valid,6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
14,Myle Ott,myleott@fb.com,2017-10-12 22:04:33-04:00,0ccd81d40790924f517617e955970981a539c1a2,https://github.com/pytorch/fairseq/commit/0ccd81d40790924f517617e955970981a539c1a2,"Update README.md

Update PyTorch install instructions",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
15,Myle Ott,myleott@fb.com,2017-10-13 09:44:14-04:00,9c5fa5c54413f8c954ab34fa41e4ed556896b73e,https://github.com/pytorch/fairseq/commit/9c5fa5c54413f8c954ab34fa41e4ed556896b73e,"Update README.md

Don't suggest Miniconda (see #24)",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
16,Myle Ott,myleott@fb.com,2017-10-14 23:37:16-04:00,2b482f638a2ff51eeed595b15f338333b5dd8856,https://github.com/pytorch/fairseq/commit/2b482f638a2ff51eeed595b15f338333b5dd8856,Fix --no-progress-bar option in generate.py (#115),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
17,Sergey Edunov,edunov@fb.com,2017-10-17 11:46:04-07:00,af86c1ac3db83b3c1475cc3f7d0014b6897fd704,https://github.com/pytorch/fairseq/commit/af86c1ac3db83b3c1475cc3f7d0014b6897fd704,Update En2Fr model,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
18,Myle Ott,myleott@fb.com,2017-09-25 10:21:13-07:00,59d599a27725452d4711ff610f5a65b87517a2e3,https://github.com/pytorch/fairseq/commit/59d599a27725452d4711ff610f5a65b87517a2e3,Move helper functions from generate.py to fairseq/dictionary.py,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
19,Myle Ott,myleott@fb.com,2017-09-25 10:49:16-07:00,7333d04d3f9b9b00f73f7f95fde303160eedb453,https://github.com/pytorch/fairseq/commit/7333d04d3f9b9b00f73f7f95fde303160eedb453,Support configurable BPE symbol,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
20,Louis Martin,louismartin@fb.com,2017-09-25 10:17:43-07:00,cb0d7b2ad1d3abc2ff4c270552f6278d0df59fdd,https://github.com/pytorch/fairseq/commit/cb0d7b2ad1d3abc2ff4c270552f6278d0df59fdd,Fix flake8 warnings,10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
21,Myle Ott,myleott@fb.com,2017-09-27 08:22:37-07:00,813352e12c519dbf0af0123443bbaa6a6b54ef0c,https://github.com/pytorch/fairseq/commit/813352e12c519dbf0af0123443bbaa6a6b54ef0c,Don't save/restore convolutional layers in incremental inference,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
22,Myle Ott,myleott@fb.com,2017-09-27 08:23:25-07:00,48631f7a3ccff3506de5cf212037626a6be727c8,https://github.com/pytorch/fairseq/commit/48631f7a3ccff3506de5cf212037626a6be727c8,Allow --max-len-a to be a float,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
23,Myle Ott,myleott@fb.com,2017-09-27 19:33:19-07:00,e432459b370643d859ba65f93bf60208d368d0a7,https://github.com/pytorch/fairseq/commit/e432459b370643d859ba65f93bf60208d368d0a7,Add optimizer history to checkpoints (and rearrange criterions slightly),6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
24,Myle Ott,myleott@fb.com,2017-09-28 12:08:44-07:00,8bafae2ee7044529543768eec63d8460d894f5c6,https://github.com/pytorch/fairseq/commit/8bafae2ee7044529543768eec63d8460d894f5c6,Better logging from criterions,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
25,Myle Ott,myleott@fb.com,2017-10-02 18:05:18-04:00,376c265f3582b06893cb9fe728239410644aff00,https://github.com/pytorch/fairseq/commit/376c265f3582b06893cb9fe728239410644aff00,Add support for NCCL v2,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
26,Louis Martin,louismartin@fb.com,2017-10-03 09:50:27-07:00,84754894b925e1e9af076dc875b70a51bdfcd7ba,https://github.com/pytorch/fairseq/commit/84754894b925e1e9af076dc875b70a51bdfcd7ba,Add attention matrix to output of SequenceGenerator,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
27,Louis Martin,louismartin@fb.com,2017-10-06 01:37:38-07:00,d92ce54c656b686243a6c7a87f7738e0a16e5234,https://github.com/pytorch/fairseq/commit/d92ce54c656b686243a6c7a87f7738e0a16e5234,Ignore generated files for temporal convolution tbc,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
28,Myle Ott,myleott@fb.com,2017-10-06 17:06:40-04:00,88a8bd42c825c889a69f1c3d0d097fcdd7ab95ec,https://github.com/pytorch/fairseq/commit/88a8bd42c825c889a69f1c3d0d097fcdd7ab95ec,Fix smoothed (sentence-level) BLEU calculation,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
29,Myle Ott,myleott@fb.com,2017-10-06 17:26:33-04:00,3f9700868da4d09988b7dfbd7d5b3a9d5ea7d401,https://github.com/pytorch/fairseq/commit/3f9700868da4d09988b7dfbd7d5b3a9d5ea7d401,More flexible gradient normalization,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
30,Myle Ott,myleott@fb.com,2017-10-12 10:21:09-04:00,eea50f3869d720a0b4ae64960da11bc3bc59881c,https://github.com/pytorch/fairseq/commit/eea50f3869d720a0b4ae64960da11bc3bc59881c,Refactor model saving/loading to be more reusable,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
31,Louis Martin,louismartin@fb.com,2017-10-12 02:30:10-07:00,cab76554bff7f65c1d423fbe6960012b52adaeb0,https://github.com/pytorch/fairseq/commit/cab76554bff7f65c1d423fbe6960012b52adaeb0,Refactor code in Tokenizer,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
32,Myle Ott,myleott@fb.com,2017-10-17 17:27:19-04:00,d646a4a8fc315ee0146365ba86c4f8afd9cfdb2c,https://github.com/pytorch/fairseq/commit/d646a4a8fc315ee0146365ba86c4f8afd9cfdb2c,Add support for additional optimizers,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
33,Myle Ott,myleott@fb.com,2017-10-17 17:29:31-04:00,84b82dc6d49f386675aee11b26eced623f24db40,https://github.com/pytorch/fairseq/commit/84b82dc6d49f386675aee11b26eced623f24db40,Simplify deps of build_model to only depend on dict (instead of dataset),5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
34,Myle Ott,myleott@fb.com,2017-10-17 15:17:58-07:00,8943fc78fc62343cfef53ca23807eebf1e63463f,https://github.com/pytorch/fairseq/commit/8943fc78fc62343cfef53ca23807eebf1e63463f,Fix language inference in generate.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
35,Myle Ott,myleott@fb.com,2017-10-17 18:19:54-07:00,ff2e8cf2b6dfde1e59d766c9e3e7e363dcf02ce1,https://github.com/pytorch/fairseq/commit/ff2e8cf2b6dfde1e59d766c9e3e7e363dcf02ce1,Fix handling of continuation tokens that precede <unk> in generate.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
36,Louis Martin,louismartin@users.noreply.github.com,2017-10-19 16:38:24+02:00,8b4c45a225c4737e2fcd14192891ad386403ef57,https://github.com/pytorch/fairseq/commit/8b4c45a225c4737e2fcd14192891ad386403ef57,Prevent math overflow when loss is too high,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
37,Myle Ott,myleott@fb.com,2017-10-19 10:54:08-04:00,104cead16ef010465228635158ae02b44b2e8210,https://github.com/pytorch/fairseq/commit/104cead16ef010465228635158ae02b44b2e8210,Set seed after each epoch to improve consistency when resuming,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
38,James Reed,jamesr66@vt.edu,2017-10-24 14:29:52-07:00,30953d8bc1155fcf734b9761de7647819797e2d7,https://github.com/pytorch/fairseq/commit/30953d8bc1155fcf734b9761de7647819797e2d7,Fix for building under clang: specify C++ build and use C++ linkage (#42),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
39,Myle Ott,myleott@fb.com,2017-11-01 15:37:19-04:00,bb3be24de31140ed9c6e36696b0017c8a0a034d0,https://github.com/pytorch/fairseq/commit/bb3be24de31140ed9c6e36696b0017c8a0a034d0,Update README with note about Docker (#49),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
40,Sergey Edunov,edunov@fb.com,2017-11-01 17:34:56-07:00,f6ac1aecb3329d2cbf3f1f17106b74ac51971e8a,https://github.com/pytorch/fairseq/commit/f6ac1aecb3329d2cbf3f1f17106b74ac51971e8a,Force UTF-8 encoding for dictionary files ( #41 ),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
41,Myle Ott,myleott@fb.com,2017-10-23 15:41:46-04:00,19a3865d0847824945065a0f6390d8a4e319adb4,https://github.com/pytorch/fairseq/commit/19a3865d0847824945065a0f6390d8a4e319adb4,Only consider EOS in beam search if it's among top-k candidates,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
42,Myle Ott,myleott@fb.com,2017-10-23 12:44:14-07:00,415bf630bdaaa3adfc71fda1bd8fad14979553bf,https://github.com/pytorch/fairseq/commit/415bf630bdaaa3adfc71fda1bd8fad14979553bf,Fix description for `--sample-without-replacement` option,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
43,Myle Ott,myleott@fb.com,2017-10-23 17:45:13-04:00,3af8ec824c657af6fcc49db1a1199e609a4ccc7b,https://github.com/pytorch/fairseq/commit/3af8ec824c657af6fcc49db1a1199e609a4ccc7b,Support custom dictionary in preprocess.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
44,Myle Ott,myleott@fb.com,2017-10-25 14:31:06-04:00,820f796fd9f67014bf933bc25b0943c70d2d539d,https://github.com/pytorch/fairseq/commit/820f796fd9f67014bf933bc25b0943c70d2d539d,Add `--curriculum` option,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
45,Myle Ott,myleott@fb.com,2017-10-25 15:14:11-04:00,6e4b7e22eeb79f7e1c39d862f10ec3e61e51c979,https://github.com/pytorch/fairseq/commit/6e4b7e22eeb79f7e1c39d862f10ec3e61e51c979,"Refactor model definitions

* Move some functionality out of FConvModel into FairseqModel base class
* Move incremental decoding functionality into FairseqIncrementalDecoder module
* Refactor positional embeddings to be more specific to FConvModel",17,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
46,Michael Auli,3248766+michaelauli@users.noreply.github.com,2017-10-25 12:21:10-07:00,5fe8ea46fce75e2aafaa53a16bbe435f6605799d,https://github.com/pytorch/fairseq/commit/5fe8ea46fce75e2aafaa53a16bbe435f6605799d,Added -unkpen flag to generate.py following logic of Lua/Torch version,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
47,Myle Ott,myleott@fb.com,2017-10-25 21:07:24-04:00,2f781c5a16b05221c54094e1fb44e9692c1e433d,https://github.com/pytorch/fairseq/commit/2f781c5a16b05221c54094e1fb44e9692c1e433d,Support different max_source_positions and max_target_positions,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
48,Louis Martin,louismartin@fb.com,2017-10-26 01:38:31-07:00,ae6f6d56984ec7ab08ec232a1365374b933ee834,https://github.com/pytorch/fairseq/commit/ae6f6d56984ec7ab08ec232a1365374b933ee834,Fix call to non-existing to_string method,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
49,Myle Ott,myleott@fb.com,2017-10-26 13:11:41-04:00,5ef59abd1fb2cde1615d316ecc5185ee7b9ccfc7,https://github.com/pytorch/fairseq/commit/5ef59abd1fb2cde1615d316ecc5185ee7b9ccfc7,Fix seed so that data is properly shuffled between epochs,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
50,Myle Ott,myleott@fb.com,2017-10-27 12:21:33-04:00,8df95dcc1c3dd29096dde3417a832d3279f6f47f,https://github.com/pytorch/fairseq/commit/8df95dcc1c3dd29096dde3417a832d3279f6f47f,Upgrade args with max_source_positions and max_target_positions,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
51,Louis Martin,louismartin@users.noreply.github.com,2017-10-30 17:21:30+01:00,7ae79c126ca9e1d432aaa4f495400fdccc828893,https://github.com/pytorch/fairseq/commit/7ae79c126ca9e1d432aaa4f495400fdccc828893,"Refactor generation

* Split generate.py to generate.py and interactive.py and refactor code

The main motivation behind these changes is to try to decorrelate use
cases in order to implement future improvements such as unk replacement
with original string during evaluation on test and writing predictions
to output file.
The previous implementation worked well but I found it difficult to
integrate these future improvements.

* Add --replace-unk arg to be used without align dict

Replacing <unk> tokens can be beneficial even without an alignment
dictionary.",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
52,Myle Ott,myleott@fb.com,2017-10-31 09:45:07-04:00,97d7fcb973e52800a5387c6f88bb6c38be6847de,https://github.com/pytorch/fairseq/commit/97d7fcb973e52800a5387c6f88bb6c38be6847de,Left pad source and right pad target,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
53,Myle Ott,myleott@fb.com,2017-10-31 09:46:43-04:00,8f9dd964ca77ddbceafd1e1b74347d6781c096e1,https://github.com/pytorch/fairseq/commit/8f9dd964ca77ddbceafd1e1b74347d6781c096e1,Improvements to data loader,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
54,Myle Ott,myleott@fb.com,2017-10-31 19:25:50-04:00,e21901e835531afdda00f6675582cee1b12a36f7,https://github.com/pytorch/fairseq/commit/e21901e835531afdda00f6675582cee1b12a36f7,Fix interactive.py,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
55,Myle Ott,myleott@fb.com,2017-11-01 12:16:02-04:00,56c28099f5e02bccb52af2975936b422959a12a3,https://github.com/pytorch/fairseq/commit/56c28099f5e02bccb52af2975936b422959a12a3,Use `--lrshrink` as the reduction factor in ReduceLROnPlateau,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
56,Myle Ott,myleott@fb.com,2017-11-01 16:15:13-06:00,3278e8540ad5da258ebee3ea868e49311e30359d,https://github.com/pytorch/fairseq/commit/3278e8540ad5da258ebee3ea868e49311e30359d,Fix flake8 lint,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
57,Myle Ott,myleott@fb.com,2017-11-01 18:09:09-06:00,495530184d1356807f200b43a6c41eea043940f6,https://github.com/pytorch/fairseq/commit/495530184d1356807f200b43a6c41eea043940f6,Add dim to F.softmax calls,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
58,Louis Martin,louismartin@users.noreply.github.com,2017-11-02 10:53:28+01:00,2ef422f65f85fd332b52c8fc90541ecacc523bfc,https://github.com/pytorch/fairseq/commit/2ef422f65f85fd332b52c8fc90541ecacc523bfc,Update README with interactive.py and fix it,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
59,Myle Ott,myleott@fb.com,2017-11-04 18:28:49-04:00,f442f8967126a35ff8ac208f81998aff44acb8f4,https://github.com/pytorch/fairseq/commit/f442f8967126a35ff8ac208f81998aff44acb8f4,Add --max-sentence option for batching based on # sentences,8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
60,Myle Ott,myleott@fb.com,2017-11-04 18:45:51-04:00,7d44181dd46c60ae757937e071b89e75a7225bf3,https://github.com/pytorch/fairseq/commit/7d44181dd46c60ae757937e071b89e75a7225bf3,Loop over evaluation dataloader in descending order,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
61,Louis Martin,louismartin@users.noreply.github.com,2017-11-06 16:58:30+01:00,42a0150c377485952c292a9057da2eaf6f41f2fc,https://github.com/pytorch/fairseq/commit/42a0150c377485952c292a9057da2eaf6f41f2fc,"Replace unk with original string

* Add <eos> for unk replacement
* Add IndexedRawTextDataset to load raw text files
* Replace unk with original string
* Add load_raw_text_dataset() and --output-format
* Move has_binary_files to data.py",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
62,Myle Ott,myleott@fb.com,2017-11-07 09:26:12-05:00,b1dfd39eb2de74bb2259c7397fa873711952b660,https://github.com/pytorch/fairseq/commit/b1dfd39eb2de74bb2259c7397fa873711952b660,Revert `dim` in `F.softmax` for backwards compatibility,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
63,Myle Ott,myleott@fb.com,2017-11-07 17:12:23-05:00,e1f49695eee076a3f35e4ce9eebea9bc6c515780,https://github.com/pytorch/fairseq/commit/e1f49695eee076a3f35e4ce9eebea9bc6c515780,Rename LabelSmoothedCrossEntropy to LabelSmoothedNLLLoss,2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
64,Myle Ott,myleott@fb.com,2017-11-08 09:52:06-05:00,5c7f495484c89dc48aecad53a2836bcefe071e42,https://github.com/pytorch/fairseq/commit/5c7f495484c89dc48aecad53a2836bcefe071e42,Add LSTM,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
65,Myle Ott,myleott@fb.com,2017-11-08 20:55:06-05:00,ba5d7dcd1b43f3ce973b7c91a7ef6759e968177d,https://github.com/pytorch/fairseq/commit/ba5d7dcd1b43f3ce973b7c91a7ef6759e968177d,Only save most recent optimizer state in checkpoints (#53),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
66,Myle Ott,myleott@fb.com,2017-11-11 19:24:25-05:00,50fdf591464ca63940a2c1c5e7057b2f4df034f5,https://github.com/pytorch/fairseq/commit/50fdf591464ca63940a2c1c5e7057b2f4df034f5,Don't call forward directly (prefer module(x) to module.forward(x)),4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
67,Myle Ott,myleott@fb.com,2017-11-11 19:25:10-05:00,c6d6256ba52387066a47ce61da234e6a6a7d319c,https://github.com/pytorch/fairseq/commit/c6d6256ba52387066a47ce61da234e6a6a7d319c,Add `--log-format` option and JSON logger,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
68,Myle Ott,myleott@fb.com,2017-11-11 18:37:01-07:00,55a989e8cd9644412c9b354e806afb89dc19546c,https://github.com/pytorch/fairseq/commit/55a989e8cd9644412c9b354e806afb89dc19546c,Fix max_positions_valid in train.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
69,Myle Ott,myleott@fb.com,2017-11-11 19:26:53-07:00,83053f97b14c8d3c011f34297b281f59eb0890bb,https://github.com/pytorch/fairseq/commit/83053f97b14c8d3c011f34297b281f59eb0890bb,Fixes for `--log-format`,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
70,Myle Ott,myleott@fb.com,2017-11-09 12:49:08-05:00,d7d82715f968097bba08c92416d332d969bd1f06,https://github.com/pytorch/fairseq/commit/d7d82715f968097bba08c92416d332d969bd1f06,"Fix all-reduce for new versions of PyTorch

We previously assumed that once a model parameter's gradient buffer was allocated, it stayed fixed during training.
However, this assumption is violated in recent versions of PyTorch (i.e., the gradient buffer may be reallocated during
training), and it's no longer a safe assumption to make.

This is primarily relevant when we do the all-reduce, since we all-reduce a flattened (i.e., contiguous) copy of the
gradients. We can make this more robust by copying the result of the all-reduce back into the model parameter's gradient
buffers after each update. Intra-device copies are cheap, so this doesn't affect performance.",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
71,Myle Ott,myleott@fb.com,2017-11-08 08:04:28-07:00,13a3c811ca1ed2bf8c54386a255185af3f31b6b2,https://github.com/pytorch/fairseq/commit/13a3c811ca1ed2bf8c54386a255185af3f31b6b2,"Version 0.1.0 -> 0.2.0

Release notes:
- 5c7f495: Added simple LSTM model with input feeding and attention
- 6e4b7e2: Refactored model definitions and incremental generation to be cleaner
- 7ae79c1: Split interactive generation out of generate.py and into a new binary: interactive.py
- 19a3865: Subtle correctness fix in beam search decoder. Previously, for a beam size of k, we might emit a hypotheses
           if the <eos> was among the top 2*k candidates. Now we only emit hypotheses for which the <eos> is among the
           top-k candidates. This may subtly change generation results, and in the case of k=1 we will now produce
           strictly greedy outputs.
- 97d7fcb: Fixed bug in padding direction, where previously we right-padded the source and left-padded the target. We
           now left-pad the source and right-pad the target. This should not effect existing trained models, but may
           change (usually improves) the quality of new models.
- f442f89: Add support for batching based on the number of sentences (`--max-sentences`) in addition to the number of
           tokens (`--max-tokens`). When batching by the number of sentences, one can optionally normalize the gradients
           by the number of sentences with `--sentence-avg` (the default is to normalize by the number of tokens).
- c6d6256: Add `--log-format` option and JSON logger",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
72,Myle Ott,myleott@fb.com,2017-11-12 08:15:18-07:00,1b42c8c4c0c0aadc143c4ceb830e570f2cd2dd9a,https://github.com/pytorch/fairseq/commit/1b42c8c4c0c0aadc143c4ceb830e570f2cd2dd9a,Fallback to `--log-format=simple` for non-TTY terminals,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
73,Myle Ott,myleott@fb.com,2017-11-12 08:33:16-07:00,557b99d1bd54c8d0db9bf691b1939cd8a3016b16,https://github.com/pytorch/fairseq/commit/557b99d1bd54c8d0db9bf691b1939cd8a3016b16,Fix Flake8,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
74,Myle Ott,myleott@fb.com,2017-11-12 15:59:54-07:00,63dc27e89f33b2e2ef2e4a374c1c9e21612b0f0b,https://github.com/pytorch/fairseq/commit/63dc27e89f33b2e2ef2e4a374c1c9e21612b0f0b,Flush non-TTY logging output after each log interval,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
75,Myle Ott,myleott@fb.com,2017-11-12 20:00:49-07:00,7b08602146cc25ce8cad1bc3088bfa7315af1745,https://github.com/pytorch/fairseq/commit/7b08602146cc25ce8cad1bc3088bfa7315af1745,Make LSTM backwards compatible and fix incremental generation,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
76,Myle Ott,myleott@fb.com,2017-11-12 20:41:50-07:00,3524b661b7926e07f0b9fe4a7c82c1d7478ac28a,https://github.com/pytorch/fairseq/commit/3524b661b7926e07f0b9fe4a7c82c1d7478ac28a,Remove Python 3.6 format strings (fixes #55),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
77,Myle Ott,myleott@fb.com,2017-11-12 23:00:04-05:00,15dccfbfa449839d04492cb1b95ec8dbf9c161df,https://github.com/pytorch/fairseq/commit/15dccfbfa449839d04492cb1b95ec8dbf9c161df,Remove more Python 3.6 format strings (fixes #57) (#58),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
78,Myle Ott,myleott@fb.com,2017-11-13 14:22:43-05:00,3e3529e587c9a77df327d0c9f9cc0fde9b4d6287,https://github.com/pytorch/fairseq/commit/3e3529e587c9a77df327d0c9f9cc0fde9b4d6287,Remove Python3.6 format string from preprocess.py (fixes #60) (#61),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
79,Myle Ott,myleott@fb.com,2017-11-13 14:41:40-05:00,884e30464b8f6a74bfaea5688b31e2005e0fbef5,https://github.com/pytorch/fairseq/commit/884e30464b8f6a74bfaea5688b31e2005e0fbef5,Update requirements.txt and fix flake8 (#62),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
80,Zrachel,zhangruiqing01@baidu.com,2017-11-24 13:14:06+08:00,90b2d8de7ed058dd11d5dc08896d204ce8bfd26e,https://github.com/pytorch/fairseq/commit/90b2d8de7ed058dd11d5dc08896d204ce8bfd26e,fix bug in lstm model (#68),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
81,toothlessdragon,oojensen@alumni.stanford.edu,2017-12-01 22:07:32-08:00,d74f200aa800aa418841e74568c1ce3e1b1b8a39,https://github.com/pytorch/fairseq/commit/d74f200aa800aa418841e74568c1ce3e1b1b8a39,Fixed 2 typos (#75),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
82,Myle Ott,myleott@fb.com,2017-11-15 14:19:57-07:00,be274623e07c8309d30d14cff4c083187d73fe3b,https://github.com/pytorch/fairseq/commit/be274623e07c8309d30d14cff4c083187d73fe3b,Improve error when resuming training with a different model architecture,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
83,Myle Ott,myleott@fb.com,2017-11-17 16:09:11-05:00,a233fceb859593143dbc29226f483f29253976ae,https://github.com/pytorch/fairseq/commit/a233fceb859593143dbc29226f483f29253976ae,Improve memory handling (recover from OOM and periodically empty caching allocator),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
84,Myle Ott,myleott@fb.com,2017-11-21 12:54:38-05:00,19fafae695806c177e42b09a6f86a22808f03766,https://github.com/pytorch/fairseq/commit/19fafae695806c177e42b09a6f86a22808f03766,Allow --lr to specify a fixed learning rate schedule,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
85,Myle Ott,myleott@fb.com,2017-11-21 19:21:49-05:00,bd46c5ecfbdac7006162a7adb34ba256a902d1d5,https://github.com/pytorch/fairseq/commit/bd46c5ecfbdac7006162a7adb34ba256a902d1d5,Prefer command-line configuration over checkpoint for optimizer state,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
86,Myle Ott,myleott@fb.com,2017-12-02 11:28:20-08:00,99493a854540927bcbef0a9a067492fd75a2b5ea,https://github.com/pytorch/fairseq/commit/99493a854540927bcbef0a9a067492fd75a2b5ea,Save number of GPUs in args (and checkpoints),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
87,Myle Ott,myleott@fb.com,2017-12-04 14:40:08-08:00,9f3ccaa60c6eb1ed7877b07f3863d418efaf875f,https://github.com/pytorch/fairseq/commit/9f3ccaa60c6eb1ed7877b07f3863d418efaf875f,Fix weight norm dimension in decoder (fixes #73),5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
88,Myle Ott,myleott@fb.com,2017-12-04 16:43:24-08:00,10bf4074f1419cb66e3b865e5fdd83f233329778,https://github.com/pytorch/fairseq/commit/10bf4074f1419cb66e3b865e5fdd83f233329778,Rebuild optimizer when loading checkpoints,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
89,Myle Ott,myleott@fb.com,2017-12-06 11:34:12-08:00,0a836276129ef71fa6c44975dd02ab70bccc496d,https://github.com/pytorch/fairseq/commit/0a836276129ef71fa6c44975dd02ab70bccc496d,Fix conv padding for even kernel widths,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
90,Yann N. Dauphin,yann@dauphin.io,2018-01-05 10:31:06-08:00,9430544a3b95dbb2e3b86c303776e37f18188ad3,https://github.com/pytorch/fairseq/commit/9430544a3b95dbb2e3b86c303776e37f18188ad3,"Directly decay weight instead of L2 penalty (#157)

See https://arxiv.org/pdf/1711.05101.pdf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
91,Sergey Edunov,edunov@fb.com,2018-01-12 10:00:11-08:00,71d2d44c80886be700cebbef7d3a7d413e152c42,https://github.com/pytorch/fairseq/commit/71d2d44c80886be700cebbef7d3a7d413e152c42,Prepare scripts for WMT14,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
92,Myle Ott,myleott@fb.com,2017-12-07 22:12:52-08:00,cc7705d3b68a1f59ada2425498635be06c611c08,https://github.com/pytorch/fairseq/commit/cc7705d3b68a1f59ada2425498635be06c611c08,Fix generation bug with large beam sizes (>50),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
93,Myle Ott,myleott@fb.com,2017-12-21 15:45:50-05:00,9f7c3ec6a6f8c9a12aed4ae58c39e7fcd5186859,https://github.com/pytorch/fairseq/commit/9f7c3ec6a6f8c9a12aed4ae58c39e7fcd5186859,Add support for sharded generation,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
94,Myle Ott,myleott@fb.com,2017-12-22 09:52:42-05:00,a09fe8035d51d1a6adb29f688cdf6d19c91febe9,https://github.com/pytorch/fairseq/commit/a09fe8035d51d1a6adb29f688cdf6d19c91febe9,Fix BeamableMM,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
95,Myle Ott,myleott@fb.com,2017-12-22 10:11:22-05:00,a4f86a898dff01faa06edcad05493d84a2b49c16,https://github.com/pytorch/fairseq/commit/a4f86a898dff01faa06edcad05493d84a2b49c16,Better error message for --decoder-attention,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
96,Myle Ott,myleott@fb.com,2017-12-22 14:14:52-05:00,5637d54eeb5080ec60b37adde8ab71d3bfe76af7,https://github.com/pytorch/fairseq/commit/5637d54eeb5080ec60b37adde8ab71d3bfe76af7,Minor fix for strip_pad functions,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
97,Myle Ott,myleott@fb.com,2017-12-22 20:56:42-05:00,7da4e0629e39a9841fceea6db711aed6447b2a4f,https://github.com/pytorch/fairseq/commit/7da4e0629e39a9841fceea6db711aed6447b2a4f,Support deprecation of volatile Variables in latest PyTorch,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
98,Myle Ott,myleott@fb.com,2017-12-26 09:50:37-05:00,18a6d85c8830afe1bc49185635a2f8384d07b745,https://github.com/pytorch/fairseq/commit/18a6d85c8830afe1bc49185635a2f8384d07b745,Add explicit dimension to softmax calls,7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
99,Myle Ott,myleott@fb.com,2017-12-26 11:22:48-05:00,fa508492dffcaebd0a80b13a1918852229cd123b,https://github.com/pytorch/fairseq/commit/fa508492dffcaebd0a80b13a1918852229cd123b,Output number of model parameters in train.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
100,Myle Ott,myleott@fb.com,2017-12-26 12:53:14-08:00,dcbf5e7533dfaf3f7903b21e159243e63910ae67,https://github.com/pytorch/fairseq/commit/dcbf5e7533dfaf3f7903b21e159243e63910ae67,Raise FileNotFoundError if dictionary files don't exist,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
101,Myle Ott,myleott@fb.com,2018-01-01 09:33:43-08:00,6f6cb4ab45ff87b50d361992aad022528a1cb747,https://github.com/pytorch/fairseq/commit/6f6cb4ab45ff87b50d361992aad022528a1cb747,Add reduce kwarg to criterions,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
102,Myle Ott,myleott@fb.com,2018-01-01 10:23:08-08:00,eb005cdb2bdfc574a2d8a317a965eb7cb2023205,https://github.com/pytorch/fairseq/commit/eb005cdb2bdfc574a2d8a317a965eb7cb2023205,Streamline data formatting utils,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
103,Myle Ott,myleott@fb.com,2018-01-01 10:34:00-08:00,c542884deca99ba7e9d96e02e02619621d311275,https://github.com/pytorch/fairseq/commit/c542884deca99ba7e9d96e02e02619621d311275,Add --max-sentences-valid to train.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
104,Myle Ott,myleott@fb.com,2018-01-01 10:34:50-08:00,dccf79092eb28fa7b176945b6c01906a261522f2,https://github.com/pytorch/fairseq/commit/dccf79092eb28fa7b176945b6c01906a261522f2,Add option to SequenceGenerator to retain dropout,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
105,Myle Ott,myleott@fb.com,2018-01-06 11:51:07-08:00,185a0df5991220e9cb5b7e6f639e22332f786e2d,https://github.com/pytorch/fairseq/commit/185a0df5991220e9cb5b7e6f639e22332f786e2d,Fix warning about deprecated `volatile` kwarg for Variables,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
106,Myle Ott,myleott@fb.com,2018-01-06 15:51:49-05:00,c21a6e29933f91b83ab417b182c328958dad0bbf,https://github.com/pytorch/fairseq/commit/c21a6e29933f91b83ab417b182c328958dad0bbf,Move positional embeddings into LearnedPositionalEmbedding module,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
107,Myle Ott,myleott@fb.com,2018-01-07 12:34:36-05:00,4db6579a6346b478d09166e672c9051f102e468b,https://github.com/pytorch/fairseq/commit/4db6579a6346b478d09166e672c9051f102e468b,"Move normalization of model output (e.g., via LSM) into model definition",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
108,Myle Ott,myleott@fb.com,2018-01-08 20:22:47-08:00,fd28c8806b6f3805194a37df2b8b95f7b076ce74,https://github.com/pytorch/fairseq/commit/fd28c8806b6f3805194a37df2b8b95f7b076ce74,Fix LearnedPositionalEmbedding,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
109,Myle Ott,myleott@fb.com,2018-01-09 07:20:13-08:00,08a74a326fdc5c96af00d002eb3254e449b3c5a4,https://github.com/pytorch/fairseq/commit/08a74a326fdc5c96af00d002eb3254e449b3c5a4,Fix gradient clipping when --clip-norm=0,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
110,Myle Ott,myleott@fb.com,2018-01-09 07:24:30-08:00,5eddda8b8adc0e2015e5664a26d6898c937130e9,https://github.com/pytorch/fairseq/commit/5eddda8b8adc0e2015e5664a26d6898c937130e9,Save dictionary in model base classes,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
111,Myle Ott,myleott@fb.com,2018-01-09 15:49:12-08:00,0b84ab197ae7ddb1a4098d1a0d0f0cec80a577e1,https://github.com/pytorch/fairseq/commit/0b84ab197ae7ddb1a4098d1a0d0f0cec80a577e1,Fix training,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
112,Myle Ott,myleott@fb.com,2018-01-12 10:27:50-08:00,907ca927ebb21a0a186c25ba3b9e79823ac0db14,https://github.com/pytorch/fairseq/commit/907ca927ebb21a0a186c25ba3b9e79823ac0db14,Better support for torch.no_grad (since volatile is deprecated),3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
113,Sergey Edunov,edunov@apache.org,2018-01-16 17:09:20-08:00,c5378602d488ba0e9c3abee362c259c1fe5a51f7,https://github.com/pytorch/fairseq/commit/c5378602d488ba0e9c3abee362c259c1fe5a51f7,Share input/output embed,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
114,Sergey Edunov,edunov@apache.org,2018-01-16 17:37:35-08:00,dd31fa92d0b98aa697bdcc4d1c5f9870e8b2dfbd,https://github.com/pytorch/fairseq/commit/dd31fa92d0b98aa697bdcc4d1c5f9870e8b2dfbd,Report log likelihood for label smoothing,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
115,Michael Auli,michaelauli@devfair026.maas,2018-01-16 18:58:55-08:00,173c577b9d747fb258bb44901511d11b1d9f3a4a,https://github.com/pytorch/fairseq/commit/173c577b9d747fb258bb44901511d11b1d9f3a4a,Momentum correction,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
116,Michael Auli,michaelauli@devfair026.maas,2018-01-17 18:17:39-08:00,66314a60d86d39763ff6d02306eb8d96deab812f,https://github.com/pytorch/fairseq/commit/66314a60d86d39763ff6d02306eb8d96deab812f,ATen Fix,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
117,Myle Ott,myleott@fb.com,2018-01-18 17:44:03-08:00,334694363b1e6b3bb8f1f999dd29fd0c2a926e9a,https://github.com/pytorch/fairseq/commit/334694363b1e6b3bb8f1f999dd29fd0c2a926e9a,Better warning message for inputs that are too long,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
118,Myle Ott,myleott@fb.com,2018-01-19 11:28:12-05:00,81ace092ef4ea9af523f9d50e8653a67124b5397,https://github.com/pytorch/fairseq/commit/81ace092ef4ea9af523f9d50e8653a67124b5397,Fix max_positions calculation in train.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
119,Myle Ott,myleott@fb.com,2018-01-19 18:27:54-05:00,f9362e87bd07346492802c30fe71192401f9195e,https://github.com/pytorch/fairseq/commit/f9362e87bd07346492802c30fe71192401f9195e,Output correct perplexity when training with --sentence-avg,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
120,Myle Ott,myleott@fb.com,2018-01-22 08:19:23-07:00,66d9fcf5c8f2fb6aded3b439b7b58b01f760f265,https://github.com/pytorch/fairseq/commit/66d9fcf5c8f2fb6aded3b439b7b58b01f760f265,Fix tests,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
121,Michael Auli,3248766+michaelauli@users.noreply.github.com,2018-01-19 14:48:02-08:00,ee36a6f3e310ff905f6d74b8d9eac4c3a220b188,https://github.com/pytorch/fairseq/commit/ee36a6f3e310ff905f6d74b8d9eac4c3a220b188,"Fixed Weight Decay Regularization in Adam

See https://arxiv.org/abs/1711.05101",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
122,Sergey Edunov,edunov@fb.com,2018-01-26 17:32:00-08:00,4185d3ed029cea29e652a7cb1bcb2849adecf1dc,https://github.com/pytorch/fairseq/commit/4185d3ed029cea29e652a7cb1bcb2849adecf1dc,Switch to news-commentary-v12,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
123,Joost Bastings,joost.bastings@gmail.com,2018-01-29 10:42:07-05:00,1ff3efce63f8ae45fbe64b2d566b73871a520147,https://github.com/pytorch/fairseq/commit/1ff3efce63f8ae45fbe64b2d566b73871a520147,"Ratio should be predlen/reflen not reflen/predlen

To be compatible with multi-bleu.
This seems to only affect the result_string.",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
124,Sergey Edunov,edunov@fb.com,2018-01-31 10:18:26-08:00,971c2d6363bb525c420338ccef8c20d2cafa12f1,https://github.com/pytorch/fairseq/commit/971c2d6363bb525c420338ccef8c20d2cafa12f1,Adding README and more parameters to En2De script,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
125,Sergey Edunov,edunov@fb.com,2018-01-31 14:47:47-08:00,2c18c2736582ac717ae79f5065bb1a3a6d6ad648,https://github.com/pytorch/fairseq/commit/2c18c2736582ac717ae79f5065bb1a3a6d6ad648,Update README with new models,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
126,Sergey Edunov,edunov@fb.com,2018-01-31 14:56:51-08:00,52b6119a533a6fe88ebaabf096e39083d254e93d,https://github.com/pytorch/fairseq/commit/52b6119a533a6fe88ebaabf096e39083d254e93d,spelling,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
127,Sergey Edunov,edunov@fb.com,2018-02-08 17:04:56-08:00,9a95121633a268bb4574ade8509c593c4d0a110c,https://github.com/pytorch/fairseq/commit/9a95121633a268bb4574ade8509c593c4d0a110c,Adjust weight decay by the current learning rate to make it work correctly during annealing,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
128,Myle Ott,myleott@fb.com,2018-02-12 15:38:55-05:00,7e86e30cc5b97db30403b738de2dbbb55f06a92b,https://github.com/pytorch/fairseq/commit/7e86e30cc5b97db30403b738de2dbbb55f06a92b,Allow larger maxlen (fixes #100) (#101),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
129,Myle Ott,myleott@fb.com,2018-02-27 17:09:42-05:00,66415206127c9e36586792f3217f8b0c42d9343c,https://github.com/pytorch/fairseq/commit/66415206127c9e36586792f3217f8b0c42d9343c,"fairseq-py goes distributed (#106)

This PR includes breaking API changes to modularize fairseq-py and adds support for distributed training across multiple nodes.

Changes:
- c7033ef: add support for distributed training! See updated README for usage.
- e016299: modularize fairseq-py, adding support for register_model, register_criterion, register_optimizer, etc.
- 154e440: update LSTM implementation to use PackedSequence objects in the encoder, better following best practices and improving perf
- 90c2973 and 1da6265: improve unit test coverage",62,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],4,79,1,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestBinaries(unittest.TestCase):', 'class TestSequenceGenerator(unittest.TestCase):', 'class TestSequenceScorer(unittest.TestCase):', 'class TestUtils(unittest.TestCase):']","[('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)'), ('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0])'), ('HypoTokens', '(hypos[0][1], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.9, 0.9, 1.0])'), ('HypoTokens', '(hypos[1][0], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.4, 1.0])'), ('HypoTokens', '(hypos[1][1], [w1, w2, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.6])'), ('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0], normalized=False)'), ('HypoTokens', '(hypos[0][1], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.9, 0.9, 1.0], normalized=False)'), ('HypoTokens', '(hypos[1][0], [w1, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.6], normalized=False)'), ('HypoTokens', '(hypos[1][1], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.4, 1.0], normalized=False)'), ('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[0][1], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.9, 0.9, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[1][0], [w1, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.6], lenpen=lenpen)'), ('HypoTokens', '(hypos[1][1], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.4, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[0][0], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][0], [0.1, 0.9, 0.9, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[0][1], [w1, eos])'), ('HypoScore', '(hypos[0][1], [0.9, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[1][0], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.4, 1.0], lenpen=lenpen)'), ('HypoTokens', '(hypos[1][1], [w1, w2, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.6], lenpen=lenpen)'), ('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0])'), ('HypoTokens', '(hypos[0][1], [w2, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.1, 0.6])'), ('HypoTokens', '(hypos[1][0], [w1, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.6])'), ('HypoTokens', '(hypos[1][1], [w2, w2, eos])'), ('HypoScore', '(hypos[1][1], [0.3, 0.9, 0.01])'), ('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0])'), ('HypoTokens', '(hypos[0][1], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.9, 0.9, 1.0])'), ('HypoTokens', '(hypos[1][0], [w2, w2, w2, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.3, 0.9, 0.99, 0.4, 1.0])'), ('HypoTokens', '(hypos[1][1], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.4, 1.0])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)'), ('HypoTokens', '(hypos[0], data[id][])'), ('HypoScore', '(hypos[0], expected_scores[id])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('AlmostEqual', '('), ('AlmostEqual', '('), ('True', '(isinstance(v, Variable))'), ('False', '(v.data.is_cuda)'), ('Equal', '(v.data.is_cuda, torch.cuda.is_available())'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)')]",['def setUp(self):'],[],[],[],[],[],[],[],"['import unittest', 'import unittest', 'import unittest', 'import unittest']",[],[],[],[],[],[],[],[],[],[],[],"['hasattr(args, )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
130,Myle Ott,myleott@fb.com,2018-02-14 07:39:06-08:00,b9f2d427471a170b3a34bcd46b30ed42c50886b8,https://github.com/pytorch/fairseq/commit/b9f2d427471a170b3a34bcd46b30ed42c50886b8,Add OOM counter back to logging output,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
131,Myle Ott,myleott@fb.com,2018-02-15 08:13:09-08:00,29c82741288703b6bd146017fa2ef6a5e09a2490,https://github.com/pytorch/fairseq/commit/29c82741288703b6bd146017fa2ef6a5e09a2490,Fix tests and flake8,4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],['from unittest import mock'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
132,Myle Ott,myleott@fb.com,2018-02-15 11:12:19-08:00,0d90e35f3b14bd242a64a7a2af744a258e7d0298,https://github.com/pytorch/fairseq/commit/0d90e35f3b14bd242a64a7a2af744a258e7d0298,More unit test fixes,6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],['from unittest import mock'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
133,Dario Pavllo,30368978+dariopavllo@users.noreply.github.com,2018-02-23 10:42:06-08:00,866b27d51a5e9857c10b21e34cf74d5cedb1714c,https://github.com/pytorch/fairseq/commit/866b27d51a5e9857c10b21e34cf74d5cedb1714c,"Add support to prefixes (#221)

* Add prefix

* Fixes

* Keep original scores with prefix

* Improve prefix code

* Replace 'repeat' with 'expand'",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
134,Myle Ott,myleott@fb.com,2018-02-23 19:59:35-08:00,78a6ef02660a9862942f06fa830dc16dd1b2a4f2,https://github.com/pytorch/fairseq/commit/78a6ef02660a9862942f06fa830dc16dd1b2a4f2,pytorch update: no need to rewrap variable in backward(),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
135,Myle Ott,myleott@fb.com,2018-02-23 22:05:49-08:00,e7094b14c90f9a69a1913e65446a7447f9ed9f30,https://github.com/pytorch/fairseq/commit/e7094b14c90f9a69a1913e65446a7447f9ed9f30,Fix LabelSmoothedCrossEntropy test,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
136,Myle Ott,myleott@fb.com,2018-02-24 01:19:59-05:00,9438019ff066116534c94e7f0a154aff6c51409e,https://github.com/pytorch/fairseq/commit/9438019ff066116534c94e7f0a154aff6c51409e,Refactor incremental generation to be more explicit and less magical (#222),9,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
137,Sergey Edunov,edunov@apache.org,2018-02-27 11:41:20-08:00,2f976aae0acb68941e45bb168d2d7a5ede91dd7d,https://github.com/pytorch/fairseq/commit/2f976aae0acb68941e45bb168d2d7a5ede91dd7d,"Making our code compatible with the latest pytorch (#223)

* Making our code compatible with the latest pytorch

* revert

* torch.nn.utils.clip_grad_norm now returns tensor",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
138,Myle Ott,myleott@fb.com,2018-03-01 13:03:17-05:00,3bde773d66ff8472c5e9fb4b63d9a33554e48c09,https://github.com/pytorch/fairseq/commit/3bde773d66ff8472c5e9fb4b63d9a33554e48c09,More fixes for recent PyTorch (incl. topk issue) (#113),4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
139,Myle Ott,myleott@fb.com,2018-03-01 14:04:08-05:00,6e4d370af9b5674b2f545843115c985c814d6e3c,https://github.com/pytorch/fairseq/commit/6e4d370af9b5674b2f545843115c985c814d6e3c,More updates for PyTorch (#114),4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Less', '(utils.item((t1 - t2).abs().max()), 1e-4)')]",[],[],[],[],[],[],[],[],[],[],"[('Less', '((t1 - t2).abs().max(), 1e-4)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
140,James Reed,jamesreed@fb.com,2018-03-01 20:09:54-08:00,56f9ec3c389a52a5786afd8152c4328d5ebea32a,https://github.com/pytorch/fairseq/commit/56f9ec3c389a52a5786afd8152c4328d5ebea32a,"Use ATen built-in conv_tbc method (#66)

Remove custom ConvTBC code",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
141,Myle Ott,myleott@fb.com,2018-03-02 19:26:42-08:00,5f29d1236c9d58989d59f52633df88ed92787460,https://github.com/pytorch/fairseq/commit/5f29d1236c9d58989d59f52633df88ed92787460,Small fixes,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
142,Myle Ott,myleott@fb.com,2018-03-04 15:44:05-05:00,e73fddf45377e8a3c0ea2e8281fae18f7b498dd6,https://github.com/pytorch/fairseq/commit/e73fddf45377e8a3c0ea2e8281fae18f7b498dd6,Filter padding properly in LabelSmoothedCrossEntropyCriterion (#229),3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,11,1,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(vocab, 4 + 3)  # 4 special + 3 tokens'), ('Equal', '(self.d.pad(), 1)'), ('Equal', '(self.d.eos(), 2)'), ('Equal', '(self.d.unk(), 3)'), ('Less', '(abs(nll_loss - nll_logging_output[]), 1e-6)'), ('Less', '(abs(nll_loss - smooth_logging_output[]), 1e-6)'), ('AlmostEqual', '(loss, loss1 + loss2)'), ('AlmostEqual', '(loss, unreduced_loss.sum())'), ('AlmostEqual', '(nll_loss, smooth_loss)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-6)')]",['def setUp(self):'],[],[],[],[],[],[],[],[],[],"[('True', '(gradcheck('), ('True', '(gradcheck(lambda x, y: criterion.apply(x, y, 0.1, None, weights), (input, target)))'), ('True', '(gradcheck(lambda x, y: criterion.apply(x, y, 0.1, None, None), (input, target)))')]",[],[],[],[],[],[],[],[],[],"['hasattr(args, )', 'self.args.probs.dim() == 3, \\']",[],[],[],[],[],[],[],[],[],[],[],[],"['hasattr(args, )']",[],[],[],[],[],[],[],[],[],[],[],[]
143,Sergey Edunov,edunov@apache.org,2018-03-05 14:12:27-08:00,b03b53b426deaa8bdf6c93d91e424c5571acf7e5,https://github.com/pytorch/fairseq/commit/b03b53b426deaa8bdf6c93d91e424c5571acf7e5,"Allow more flexible pre-processing and generation (#227)

* Allow more flexible pre-processing and generation

* Addressing CR comments

* small fix",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
144,Myle Ott,myleott@fb.com,2018-03-07 11:50:08-05:00,49aeab2d59b96933424e06685392eaed6a06e324,https://github.com/pytorch/fairseq/commit/49aeab2d59b96933424e06685392eaed6a06e324,Enforce upper-bound on maximum generation length (#121),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
145,Runqi Yang,runqiyang@gmail.com,2018-03-25 22:40:40+08:00,261d1822c903adf7cd8ea7c895e6ebc84dda5d53,https://github.com/pytorch/fairseq/commit/261d1822c903adf7cd8ea7c895e6ebc84dda5d53,"fix typo in data/README

Change ""awailable"" to ""available"".",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
146,Runqi Yang,runqiyang@gmail.com,2018-03-26 21:47:53+08:00,6268f20ef340b72ea549f1d20f86ef5ff988425d,https://github.com/pytorch/fairseq/commit/6268f20ef340b72ea549f1d20f86ef5ff988425d,"fix typo in data/README (#131)

Change ""awailable"" to ""available"".",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
147,Runqi Yang,runqiyang@gmail.com,2018-03-28 09:11:55+08:00,3c07295885c6283def573e7a6811464f250c3b28,https://github.com/pytorch/fairseq/commit/3c07295885c6283def573e7a6811464f250c3b28,"Update training commands

Update training commands in data/README to match the latest version of this project according to #132.

- Motivation: in the previous data/README, the commands are obsolete and will cause the error ""unrecognized arguments: --label-smoothing 0.1 --force-anneal 50"". 
- What's changed: add arguments ""--criterion label_smoothed_cross_entropy"" and ""--lr-scheduler fixed"" to the training commands of all 3 datasets.
- Result: the new commands run without error on all 3 datasets.",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
148,Runqi Yang,runqiyang@gmail.com,2018-03-28 09:20:43+08:00,435ed3517808a51492296f16f6cd75d586d95950,https://github.com/pytorch/fairseq/commit/435ed3517808a51492296f16f6cd75d586d95950,"Update training commands

Update training commands in data/README to match the latest version of this project according to #132.

Continue from 3c07295885c6283def573e7a6811464f250c3b28: add omitted ""\"".",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
149,Runqi Yang,runqiyang@gmail.com,2018-03-29 00:50:19+08:00,0a141e3f45f53bf884442204309e5caa0588d19f,https://github.com/pytorch/fairseq/commit/0a141e3f45f53bf884442204309e5caa0588d19f,"Update training command for IWSLT14

specify a single GPU setup for IWSLT14",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
150,Myle Ott,myleott@fb.com,2018-04-02 10:13:07-04:00,d3795d6cd1c66ac05dc0f4861ce69ab4680bff3d,https://github.com/pytorch/fairseq/commit/d3795d6cd1c66ac05dc0f4861ce69ab4680bff3d,"Merge internal changes (#136)

Changes:
- 7d19e36: Add `--sampling` flag to generate.py to sample instead of doing beam search
- c777340: Add `scripts/average_checkpoints.py` to average multiple checkpoints into a combined model
- 3ea882c: Add `--max-update` option to train.py to stop training after a given number of updates
- small bugfixes for distributed training, LSTM, inverse square root LR scheduler",35,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,3,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestAverageCheckpoints(unittest.TestCase):'],"[('Equal', '('), ('AlmostEqual', '('), ('AlmostEqual', '(')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
151,alexeib,alexei.b@gmail.com,2018-04-11 00:15:31-07:00,6532e32bdb7b718fb51b33378071a4da9f27acd8,https://github.com/pytorch/fairseq/commit/6532e32bdb7b718fb51b33378071a4da9f27acd8,make interactive mode print out alignment nicely,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
152,Myle Ott,myleott@fb.com,2018-05-01 08:09:15-06:00,56099c74662421b54a6a0caee9356c74f554864b,https://github.com/pytorch/fairseq/commit/56099c74662421b54a6a0caee9356c74f554864b,Disallow --batch-size in interactive.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
153,Myle Ott,myleott@fb.com,2018-05-01 10:06:45-04:00,66ee3df9071ea17b32f2ccc40d6dd092a82a551f,https://github.com/pytorch/fairseq/commit/66ee3df9071ea17b32f2ccc40d6dd092a82a551f,Update README.md,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
154,ngimel,ngimelshein@nvidia.com,2018-05-09 05:43:41-07:00,48c4c6d3508cacf60a83212f2ee688cf356ee272,https://github.com/pytorch/fairseq/commit/48c4c6d3508cacf60a83212f2ee688cf356ee272,use implicit padding when possible (#152),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
155,Sai,sai-prasanna@users.noreply.github.com,2018-05-09 18:31:58+05:18,e40363d7089d202d1ffbe66ffedcb628a0db27e8,https://github.com/pytorch/fairseq/commit/e40363d7089d202d1ffbe66ffedcb628a0db27e8,Add pretrained embedding support (#151),3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
156,Myle Ott,myleott@fb.com,2018-05-09 07:15:13-06:00,4973d05ac7651f46e1c7edc8060600b44baf9e71,https://github.com/pytorch/fairseq/commit/4973d05ac7651f46e1c7edc8060600b44baf9e71,Flake8,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
157,Myle Ott,myleott@fb.com,2018-05-21 09:26:15-04:00,3ae97589bcd4083130a511af18106709b6cb1f66,https://github.com/pytorch/fairseq/commit/3ae97589bcd4083130a511af18106709b6cb1f66,Fix old model checkpoints after #151 (fixes #156) (#157),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
158,theweiho,theweiho@users.noreply.github.com,2018-05-22 13:41:01-07:00,29153e279f659cb27732622b52026d0d7d5de817,https://github.com/pytorch/fairseq/commit/29153e279f659cb27732622b52026d0d7d5de817,Update dataset code for use by https://github.com/pytorch/translate/pull/62 (#161),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
159,Myle Ott,myleott@fb.com,2018-05-24 13:38:12-04:00,ec0031df7b2c7d583b13efaffbb7a2f03e87378c,https://github.com/pytorch/fairseq/commit/ec0031df7b2c7d583b13efaffbb7a2f03e87378c,Merge internal changes (#163),6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
160,Myle Ott,myleott@fb.com,2018-06-14 17:39:04-04:00,d62a86511e6b24b29fbef6db25246cd05eb2878b,https://github.com/pytorch/fairseq/commit/d62a86511e6b24b29fbef6db25246cd05eb2878b,0.4.0 -> 0.5.0,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
161,Myle Ott,myleott@fb.com,2018-04-02 14:22:30-06:00,0e8414f9ae8d86176221c67dba9925adabfe0dd8,https://github.com/pytorch/fairseq/commit/0e8414f9ae8d86176221c67dba9925adabfe0dd8,Remove sweep_log prefix from json progress bar,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
162,Myle Ott,myleott@fb.com,2018-04-04 10:38:57-04:00,871be38952f0e00c22b56338f70f034dfe65054d,https://github.com/pytorch/fairseq/commit/871be38952f0e00c22b56338f70f034dfe65054d,Faster fconv generation,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
163,Myle Ott,myleott@fb.com,2018-04-06 16:24:30-06:00,b84070b737f4913d8a2ff4385d346924aa608e05,https://github.com/pytorch/fairseq/commit/b84070b737f4913d8a2ff4385d346924aa608e05,Fix LSTM,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
164,alexeib,alexei.b@gmail.com,2018-04-08 23:43:21-07:00,fe54ea547779f173a78832a81c0b51df064fc24a,https://github.com/pytorch/fairseq/commit/fe54ea547779f173a78832a81c0b51df064fc24a,fix optim history,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
165,alexeib,alexei.b@gmail.com,2018-04-09 11:03:03-07:00,6a7c8d0d68d129941162ab56998822bcfd5eb79d,https://github.com/pytorch/fairseq/commit/6a7c8d0d68d129941162ab56998822bcfd5eb79d,address comments,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
166,Myle Ott,myleott@fb.com,2018-03-05 07:07:49-07:00,97b58b46c76d1b165fc9d13a43768b188deac309,https://github.com/pytorch/fairseq/commit/97b58b46c76d1b165fc9d13a43768b188deac309,Add Transformer model,6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
167,Myle Ott,myleott@fb.com,2018-03-06 10:09:09-07:00,559eca819149a137922ac8f43697964610818887,https://github.com/pytorch/fairseq/commit/559eca819149a137922ac8f43697964610818887,Remove Google batching stategy (it's not needed),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
168,Myle Ott,myleott@fb.com,2018-03-13 16:52:06-06:00,1235aa0809633134fd2e5d1de81721c344d6d649,https://github.com/pytorch/fairseq/commit/1235aa0809633134fd2e5d1de81721c344d6d649,Pass args around to cleanup parameter lists,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
169,Myle Ott,myleott@fb.com,2018-04-01 15:35:03-06:00,f68a44359b6596997b931d2e662a899ffba9d407,https://github.com/pytorch/fairseq/commit/f68a44359b6596997b931d2e662a899ffba9d407,Bug fixes,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
170,Myle Ott,myleott@fb.com,2018-04-02 11:34:03-06:00,5935fe2fbfe5c84c64e7b72ff58f2e7bafa77da1,https://github.com/pytorch/fairseq/commit/5935fe2fbfe5c84c64e7b72ff58f2e7bafa77da1,Fix flake8,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
171,Myle Ott,myleott@fb.com,2018-04-03 12:01:16-06:00,81b47e7ee6836f999ef027e41f07bd473b4d6170,https://github.com/pytorch/fairseq/commit/81b47e7ee6836f999ef027e41f07bd473b4d6170,Fix buffers in sinusoidal positional embeddings,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
172,Alexei Baevski,alexei.b@gmail.com,2018-04-05 13:00:35-07:00,b2374e52f84e138811864dfb10d8ff768d396b2e,https://github.com/pytorch/fairseq/commit/b2374e52f84e138811864dfb10d8ff768d396b2e,"caching v3 (cache keys, values, process only last time step) (#241)

- process only last time step during generation
- cache keys and values
- dont apply masking during generation",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
173,alexeib,alexei.b@gmail.com,2018-04-05 13:56:10-07:00,fc8306852ea5bdf2ed73543cdf6a2f84dc6faa14,https://github.com/pytorch/fairseq/commit/fc8306852ea5bdf2ed73543cdf6a2f84dc6faa14,smarter way to avoid applying encoder key mask,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
174,Myle Ott,myleott@fb.com,2018-04-05 14:53:17-06:00,36e360d907130333c2823f3e9556f71585d16707,https://github.com/pytorch/fairseq/commit/36e360d907130333c2823f3e9556f71585d16707,Use PyTorch LayerNorm and improve weight init,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
175,Myle Ott,myleott@fb.com,2018-04-06 17:07:12-06:00,60c4081b061c1fa813fc319087231478901f2e73,https://github.com/pytorch/fairseq/commit/60c4081b061c1fa813fc319087231478901f2e73,More improvements to weight init and FP16 support,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
176,Sergey Edunov,edunov@apache.org,2018-04-07 14:29:58-07:00,2d27ae084ab1a376344d3fe803e35b8d5cbd81b6,https://github.com/pytorch/fairseq/commit/2d27ae084ab1a376344d3fe803e35b8d5cbd81b6,Simulated big batches,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
177,Myle Ott,myleott@fb.com,2018-04-07 15:13:30-06:00,d6be0c7e001c51fc7639304a463dae4041d535f6,https://github.com/pytorch/fairseq/commit/d6be0c7e001c51fc7639304a463dae4041d535f6,Use FP32 for multi-head attention softmax,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
178,Sergey Edunov,edunov@apache.org,2018-04-07 15:05:02-07:00,c52f6ea4fc8f58b389fd806c117f501fb8234976,https://github.com/pytorch/fairseq/commit/c52f6ea4fc8f58b389fd806c117f501fb8234976,better batching,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
179,Myle Ott,myleott@fb.com,2018-04-07 18:21:45-04:00,4fa8760e9a1f93320007aa9060e7c60fd2e90cc7,https://github.com/pytorch/fairseq/commit/4fa8760e9a1f93320007aa9060e7c60fd2e90cc7,Improve dataloader speed and deprecate concept of batch_offset (use --sample-without-replacement instead),3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
180,Myle Ott,myleott@fb.com,2018-04-07 21:44:29-06:00,47b3b81c0d0266d18f765e5656ad6667bf592749,https://github.com/pytorch/fairseq/commit/47b3b81c0d0266d18f765e5656ad6667bf592749,Allow schedule for update-freq,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
181,Myle Ott,myleott@fb.com,2018-04-07 20:14:53-06:00,73a87327ed703284b42eaff077e9228a9f579498,https://github.com/pytorch/fairseq/commit/73a87327ed703284b42eaff077e9228a9f579498,Fix batching during generation,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
182,Myle Ott,myleott@fb.com,2018-04-10 22:35:01-04:00,7ee1d28458da68a76903a38dda5164e6abcaebf1,https://github.com/pytorch/fairseq/commit/7ee1d28458da68a76903a38dda5164e6abcaebf1,Add FP16 support,9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
183,Myle Ott,myleott@fb.com,2018-04-11 11:17:39-06:00,26f87c7d6b8c4f5be00424598bbc71c224b1870a,https://github.com/pytorch/fairseq/commit/26f87c7d6b8c4f5be00424598bbc71c224b1870a,Make dictionary size a multiple of 8,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
184,Myle Ott,myleott@fb.com,2018-04-12 08:23:45-06:00,4cd2bb702b2715d3cd38e7106c9d52c3b229eba2,https://github.com/pytorch/fairseq/commit/4cd2bb702b2715d3cd38e7106c9d52c3b229eba2,"Revert ""Make dictionary size a multiple of 8""

This reverts commit b2e119c209363e6ff6d2878a69c7d1a507a2e9be.",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
185,Myle Ott,myleott@fb.com,2018-04-12 08:34:25-06:00,745d5fbd7f640e1fd04f17981c4816659ad64c04,https://github.com/pytorch/fairseq/commit/745d5fbd7f640e1fd04f17981c4816659ad64c04,Pad dictionary to be a multiple of 8 in preprocessing,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
186,Myle Ott,myleott@fb.com,2018-04-12 10:45:42-07:00,bcdc27dcf1d6ab324af160cbb75fd3031c9dc567,https://github.com/pytorch/fairseq/commit/bcdc27dcf1d6ab324af160cbb75fd3031c9dc567,No more magical --fp16,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
187,Alexei Baevski,alexei.b@gmail.com,2018-04-12 13:50:15-07:00,2a84f46bf0a05ae5b0caaf6724808dfe65dac638,https://github.com/pytorch/fairseq/commit/2a84f46bf0a05ae5b0caaf6724808dfe65dac638,"remove completed sentences from batch

remove completed sentences from batch and allow batching uneven lengths (with fixes to make padded sequences work correctly in all models)",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
188,Myle Ott,myleott@fb.com,2018-04-12 15:43:41-06:00,8fcdb9b7267b9864ff9da0422f7c81c5615800f9,https://github.com/pytorch/fairseq/commit/8fcdb9b7267b9864ff9da0422f7c81c5615800f9,Fix Flake8,8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
189,Myle Ott,myleott@fb.com,2018-04-13 07:25:49-06:00,f607d9e89a17722a82fc2a0034ef9faf369c4cd2,https://github.com/pytorch/fairseq/commit/f607d9e89a17722a82fc2a0034ef9faf369c4cd2,Small optimization for LSTM,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
190,Myle Ott,myleott@fb.com,2018-04-12 15:58:25-06:00,fa7c575a1c8d698e4eb8c58597ae8461a3d7f007,https://github.com/pytorch/fairseq/commit/fa7c575a1c8d698e4eb8c58597ae8461a3d7f007,Fix preprocess.py,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
191,Myle Ott,myleott@fb.com,2018-04-17 16:07:08-04:00,1ec5f0a0e4c667135e651206383bf4defbcee4f7,https://github.com/pytorch/fairseq/commit/1ec5f0a0e4c667135e651206383bf4defbcee4f7,Use eval() to parse args.lr,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
192,Alexei Baevski,alexei.b@gmail.com,2018-04-23 00:03:51-07:00,c6d4386c1574d715263e1cd8200f94a26f74ec13,https://github.com/pytorch/fairseq/commit/c6d4386c1574d715263e1cd8200f94a26f74ec13,Fix embedding initialization for padding,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
193,Myle Ott,myleott@fb.com,2018-04-21 19:56:51-06:00,dc40ac58f701bac58401d64ac1562c50c1999636,https://github.com/pytorch/fairseq/commit/dc40ac58f701bac58401d64ac1562c50c1999636,Simplify train.py (merge with singleprocess_train.py),4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
194,Myle Ott,myleott@fb.com,2018-04-21 19:07:53-06:00,0daba38ecbefc8182412a237e9b6e4135f5d83ae,https://github.com/pytorch/fairseq/commit/0daba38ecbefc8182412a237e9b6e4135f5d83ae,Save and restore wall time in checkpoints,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
195,Myle Ott,myleott@fb.com,2018-04-22 09:15:10-06:00,7c7634f638975454a981b75eaa61f8817081faca,https://github.com/pytorch/fairseq/commit/7c7634f638975454a981b75eaa61f8817081faca,Support --warmup-updates with fixed LR schedule,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
196,Myle Ott,myleott@fb.com,2018-04-24 12:46:54-06:00,8afb77612ce35fd9a3457c3faaaf1c61195ea8df,https://github.com/pytorch/fairseq/commit/8afb77612ce35fd9a3457c3faaaf1c61195ea8df,Fix tests,4,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
197,Myle Ott,myleott@fb.com,2018-04-25 10:12:56-04:00,88df72c0cf951f477f9f61edfa90c2333d6ffabf,https://github.com/pytorch/fairseq/commit/88df72c0cf951f477f9f61edfa90c2333d6ffabf,Remove src-padding from generation output,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
198,Alexei Baevski,alexei.b@gmail.com,2018-04-25 15:47:04+01:00,2a681d99e637e2bed0dedd8ada27192956ee8741,https://github.com/pytorch/fairseq/commit/2a681d99e637e2bed0dedd8ada27192956ee8741,make sure tensor used to index is cuda if on gpu,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
199,Myle Ott,myleott@fb.com,2018-04-30 07:44:17-06:00,7f538f54d94601b6406854e60cf4ece0e92f9f29,https://github.com/pytorch/fairseq/commit/7f538f54d94601b6406854e60cf4ece0e92f9f29,Fix --prefix-size,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
200,Angela Fan,angela.h.fan@gmail.com,2018-05-01 15:50:15-06:00,d85b61d65e81960440656035f025802027b2d192,https://github.com/pytorch/fairseq/commit/d85b61d65e81960440656035f025802027b2d192,fix to adding tokens to dictionary while thresholding,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
201,Alexei Baevski,alexei.b@gmail.com,2018-05-03 09:55:39+01:00,23211c457d55dced35ca3b6d511b0dff15fd4968,https://github.com/pytorch/fairseq/commit/23211c457d55dced35ca3b6d511b0dff15fd4968,make attn dropout 0.1 default for big en-de transformer,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
202,Alexei Baevski,alexei.b@gmail.com,2018-05-04 11:53:09+01:00,f6a5a54e2442faabde6c67db014061dd34983cde,https://github.com/pytorch/fairseq/commit/f6a5a54e2442faabde6c67db014061dd34983cde,add support for averaging last n checkpoints,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
203,alexeib,alexei.b@gmail.com,2018-05-08 17:20:09+01:00,12a47a64ce49e49b1801e69973e4b5cd3c3e1ab7,https://github.com/pytorch/fairseq/commit/12a47a64ce49e49b1801e69973e4b5cd3c3e1ab7,fix flag copy paste (decoder-normalize-before),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
204,Myle Ott,myleott@fb.com,2018-05-08 13:10:12-06:00,c53b2ee06a3d4f9e18014a75bfacba8d731071ab,https://github.com/pytorch/fairseq/commit/c53b2ee06a3d4f9e18014a75bfacba8d731071ab,Remove padding from --score-reference,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
205,Myle Ott,myleott@fb.com,2018-05-08 14:25:45-07:00,a04c4cf46f5ecf9fff9487b94800308d0e0cdae4,https://github.com/pytorch/fairseq/commit/a04c4cf46f5ecf9fff9487b94800308d0e0cdae4,Fix --remove-bpe to strip trailing BPE symbols,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
206,Sergey Edunov,edunov@fb.com,2018-05-10 12:25:06-07:00,4ce453b18fc2ff9cb7e42b86313b31a8a5d08d18,https://github.com/pytorch/fairseq/commit/4ce453b18fc2ff9cb7e42b86313b31a8a5d08d18,Sampling doesn't work with interactive,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
207,Alexei Baevski,alexei.b@gmail.com,2018-05-11 15:39:34+01:00,663fd8060e367b5db740720aa3ad0fa12ac733e5,https://github.com/pytorch/fairseq/commit/663fd8060e367b5db740720aa3ad0fa12ac733e5,implement batching in interactive mode,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
208,Alexei Baevski,alexei.b@gmail.com,2018-05-11 15:51:18+01:00,9f1b37ddcd5c7e757993f071f919b1b5a231a80c,https://github.com/pytorch/fairseq/commit/9f1b37ddcd5c7e757993f071f919b1b5a231a80c,fix alignment when using uneven batches and left pad,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
209,Myle Ott,myleott@fb.com,2018-05-14 18:04:45-04:00,a5e493640a3ea98ee1298d8eb983e628088bc780,https://github.com/pytorch/fairseq/commit/a5e493640a3ea98ee1298d8eb983e628088bc780,Support integer learning rates,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
210,Alexei Baevski,alexei.b@gmail.com,2018-05-15 18:54:01+01:00,67af40c9cca0241d797be13ae557d59c3732b409,https://github.com/pytorch/fairseq/commit/67af40c9cca0241d797be13ae557d59c3732b409,allow specifying max_tokens for generation,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
211,Alexei Baevski,alexei.b@gmail.com,2018-05-17 13:57:34+01:00,bf47b95679e767230a24e92d42472b89e74120e0,https://github.com/pytorch/fairseq/commit/bf47b95679e767230a24e92d42472b89e74120e0,also report sentence/s timing when generating,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
212,alexeib,alexei.b@gmail.com,2018-05-17 14:17:39+01:00,58e2c449cd81895748eac7b39309e9ff77557dc9,https://github.com/pytorch/fairseq/commit/58e2c449cd81895748eac7b39309e9ff77557dc9,default dropout to correct value for big transformer,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
213,Alexei Baevski,alexei.b@gmail.com,2018-05-23 13:59:56+01:00,fc312d28d3b352fb479e022469642d400d575ae2,https://github.com/pytorch/fairseq/commit/fc312d28d3b352fb479e022469642d400d575ae2,ability to checkpoint when reaching certain number of updates,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
214,Myle Ott,myleott@fb.com,2018-05-23 09:52:10-04:00,7c07e87cb8e80777e361cbf4cc2fc65eec6ac88e,https://github.com/pytorch/fairseq/commit/7c07e87cb8e80777e361cbf4cc2fc65eec6ac88e,All-reduce in FP16,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
215,alexeib,alexei.b@gmail.com,2018-05-23 15:15:40+01:00,a3e4c4c32a4ac3a15b1686163ce0fcf19dfea347,https://github.com/pytorch/fairseq/commit/a3e4c4c32a4ac3a15b1686163ce0fcf19dfea347,remove unused verbose option & make arguments to averaging script nicer,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
216,Alexei Baevski,alexei.b@gmail.com,2018-04-17 15:48:16-07:00,1b5a498c2c0283a27428861255743ba7023ca0a3,https://github.com/pytorch/fairseq/commit/1b5a498c2c0283a27428861255743ba7023ca0a3,allow overwriting args for different architectures,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
217,Myle Ott,myleott@fb.com,2018-05-24 08:04:48-06:00,ae2585d9fd14a6006dad71fbba0bdf1e6d53a296,https://github.com/pytorch/fairseq/commit/ae2585d9fd14a6006dad71fbba0bdf1e6d53a296,Fix tests,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
218,ngimel,ngimelshein@nvidia.com,2018-05-09 05:43:41-07:00,8300a521882c9da59010ee41e35632a98ce97f6d,https://github.com/pytorch/fairseq/commit/8300a521882c9da59010ee41e35632a98ce97f6d,use implicit padding when possible,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
219,Sai,sai-prasanna@users.noreply.github.com,2018-05-09 18:31:58+05:18,6f96ad78744d862bec9b8431a79b08921e2e1e32,https://github.com/pytorch/fairseq/commit/6f96ad78744d862bec9b8431a79b08921e2e1e32,Add pretrained embedding support,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
220,Myle Ott,myleott@fb.com,2018-05-09 07:15:13-06:00,81e99d8d9f26678992913744188065b8cc63e9ea,https://github.com/pytorch/fairseq/commit/81e99d8d9f26678992913744188065b8cc63e9ea,Flake8,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
221,Myle Ott,myleott@fb.com,2018-05-21 09:26:15-04:00,d48160349431146064327caffbff887784560980,https://github.com/pytorch/fairseq/commit/d48160349431146064327caffbff887784560980,Fix old model checkpoints,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
222,myleott,myleott@fb.com,2018-05-23 13:37:54-07:00,4e1ec2d8833ff3a2f91443593f96cb6c178b4fc5,https://github.com/pytorch/fairseq/commit/4e1ec2d8833ff3a2f91443593f96cb6c178b4fc5,Merge OSS + internal changes,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
223,alexeib,alexei.b@gmail.com,2018-05-25 14:43:37+01:00,4c2ef2de74a2b80864591bb50d997da749c6461c,https://github.com/pytorch/fairseq/commit/4c2ef2de74a2b80864591bb50d997da749c6461c,"Conv lm implementation

This implements convolutional language model from https://arxiv.org/pdf/1612.08083.pdf

There are 3 modes for constructing batches:

- token block: fill each sample with a specified number of tokens without regard for sentence delimiters - this is what was used for training in the paper
- complete: fill each sample with a specified number of tokens but make sure it contains only complete sentences (i.e. if next sentence goes over token block limit, move it to the next sample) - this was used for evaluation in the paper
- eos: one sentence per sample (skip blank lines)

some results:

GCNN-13 - GBW - 37.46
GCNN-14B - GBW - 33.88
GCNN-8 - Wiki103 - 43.76
GCNN-14 - Wiki103 - 35.66

train:

python train.py /private/home/abaevski/data/wiki103 --save-dir /tmp --fp16 --max-epoch 35 --save-interval 1 --save-interval-updates 1000 --keep-interval-updates 25 --arch fconv_lm --optimizer nag --lr 1.0 --lr-scheduler reduce_lr_on_plateau --lr-shrink 0.5 --decoder-embed-dim 280 --decoder-layers '[(850, 6)] * 3 + [(850,1)] + [(850,5)] * 4 + [(850,1)] + [(850,4)] * 3 + [(1024,4)] + [(2048, 4)]' --clip-norm 0.1 --dropout 0.2 --weight-decay 5e-06 --criterion cross_entropy --max-tokens 1024 --max-target-positions 1024 --seed 1 --log-format json --log-interval 500

eval:

python eval_lm.py ~abaevski/data/wiki103 --path '/checkpoint02/abaevski/2018-04-27/lm_wiki.fp16.mxup300000.fconv.adam.lrs=reduce_lr_on_plateau.emb280.layers(850,6)*3+(850,1)+(850,5)*4+(850,1)+(850,4)*3+(1024,1)+(2048,4).lr0.0005.clp0.1.drp0.3.wd0.0.crt=cross_entropy.mxtk2048.smptk256.seed1.ngpu8/checkpoint_last.pt'",34,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,9,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestLoadCheckpoint(unittest.TestCase):'],"[('Equal', '(epoch, 2)'), ('Equal', '(len(ds), 50)'), ('NotIsInstance', '(ds, MagicMock)'), ('Equal', '(epoch, 2)'), ('Equal', '(len(ds), 150)'), ('IsInstance', '(ds, MagicMock)'), ('Equal', '(epoch, 1)'), ('Equal', '(len(ds), 150)'), ('IsInstance', '(ds, MagicMock)')]",['def setUp(self):'],[],['def tearDown(self):'],[],[],[],[],[' unittest.mock '],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
224,alexeib,alexei.b@gmail.com,2018-05-25 08:00:21-06:00,e774fda7b7d5bfd5aeb56e0625065eeaac3c9dc1,https://github.com/pytorch/fairseq/commit/e774fda7b7d5bfd5aeb56e0625065eeaac3c9dc1,default normalization constant for older models,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
225,alexeib,alexei.b@gmail.com,2018-05-25 15:08:12+01:00,09379ad86de3bd0bb146b06d9f8650c73fef7da7,https://github.com/pytorch/fairseq/commit/09379ad86de3bd0bb146b06d9f8650c73fef7da7,add big en_fr transformer architecture,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
226,Myle Ott,myleott@fb.com,2018-05-25 10:41:17-04:00,386847ee51925dfb91aa7d547368051cbd637d4e,https://github.com/pytorch/fairseq/commit/386847ee51925dfb91aa7d547368051cbd637d4e,Generalize eval_str_list,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
227,alexeib,alexei.b@gmail.com,2018-05-27 23:40:14+01:00,978c125aee0758273442e91fcc1164920f87f9bd,https://github.com/pytorch/fairseq/commit/978c125aee0758273442e91fcc1164920f87f9bd,fix restoring from middle of epoch; fix defaulting transformer dropout params,5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(next(ds), 50)'), ('Equal', '(next(iter(ds)), 0)'), ('Equal', '(next(iter(ds)), 0)')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(len(ds), 50)'), ('NotIsInstance', '(ds, MagicMock)'), ('Equal', '(len(ds), 150)'), ('IsInstance', '(ds, MagicMock)'), ('Equal', '(len(ds), 150)'), ('IsInstance', '(ds, MagicMock)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
228,alexeib,alexei.b@gmail.com,2018-05-28 13:01:42+01:00,7d5604024ba41a4347af023749c35174f97872ba,https://github.com/pytorch/fairseq/commit/7d5604024ba41a4347af023749c35174f97872ba,record end_of_epoch in checkpoint,2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(epoch, 3)')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(epoch, 2)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
229,alexeib,alexei.b@gmail.com,2018-05-29 16:30:41+01:00,7f5f2461aa542c886d23c0e6588ac9956943ec76,https://github.com/pytorch/fairseq/commit/7f5f2461aa542c886d23c0e6588ac9956943ec76,use adaptive softmax only with adaptive loss,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
230,alexeib,alexei.b@gmail.com,2018-05-29 17:09:12+01:00,50931d69465c2a7c255cb321ee87283403189565,https://github.com/pytorch/fairseq/commit/50931d69465c2a7c255cb321ee87283403189565,fix default params,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
231,Angela Fan,angela.h.fan@gmail.com,2018-05-09 20:19:48-06:00,b59815bc2b6fbc68c2fa26833aea210061391bb9,https://github.com/pytorch/fairseq/commit/b59815bc2b6fbc68c2fa26833aea210061391bb9,"added multiscale gated self attention layer with multiple heads, and pretrained fusion models",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
232,Angela Fan,angela.h.fan@gmail.com,2018-05-30 01:13:02-06:00,7a36da4244cb3418ae9ee85a7c46d9947026f9c9,https://github.com/pytorch/fairseq/commit/7a36da4244cb3418ae9ee85a7c46d9947026f9c9,modified writing prompts model parameters to make readme cleaner,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
233,Angela Fan,angela.h.fan@gmail.com,2018-05-30 01:30:43-06:00,316744d68bfd28c385acfe488995225bb18be8c0,https://github.com/pytorch/fairseq/commit/316744d68bfd28c385acfe488995225bb18be8c0,minor parameter fixes for stories model,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
234,Alexei Baevski,alexei.b@gmail.com,2018-05-30 14:55:29+01:00,295ccee99e0dcb2db5c41ec8cc0b3f3094ab1af7,https://github.com/pytorch/fairseq/commit/295ccee99e0dcb2db5c41ec8cc0b3f3094ab1af7,"save best val loss in checkpoint

save best val loss in checkpoint and also print best so far

this way when training continues from an existing checkpoint, we dont immediately override checkpoint_best with a worse loss",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
235,alexeib,alexei.b@gmail.com,2018-05-30 15:38:52+01:00,6eda8e47c39906ba75a25571b77029766e558c07,https://github.com/pytorch/fairseq/commit/6eda8e47c39906ba75a25571b77029766e558c07,fix model loading in eval_lm,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
236,Myle Ott,myleott@fb.com,2018-05-30 13:45:41-04:00,cf1c64a5f7cffe7a362cc772752f8f226fb04f22,https://github.com/pytorch/fairseq/commit/cf1c64a5f7cffe7a362cc772752f8f226fb04f22,Nits,17,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
237,Myle Ott,myleott@fb.com,2018-05-30 10:06:56-06:00,76b5ecab61765a6fb83eb50f8d9c904548e925c5,https://github.com/pytorch/fairseq/commit/76b5ecab61765a6fb83eb50f8d9c904548e925c5,Migrate all binaries to use options.parse_args_and_arch,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
238,Myle Ott,myleott@fb.com,2018-05-30 15:58:21-04:00,24d7de4465c3d6e75065974d36fd8d8cdd4bfc29,https://github.com/pytorch/fairseq/commit/24d7de4465c3d6e75065974d36fd8d8cdd4bfc29,Unify various sharding into ShardedIterator,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
239,Myle Ott,myleott@fb.com,2018-05-30 19:12:31-04:00,6643d52548efd9558f96d0530c6306f9265bcf04,https://github.com/pytorch/fairseq/commit/6643d52548efd9558f96d0530c6306f9265bcf04,Use symlinks for redundant checkpoints,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
240,Myle Ott,myleott@fb.com,2018-05-30 19:13:28-04:00,a919570b76fbf393965f6eec89c620187718425d,https://github.com/pytorch/fairseq/commit/a919570b76fbf393965f6eec89c620187718425d,Merge validate and val_loss functions (simplify train.py),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
241,Alexei Baevski,alexei.b@gmail.com,2018-05-31 13:51:51+01:00,c778a31e2b6ae4d089d9a213ba023140438725b2,https://github.com/pytorch/fairseq/commit/c778a31e2b6ae4d089d9a213ba023140438725b2,create examples dir and add conv lm + stories readme,8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
242,Myle Ott,myleott@fb.com,2018-05-31 10:03:54-04:00,13aa36cf4f1deaba804a6e15b8bf08af3c5f7e19,https://github.com/pytorch/fairseq/commit/13aa36cf4f1deaba804a6e15b8bf08af3c5f7e19,Small fixes,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
243,Myle Ott,myleott@fb.com,2018-06-04 07:11:23-06:00,736fbee2a1bb1548162888ae86946c840f8e5021,https://github.com/pytorch/fairseq/commit/736fbee2a1bb1548162888ae86946c840f8e5021,Suppress stdout in test_train,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,6,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(epoch, 2)'), ('Equal', '(next(ds), 50)'), ('Equal', '(epoch, 3)'), ('Equal', '(next(iter(ds)), 0)'), ('Equal', '(epoch, 1)'), ('Equal', '(next(iter(ds)), 0)')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(epoch, 2)'), ('Equal', '(next(ds), 50)'), ('Equal', '(epoch, 3)'), ('Equal', '(next(iter(ds)), 0)'), ('Equal', '(epoch, 1)'), ('Equal', '(next(iter(ds)), 0)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
244,Myle Ott,myleott@fb.com,2018-06-04 14:24:08-06:00,16a72b4dd1d4946bbc9b6a8eec40c91e644b3b3d,https://github.com/pytorch/fairseq/commit/16a72b4dd1d4946bbc9b6a8eec40c91e644b3b3d,"Add more integration tests (LM, stories, transformer, lstm)",1,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],3,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestTranslation(unittest.TestCase):', 'class TestStories(unittest.TestCase):', 'class TestLanguageModeling(unittest.TestCase):']",[],[],[],[],[],[],[],[],[],[],['class TestBinaries(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
245,Myle Ott,myleott@fb.com,2018-06-04 22:51:08-05:00,92050ef2ecaeab5ecea2105489f897f2c4d570fc,https://github.com/pytorch/fairseq/commit/92050ef2ecaeab5ecea2105489f897f2c4d570fc,Update README.md,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
246,Alexei Baevski,alexei.b@gmail.com,2018-06-05 12:43:25-04:00,f41089090e4c3c319d039e2fb9b975d4ce647d6c,https://github.com/pytorch/fairseq/commit/f41089090e4c3c319d039e2fb9b975d4ce647d6c,"build optimizer only once, otherwise it leaks cuda memory",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
247,alexeib,alexei.b@gmail.com,2018-06-07 14:08:17-04:00,1fb22af44fe2d2c67f1b3a219202ae72fbf292e7,https://github.com/pytorch/fairseq/commit/1fb22af44fe2d2c67f1b3a219202ae72fbf292e7,initialize normalization constant for fconv_lm,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
248,Myle Ott,myleott@fb.com,2018-06-11 12:27:16-05:00,fc87eea2895ae736816fffae924ecf1f0c2838ff,https://github.com/pytorch/fairseq/commit/fc87eea2895ae736816fffae924ecf1f0c2838ff,"Fix length penalty when combined with --no-early-stop

Co-authored-by: pmichel31415 <pmichel@fb.com>",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
249,Sergey Edunov,edunov@fb.com,2018-06-11 16:33:44-07:00,2de9353273bc2f10c9ca844d1539e22a43fb3c2e,https://github.com/pytorch/fairseq/commit/2de9353273bc2f10c9ca844d1539e22a43fb3c2e,torch.arange default return type is changed in the latest pytorch version https://github.com/pytorch/pytorch/pull/7016,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
250,Myle Ott,myleott@fb.com,2018-06-12 13:39:41-04:00,ff68a9ef501e7286501dba1719024dfaaab4b473,https://github.com/pytorch/fairseq/commit/ff68a9ef501e7286501dba1719024dfaaab4b473,"Add FairseqTask

A Task defines the data format, stores shared state (e.g., dictionaries) and provides helpers for building the model/criterion and calculating the loss.

Changes:
- Add TranslationTask and LanguageModelingTask. New tasks can be registered with @register_task decorator.
- Add EpochBatchIterator to encapsulate batching and saving/restoring dataloader position
- Remove LEFT_PAD_* constants and make them configurable per task",47,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,18,0,0,0,0,0,0,0,0,1,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDataUtils(unittest.TestCase):'],"[('True', '(itr.has_next())'), ('Equal', '(next(itr), 0)'), ('Equal', '(next(itr), 1)'), ('Equal', '(next(itr), 5)'), ('Equal', '(next(itr), 9)'), ('False', '(itr.has_next())'), ('Equal', '(epoch_itr.epoch, 2)'), ('Equal', '(epoch_itr.iterations_in_epoch, 50)'), ('Equal', '(epoch_itr.epoch, 2)'), ('Equal', '(epoch_itr.iterations_in_epoch, 50)'), ('Equal', '(next(itr)[][0].item(), 50)'), ('Equal', '(epoch_itr.iterations_in_epoch, 51)'), ('Equal', '(epoch_itr.epoch, 3)'), ('Equal', '(epoch_itr.iterations_in_epoch, 0)'), ('Equal', '(next(itr)[][0].item(), 0)'), ('Equal', '(epoch_itr.epoch, 1)'), ('Equal', '(epoch_itr.iterations_in_epoch, 0)'), ('Equal', '(next(itr)[][0].item(), 0)')]",[],[],[],[],[],[],[],[],['import unittest'],[],"[('Equal', '(epoch, 2)'), ('Equal', '(next(ds), 50)'), ('Equal', '(epoch, 3)'), ('Equal', '(next(iter(ds)), 0)'), ('Equal', '(epoch, 1)'), ('Equal', '(next(iter(ds)), 0)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
251,Myle Ott,myleott@fb.com,2018-06-12 21:10:15-04:00,e89329d6657e0dbc35d93f663363141ac2da5cdb,https://github.com/pytorch/fairseq/commit/e89329d6657e0dbc35d93f663363141ac2da5cdb,Updates for latest PyTorch,19,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],"[('True', '(isinstance(v, Variable))'), ('False', '(v.data.is_cuda)'), ('Equal', '(v.data.is_cuda, torch.cuda.is_available())')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
252,Myle Ott,myleott@fb.com,2018-06-12 19:18:57-06:00,55dc4842b28830f675ef1e9f76dec152d893ba44,https://github.com/pytorch/fairseq/commit/55dc4842b28830f675ef1e9f76dec152d893ba44,Fix tests,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
253,Myle Ott,myleott@fb.com,2018-06-12 14:12:50-06:00,bfcc6ec7398993c8156176fa8189ff917935785d,https://github.com/pytorch/fairseq/commit/bfcc6ec7398993c8156176fa8189ff917935785d,Fix bidirectional lstm,2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
254,Myle Ott,myleott@fb.com,2018-06-14 11:04:20-04:00,ef17941545c6d742de717d9769b2a412d9924e4e,https://github.com/pytorch/fairseq/commit/ef17941545c6d742de717d9769b2a412d9924e4e,Faster generation when using a single model (rather than ensemble),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
255,Myle Ott,myleott@fb.com,2018-06-14 11:38:55-04:00,16caed313e8943f9263a5dbb3e3cede32b326ead,https://github.com/pytorch/fairseq/commit/16caed313e8943f9263a5dbb3e3cede32b326ead,Change --path to be colon-separated instead of comma-separated,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
256,alexeib,alexei.b@gmail.com,2018-06-14 17:53:18-07:00,fbc42f2d8f9ba4a040455d704a14fea42a537656,https://github.com/pytorch/fairseq/commit/fbc42f2d8f9ba4a040455d704a14fea42a537656,add default architecture for gbw fconv lm,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
257,Alexei Baevski,alexei.b@gmail.com,2018-06-14 18:34:47-07:00,2845d033ab4efc1dd017350fda1dac27247a0f29,https://github.com/pytorch/fairseq/commit/2845d033ab4efc1dd017350fda1dac27247a0f29,add links to pretrained language models,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
258,Myle Ott,myleott@fb.com,2018-06-15 12:39:51-04:00,5383b5dbab55f7d7e49ee6716c70559c6b712130,https://github.com/pytorch/fairseq/commit/5383b5dbab55f7d7e49ee6716c70559c6b712130,"Update README.md

Add transformer models and replace list with table",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
259,Myle Ott,myleott@fb.com,2018-06-15 23:10:42-04:00,8cba7b143efd5acb14d04d7c6119028bf2e6eba2,https://github.com/pytorch/fairseq/commit/8cba7b143efd5acb14d04d7c6119028bf2e6eba2,"Update README.md

Fix preprocessed test set download links",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
260,ngimel,ngimelshein@nvidia.com,2018-06-20 09:17:32-07:00,411018fc63bb1a55422a2296816e82f2c77a6352,https://github.com/pytorch/fairseq/commit/411018fc63bb1a55422a2296816e82f2c77a6352,add count for padding words (#180),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
261,Angela Fan,angela.h.fan@gmail.com,2018-06-20 10:24:21-06:00,a8f0b75259b5720fdd16ebdc4e0117e425cfd833,https://github.com/pytorch/fairseq/commit/a8f0b75259b5720fdd16ebdc4e0117e425cfd833,gzip instead of bzip change to stories download,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
262,Angela Fan,angela.h.fan@gmail.com,2018-06-20 12:20:26-06:00,12515c237b91774a2c35526d9954d4a457816710,https://github.com/pytorch/fairseq/commit/12515c237b91774a2c35526d9954d4a457816710,added clarification on the newline token we model,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
263,Angela Fan,angela.h.fan@gmail.com,2018-06-20 12:21:22-06:00,0b37f5a6acbb544cb7dd9decf96914e6295c2c3b,https://github.com/pytorch/fairseq/commit/0b37f5a6acbb544cb7dd9decf96914e6295c2c3b,fixed newline word not appearing,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
264,Myle Ott,myleott@fb.com,2018-06-21 08:04:21-04:00,70d61db4b72660c1c057196ce200f56f0d96a3bc,https://github.com/pytorch/fairseq/commit/70d61db4b72660c1c057196ce200f56f0d96a3bc,Fix translation README (fixes #186) (#189),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
265,Myle Ott,myleott@fb.com,2018-06-21 08:19:16-04:00,572a1d55df139bc109868918838ca1a54daf2e06,https://github.com/pytorch/fairseq/commit/572a1d55df139bc109868918838ca1a54daf2e06,Fix `--output-format raw` option to preprocess.py (Fixes #188) (#190),2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
266,Mehdi Drissi,mdrissi@hmc.edu,2018-06-21 10:55:44-07:00,762956a559e65e1e48df8f8b4df515d23b66fddb,https://github.com/pytorch/fairseq/commit/762956a559e65e1e48df8f8b4df515d23b66fddb,"Two tiny changes to train/eval_lm. For train fix an off by one, while for eval_lm make it work when the task is translation'",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
267,Louis MARTIN,louisrtm@gmail.com,2018-06-18 09:57:52-06:00,a846b2138d361dace344ba4b373e9c271736f473,https://github.com/pytorch/fairseq/commit/a846b2138d361dace344ba4b373e9c271736f473,"Ignore files in examples (other than .sh and .md)

When downloading files in examples directory (e.g. when running
`prepare-iwslt14.sh`), git sees them as untracked files but they should
not be committed.
Add a .gitignore script that ignore everything in the examples
subdirectories except for .sh and .md files.",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
268,Myle Ott,myleott@fb.com,2018-06-18 17:35:40-04:00,d9a13180fd12923a501998879927d3e4a7e81aa4,https://github.com/pytorch/fairseq/commit/d9a13180fd12923a501998879927d3e4a7e81aa4,Better failure message when loss explodes during FP16 training,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
269,Myle Ott,myleott@fb.com,2018-06-18 18:34:09-04:00,9dcee4c759805360109cdc712eb0efefd961ff4f,https://github.com/pytorch/fairseq/commit/9dcee4c759805360109cdc712eb0efefd961ff4f,Store full checkpoints instead of symlinking,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
270,Stephen Roller,roller@fb.com,2018-06-19 08:29:15-06:00,cc85d411cb408cbb212c040243081f7f85dd8876,https://github.com/pytorch/fairseq/commit/cc85d411cb408cbb212c040243081f7f85dd8876,Fix a bug with using GloVe 840B tokens for initialization.,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
271,alexeib,alexei.b@gmail.com,2018-06-19 11:47:05-07:00,a091d23941515a815d4bc07c3bbba10253ffede3,https://github.com/pytorch/fairseq/commit/a091d23941515a815d4bc07c3bbba10253ffede3,respect max tokens and ignore invalid inputs when evaluating lm,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
272,alexeib,alexei.b@gmail.com,2018-06-19 11:59:18-07:00,5bf0772450927648657f210ccc9eeb3b8d066cab,https://github.com/pytorch/fairseq/commit/5bf0772450927648657f210ccc9eeb3b8d066cab,sort descending when evaluating lm because it is faster (17k wps vs 11k) and will fail early if oom,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
273,Stephen Roller,roller@fb.com,2018-06-19 08:27:59-06:00,9a88b71d4eb5ac6d1041e8eba5feb64c6a5fa8d9,https://github.com/pytorch/fairseq/commit/9a88b71d4eb5ac6d1041e8eba5feb64c6a5fa8d9,"Support pretrained embeddings for Transformer.

Also show a nicer error message.",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
274,Myle Ott,myleott@fb.com,2018-06-19 20:27:00-04:00,930c9580b6df5c6be140adcbfc45820903830a54,https://github.com/pytorch/fairseq/commit/930c9580b6df5c6be140adcbfc45820903830a54,Support FP16 during inference,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
275,Alexei Baevski,alexei.b@gmail.com,2018-06-20 15:46:08-07:00,03fffb98c058d0d6cfe9ea5f5fbcda4034d3f3f0,https://github.com/pytorch/fairseq/commit/03fffb98c058d0d6cfe9ea5f5fbcda4034d3f3f0,fix sinusoidal embedding init size,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
276,alexeib,alexei.b@gmail.com,2018-06-20 21:20:07-07:00,678d0bbcb2cd0d1d72c8b7049a3e3a8b1d0bdf5d,https://github.com/pytorch/fairseq/commit/678d0bbcb2cd0d1d72c8b7049a3e3a8b1d0bdf5d,add sinusoidal pos initialization,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
277,Myle Ott,myleott@fb.com,2018-06-21 11:55:21-06:00,e9967cd334783f5da50deadc17cf8a4fc3380171,https://github.com/pytorch/fairseq/commit/e9967cd334783f5da50deadc17cf8a4fc3380171,Fix interpretation of --max-epoch,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
278,Myle Ott,myleott@fb.com,2018-06-21 14:34:29-04:00,6ec5022e5713d73207a979ef9ff23fe3238e9218,https://github.com/pytorch/fairseq/commit/6ec5022e5713d73207a979ef9ff23fe3238e9218,Move reorder_encoder_out to FairseqEncoder and fix non-incremental decoding,9,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
279,Myle Ott,myleott@fb.com,2018-06-21 14:51:49-04:00,b458977a61ec448e294de37614e4912de092f19a,https://github.com/pytorch/fairseq/commit/b458977a61ec448e294de37614e4912de092f19a,Add steps to reproduce WMT En-De results from Scaling NMT paper,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
280,Myle Ott,myleott@fb.com,2018-06-22 15:24:00-04:00,7bcb487aad8504043d13c9b869d555aa565a46c7,https://github.com/pytorch/fairseq/commit/7bcb487aad8504043d13c9b869d555aa565a46c7,Fix typo,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
281,Myle Ott,myleott@fb.com,2018-06-24 12:25:07-04:00,c6fe9fc5e06aa63c0c202be63e7383cc1169290f,https://github.com/pytorch/fairseq/commit/c6fe9fc5e06aa63c0c202be63e7383cc1169290f,Fix for Dictionary.finalize,2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDictionary(unittest.TestCase):'],"[('Equal', '(toks.size(), ref_toks.size())'), ('Equal', '(0, (toks != ref_toks).sum().item())')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
282,Myle Ott,myleott@fb.com,2018-06-24 12:59:59-04:00,bd4db8fbd09ccd99efbb49f4280e7f690b82bddc,https://github.com/pytorch/fairseq/commit/bd4db8fbd09ccd99efbb49f4280e7f690b82bddc,Misc changes for pytorch-translate,8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
283,Myle Ott,myleott@fb.com,2018-06-25 12:16:10-04:00,74efc21403477d103bd426ae64c37b7a30d8f4bf,https://github.com/pytorch/fairseq/commit/74efc21403477d103bd426ae64c37b7a30d8f4bf,Fix attention order in unit tests (fixes #195) (#197),1,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
284,Myle Ott,myleott@fb.com,2018-06-25 12:23:04-04:00,6edf81ddfe8fe014bc8a201160b7d7d386058e52,https://github.com/pytorch/fairseq/commit/6edf81ddfe8fe014bc8a201160b7d7d386058e52,Remove more Variable() calls (#198),7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
285,Myle Ott,myleott@fb.com,2018-06-26 09:34:58-04:00,1f08602f8f5adf7970e2b675fc50e72a637f8cad,https://github.com/pytorch/fairseq/commit/1f08602f8f5adf7970e2b675fc50e72a637f8cad,Remove unnecessary assert (fixes #199) (#200),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
286,Myle Ott,myleott@fb.com,2018-06-28 14:19:31-04:00,a75c30923be7bda98592561050432211f9227009,https://github.com/pytorch/fairseq/commit/a75c30923be7bda98592561050432211f9227009,Fix preprocessing for WMT14 En-De to replicate Scaling NMT paper (#203),2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
287,Angela Fan,angela.h.fan@gmail.com,2018-07-02 03:40:31-06:00,62578bbe1f0fcd41093b9ed9efc4405e58974a67,https://github.com/pytorch/fairseq/commit/62578bbe1f0fcd41093b9ed9efc4405e58974a67,adding pretrained stories model,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
288,ngimel,ngimelshein@nvidia.com,2018-07-02 06:00:53-07:00,ff3db3cddff576bdfdbfa5c9c400997ed8c8e009,https://github.com/pytorch/fairseq/commit/ff3db3cddff576bdfdbfa5c9c400997ed8c8e009,fix decoder_normalize_before typo (#205),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
289,Angela Fan,angela.h.fan@gmail.com,2018-07-07 20:56:26-06:00,30ef667d40b9b34ca9caa43706a89b57531f19a7,https://github.com/pytorch/fairseq/commit/30ef667d40b9b34ca9caa43706a89b57531f19a7,"add model override argument from load_ensemble_for_inference at generation time, updating readme for stories",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
290,Angela Fan,angela.h.fan@gmail.com,2018-07-08 05:07:14-06:00,1d79ed9b5f67a51e468d012b2ba1bded6d8ebbaf,https://github.com/pytorch/fairseq/commit/1d79ed9b5f67a51e468d012b2ba1bded6d8ebbaf,adding model arg override at generation time for interactive.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
291,Alexei Baevski,alexei.b@gmail.com,2018-07-10 16:29:26-07:00,f26b6affdaf67d271e0d39f4c4c8384c4e8160d9,https://github.com/pytorch/fairseq/commit/f26b6affdaf67d271e0d39f4c4c8384c4e8160d9,assert that vocab size >= adaptive softmax cutoff (#214),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
292,Mehdi Drissi,mdrissi@cs.hmc.edu,2018-07-11 04:31:10-06:00,ac5fddfc691267285a84c81d39475411da5ed1c6,https://github.com/pytorch/fairseq/commit/ac5fddfc691267285a84c81d39475411da5ed1c6,Fix up model defaults (#211),4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
293,Sergey Edunov,edunov@fb.com,2018-07-19 10:58:15-07:00,eaa576b07322f7f7a03c92eef8ab7fddb6c364b6,https://github.com/pytorch/fairseq/commit/eaa576b07322f7f7a03c92eef8ab7fddb6c364b6,Pass sampling-temperature trough to the generator in interactive.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
294,Angela Fan,angela.h.fan@gmail.com,2018-07-19 17:59:07-07:00,28adb200a6bd82111742e8895894195cff0b0274,https://github.com/pytorch/fairseq/commit/28adb200a6bd82111742e8895894195cff0b0274,"stories data preprocessing needs padding factor 1 to match pretrained model, updating readme",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
295,higgsfield,x.comb.kz@gmail.com,2018-07-25 19:26:58+06:00,7358296b07f05dc055aaaa820d4b1c54a90d7812,https://github.com/pytorch/fairseq/commit/7358296b07f05dc055aaaa820d4b1c54a90d7812,fixed output_proj's input_dim in attention (#226),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
296,Alexei Baevski,alexei.b@gmail.com,2018-06-27 21:42:07-07:00,d494485fcbf6d3011fa8055ecbe1171f18ea4054,https://github.com/pytorch/fairseq/commit/d494485fcbf6d3011fa8055ecbe1171f18ea4054,fix raw text for language modeling,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
297,alexeib,alexei.b@gmail.com,2018-07-02 12:47:43-07:00,0e9e7f7b53c05bfcb5381eee0901bdde456eff14,https://github.com/pytorch/fairseq/commit/0e9e7f7b53c05bfcb5381eee0901bdde456eff14,make model access saner,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
298,alexeib,alexei.b@gmail.com,2018-07-02 16:08:26-07:00,a7d0bd0e344c3ab00cc72eade567052fa0de46ab,https://github.com/pytorch/fairseq/commit/a7d0bd0e344c3ab00cc72eade567052fa0de46ab,fix token block rotation,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
299,Stephen Roller,roller@fb.com,2018-07-03 13:49:23-04:00,f472d14104a388103562e92d8d989ab76b175d51,https://github.com/pytorch/fairseq/commit/f472d14104a388103562e92d8d989ab76b175d51,Support tied embeddings in LSTM encoder/decoder,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
300,Alexei Baevski,alexei.b@gmail.com,2018-07-10 09:57:53-07:00,89e19d42ec22d5040aeb2e77957dc2fb6c905079,https://github.com/pytorch/fairseq/commit/89e19d42ec22d5040aeb2e77957dc2fb6c905079,disable printing alignment by default (for perf) and add a flag to enable it,9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
301,alexeib,alexei.b@gmail.com,2018-07-10 14:41:16-07:00,1018c3335adfcf649ded14791f8399923ebf9b69,https://github.com/pytorch/fairseq/commit/1018c3335adfcf649ded14791f8399923ebf9b69,default need_attn to False,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
302,Stephen Roller,roller@fb.com,2018-07-11 06:20:20-04:00,498a186d777414f429a557b8ad6d8eff8b664011,https://github.com/pytorch/fairseq/commit/498a186d777414f429a557b8ad6d8eff8b664011,Fix bug when --share-all-embeddings but no --encoder-embed-path,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
303,Myle Ott,myleott@fb.com,2018-07-12 14:13:28+02:00,bb5f15d137e3d9b1723a7c06d588e184ab3397c5,https://github.com/pytorch/fairseq/commit/bb5f15d137e3d9b1723a7c06d588e184ab3397c5,Iterate on need_attn and fix tests,10,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
304,Myle Ott,myleott@fb.com,2018-07-12 14:14:41+02:00,c37fc8fd3c624a954d90bda180a5844d25309448,https://github.com/pytorch/fairseq/commit/c37fc8fd3c624a954d90bda180a5844d25309448,Output positional scores in interactive.py,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
305,Myle Ott,myleott@fb.com,2018-07-12 15:48:36+02:00,0ef2856c3fcbbf732f8f7213d19ec3616139b36b,https://github.com/pytorch/fairseq/commit/0ef2856c3fcbbf732f8f7213d19ec3616139b36b,Don't compute unnecessary attention averages during training,4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
306,Alexei Baevski,alexei.b@gmail.com,2018-07-18 14:37:02-07:00,d2e2a1d468e8048e9650ec9c8818841cead0645d,https://github.com/pytorch/fairseq/commit/d2e2a1d468e8048e9650ec9c8818841cead0645d,"Transformer lm

This implements transformer based language model. It already obtains better perplexity on wikitext103 without any tuning. I will also train it on gbw where I also expect to get better ppl

Example training command:

python train.py /private/home/abaevski/data/wiki103 —save-dir /tmp —fp16 —max-epoch 80 —save-interval 1 —arch transformer_lm —task language_modeling —optimizer nag —lr 0.008 —lr-scheduler reduce_lr_on_plateau —lr-shrink 0.6 —dropout 0.2 —criterion adaptive_loss —adaptive-softmax-cutoff 10000,50000,200000 —max-tokens 512 —tokens-per-sample 512 —seed 1 —sample-break-mode none —log-format json —log-interval 50 —save-interval-updates 2500 —keep-interval-updates 25
small transformer got to 31.3 ppl on wiki text 103 (compared to 35 with fconv) while @myleott got a big transformer lm to 27 something ppl on wiki text 103",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
307,alexeib,alexei.b@gmail.com,2018-07-18 17:07:31-07:00,67ee6d1faf1dbcbf70aaec4340d519652ed84e77,https://github.com/pytorch/fairseq/commit/67ee6d1faf1dbcbf70aaec4340d519652ed84e77,remove right-to-left lm support,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
308,alexeib,alexei.b@gmail.com,2018-07-19 08:58:30-07:00,e7b494f8bcae5988fa014f61fcac120606988d9f,https://github.com/pytorch/fairseq/commit/e7b494f8bcae5988fa014f61fcac120606988d9f,default decoder_learned_pos for lm,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
309,Alexei Baevski,alexei.b@gmail.com,2018-07-20 13:18:31-07:00,dbe963717c69c69a5409a3b427544481ade5e70c,https://github.com/pytorch/fairseq/commit/dbe963717c69c69a5409a3b427544481ade5e70c,option to print language model words and their log probs during evaluation,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
310,Sergey Edunov,edunov@apache.org,2018-07-23 12:11:29-07:00,c2794070108e3a906bb5f7ae6dc00911e13ebb4e,https://github.com/pytorch/fairseq/commit/c2794070108e3a906bb5f7ae6dc00911e13ebb4e,Update IWSLT configuration for transformer,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
311,Myle Ott,myleott@fb.com,2018-07-24 20:36:59-07:00,5aa4a627f92e41e4bcfdf877bbd8e6768b4e2011,https://github.com/pytorch/fairseq/commit/5aa4a627f92e41e4bcfdf877bbd8e6768b4e2011,Don't use 0-dimensional buffers in sinusoidal positional embeddings,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
312,Myle Ott,myleott@fb.com,2018-07-25 06:25:45-07:00,93fec886df8615350023fc25e8c4c7e51b70399e,https://github.com/pytorch/fairseq/commit/93fec886df8615350023fc25e8c4c7e51b70399e,Fix comment,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
313,Myle Ott,myleott@fb.com,2018-07-25 10:17:19-04:00,2fbfda0d3e4dba56eaa7bef0eb98b01d9277a391,https://github.com/pytorch/fairseq/commit/2fbfda0d3e4dba56eaa7bef0eb98b01d9277a391,Merge internal changes,9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
314,myleott,myleott@fb.com,2018-07-25 11:19:24-07:00,5d99e13925a1aa31981de759adb056f969e80d14,https://github.com/pytorch/fairseq/commit/5d99e13925a1aa31981de759adb056f969e80d14,fbshipit-source-id: 3f76eab2b42792fc8ed087fa0e2f4968bf980ad7,0,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
315,theweiho,theweiho@users.noreply.github.com,2018-07-26 17:57:37-07:00,79bbe1d8507dd55a3204a93d9a50508af8f9fcdb,https://github.com/pytorch/fairseq/commit/79bbe1d8507dd55a3204a93d9a50508af8f9fcdb,Add load_optim option to load checkpoint but not optimizer state (#229),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
316,alvations,alvations@gmail.com,2018-07-27 08:58:04+08:00,cabcc2547bf751d804644b45e1566734068630bf,https://github.com/pytorch/fairseq/commit/cabcc2547bf751d804644b45e1566734068630bf,Correct path in the pre-processing example (#230),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
317,alvations,alvations@gmail.com,2018-07-31 22:18:06+08:00,9143dfab61d3211a665c2611c6389c750efcd595,https://github.com/pytorch/fairseq/commit/9143dfab61d3211a665c2611c6389c750efcd595,Correct the help name of the prefixes arguments (#234),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
318,Myle Ott,myleott@fb.com,2018-07-31 21:56:06-04:00,202e0bbee73f70c15cfea1350339e68805a401e2,https://github.com/pytorch/fairseq/commit/202e0bbee73f70c15cfea1350339e68805a401e2,Fix bug when training with FP32 and --update-freq (#236),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
319,Maggie Li,MaggieMeow@users.noreply.github.com,2018-08-01 19:41:44+08:00,f7f2dd01ea141cc3303d6735e0fdcf8402b1557b,https://github.com/pytorch/fairseq/commit/f7f2dd01ea141cc3303d6735e0fdcf8402b1557b,Add ensemble for different architectures (#235),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
320,ngimel,ngimelshein@nvidia.com,2018-08-16 14:30:32-07:00,fedc55ecec890fe27a968d0bc701f2f019a305e9,https://github.com/pytorch/fairseq/commit/fedc55ecec890fe27a968d0bc701f2f019a305e9,add end-of-stack normalizations in case normalize_before has been set (#244),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
321,Myle Ott,myleott@fb.com,2018-08-16 14:31:20-07:00,53c7d271f3d97b0850d711c3f12a4efd92bc2898,https://github.com/pytorch/fairseq/commit/53c7d271f3d97b0850d711c3f12a4efd92bc2898,Fix comment,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
322,Myle Ott,myleott@fb.com,2018-08-16 17:33:23-04:00,af38ed48bb88cc489d1251e16519c7eef3a2fba7,https://github.com/pytorch/fairseq/commit/af38ed48bb88cc489d1251e16519c7eef3a2fba7,Fix bidirectional LSTM concatenation (#249),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
323,Alexei Baevski,alexei.b@gmail.com,2018-07-26 13:53:27-07:00,f69206c8624c62fb3f89f92fd1448fde05afa966,https://github.com/pytorch/fairseq/commit/f69206c8624c62fb3f89f92fd1448fde05afa966,fix adaptive softmax indexing,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
324,Alexei Baevski,alexei.b@gmail.com,2018-07-28 01:49:05-07:00,616afddd8979958a80308ad565c5a0625871e826,https://github.com/pytorch/fairseq/commit/616afddd8979958a80308ad565c5a0625871e826,option for a smaller adaptive softmax,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
325,Alexei Baevski,alexei.b@gmail.com,2018-07-28 01:50:50-07:00,885e7ec9ecbf19c36471e9aea095e2c8f238be83,https://github.com/pytorch/fairseq/commit/885e7ec9ecbf19c36471e9aea095e2c8f238be83,character token embeddings for word level predictions,5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestCharacterTokenEmbedder(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['embs.size() == (len(test_sents), max_len + 2, 5)', 'embs[0][0].equal(embs[1][0])', 'embs[0][0].equal(embs[0][-1])', 'embs[0][1].equal(embs[2][1])', 'embs[0][3].equal(embs[1][1])', 'embedder.char_embeddings.weight.grad is not None']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
326,alexeib,alexei.b@gmail.com,2018-07-28 01:54:39-07:00,d899817311d7c10676809fcf212a2daff93a8f7c,https://github.com/pytorch/fairseq/commit/d899817311d7c10676809fcf212a2daff93a8f7c,remove unneeded defaults,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
327,Alexei Baevski,alexei.b@gmail.com,2018-07-28 11:47:08-07:00,19c25f471d4b2c1939c3845ca1b4aefd2609f8db,https://github.com/pytorch/fairseq/commit/19c25f471d4b2c1939c3845ca1b4aefd2609f8db,"Always smaller soft

no need to have half-size option as behavior can be reproduced with existing flags",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
328,Alexei Baevski,alexei.b@gmail.com,2018-07-28 12:13:34-07:00,6e3685ad15ad28e49cd9a42f93daa73bedf5b1ad,https://github.com/pytorch/fairseq/commit/6e3685ad15ad28e49cd9a42f93daa73bedf5b1ad,make adaptive softmax dropout an optional arg,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
329,alexeib,alexei.b@gmail.com,2018-07-28 18:59:45-07:00,2dc074d8f2302755a07e75bb8489ef00118d4add,https://github.com/pytorch/fairseq/commit/2dc074d8f2302755a07e75bb8489ef00118d4add,"add flag that allows keeping optimizer config

adds -reset-optimizer, --reset-lr-scheduler, and --optimizer-overrides flags",6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
330,alexeib,alexei.b@gmail.com,2018-07-31 18:57:15-07:00,0b5166db2ec6f10c7b579e8627e15a0112a80edf,https://github.com/pytorch/fairseq/commit/0b5166db2ec6f10c7b579e8627e15a0112a80edf,fix tests,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
331,alexeib,alexei.b@gmail.com,2018-08-02 09:51:53-07:00,45082e482810c01898ffaca01af63faccc2fb5c8,https://github.com/pytorch/fairseq/commit/45082e482810c01898ffaca01af63faccc2fb5c8,make batching faster for monolingual dataset,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
332,alexeib,alexei.b@gmail.com,2018-08-03 15:51:41-07:00,e4f51e18508dc20ca379025108e59490dd59a6e0,https://github.com/pytorch/fairseq/commit/e4f51e18508dc20ca379025108e59490dd59a6e0,load args from model for eval_lm,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
333,alexeib,alexei.b@gmail.com,2018-08-04 01:41:31-07:00,1d38624f24a532afa1e284066326ceb758b543e1,https://github.com/pytorch/fairseq/commit/1d38624f24a532afa1e284066326ceb758b543e1,parameters to separate input/inner/out dims,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
334,Alexei Baevski,alexei.b@gmail.com,2018-08-08 10:54:29-07:00,75e12a27fb885ce399a9290ac950e1be08892a10,https://github.com/pytorch/fairseq/commit/75e12a27fb885ce399a9290ac950e1be08892a10,cosine + triangular lr scheduler,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
335,Myle Ott,myleott@fb.com,2018-08-09 13:19:14-04:00,ef43da72d3d5f4e66857b61e9aeb60fe18cc439f,https://github.com/pytorch/fairseq/commit/ef43da72d3d5f4e66857b61e9aeb60fe18cc439f,Factor out search logic in SequenceGenerator,4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
336,Sergey Edunov,edunov@fb.com,2018-08-09 11:05:04-07:00,97a6b13903f1417a75d2f015bbe98b28f8ed14c3,https://github.com/pytorch/fairseq/commit/97a6b13903f1417a75d2f015bbe98b28f8ed14c3,Reset gnorm after each epoch,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
337,alexeib,alexei.b@gmail.com,2018-08-09 22:31:50-07:00,f1d81db8b7f0a8d5a1888b787057c23bcbb7811d,https://github.com/pytorch/fairseq/commit/f1d81db8b7f0a8d5a1888b787057c23bcbb7811d,fix tests,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('AlmostEqual', '(embs[0][0], embs[1][0])'), ('AlmostEqual', '(embs[0][0], embs[0][-1])'), ('AlmostEqual', '(embs[0][1], embs[2][1])'), ('AlmostEqual', '(embs[0][3], embs[1][1])'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-6)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],"['embs[0][0].equal(embs[1][0])', 'embs[0][0].equal(embs[0][-1])', 'embs[0][1].equal(embs[2][1])', 'embs[0][3].equal(embs[1][1])']",[],[],[],[],[],[],[],[],[],[],[],[]
338,Myle Ott,myleott@fb.com,2018-08-13 10:32:44-07:00,5b533aa682eec0f0de8e64a49cdb77ec53c317f0,https://github.com/pytorch/fairseq/commit/5b533aa682eec0f0de8e64a49cdb77ec53c317f0,Increase max buffer size in all_gather_list,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
339,Alexei Baevski,alexei.b@gmail.com,2018-08-14 00:36:01-07:00,cd6590b6ff2dde41e5b3621641d98ab9bdc3a71a,https://github.com/pytorch/fairseq/commit/cd6590b6ff2dde41e5b3621641d98ab9bdc3a71a,script to read binarized data,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
340,Myle Ott,myleott@fb.com,2018-08-14 06:29:37-07:00,e74221925938a5a2bd3737c4d92db30853c2aa1e,https://github.com/pytorch/fairseq/commit/e74221925938a5a2bd3737c4d92db30853c2aa1e,Move read_binarized.py to scripts/,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
341,Myle Ott,myleott@fb.com,2018-08-14 06:48:49-07:00,8d6665f2c1c211e508772f2742672a5f7654221e,https://github.com/pytorch/fairseq/commit/8d6665f2c1c211e508772f2742672a5f7654221e,Warn when using FP16 on pre-Volta GPUs,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
342,alexeib,alexei.b@gmail.com,2018-08-16 19:13:06-07:00,ba9f32cc64d4ef3817c42523c644ad336d109d00,https://github.com/pytorch/fairseq/commit/ba9f32cc64d4ef3817c42523c644ad336d109d00,add warmup support back to cosine lr sched (important for mt),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
343,Myle Ott,myleott@fb.com,2018-08-10 10:34:29-07:00,8c0ca1a0c1c88d048c0060fa7c880bd619426f9b,https://github.com/pytorch/fairseq/commit/8c0ca1a0c1c88d048c0060fa7c880bd619426f9b,Diverse Beam Search,6,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,19,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDiverseBeamSearch(unittest.TestCase):'],"[('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)'), ('HypoTokens', '(hypos[0][0], [w1, w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 0.6, 1.0])'), ('HypoTokens', '(hypos[0][1], [w1, w1, eos])'), ('HypoScore', '(hypos[0][1], [0.9, 0.6, 1.0])'), ('HypoTokens', '(hypos[1][0], [w1, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.9])'), ('HypoTokens', '(hypos[1][1], [w1, w2, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.9])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)')]",['def setUp(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
344,Myle Ott,myleott@fb.com,2018-08-21 21:13:02-04:00,47f095fdd07b55986234ecdd326443a1e65e7d7e,https://github.com/pytorch/fairseq/commit/47f095fdd07b55986234ecdd326443a1e65e7d7e,Remove --normalization-constant from fconv,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
345,Louis Martin,louismartin@users.noreply.github.com,2018-08-23 19:58:13+02:00,81ba4c4c48dfaa133888dbf0d8f72c253e2ed0a1,https://github.com/pytorch/fairseq/commit/81ba4c4c48dfaa133888dbf0d8f72c253e2ed0a1,Fix adaptive softmax cutoff comment,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
346,Alexei Baevski,alexei.b@gmail.com,2018-08-23 18:05:50-07:00,f84e1ed47bbf6f771cd3239f7da14d2112cdf8f4,https://github.com/pytorch/fairseq/commit/f84e1ed47bbf6f771cd3239f7da14d2112cdf8f4,disable final layer norm for transformer decoder as it makes things worse,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
347,Myle Ott,myleott@fb.com,2018-08-24 14:36:41-04:00,9c102784b850f8f3c90cfdc96a71800a939f0346,https://github.com/pytorch/fairseq/commit/9c102784b850f8f3c90cfdc96a71800a939f0346,Add training wall time meter,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
348,Sergey Edunov,edunov@fb.com,2018-08-24 15:17:42-07:00,c9b800d21239a7e5c5fb7abc4b22b49a2c0409cf,https://github.com/pytorch/fairseq/commit/c9b800d21239a7e5c5fb7abc4b22b49a2c0409cf,Old checkpoints can't be loaded because of a new meter,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
349,Alexei Baevski,alexei.b@gmail.com,2018-08-26 18:43:14-07:00,c7c567a7750605f2168af4d7ed5af918a3cd6702,https://github.com/pytorch/fairseq/commit/c7c567a7750605f2168af4d7ed5af918a3cd6702,word stats in eval_lm,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
350,Myle Ott,myleott@fb.com,2018-08-27 10:48:22-04:00,753935efe83513298cb7d5e2bf1995eda7b48817,https://github.com/pytorch/fairseq/commit/753935efe83513298cb7d5e2bf1995eda7b48817,Merge internal changes,6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
351,Myle Ott,myleott@fb.com,2018-08-27 07:29:06-07:00,b9956a6a56372c86cdb5f23a8a288b183b123faa,https://github.com/pytorch/fairseq/commit/b9956a6a56372c86cdb5f23a8a288b183b123faa,Fix FP16 version comparison,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
352,Alexei Baevski,alexei.b@gmail.com,2018-08-28 16:09:06-07:00,343819f93523ae4f25bd1e9ee192af74cf6962ee,https://github.com/pytorch/fairseq/commit/343819f93523ae4f25bd1e9ee192af74cf6962ee,"dont send dummy batch when reloading from checkpoint

also don't crash if param does not recieve grads",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
353,Li Zhao,lizhao@devfair0160.h2.fair,2018-08-28 17:43:05-07:00,5852d3a0f815c583119ef6a40f09bce39e2b8ed2,https://github.com/pytorch/fairseq/commit/5852d3a0f815c583119ef6a40f09bce39e2b8ed2,Add adaptive softmax changes for lstm model,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
354,Myle Ott,myleott@fb.com,2018-08-28 09:02:42-07:00,6296de825a801f96219ba20352de335be9f45883,https://github.com/pytorch/fairseq/commit/6296de825a801f96219ba20352de335be9f45883,Add --upsample-primary,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
355,Myle Ott,myleott@fb.com,2018-08-30 12:17:33-04:00,2e507d3cb43073a5656c7d3e74deef0b6a3fec04,https://github.com/pytorch/fairseq/commit/2e507d3cb43073a5656c7d3e74deef0b6a3fec04,Clean up FairseqTask so that it's easier to extend/add new tasks,12,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
356,alexeib,alexei.b@gmail.com,2018-08-30 12:00:49-07:00,b3cd43b29002e13758914be5527969344bd313b8,https://github.com/pytorch/fairseq/commit/b3cd43b29002e13758914be5527969344bd313b8,fix max_positions comparison,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
357,Myle Ott,myleott@fb.com,2018-08-30 14:07:58-07:00,75f6ba0597622e68647c98838e0b2b4fdce09e17,https://github.com/pytorch/fairseq/commit/75f6ba0597622e68647c98838e0b2b4fdce09e17,Fix comment,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
358,Myle Ott,myleott@fb.com,2018-08-31 10:09:47-04:00,0a7f9e64bb7a306efb0ad25d6f03dc05a92dbfb9,https://github.com/pytorch/fairseq/commit/0a7f9e64bb7a306efb0ad25d6f03dc05a92dbfb9,Further generalize EpochBatchIterator and move iterators into new file,6,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestIterators(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],['class TestDataUtils(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
359,alexeib,alexei.b@gmail.com,2018-09-02 10:24:28-07:00,dfd77717b91a6e233829735795ab49d6fd85c0b3,https://github.com/pytorch/fairseq/commit/dfd77717b91a6e233829735795ab49d6fd85c0b3,fix cosine lr sched for t_mult=1 with warmup,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
360,Myle Ott,myleott@fb.com,2018-09-02 20:05:38-07:00,d473620e39d67d6cc0fde9f07918400e4a1216a4,https://github.com/pytorch/fairseq/commit/d473620e39d67d6cc0fde9f07918400e4a1216a4,Test max_positions,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Raises', '(Exception) as context:'), ('True', '('), ('Raises', '(Exception) as context:')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
361,Myle Ott,myleott@fb.com,2018-09-02 23:15:26-04:00,0e101e9c55140b915ea7fd17eeed29461e6c4bc7,https://github.com/pytorch/fairseq/commit/0e101e9c55140b915ea7fd17eeed29461e6c4bc7,Misc changes to simplify upcoming tutorial,7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
362,Myle Ott,myleott@fb.com,2018-09-03 08:57:53-07:00,6381cc977fa909bd53c8915a18684e49b3507332,https://github.com/pytorch/fairseq/commit/6381cc977fa909bd53c8915a18684e49b3507332,Add documentation,44,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
363,Myle Ott,myleott@fb.com,2018-09-03 16:33:36-07:00,4a47b889927af031a21af9b841b001f21ba7135f,https://github.com/pytorch/fairseq/commit/4a47b889927af031a21af9b841b001f21ba7135f,Update documentation,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
364,Angela Fan,angela.h.fan@gmail.com,2018-09-07 15:09:51-07:00,5d00e8eea2644611f397d05c6c8f15083388b8b4,https://github.com/pytorch/fairseq/commit/5d00e8eea2644611f397d05c6c8f15083388b8b4,modified stories readme to include sample preprocessing code to split stories to 1k tokens,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
365,Sergey Edunov,edunov@fb.com,2018-09-17 21:23:48-07:00,5d944b06821dcff1101a4612dd9e4cf4a7c173f3,https://github.com/pytorch/fairseq/commit/5d944b06821dcff1101a4612dd9e4cf4a7c173f3,Fix readme,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
366,Sergey Edunov,edunov@fb.com,2018-09-17 21:24:40-07:00,fe2d1581a4360e19d90d6ab8cbf3d9dd2556b2b1,https://github.com/pytorch/fairseq/commit/fe2d1581a4360e19d90d6ab8cbf3d9dd2556b2b1,Fix docs,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
367,Sergey Edunov,edunov@fb.com,2018-09-17 22:30:35-07:00,74b3f1e999caab20dbcfb8d2949d1a0cc32021e8,https://github.com/pytorch/fairseq/commit/74b3f1e999caab20dbcfb8d2949d1a0cc32021e8,Readme fix,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
368,Sergey Edunov,edunov@apache.org,2018-09-24 13:03:18-07:00,86b5cfe49c0dd435d389663296bd765160c83277,https://github.com/pytorch/fairseq/commit/86b5cfe49c0dd435d389663296bd765160c83277,Update readme with WMT'18 model (#433),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
369,Stephen Roller,roller@fb.com,2018-09-05 13:49:05-04:00,e6d45d5cd79cfdbf7e36eea8ff90fe1b93581171,https://github.com/pytorch/fairseq/commit/e6d45d5cd79cfdbf7e36eea8ff90fe1b93581171,Generator: net_input instead of manual src_tokens.,2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
370,Stephen Roller,roller@fb.com,2018-09-05 14:21:39-04:00,0714080b72694b283b0c57c565e51c50a6ebd702,https://github.com/pytorch/fairseq/commit/0714080b72694b283b0c57c565e51c50a6ebd702,Sequence generator bug fix.,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
371,Myle Ott,myleott@fb.com,2018-09-06 14:04:40-04:00,311d2c6ca9c3991bc9d249de095a8613f9471cc3,https://github.com/pytorch/fairseq/commit/311d2c6ca9c3991bc9d249de095a8613f9471cc3,Revert sequence generator changes,2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
372,Sergey Edunov,edunov@apache.org,2018-09-06 11:46:54-07:00,1082ba352c5f1d524b1fcba43ee611280b169224,https://github.com/pytorch/fairseq/commit/1082ba352c5f1d524b1fcba43ee611280b169224,"Switch to DistributedDataParallelC10d and bump version 0.5.0 -> 0.6.0

- no more FP16Trainer, we just have an FP16Optimizer wrapper
- most of the distributed code is moved to a new wrapper class called DistributedFairseqModel, which behaves like DistributedDataParallel and a FairseqModel at the same time
- Trainer now requires an extra dummy_batch argument at initialization, which we do fwd/bwd on when there's an uneven number of batches per worker. We hide the gradients from these dummy batches by multiplying the loss by 0
- Trainer.train_step now takes a list of samples, which will allow cleaner --update-freq",20,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
373,Myle Ott,myleott@fb.com,2018-09-06 17:41:00-04:00,f66e9cb5998e28a5a10546948c54b9c21801328b,https://github.com/pytorch/fairseq/commit/f66e9cb5998e28a5a10546948c54b9c21801328b,Disable c10d for AdaptiveLoss,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
374,Myle Ott,myleott@fb.com,2018-09-07 12:55:16-07:00,8bd8ec8fa8e9ee460d28728a910495335606a0b2,https://github.com/pytorch/fairseq/commit/8bd8ec8fa8e9ee460d28728a910495335606a0b2,Update LM test with --no-c10d,1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
375,Stephen Roller,roller@fb.com,2018-09-08 17:53:10-04:00,bfeb7732147a9fb885e3496110764df5d8bdc571,https://github.com/pytorch/fairseq/commit/bfeb7732147a9fb885e3496110764df5d8bdc571,"Pass encoder_input to generator, rather than src_tokens/src_lengths.",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
376,Myle Ott,myleott@fb.com,2018-09-09 12:41:26-07:00,83e08b6f6e4b08a77ff3cb3538fbeb93ae65b79f,https://github.com/pytorch/fairseq/commit/83e08b6f6e4b08a77ff3cb3538fbeb93ae65b79f,Fix validation loss,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
377,Myle Ott,myleott@fb.com,2018-09-09 13:09:07-07:00,e775877f6893480f6353262304c6c312920d16a2,https://github.com/pytorch/fairseq/commit/e775877f6893480f6353262304c6c312920d16a2,Add unit test to verify reproducibility after reloading checkpoints,4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestReproducibility(unittest.TestCase):'],"[('Equal', '(cast(train_log[k]), cast(train_res_log[k]))'), ('Equal', '(cast(valid_log[k]), cast(valid_res_log[k]))')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
378,Sergey Edunov,edunov@fb.com,2018-09-10 14:51:11-07:00,ee46c63ba516c72a223252f59871f917b81b7fd0,https://github.com/pytorch/fairseq/commit/ee46c63ba516c72a223252f59871f917b81b7fd0,Fix adaptive loss logging,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
379,Sergey Edunov,edunov@apache.org,2018-09-12 17:25:57-07:00,862cad112ac51be1b20eed9efc5e40ef865a9457,https://github.com/pytorch/fairseq/commit/862cad112ac51be1b20eed9efc5e40ef865a9457,Parallel preprocessing,3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
380,Myle Ott,myleott@fb.com,2018-09-12 20:28:54-07:00,78071e0f8cf9607b25462117edb1b6dd457b3e5e,https://github.com/pytorch/fairseq/commit/78071e0f8cf9607b25462117edb1b6dd457b3e5e,Fix type of c10d bucket size,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
381,Myle Ott,myleott@fb.com,2018-09-17 11:33:17-04:00,fbe8ce65d3c81a7c6f042531b8301ca626c3329c,https://github.com/pytorch/fairseq/commit/fbe8ce65d3c81a7c6f042531b8301ca626c3329c,Better support for various c10d API changes,6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
382,Alexei Baevski,alexei.b@gmail.com,2018-09-20 17:14:05-07:00,cfd2a3a048ca2d66d7c40d58f21179c8eb6c264c,https://github.com/pytorch/fairseq/commit/cfd2a3a048ca2d66d7c40d58f21179c8eb6c264c,core changes to support latte collab,13,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
383,alexeib,alexei.b@gmail.com,2018-09-21 18:47:53-07:00,28069cf4895469812d699ae31d75e896e80ea474,https://github.com/pytorch/fairseq/commit/28069cf4895469812d699ae31d75e896e80ea474,fix issue with truncated dict,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
384,Myle Ott,myleott@fb.com,2018-09-24 11:59:27-04:00,535ca9915b7edea930c946de74b49b963ef86347,https://github.com/pytorch/fairseq/commit/535ca9915b7edea930c946de74b49b963ef86347,Merge internal changes,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
385,Sergey Edunov,edunov@apache.org,2018-09-24 13:52:01-07:00,a4fe8c990a6195ec2ab8510d4b384b13733ee151,https://github.com/pytorch/fairseq/commit/a4fe8c990a6195ec2ab8510d4b384b13733ee151,Add back secondary set,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
386,Myle Ott,myleott@fb.com,2018-09-25 14:02:34-04:00,864b89d044ffdd33f13802af20da8714bf2855f5,https://github.com/pytorch/fairseq/commit/864b89d044ffdd33f13802af20da8714bf2855f5,"Online backtranslation module

Co-authored-by: liezl200 <lie@fb.com>",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,4,1,0,0,0,0,0,0,0,1,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestBacktranslationDataset(unittest.TestCase):'],"[('TensorEqual', '(expected_src, generated_src)'), ('TensorEqual', '(expected_tgt, tgt_tokens)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)')]",['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],"[('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
387,myleott,myleott@fb.com,2018-09-30 12:25:59-07:00,f8377a704cda050dec1376d6bea9a8888ed57fbf,https://github.com/pytorch/fairseq/commit/f8377a704cda050dec1376d6bea9a8888ed57fbf,fbshipit-source-id: 6a835d32f9dc5e0de118f1b46d365d0e0cc85e11,5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,11,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDataNoising(unittest.TestCase):'],"[('Equal', '(x_len[0] - 2, l_noised[0])'), ('Equal', '(x_noised[i][0], x[i+2][0])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_noised[i][0], vocab.unk())'), ('Equal', '(x_noised[i][0], x[i][0])'), ('Equal', '(x[j][i], x_noised[j][i])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x[i][0], x_noised[i][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
388,myleott,myleott@fb.com,2018-09-30 12:40:34-07:00,0bc5c2e9cc9b4378dfb474593e82b49e1d4037a8,https://github.com/pytorch/fairseq/commit/0bc5c2e9cc9b4378dfb474593e82b49e1d4037a8,fbshipit-source-id: 17992f6a5908f078942544b769eda7a340a5e359,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
389,Myle Ott,myleott@fb.com,2018-09-30 14:05:19-07:00,b87c536651649ad71dd8766a409ef1c032b55afa,https://github.com/pytorch/fairseq/commit/b87c536651649ad71dd8766a409ef1c032b55afa,"Merge internal changes (#295)

Summary:
Changelog:
- `90f52a1`: Support loading subsets of the data on each worker with the `--fix-batches-to-gpus` flag. This should fix #217 and #266.
- `6eda0a9`: Update README for replicating the ""Scaling Neural Machine Translation"" paper
- `b14c7cf`: Fallback to no_c10d backend for pytorch 0.4.1 (fixes #294)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/295

Differential Revision: D10121559

Pulled By: myleott

fbshipit-source-id: 41c84d0ee4cdd113544b5d3aa38ae8b23acc2c27",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
390,alexeib,alexei.b@gmail.com,2018-09-30 16:55:01-07:00,22e535e23b4c2e95e7e72198ae98c7e0d4ac8c18,https://github.com/pytorch/fairseq/commit/22e535e23b4c2e95e7e72198ae98c7e0d4ac8c18,"Merge internal changes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/296

Differential Revision: D10121830

Pulled By: alexeib

fbshipit-source-id: 1b73430bdfdcb20a9a6123abfca3472a0d307b3b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
391,Liezl Puzon,lie@fb.com,2018-10-02 15:10:21-07:00,86e93f2bcf0f50f607658794c87f766b74a52585,https://github.com/pytorch/fairseq/commit/86e93f2bcf0f50f607658794c87f766b74a52585,"Explicitly list out generation args for backtranslation dataset

Summary:
Using argparse Namespace hides the actual args that are expected and makes code harder to read.

Note the difference in style for the args list

    def __init__(
        self,
        tgt_dataset,
        tgt_dict,
        backtranslation_model,
        unkpen,
        sampling,
        beam,
        max_len_a,
        max_len_b,
    ):

instead of

    def __init__(
        self, tgt_dataset, tgt_dict, backtranslation_model, unkpen, sampling,
        beam,  max_len_a, max_len_b,
    ):

Reviewed By: dpacgopinath

Differential Revision: D10152331

fbshipit-source-id: 6539ccba09d48acf23759996b7e32fb329b3e3f6",2,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
392,Michael Auli,3248766+michaelauli@users.noreply.github.com,2018-10-02 16:11:23-07:00,df88ba95c7028e8c09430efa6fe2c9c98e0e13d5,https://github.com/pytorch/fairseq/commit/df88ba95c7028e8c09430efa6fe2c9c98e0e13d5,"Update README.md

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/300

Differential Revision: D10154711

Pulled By: edunov

fbshipit-source-id: 859d1ac59923b67c1547b6f7acb94f801b0c3318",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
393,Liezl Puzon,lie@fb.com,2018-10-02 18:20:01-07:00,f766c9a0d5d431fd16fd894b348b2d21eee259f5,https://github.com/pytorch/fairseq/commit/f766c9a0d5d431fd16fd894b348b2d21eee259f5,"Pass in kwargs and SequenceGenerator class to init BacktranslationDataset

Summary: This generalizes BacktranslationDataset to allow us to use any SequenceGenerator class. For example, if we want to use this model in PyTorch Translate, we can pass the following to BacktraanslationDataset init: (1) a PyTorch Translate SequenceGenerator class as generator_class and (2) the appropriate args for initializing that class as kwargs.

Reviewed By: xianxl

Differential Revision: D10156552

fbshipit-source-id: 0495d825bf4727da96d0d9a40dc434135ff3486c",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
394,Myle Ott,myleott@fb.com,2018-10-03 15:37:31-07:00,fc677c945e73ecf1cdad8bc2562b0c1d842ba0cb,https://github.com/pytorch/fairseq/commit/fc677c945e73ecf1cdad8bc2562b0c1d842ba0cb,"Fix proxying in DistributedFairseqModel

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/302

Differential Revision: D10174608

Pulled By: myleott

fbshipit-source-id: 4e2dfc76eae97afc5488f29b47e74f9897a643ff",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
395,Liezl Puzon,lie@fb.com,2018-10-03 18:18:03-07:00,b9e29a4711a6c0b7923879d5d59f3e879f0f228a,https://github.com/pytorch/fairseq/commit/b9e29a4711a6c0b7923879d5d59f3e879f0f228a,"Option to remove EOS at source in backtranslation dataset

Summary:
If we want our parallel data to have EOS at the end of source, we keep the EOS at the end of the generated source dialect backtranslation.
If we don't want our parallel data to have EOS at the end of source, we **remove** the EOS at the end of the generated source dialect backtranslation.

Note: we always want EOS at the end of our target / reference in parallel data so our model can learn to generate a sentence at any arbitrary length. So we make sure that the original target has an EOS before returning a batch of {generated src, original target}. If our original targets in tgt dataset doesn't have an EOS, we append EOS to each tgt sample before collating.
We only do this for the purpose of collating a {generated src, original tgt} batch AFTER generating the backtranslations. We don't enforce any EOS before passing tgt to the tgt->src model for generating the backtranslation. The users of this dataset is expected to format tgt dataset examples in the correct format that the tgt->src model expects.

Reviewed By: jmp84

Differential Revision: D10157725

fbshipit-source-id: eb6a15f13c651f7c435b8db28103c9a8189845fb",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
396,James Cross,jcross@fb.com,2018-10-05 11:06:46-07:00,265f42b7d52df824364f6709678a0827ef2f19e7,https://github.com/pytorch/fairseq/commit/265f42b7d52df824364f6709678a0827ef2f19e7,"multihead_attention: pre-transpose incremental state (#232)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/232

Though transpose operations are essentially free during PyTorch execution, they can result in costly operations when exported to Caffe2 inference nets via ONNX tracing, especially when applied repeatedly to large tensors.

For this reason, we update `MultiheadAttention` to store its incremental state with shape (bsz, num_heads, seq_len, head_dim), that is after transposing the projected input. This should result in non-trivially faster exported models without changing the semantics or speed of PyTorch execution.

Reviewed By: myleott

Differential Revision: D10186506

fbshipit-source-id: 8a42712423ee767ea49ed88d2a4653f900d14fba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
397,Liezl Puzon,lie@fb.com,2018-10-05 18:19:42-07:00,8798a24031f40a04c32118c6657f72a3c88819ea,https://github.com/pytorch/fairseq/commit/8798a24031f40a04c32118c6657f72a3c88819ea,"Have noising account for sentences with and without EOS (#305)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/305

Previously, noising code assumed that every sentence had an EOS which had to be excluded from noising operations (since we shouldn't drop, blank, or shuffle EOS). This logic allows the noising module to handle sentences with EOS and without EOS

Reviewed By: xianxl

Differential Revision: D10114425

fbshipit-source-id: 04ec8547343eb94266bda1ac7fca3d8a1991c9f4",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,29,0,0,0,0,0,0,0,0,0,0,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '('), ('Equal', '(x_len[0] - 2, l_noised[0])'), ('Equal', '(x_noised[i][0], x[i + 2][0])'), ('_word_dropout_correct', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_noised[i][0], unk)'), ('Equal', '(x_noised[i][0], x[i][0])'), ('_word_blanking_correct', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('Equal', '(x[j][i], x_noised[j][i])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x[i][0], x_noised[i][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])'), ('_no_shuffle_with_0_distance', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_shuffle_with_distance_3', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('NotEqual', '('), ('_word_dropout_correct', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_blanking_correct', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_no_shuffle_with_0_distance', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_shuffle_with_distance_3', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(x_len[0] - 2, l_noised[0])'), ('Equal', '(x_noised[i][0], x[i+2][0])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_noised[i][0], vocab.unk())'), ('Equal', '(x_noised[i][0], x[i][0])'), ('Equal', '(x[j][i], x_noised[j][i])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x[i][0], x_noised[i][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
398,Liezl Puzon,lie@fb.com,2018-10-05 18:19:44-07:00,e286243c68f1589a781488580fc19388714612be,https://github.com/pytorch/fairseq/commit/e286243c68f1589a781488580fc19388714612be,"Add denoising dataset for denoising autoencoder (#306)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/306

This uses a source dataset to generate a batch of {source: noisy source, target: original clean source} which allows us to train a denoising autoencoding component as part of a seq2seq model.

Reviewed By: xianxl

Differential Revision: D10078981

fbshipit-source-id: 026225984d4a97062ac05dc3a36e79b5c841fe9c",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('TensorEqual', '(expected_src, generated_src)'), ('TensorEqual', '(expected_tgt, tgt_tokens)'), ('TensorEqual', '(expected_src, generated_src)'), ('TensorEqual', '(expected_tgt, tgt_tokens)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
399,James Cross,jcross@fb.com,2018-10-17 13:54:59-07:00,0eea6923b9d7f408e667714709b070171ac7fe05,https://github.com/pytorch/fairseq/commit/0eea6923b9d7f408e667714709b070171ac7fe05,"fix make_positions() typo (#316)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/316

This code should actually be keeping the padded positions as `padding_idx` (though note that this is on the ONNX export path, and it has no effect in the most common case when using the exported network to do un-batched inference).

Reviewed By: myleott

Differential Revision: D10431872

fbshipit-source-id: 79fe4ac27cafcd4701e0f2a90e29d1b7362dc6f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
400,Peng-Jen Chen,pipibjc@fb.com,2018-10-19 01:12:59-07:00,0a628401adb62873899b4e13ac9415c1b330ca45,https://github.com/pytorch/fairseq/commit/0a628401adb62873899b4e13ac9415c1b330ca45,"Update upgrade_state_dict in transformer.py to upgrade_state_dict_named (#317)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/317

When upgrading `state_dict` variable, `upgrade_state_dict` function in TransformerEncoder/TransformerDecoder doesn't handle multiple encoders/decoders, however, D10052908 will be the case.

Before the change, we will hit error message [1] when loading checkpoint for multilingual_transformer model in D10052908. This diff will fix it.

Reviewed By: myleott, liezl200

Differential Revision: D10375418

fbshipit-source-id: 7104c1a463e78f3fa33d8479a37c51608be50610",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
401,Peng-Jen Chen,pipibjc@fb.com,2018-10-20 22:25:11-07:00,8441cbf35c6e1dd9c5b1dc6be07358463794ed4f,https://github.com/pytorch/fairseq/commit/8441cbf35c6e1dd9c5b1dc6be07358463794ed4f,"Manually port pull request 385

Summary:
Manually port fairinternal fairseq-py pull request #385 [1] to fbcode.

Resolve the merge conflict of removing fp16_trainer per offline discussion with Myle. Also updated codes to make generate.py works.

[1] https://github.com/fairinternal/fairseq-py/pull/385/commits/18fa6e154781cf0c4b1596429dba7e753a545069

Reviewed By: liezl200

Differential Revision: D10052908

fbshipit-source-id: c3c378d78dc1e9ac087c815f359e78c0048ff2f5",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
402,Halil Akin,halilakin@fb.com,2018-10-22 11:24:08-07:00,23e9dc2e03fe49eae61b183b18b200e82495a099,https://github.com/pytorch/fairseq/commit/23e9dc2e03fe49eae61b183b18b200e82495a099,"Fix another distributed syncing issue

Summary:
This is another failure due to distributed GPU's getting out of sync.
We are running save_and_eval (which has the inter-gpu communication calls) by
looking at number of updates. But number of updates means weight updates. Whenever
there is an issue in the training and weights can't be updated, nodes go
out of sync and nodes start failing. So we should check number of iterations instead.

I am, again, making a small change to save the day, but we should decouple/refactor
save_and_eval logic from the training, to have less headache in future.
Planning, working on that in future. But this should solve some of the
issues for now.

Reviewed By: jhcross

Differential Revision: D10478427

fbshipit-source-id: b9deacfea252b2fb66b81c799fa78e2439fa514c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
403,Deepak Gopinath,dgopinath@fb.com,2018-10-22 22:26:02-07:00,1aae5f6a8a8b5c3662dee6ce1af648cea3b5e79d,https://github.com/pytorch/fairseq/commit/1aae5f6a8a8b5c3662dee6ce1af648cea3b5e79d,"Expose BacktranslationDataset from fairseq.data (#324)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/324

BacktranslationDataset was introduced recently but was not exposed as part of the fairseq.data module

Reviewed By: liezl200

Differential Revision: D10412717

fbshipit-source-id: 8a9d4ecd43fd376e895c450d00e765a869c95eff",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
404,Deepak Gopinath,dgopinath@fb.com,2018-10-22 22:26:02-07:00,613ffeea9c7454a53f41c9131ebfe1e15f961626,https://github.com/pytorch/fairseq/commit/613ffeea9c7454a53f41c9131ebfe1e15f961626,"Add size method to BacktranslationDataset + misc fixes (#325)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/325

RoundRobinZipDataset requires size(index) method implemented in every dataset used. Also added missing return statements in a few methods.

Reviewed By: liezl200

Differential Revision: D10457159

fbshipit-source-id: 01856eb455f2f3a21e7fb723129ff35fbe29e0ae",2,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
405,Haoran Li,aimeeli@fb.com,2018-10-25 12:34:57-07:00,4afa455eee6a4991b81d6c907c6dc53c272de57d,https://github.com/pytorch/fairseq/commit/4afa455eee6a4991b81d6c907c6dc53c272de57d,"make fairseq models compatible with character inputs and use character inputs for elmo in pytext

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/321

Reviewed By: alexeib

Differential Revision: D10430186

fbshipit-source-id: 9cc8fe0f202cc49370cecf36312bcc9bf0b4deee",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
406,Deepak Gopinath,dgopinath@fb.com,2018-10-25 14:43:39-07:00,0d63cf03a949ea01d18c2b61281a5d124a6d6a7f,https://github.com/pytorch/fairseq/commit/0d63cf03a949ea01d18c2b61281a5d124a6d6a7f,"LanguagePairDataset and BacktranslationDataset changes for semi supervised task setup (#330)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/330

As part of the semi sueprvised task setup (https://github.com/pytorch/translate/pull/243), this diff adds the ability for LanguagePairDataset to remove EOS from source or append EOS to target. This functionality is required by BacktranslationDataset to use translations as source data.

Also added changes to BacktranslationDataset to make it work on GPU. We needed to transfer back-translated sentences back to CPU for the LanguagePairDataset to collate.

Reviewed By: liezl200

Differential Revision: D10846294

fbshipit-source-id: b015ecb5fcef26fba507c30f8a4992bdbc54899f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
407,Wei Ho,weiho@fb.com,2018-10-26 14:07:22-07:00,6117f82753afa0d8db6f1c8731ec02ce614d3855,https://github.com/pytorch/fairseq/commit/6117f82753afa0d8db6f1c8731ec02ce614d3855,"Fix print & add more informative logging

Summary: Fix fairseq's `force` option for disabling print suppression (otherwise, `print(..., force=True)` fails on master since the force kwarg gets passed to the builtin print).

Reviewed By: dpacgopinath

Differential Revision: D10522058

fbshipit-source-id: bbc10c021a7d21396ebfbb1bf007f6b9b162f4fd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
408,Xian Li,xianl@fb.com,2018-10-26 18:18:13-07:00,90c01b3a0bc8f6c89f715b457079d59f55721165,https://github.com/pytorch/fairseq/commit/90c01b3a0bc8f6c89f715b457079d59f55721165,"Extend WordShuffle noising function to apply to non-bpe tokens

Summary:
We'd like to resue the noising functions and DenoisingDataset in
adversarial training. However, current noising functions assume the input are
subword tokens. The goal of this diff is to extend it so the noising can be
applied to word tokens. Since we're mostly interested in the word shuffle
noising, so I only modified the WordShuffle class.

Reviewed By: liezl200

Differential Revision: D10523177

fbshipit-source-id: 1e5d27362850675010e73cd38850c890d42652ab",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(x[k][0], x_noised[v][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])'), ('_no_shuffle_with_0_distance', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_nonbpe_shuffle_with_distance_3', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
409,James Cross,jcross@fb.com,2018-10-29 23:00:35-07:00,672977c1bc3fd0d37c91ab0a2828c56bbd2b0769,https://github.com/pytorch/fairseq/commit/672977c1bc3fd0d37c91ab0a2828c56bbd2b0769,"transformer onnx trace: skip no-op transpose (#333)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/333

A tiny hack to speed up inference slightly for transformer beam search after export to graph mode. Specifically, there is no need to transpose a dimension with size 1 (the sequence length of a single decoder time step during beam search) with its neighbor immediately before a view/reshape.

Reviewed By: jmp84

Differential Revision: D12833011

fbshipit-source-id: f9c344a9ad595e6e48a8a65b31cf2b1392f9b938",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
410,John Pope,jp@bellgeorge.com,2018-10-31 18:09:50-07:00,f41088a5641d68dfd60b6a9518212f610d7b45dd,https://github.com/pytorch/fairseq/commit/f41088a5641d68dfd60b6a9518212f610d7b45dd,"match examples/stories/writingPrompts scripts to correct folder

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/290

Differential Revision: D12876759

Pulled By: myleott

fbshipit-source-id: 9f6d1c9de27dad29368a7edb923dfcf770355938",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
411,Zihao Fu,fllubo@qq.com,2018-10-31 19:01:52-07:00,f3a0939eed35dce9ea5b339911133e43f7d7b1f1,https://github.com/pytorch/fairseq/commit/f3a0939eed35dce9ea5b339911133e43f7d7b1f1,"Update bleu.py (#320)

Summary:
Modify Error message of bleu.
Fix the issue:  https://github.com/pytorch/fairseq/issues/284
Pull Request resolved: https://github.com/pytorch/fairseq/pull/320

Differential Revision: D12876721

Pulled By: myleott

fbshipit-source-id: df25885a94a584cbf4b86a1665e3e513c7eb8e9a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
412,Myle Ott,myleott@fb.com,2018-11-01 01:23:43-07:00,5bbd148e6ea1fc2d62c9503fe14bd795fea40389,https://github.com/pytorch/fairseq/commit/5bbd148e6ea1fc2d62c9503fe14bd795fea40389,"Fix tests + style nits + Python 3.5 compat

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/336

Differential Revision: D12876709

Pulled By: myleott

fbshipit-source-id: a31536e2eb93f752600b9940c28e9b9fcefc8b86",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
413,Liezl Puzon,lie@fb.com,2018-11-01 07:09:32-07:00,c9c660c0d42a8fa5b65005f1e83c5e0c2578268e,https://github.com/pytorch/fairseq/commit/c9c660c0d42a8fa5b65005f1e83c5e0c2578268e,"Denoising autoencoder task (#251)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/251

We should use shared encoder and separate decoders as in:

https://fb.facebook.com/groups/2156114531381111/permalink/2169028113423086/

Generation is a hack, ideally the net input should have the lang pair info so that when we pass the sample to the model, it can select the correct encoder/decoder pair.

diff [2/2] will be for flow integration for basic experimentation

TODO in a future diff: figure out how to generalize this so export will work??

This works with vocab reduction, but we only support vocab reduction for src-tgt, not src-src model. A future (lowpri) task could be to add word prediction vocab reduction for src-src model to speed up training.

Reviewed By: xianxl

Differential Revision: D10512576

fbshipit-source-id: 545d96cad8e814b9da7be102a48cc5cac358b758",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
414,Myle Ott,myleott@fb.com,2018-11-01 07:49:38-07:00,50a671f78d0c8de0392f924180db72ac9b41b801,https://github.com/pytorch/fairseq/commit/50a671f78d0c8de0392f924180db72ac9b41b801,"Move fairseq part of D10478427 directly into pytorch-translate (#337)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/337

Pull Request resolved: https://github.com/pytorch/translate/pull/250

Reviewed By: akinh

Differential Revision: D12880352

fbshipit-source-id: 61e9888a9cc3df07e805820b74a5fcf359dfe0ea",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
415,ngimel,ngimelshein@nvidia.com,2018-11-01 14:26:56-07:00,726a47dc7e7b98dd6496e7a225d2c7760da1366f,https://github.com/pytorch/fairseq/commit/726a47dc7e7b98dd6496e7a225d2c7760da1366f,"Fix ""ignore-case"" behavior (#339)

Summary:
Currently, if `ignore-case` is set, the same line will be yielded twice - once as lower-cased version, once as original version, leading to lower than expected uncased scores.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/339

Differential Revision: D12890386

Pulled By: myleott

fbshipit-source-id: 0570e5f6e8f848f2c6439d615e70aca6df097eef",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
416,Liezl Puzon,lie@fb.com,2018-11-01 17:11:02-07:00,0b05467dd8db89f8c2c5649eec0f33cd03c8b068,https://github.com/pytorch/fairseq/commit/0b05467dd8db89f8c2c5649eec0f33cd03c8b068,"Black formatting in fairseq/test_noising (#341)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/341

Use black formatting in test_noising.py

Reviewed By: xianxl

Differential Revision: D12810285

fbshipit-source-id: 5517dd5d2f086831f487d88acf6bc2fa18820297",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
417,Liezl Puzon,lie@fb.com,2018-11-01 17:11:02-07:00,b1521f962e4ca670311c0cd0c8b1dadf310cb242,https://github.com/pytorch/fairseq/commit/b1521f962e4ca670311c0cd0c8b1dadf310cb242,"Refactor fairseq/test_noising with a word shuffle helper function (#340)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/340

This allows us to do a lot less copy paste when adding new word shuffle function tests

Reviewed By: xianxl

Differential Revision: D12810304

fbshipit-source-id: a56b5df093d17be2b73837897c526978cab92b70",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,9,0,0,0,0,0,0,0,0,0,0,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(x[k][i], x_noised[v][i])'), ('Equal', '(pre_shuffle_length, post_shuffle_length)'), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '(')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(x[j][i], x_noised[j][i])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x[i][0], x_noised[i][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])'), ('Equal', '(x[k][0], x_noised[v][0])'), ('Equal', '(x[k][1], x_noised[v][1])'), ('Equal', '(x_len[0], l_noised[0])'), ('Equal', '(x_len[1], l_noised[1])'), ('_no_shuffle_with_0_distance', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_shuffle_with_distance_3', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_no_shuffle_with_0_distance', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_nonbpe_shuffle_with_distance_3', '('), ('_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_no_shuffle_with_0_distance', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())'), ('_word_shuffle_with_distance_3', '('), ('_no_eos_at_end', '(x=x_noised, x_len=l_noised, eos=vocab.eos())')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
418,Liezl Puzon,lie@fb.com,2018-11-06 20:38:56-08:00,2b13f3c036898140c6266d8b2d39c7fb3b3112e6,https://github.com/pytorch/fairseq/commit/2b13f3c036898140c6266d8b2d39c7fb3b3112e6,"Support BPE end of word marker suffix in fairseq noising module

Summary:
There are 2 ways to implement BPE:
1. use a continuation marker suffix to indicate that there is at least one more subtoken left in the word
2. use a end of word marker suffix to indicate that there is no more subtokens left in the word

This adds some logic to account for either kind of BPE marker suffix. This diff adds a corresponding test. I also refactored the test setup to reduce the number of boolean args when setting up test data.

Reviewed By: xianxl

Differential Revision: D12919428

fbshipit-source-id: 405e9f346dce6e736c1305288721dfc7b63e872a",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('_word_shuffle_matches_expected', '('), ('_word_shuffle_matches_expected', '(')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
419,Myle Ott,myleott@fb.com,2018-11-07 10:08:21-08:00,8eb232ce150d1afb44880a7078eb4abbae60dc32,https://github.com/pytorch/fairseq/commit/8eb232ce150d1afb44880a7078eb4abbae60dc32,"Merge internal changes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/352

Differential Revision: D12956930

Pulled By: myleott

fbshipit-source-id: 39334a79544bac570feb04be9103269d7c1563f9",19,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
420,Peng-Jen Chen,pipibjc@fb.com,2018-11-08 03:07:18-08:00,189fcabffc87c5216e7bf2a76f38f34ce41ffe9c,https://github.com/pytorch/fairseq/commit/189fcabffc87c5216e7bf2a76f38f34ce41ffe9c,"Fix error when training multilingual_translation task with multi-GPU

Summary:
D10052908 introduce multilingual_translation task, but it raises exception when training with multiple-GPUs: P60202593

With Myle's help, we found that it is because of improperly handled dummy batch data type, and it causes optimizer.backward() is not executed same number of times cross different GPUs.

Reviewed By: xianxl

Differential Revision: D12964263

fbshipit-source-id: 4991039030bf373f0c484e131acc4736487be4d8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
421,Ruty Rinott,ruty@fb.com,2018-11-09 16:16:36-08:00,880e7cd46adc72245fbc5d939106d3dc081caa4f,https://github.com/pytorch/fairseq/commit/880e7cd46adc72245fbc5d939106d3dc081caa4f,"pipeline for LM training

Summary:
step 2 of pipeline for LM training
assumes tokenized text data as input. Splits it into train/validation/test, and runs binarization
(step a_ii in https://fb.quip.com/kazzAxvZHBj9)

Reviewed By: borguz

Differential Revision: D10454705

fbshipit-source-id: 74e8679041f5507c4e404c1b719547c2ae9ed983",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
422,Liezl Puzon,lie@fb.com,2018-11-13 11:31:45-08:00,7e60d45b017f6d08c607f57b9c4f6aa2ded08c97,https://github.com/pytorch/fairseq/commit/7e60d45b017f6d08c607f57b9c4f6aa2ded08c97,"Support for BPE vocabs + denoising autoencoder in PyTorch Translate (#362)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/362

Pull Request resolved: https://github.com/pytorch/translate/pull/254

This actually uses the fairseq logic which supports BPE cont / end word marker suffixes.

Reviewed By: xianxl

Differential Revision: D12952766

fbshipit-source-id: 35a1bbc38240e4145bec0fc419f2d0a6a73ae2e5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
423,Myle Ott,myleott@fb.com,2018-11-14 07:41:22-08:00,161d1e06c9f08586d9cd4d55494392b49614a066,https://github.com/pytorch/fairseq/commit/161d1e06c9f08586d9cd4d55494392b49614a066,"Fix dummy batch when --max-tokens is small (fixes #347)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/366

Differential Revision: D13058513

Pulled By: myleott

fbshipit-source-id: a146d2cfb345d404775ed8d6b8e4a4ad4e7a33b4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
424,Haoran Li,aimeeli@fb.com,2018-11-16 14:23:16-08:00,a4e349859e6b7b5b2db4a82252021539fcbd08f0,https://github.com/pytorch/fairseq/commit/a4e349859e6b7b5b2db4a82252021539fcbd08f0,"make dictionary optional

Reviewed By: jingfeidu

Differential Revision: D13104360

fbshipit-source-id: 9636f5ee2721818f98b33af559fa24292534a72f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
425,Myle Ott,myleott@fb.com,2018-11-17 10:34:52-08:00,2625b0a48eb4eb3e39869384620fa033c9eca4a0,https://github.com/pytorch/fairseq/commit/2625b0a48eb4eb3e39869384620fa033c9eca4a0,"Add LegacyDistributedDataParallel in place of no_c10d (#370)

Summary:
This should bring back the speedup with --update-freq that we reported in the Scaling Neural Machine Translation paper.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/370

Differential Revision: D13100281

Pulled By: myleott

fbshipit-source-id: 4a81b51bb7390a197add314a4be5512bbf68c085",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
426,Myle Ott,myleott@fb.com,2018-11-18 08:25:22-08:00,0864a9c49d6c653d27796d9668f2da50fab11439,https://github.com/pytorch/fairseq/commit/0864a9c49d6c653d27796d9668f2da50fab11439,"Fix build for docs

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/372

Differential Revision: D13114426

Pulled By: myleott

fbshipit-source-id: 6c24b96a3556a0ecd3d1f350642a884254a40bd3",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
427,Naman Goyal,namangoyal@learnfair0942.h2.fair,2018-11-18 14:42:37-08:00,693894b6de4af34af10789906e7ece5be2e78989,https://github.com/pytorch/fairseq/commit/693894b6de4af34af10789906e7ece5be2e78989,"Merge small fixes from internal

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/374

Differential Revision: D13116074

Pulled By: myleott

fbshipit-source-id: 485724cc5a40e8360d21e4bf9c35821baa0ddc57",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
428,Halil Akin,halilakin@fb.com,2018-11-19 11:43:55-08:00,a442244db661a30fe70ee46a96593c94659414f7,https://github.com/pytorch/fairseq/commit/a442244db661a30fe70ee46a96593c94659414f7,"Protect against failures in case of OOMs

Summary: Fixing some distributed failures that happen when OOMs are observed.

Reviewed By: myleott

Differential Revision: D13121054

fbshipit-source-id: f71a0a695332acbaa1797e89887b8b7c7ddaa727",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
429,Myle Ott,myleott@fb.com,2018-11-25 21:24:24-08:00,3c19878f71fbff07d8c34c139d6447d3877fae64,https://github.com/pytorch/fairseq/commit/3c19878f71fbff07d8c34c139d6447d3877fae64,"Refactor BacktranslationDataset to be more reusable (#354)

Summary:
- generalize AppendEosDataset -> TransformEosDataset
- remove EOS logic from BacktranslationDataset (use TransformEosDataset instead)
- BacktranslationDataset takes a backtranslation_fn instead of building the SequenceGenerator itself
Pull Request resolved: https://github.com/pytorch/fairseq/pull/354

Reviewed By: liezl200

Differential Revision: D12970233

Pulled By: myleott

fbshipit-source-id: d5c5b0e0a75eca1bd3a50382ac24621f35c32f36",10,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
430,Myle Ott,myleott@fb.com,2018-11-25 21:27:24-08:00,14506a83a29042656297bf1d3e888e49278d3990,https://github.com/pytorch/fairseq/commit/14506a83a29042656297bf1d3e888e49278d3990,"Fix some recursive functions (e.g., reorder_incremental_state) to only touch each module once (#379)

Summary:
This can happen if a module is registered in more than one place in the network.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/379

Differential Revision: D13154498

Pulled By: myleott

fbshipit-source-id: a35575d1956a46cd35ac8b16a719ad20ac3e380a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
431,Haoran Li,aimeeli@fb.com,2018-11-26 20:02:05-08:00,a5e2d786a1a4a91efbc946910de15b52b2d58dfe,https://github.com/pytorch/fairseq/commit/a5e2d786a1a4a91efbc946910de15b52b2d58dfe,"onnx bi-transformer (#385)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/385

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/6

Pull Request resolved: https://github.com/pytorch/pytorch/pull/14292

Reviewed By: jingfeidu

Differential Revision: D10517864

fbshipit-source-id: 81008b5cc6aab70e23329c187392fb72ee057d78",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
432,Liezl Puzon,lie@fb.com,2018-11-27 01:03:25-08:00,07e34244e85e68aa6ae72bc200f1347777e3a8bb,https://github.com/pytorch/fairseq/commit/07e34244e85e68aa6ae72bc200f1347777e3a8bb,"Decoder embedding sharing in PyTorch Translate for denoising autoencoder (#386)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/386

Pull Request resolved: https://github.com/pytorch/translate/pull/266

This allows decoder embedding sharing for denoising autoencoder modules with different decoders (one for src decoding and one for tgt decoding)

Reviewed By: dpacgopinath

Differential Revision: D13133015

fbshipit-source-id: 3c98be639d705744ccf5ba3a8fd7d10ddc7aef4a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
433,Myle Ott,myleott@fb.com,2018-11-28 17:20:19-08:00,866d0d2e3d7056e7523128afd87fca15bf4e4dbc,https://github.com/pytorch/fairseq/commit/866d0d2e3d7056e7523128afd87fca15bf4e4dbc,"Fix --ddp-backend=no_c10d for params that don't require grads

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/388

Reviewed By: theweiho

Differential Revision: D13244869

fbshipit-source-id: d22c18f63f9a691ccc7245e06bc9a5b776a192b5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
434,Haoran Li,aimeeli@fb.com,2018-11-28 18:26:53-08:00,7bbe528d819824951588a692ba30990d9b806159,https://github.com/pytorch/fairseq/commit/7bbe528d819824951588a692ba30990d9b806159,"fixes on bi-transformer onnx

Summary: replace dynamic index put with copying and creating a new tensor

Reviewed By: wanchaol

Differential Revision: D13244573

fbshipit-source-id: 909f7913ad579ed035f29bb52321ff01e09a2c60",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
435,linkerr,569889194@qq.com,2018-11-30 07:28:17-08:00,9dd8724577fd01511a59df962e856a7e30be4618,https://github.com/pytorch/fairseq/commit/9dd8724577fd01511a59df962e856a7e30be4618,"fixed torch 0.4.0 , ""RuntimeError: Expected object of type torch.cuda… (#393)

Summary:
….LongTensor but found type torch.cuda.FloatTensor for argument #3 'index' "" error

in the torch.__version__ == 0.4.0 ,
new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)
will return a float dtype Tensor, when exec the ""line 321: fairseq/fairseq/models/fconv.py "" will throw a RuntimeError
Pull Request resolved: https://github.com/pytorch/fairseq/pull/393

Differential Revision: D13276496

Pulled By: myleott

fbshipit-source-id: e7986246fbe2c79fff61bcab0e5bec9dd63e0afd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
436,Myle Ott,myleott@fb.com,2018-12-04 13:31:22-08:00,776e9ce3d43ecf8b65b3ff737196a4a9cfe42ecf,https://github.com/pytorch/fairseq/commit/776e9ce3d43ecf8b65b3ff737196a4a9cfe42ecf,"Better error message if workers fall out of sync (#396)

Summary:
This kind of issue should be rare, but the exception that was thrown before (""UnpicklingError: invalid load key"") was very opaque, so let's use something a bit clearer.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/396

Differential Revision: D13325600

Pulled By: myleott

fbshipit-source-id: 2e7093752d45d6b04a3d506aca8d5694b72ab638",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
437,Teng Li,tengli@fb.com,2018-12-05 18:27:05-08:00,50591a29736110e405f4641d77e2b98a3cf6bd2a,https://github.com/pytorch/fairseq/commit/50591a29736110e405f4641d77e2b98a3cf6bd2a,"Enable check_reduction for imagenet flow and fairseq

Summary:
As the title says, better to enable this for certain use cases to make
sure things are right

Reviewed By: myleott, pietern

Differential Revision: D13351753

fbshipit-source-id: cf495960fda71ebd679c23212e19703c93a9dbdc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
438,Myle Ott,myleott@fb.com,2018-12-06 07:40:15-08:00,0693c351dff58716ae5ed0f7ec8704b8019cc499,https://github.com/pytorch/fairseq/commit/0693c351dff58716ae5ed0f7ec8704b8019cc499,"Add check that --encoder-layers matches --decoder-layers for LSTM (fixes #394)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/398

Differential Revision: D13358876

Pulled By: myleott

fbshipit-source-id: 57673f2643aac01492cb8f5728bb9f1a34ba6aa7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
439,Myle Ott,myleott@fb.com,2018-12-06 13:21:30-08:00,82a9f9230f69a215cf86053992f65b1658261016,https://github.com/pytorch/fairseq/commit/82a9f9230f69a215cf86053992f65b1658261016,"Fix arg formatting in preprocess.py and add fmt control for black formatting (#399)

Summary:
Not switching to Black formatting just yet, but adding fmt: off directives in case we decide to later.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/399

Differential Revision: D13364674

Pulled By: myleott

fbshipit-source-id: a20a11a18be3d583ee30eff770278fb4bd05b93c",21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
440,Myle Ott,myleott@fb.com,2018-12-06 15:07:40-08:00,ccd22212d80619f9f8cc194e78bc19f4828b24c0,https://github.com/pytorch/fairseq/commit/ccd22212d80619f9f8cc194e78bc19f4828b24c0,"Warn when using --update-freq on a single machine and --ddp-backend != no_c10d

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/400

Differential Revision: D13366996

Pulled By: myleott

fbshipit-source-id: b4907815e7cc1b4a2aceab11210bf64cb3d814c9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
441,Halil Akin,halilakin@fb.com,2018-12-06 16:12:34-08:00,6c006a34abbc0e8cb56445e57ebc0859739cad77,https://github.com/pytorch/fairseq/commit/6c006a34abbc0e8cb56445e57ebc0859739cad77,"Take a dummy train step under OOM to keep multiprocessing in sync

Summary: This is not a guaranteed solution (since processes may still get out of sync if OOM happens after an all_gather/all_reduce has been done) - but should still make multiprocessing training more robust in practice since it seems we usually OOM early enough.

Reviewed By: myleott

Differential Revision: D13086018

fbshipit-source-id: feb1b01c2eb8818797cfdabc0faac8056ba1b4ee",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
442,Myle Ott,myleott@fb.com,2018-12-07 15:10:19-08:00,03ef3ab8ce7575ff4501872353dec503dceb199d,https://github.com/pytorch/fairseq/commit/03ef3ab8ce7575ff4501872353dec503dceb199d,"Add --fp16-scale-tolerance (#397)

Summary:
Let's only decrease the loss scale if a large enough percentage of batches overflow.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/397

Differential Revision: D13355159

Pulled By: myleott

fbshipit-source-id: e17dde73d34a639519b4348c013fdd19d2b314e6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
443,Peng Li,pengli09@gmail.com,2018-12-08 07:23:23-08:00,00e47d7c11c307c4d9208d8b354a0fd40a04fd47,https://github.com/pytorch/fairseq/commit/00e47d7c11c307c4d9208d8b354a0fd40a04fd47,"fix data checking report bug (#403)

Summary:
The original code reports the size of a valid sample instead of an invalid one when raising an Exception , which will make people confused.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/403

Differential Revision: D13391431

Pulled By: myleott

fbshipit-source-id: 4642ed027c0f664424fc5a9baf4363791144feaf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
444,Suvrat Bhooshan,sbh@fb.com,2018-12-10 23:49:09-08:00,c37250ab1c845919af721cd3f5c4cec2993aefe1,https://github.com/pytorch/fairseq/commit/c37250ab1c845919af721cd3f5c4cec2993aefe1,"Loading PreTrained Models (#406)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/406

Static helper function in TranslationTask to load pretrained models

Reviewed By: myleott

Differential Revision: D13345276

fbshipit-source-id: 3a675ee1a144ceb8b010f30e1a6163ef670b53f3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
445,Haoran Li,aimeeli@fb.com,2018-12-18 14:01:07-08:00,9ca82a0ee650c17c1d008c8fbf2a2fcae2912b41,https://github.com/pytorch/fairseq/commit/9ca82a0ee650c17c1d008c8fbf2a2fcae2912b41,"data per gpu change

Summary: Avoid loading entire data set per gpu to reduce memory footprint

Reviewed By: rutyrinott

Differential Revision: D13163548

fbshipit-source-id: 4ba717c8021ba5723d02225bae5782e2c3a18640",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
446,Myle Ott,myleott@fb.com,2018-12-24 10:47:21-08:00,0f833526c5630bcd793796912b27dafae117c244,https://github.com/pytorch/fairseq/commit/0f833526c5630bcd793796912b27dafae117c244,"Add BufferedIterator (#419)

Summary:
This improves performance for datasets that load data lazily. Enabled by default since it shouldn't compromise performance for non-lazy datasets.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/419

Differential Revision: D13546585

Pulled By: myleott

fbshipit-source-id: f6152e2047291b0d68cd7506cd772b0caafe95be",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
447,Myle Ott,myleott@fb.com,2018-12-24 11:10:35-08:00,03a57decde62c76783ef7e2288bd61bc87f6e266,https://github.com/pytorch/fairseq/commit/03a57decde62c76783ef7e2288bd61bc87f6e266,"Improve memory efficiency of FP16 optimization (#404)

Summary:
Previously when training with --fp16, we stored a copy of the model parameters in FP32 for optimization, which consumed a lot of memory. An alternative is to just do the conversions to FP32 on the fly, which allows the caching allocator to reuse/save some memory.

This reduces peak memory usage by ~20% with a negligible reduction in training speed (~2% slower) when training a big transformer on 8 GPUs on wmt en-de with --update-freq=16.

This does not affect convergence, i.e., models will train exactly as they did before.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/404

Differential Revision: D13394376

Pulled By: myleott

fbshipit-source-id: 2b9f808548df4782110513c9cfc9f7c6159bcbbf",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
448,Emanuele Bugliarello,emanuele.bugliarello@gmail.com,2018-12-26 09:15:05-08:00,19c17b7448b11b8719e39b6cf42c3ffb91b0e525,https://github.com/pytorch/fairseq/commit/19c17b7448b11b8719e39b6cf42c3ffb91b0e525,"Add option to disable positional embeddings in TransformerModel (#421)

Summary:
Add argument `--no-token-positional-embeddings` to TransformerModel (currently only available in TransformerLanguageModel) to disable positional embeddings.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/421

Differential Revision: D13548450

Pulled By: myleott

fbshipit-source-id: b352c702ed1609e3b84d9a8404941d3274a7f883",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
449,Myle Ott,myleott@fb.com,2018-12-26 10:44:19-08:00,8ce6499dbf312fa4939bd955f012b2a76a13802e,https://github.com/pytorch/fairseq/commit/8ce6499dbf312fa4939bd955f012b2a76a13802e,"Merge internal changes (#422)

Summary:
- 04cc608: Add `--match-source-len` option to generate.py to for sequence-tagging tasks
- 19f1a40: Add `--no-repeat-ngram-size` option to generate.py for ngram blocking
Pull Request resolved: https://github.com/pytorch/fairseq/pull/422

Differential Revision: D13548445

Pulled By: myleott

fbshipit-source-id: 26d1ae83993e428fcb020dac5ae358b0e36233d9",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
450,Paul Michel,pmichel31415@gmail.com,2018-12-27 20:52:09-08:00,31a43973e43de5eb28e946aaae193db29fe3eea1,https://github.com/pytorch/fairseq/commit/31a43973e43de5eb28e946aaae193db29fe3eea1,"Fix backtranslation dataset on IndexedCachedDataset (#410)

Summary:
BacktranslationDataset would throw an error when the underlying dataset was an IndexedCachedDataset because prefetching was not handled correctly. This fixes the error.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/410

Differential Revision: D13557539

Pulled By: myleott

fbshipit-source-id: 398ab59a3ebdbf1c666d862b9f905654eece800c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
451,Myle Ott,myleott@fb.com,2018-12-27 21:20:02-08:00,58dd1862f610bfe8756bb03fb69d802c2107f119,https://github.com/pytorch/fairseq/commit/58dd1862f610bfe8756bb03fb69d802c2107f119,"Fix resuming from FP16 checkpoints (#424)

Summary:
This was broken in 03a57de.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/424

Differential Revision: D13557540

Pulled By: myleott

fbshipit-source-id: 62deda5353032aff20d35d046b0bb843da44d27c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
452,Myle Ott,myleott@fb.com,2018-12-28 07:54:49-08:00,0cb87130e7d843f68a25a44cb7443187a19b7320,https://github.com/pytorch/fairseq/commit/0cb87130e7d843f68a25a44cb7443187a19b7320,"Make multiprocessing_train.py work with multi-node setups

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/425

Differential Revision: D13558340

Pulled By: myleott

fbshipit-source-id: dff8c77027e821d8c80bfbd6a6ccce9ca1a44b78",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
453,Myle Ott,myleott@fb.com,2019-01-04 20:00:49-08:00,7633129ba8d5f0e28bd6b6d6027b14352482ef31,https://github.com/pytorch/fairseq/commit/7633129ba8d5f0e28bd6b6d6027b14352482ef31,"Merge internal changes (#283)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/283

Pull Request resolved: https://github.com/pytorch/fairseq/pull/428

Differential Revision: D13564190

Pulled By: myleott

fbshipit-source-id: 3b62282d7069c288f5bdd1dd2c120788cee4abb5",59,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
454,Myle Ott,myleott@fb.com,2019-01-05 12:45:15-05:00,df4d566d57f6da9ffc274b1e991a8717904a1dbf,https://github.com/pytorch/fairseq/commit/df4d566d57f6da9ffc274b1e991a8717904a1dbf,rm fb_train.py (#432),1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
455,Myle Ott,myleott@fb.com,2019-01-05 10:09:45-08:00,7d66726b97dee2d6e2098517f4a9b9d9b1a50894,https://github.com/pytorch/fairseq/commit/7d66726b97dee2d6e2098517f4a9b9d9b1a50894,Cleanup more files,2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
456,Myle Ott,myleott@fb.com,2019-01-07 10:40:39-08:00,14bd9c62a3084ebc1cb3b5feb1e6b83f90be6449,https://github.com/pytorch/fairseq/commit/14bd9c62a3084ebc1cb3b5feb1e6b83f90be6449,"Update docs for --lazy-load and torch.distributed.launch

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/433

Differential Revision: D13588032

Pulled By: myleott

fbshipit-source-id: 0e5ff361e27b206c4490264f0f51863367499e81",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
457,Art Matsak,amatsak@gmail.com,2019-01-09 07:32:44-08:00,73876ce3b6fa6a81680e5fcde6e29022416e24e0,https://github.com/pytorch/fairseq/commit/73876ce3b6fa6a81680e5fcde6e29022416e24e0,"Fix broken link in README.md (#436)

Summary:
https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset is not longer valid, redirects to a blog post listing page.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/436

Differential Revision: D13607961

Pulled By: myleott

fbshipit-source-id: 1a1074ffcbc454e29bc9d5aed84fdf2089a224bc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
458,Myle Ott,myleott@fb.com,2019-01-09 08:51:17-08:00,4b1f4788d8333193ae04458531fbfc103d1832dd,https://github.com/pytorch/fairseq/commit/4b1f4788d8333193ae04458531fbfc103d1832dd,"Misc fixes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/439

Differential Revision: D13608151

Pulled By: myleott

fbshipit-source-id: 198b84995a6329f8329829cc91184d88f1eab947",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
459,Wei Ho,weiho@fb.com,2019-01-09 16:01:17-08:00,315fa5cbd92938e2314a36c61319121e83af3351,https://github.com/pytorch/fairseq/commit/315fa5cbd92938e2314a36c61319121e83af3351,"Make error message for trying to train after make_generation_fast work correctly

Summary: https://github.com/pytorch/fairseq/blob/master/fairseq/trainer.py#L164 calls `train()` without any argument

Reviewed By: myleott

Differential Revision: D13599203

fbshipit-source-id: 3a096a6dd35a7a3f8309fbda3b54a36f606475e3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
460,Huihui Fan,angela.h.fan@gmail.com,2019-01-14 08:56:20-08:00,d9284ee7eac9862ed478e8667e3f851aaabd863e,https://github.com/pytorch/fairseq/commit/d9284ee7eac9862ed478e8667e3f851aaabd863e,"Fixes (#442)

Summary:
minor fixes:
1- adding fairseq logo
2- encoder padding for fconv self att
3- legacy ddp change
Pull Request resolved: https://github.com/pytorch/fairseq/pull/442

Differential Revision: D13651715

Pulled By: myleott

fbshipit-source-id: ac93c80f1dbffdfe03fbd4b8a8ea527aecb576a7",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
461,Davide Caroselli,davide.caroselli@translated.net,2019-01-14 08:57:53-08:00,b15f5f5384e3b684920339816281ec9f117c7426,https://github.com/pytorch/fairseq/commit/b15f5f5384e3b684920339816281ec9f117c7426,"New command line option '--user-dir' (#440)

Summary:
Following discussion on official fairseq (https://github.com/pytorch/fairseq/issues/438), I added the `--user-dir` option to the command line. The user can now specify a path in order to import a custom module with proprietary tasks, architectures and so on.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/440

Differential Revision: D13651721

Pulled By: myleott

fbshipit-source-id: 38b87454487f1ffa5eaf19c4bcefa0b3b15a8f43",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
462,Davide Caroselli,davide@modernmt.eu,2019-01-15 11:16:26-08:00,ebaf8c50305c879d30ed71aa0cf17fe23e98f235,https://github.com/pytorch/fairseq/commit/ebaf8c50305c879d30ed71aa0cf17fe23e98f235,"'--user-dir' documentation (correct) (#447)

Summary:
Command line option --user-dir documented in docs/overview.rst
Pull Request resolved: https://github.com/pytorch/fairseq/pull/447

Differential Revision: D13674744

Pulled By: myleott

fbshipit-source-id: 17049ee5c9f692f5298ef9fa7381ee583f269cde",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
463,Davide Caroselli,davide.caroselli@translated.net,2019-01-15 13:02:09-08:00,cefe3f8ad48a402a28e654d4392e214fe88d7760,https://github.com/pytorch/fairseq/commit/cefe3f8ad48a402a28e654d4392e214fe88d7760,"Fixed wrong help message shown on '--help' (#446)

Summary:
Correct help message was obfuscated by the transient `ArgumentParser` used only for eagerly read `--user-dir` flag.

To reproduce just try:
```bash
python3 train.py --help
```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/446

Differential Revision: D13674731

Pulled By: myleott

fbshipit-source-id: b9503a4d7ef26405be630d31c0ca02386d783031",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
464,Ruty Rinott,ruty@fb.com,2019-01-15 17:54:47-08:00,d1dc66d9718a29a4e00143c2b622dc177e00d3c0,https://github.com/pytorch/fairseq/commit/d1dc66d9718a29a4e00143c2b622dc177e00d3c0,"optimizations for token_block_dataset

Summary:
optimizing memory use of token_block_dataset by replacing python data structures with numpy arrays.
applying needed parts from D13498973, instead of rebasing it on changes

Reviewed By: edunov

Differential Revision: D13678485

fbshipit-source-id: c0c827a8b95834a6a5456476040ebdc8e42136d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
465,Myle Ott,myleott@fb.com,2019-01-16 09:41:02-08:00,bdec179bd28382f6b8ffa85fda2131d1bfc44c82,https://github.com/pytorch/fairseq/commit/bdec179bd28382f6b8ffa85fda2131d1bfc44c82,"Add --checkpoint-upper-bound to average_checkpoints.py (#452)

Summary:
This is useful for averaging the last N checkpoints, ending at some ""best"" checkpoint.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/452

Differential Revision: D13695407

Pulled By: myleott

fbshipit-source-id: 5d9d2bff3706834f01501e9259834c77fb335817",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
466,Davide Caroselli,davide.caroselli@translated.net,2019-01-16 12:05:22-08:00,7853818c2e33a63ec17a31bcfe20e4fc75d94130,https://github.com/pytorch/fairseq/commit/7853818c2e33a63ec17a31bcfe20e4fc75d94130,"FIX: '--user-dir' on multi-gpu (#449)

Summary:
On a multi-gpu training scenario, the `train.py` script spawns new processes with `torch.multiprocessing.spawn`. Unfortunately those child processes don't inherit the modules imported with `--user-dir`.

This pull request fixes this problem: custom module import in now explicit on every `main()` function.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/449

Differential Revision: D13676922

Pulled By: myleott

fbshipit-source-id: 520358d66155697885b878a37e7d0484bddbc1c6",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
467,Myle Ott,myleott@fb.com,2019-01-16 21:12:11-08:00,2210fa71564dbe11e21a58a3e83d7a161dabde50,https://github.com/pytorch/fairseq/commit/2210fa71564dbe11e21a58a3e83d7a161dabde50,"Fix initial learning rate (#453)

Summary:
There was a very subtle bug here 😢When we recently removed this line (7633129ba8d5f0e28bd6b6d6027b14352482ef31), it meant that the learning rate scheduler didn't get initialized until after the first update. Unfortunately pytorch optimizers store the learning rate in their internal state, so some learning rate schedulers use their `__init__` method to reset the learning rate to some sane initial value. This is especially problematic for LR schedulers that include a warmup, where the Optimizer is likely to contain the peak learning rate at initialization, and it's only in the LR scheduler's `__init__` that the (much smaller) warmup value is set.

For example, the inverse_sqrt scheduler resets the learning rate upon initialization:
https://github.com/pytorch/fairseq/blob/7853818c2e33a63ec17a31bcfe20e4fc75d94130/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py#L48-L50

**Impact:** For the last ~1.5 weeks, the first training update would use the optimizer's default learning rate instead of the initial rate set by the LR scheduler. All subsequent updates used the correct learning rates. This primarily affects LR schedulers with warmups.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/453

Differential Revision: D13704453

Pulled By: myleott

fbshipit-source-id: a946da30100f837c66bdc6b9b77b014ab4eb8764",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
468,Myle Ott,myleott@fb.com,2019-01-16 21:13:20-08:00,d259ffa98d90fb3100ffdab70c4db65c06b6b552,https://github.com/pytorch/fairseq/commit/d259ffa98d90fb3100ffdab70c4db65c06b6b552,"Fix stories generation

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/454

Differential Revision: D13708565

Pulled By: myleott

fbshipit-source-id: 5cd0e07e3e1885eef14e3a5e8074f24cf4bde632",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
469,frankang,namoamitabha@u.northwestern.edu,2019-01-24 08:24:31-08:00,37b9c235729f9b6146c585865122a978e564d183,https://github.com/pytorch/fairseq/commit/37b9c235729f9b6146c585865122a978e564d183,"Fix iteration bug in GroupedIterator. Correct sent size filter. (#455)

Summary:
Fix iterating from the beginning bug when initializing the GroupedIterator. (https://github.com/pytorch/fairseq/issues/441)
 Correct filter criterion for dict type sentence size. (https://github.com/pytorch/fairseq/issues/451)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/455

Differential Revision: D13725646

Pulled By: myleott

fbshipit-source-id: e698fa6f9b45460f95a75c9e9976a3aa3b6aa523",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
470,vufg,23201083+vufg@users.noreply.github.com,2019-01-24 08:55:39-08:00,8eb49c845fadeffb6d563cfe4c597d2b6c504943,https://github.com/pytorch/fairseq/commit/8eb49c845fadeffb6d563cfe4c597d2b6c504943,"change f""{args}"" to ""{}"".format(args) (#467)

Summary:
Although both are supported by Python 3.6, I think it would be better to unify the usage of string format function.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/467

Differential Revision: D13802506

Pulled By: myleott

fbshipit-source-id: 5c4877547b1c4ca806ab54c80ae483cfbaa7827a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
471,Myle Ott,myleott@fb.com,2019-01-24 09:22:03-08:00,ef3e6ab5aee9d33759486e37b889ac6cf01ca55e,https://github.com/pytorch/fairseq/commit/ef3e6ab5aee9d33759486e37b889ac6cf01ca55e,"Better error message for improperly formatted dictionaries

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/468

Differential Revision: D13802590

Pulled By: myleott

fbshipit-source-id: e374e38e74dc91bda0579ae41e26289fb0ba56a2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
472,Davide Caroselli,davide.caroselli@translated.net,2019-01-24 09:33:38-08:00,38f1dee950226982ffc69414bbf5a4bd529ec863,https://github.com/pytorch/fairseq/commit/38f1dee950226982ffc69414bbf5a4bd529ec863,"Enforce UTF-8 when open() text files (#460)

Summary:
When opening text files without specifying the encoding (i.e. `open(path, ""r"")` or `open(path, ""w"")`), python3 will use the preferred locale encoding (`locale.getpreferredencoding()`) so the result is platform dependent and can change from one machine to another.

I believe fairseq should enforce its standard (UTF-8 seems like the best choice to me). This pull request explicity specify UTF-8 encoding when reading text files.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/460

Differential Revision: D13802525

Pulled By: myleott

fbshipit-source-id: 672fd55707ee559ab36d74bc1c24026166ea2367",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
473,Myle Ott,myleott@fb.com,2019-01-24 09:36:31-08:00,d0ebcec438bac49640771527fea4b2d75cabd880,https://github.com/pytorch/fairseq/commit/d0ebcec438bac49640771527fea4b2d75cabd880,"Print model and number of trained params

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/469

Differential Revision: D13802945

Pulled By: myleott

fbshipit-source-id: b6976506a8336b96ee40505c4a7638541cc99c95",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
474,Myle Ott,myleott@fb.com,2019-01-24 11:24:19-08:00,9196c0b6544c916ef9f2178a9a45cc06d264db49,https://github.com/pytorch/fairseq/commit/9196c0b6544c916ef9f2178a9a45cc06d264db49,"LSTM improvements (fixes #414)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/470

Differential Revision: D13803964

Pulled By: myleott

fbshipit-source-id: 91b66599e9a539833fcedea07c608b349ba3b449",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
475,Myle Ott,myleott@fb.com,2019-01-25 08:39:24-08:00,7e0d222cdd93484bbb68c3736f866dcc50f3fd00,https://github.com/pytorch/fairseq/commit/7e0d222cdd93484bbb68c3736f866dcc50f3fd00,"Only use c10d distributed primitives

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/471

Differential Revision: D13818918

Pulled By: myleott

fbshipit-source-id: d3b8dc50e81ee1d2dcc5efc5815998be8461085f",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
476,Lucio Dery,derylucio@fb.com,2019-01-25 10:33:05-08:00,3e67386bbcfe083f4c8fc1e9d00ff466206e7c87,https://github.com/pytorch/fairseq/commit/3e67386bbcfe083f4c8fc1e9d00ff466206e7c87,"Adafactor Optimizer (#472)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/472

Implementation of ""Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"" (https://arxiv.org/abs/1804.04235)

Differential Revision: D13388049

fbshipit-source-id: 24ad30f4bac248e6aeaced5064bb83784058f03d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
477,Xian Li,xianl@fb.com,2019-01-25 13:37:17-08:00,bc8ae449321fb5587590b32eac13ec2f95ac47b7,https://github.com/pytorch/fairseq/commit/bc8ae449321fb5587590b32eac13ec2f95ac47b7,"refactor AdversarialTrainer factor out helper functions

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/474

Reviewed By: theweiho, akinh

Differential Revision: D13701447

fbshipit-source-id: 34036dce7601835b605e3b169210edc7a6715de6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
478,Myle Ott,myleott@fb.com,2019-01-25 15:35:01-08:00,b41c74dc5be15918d5fd21f199b66b78a601192c,https://github.com/pytorch/fairseq/commit/b41c74dc5be15918d5fd21f199b66b78a601192c,"Add code for ""Pay Less Attention with Lightweight and Dynamic Convolutions"" (#473)

Summary:
Changelog:
- `e330f56`: Add code for the ""Pay Less Attention with Lightweight and Dynamic Convolutions"" paper
- `5e3b98c`: Add scripts for computing tokenized BLEU with compound splitting and sacrebleu
- update READMEs
- misc fixes
Pull Request resolved: https://github.com/pytorch/fairseq/pull/473

Differential Revision: D13819717

Pulled By: myleott

fbshipit-source-id: f2dc12ea89a436b950cafec3593ed1b04af808e9",21,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
479,Jingfei Du,jingfeidu@fb.com,2019-01-29 12:41:54-08:00,66ce2175f860e1e065dcd13755f9dd915773fa96,https://github.com/pytorch/fairseq/commit/66ce2175f860e1e065dcd13755f9dd915773fa96,"make dictionary class as input for fairseq preprocess functions (#482)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/482

With this change, we can use different dictionary classes when calling build_dictionary and build_and_save_dictionary

Reviewed By: liaimi

Differential Revision: D13855100

fbshipit-source-id: 62e6db310b5f078e05c547d2671252233be7b7f0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
480,Myle Ott,myleott@fb.com,2019-01-30 08:58:37-08:00,42be3ebd41100f9e37a0e0d30c73fe3d4ab6b105,https://github.com/pytorch/fairseq/commit/42be3ebd41100f9e37a0e0d30c73fe3d4ab6b105,"Merge internal changes (#483)

Summary:
Changelog:
- `4889802`: can now remove detokenize sentencepiece output with `--remove-bpe=sentencepiece` (fixes #331). Also added `--sacrebleu` for computing detokenized BLEU.
- `0d76427`: fix assertion error when training language model with dataset containing empty sentences
- minor bug and style fixes
Pull Request resolved: https://github.com/pytorch/fairseq/pull/483

Differential Revision: D13867899

Pulled By: myleott

fbshipit-source-id: 25c940b847fe270262ac8f5ac838407b3977fdda",12,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,15,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestTokenBlockDataset(unittest.TestCase):'],"[('Equal', '(ds[0].tolist(), [5, 4, 3, 2, 1])'), ('Equal', '(ds[1].tolist(), [1])'), ('Equal', '(ds[2].tolist(), [8, 7, 6, 1])'), ('Equal', '(ds[0].tolist(), [5, 4, 3, 2, 1])'), ('Equal', '(ds[1].tolist(), [8, 7, 6, 1])'), ('Equal', '(ds[2].tolist(), [1])'), ('Equal', '(ds[0].tolist(), [5, 4, 3])'), ('Equal', '(ds[1].tolist(), [2, 1, 8])'), ('Equal', '(ds[2].tolist(), [7, 6, 1])'), ('Equal', '(ds[3].tolist(), [9, 1])'), ('Equal', '(ds[0].tolist(), [5, 4, 3, 2, 1])'), ('Equal', '(ds[1].tolist(), [8, 7, 6, 1, 9, 1])'), ('Equal', '(ds[0].tolist(), [4, 3, 2, 1])'), ('Equal', '(ds[1].tolist(), [5, 1, 1])'), ('Equal', '(ds[2].tolist(), [6, 1])')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
481,Myle Ott,myleott@fb.com,2019-01-30 09:43:15-08:00,3dce7c9fc0c3f14e5dded19056778c35e67684e5,https://github.com/pytorch/fairseq/commit/3dce7c9fc0c3f14e5dded19056778c35e67684e5,"Add --input option to interactive.py to support reading from file

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/484

Differential Revision: D13880636

Pulled By: myleott

fbshipit-source-id: 984b2e1c3b281c28243102eb971ea45ec891d94e",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
482,Myle Ott,myleott@fb.com,2019-01-30 13:24:55-08:00,ec6f8ef99a8c6942133e01a610def197e1d6d9dd,https://github.com/pytorch/fairseq/commit/ec6f8ef99a8c6942133e01a610def197e1d6d9dd,"Do distributed init after data loading

Summary:
FACEBOOK

This switches back to torch.multiprocessing.spawn, instead of directly calling fb_train.par using a subprocess.Process. This has the advantage that exceptions are propagated properly. It also moves the distributed_init part to happen after data loading, which gets around the timeout issue.

The downside of this approach is that it's not so easy to pipe stdout to multiple places, which was nice when using the sweep.py scripts. I'm still working on a fix for that.

Reviewed By: rutyrinott, ngoyal2707

Differential Revision: D13873224

fbshipit-source-id: 08d593233b8d23590c01c723363630a79804a8b0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
483,Davide Caroselli,davide.caroselli@translated.net,2019-02-01 09:43:06-08:00,bbb4120b00e9ac7b12a52608349b7ad753fc0d19,https://github.com/pytorch/fairseq/commit/bbb4120b00e9ac7b12a52608349b7ad753fc0d19,"Support custom Dictionary implementations in 'preprocess.py' (#448)

Summary:
The `preprocess.py` script has been refactored in order to:

1. Use the `options` module for command line arguments  parsing. This will give to `preprocess.py` the ability to load custom modules with `--user-dir` flag (already implemented to all other binaries)
2. Dictionary loading and building code has moved to Task implementation. This allows custom Dictionary classes to be used during the data generation step.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/448

Differential Revision: D13674819

Pulled By: myleott

fbshipit-source-id: b40648a98ed6c08284577e5ec25876e018d8c822",6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
484,Myle Ott,myleott@fb.com,2019-02-05 07:46:44-08:00,829bd8ce5fc336991c828f178ac5f231250ef77d,https://github.com/pytorch/fairseq/commit/829bd8ce5fc336991c828f178ac5f231250ef77d,"Add standalone binaries

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/489

Differential Revision: D13956810

Pulled By: myleott

fbshipit-source-id: 61ace179d1d3790226c38b3f3e47f5452b5ec514",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
485,Wei Ho,weiho@fb.com,2019-02-06 12:24:30-08:00,c49c292c79f3460c22483cd0a964052264721d4a,https://github.com/pytorch/fairseq/commit/c49c292c79f3460c22483cd0a964052264721d4a,"Add CheckpointManager to keep avg checkpoint weights in memory to reduce disk read when averaging + various checkpoint refactoring

Summary: Pull Request resolved: https://github.com/pytorch/translate/pull/315

Reviewed By: akinh

Differential Revision: D13510446

fbshipit-source-id: 22a6594af9253130a93e638285a47183a974e0de",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
486,Ruty Rinott,ruty@fb.com,2019-02-06 23:04:26-08:00,cea0e4b9ead83adec919bb21c62360491317d351,https://github.com/pytorch/fairseq/commit/cea0e4b9ead83adec919bb21c62360491317d351,"stitch preprocessing pipeline

Summary:
1. add call to binarization to complete preprocessing pipeline
2. add ability to specify task to select the dictionary, and add a bert task
3. Get rid of function calls that are no longer needed after moving functions from fairseq here

Reviewed By: jingfeidu

Differential Revision: D13977842

fbshipit-source-id: ec9bbb4e98e62e12c20ba68bb52b8bcc94aee91d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
487,Myle Ott,myleott@fb.com,2019-02-08 22:00:46-08:00,fbd4cef9a575b5f77ca05d4b7c3ad3adb11141ac,https://github.com/pytorch/fairseq/commit/fbd4cef9a575b5f77ca05d4b7c3ad3adb11141ac,"Add fairseq to PyPI (#495)

Summary:
- fairseq can now be installed via pip: `pip install fairseq`
- command-line tools are globally accessible: `fairseq-preprocess`, `fairseq-train`, `fairseq-generate`, etc.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/495

Differential Revision: D14017761

Pulled By: myleott

fbshipit-source-id: 10c9f6634a3056074eac2f33324b4f1f404d4235",30,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
488,Juan Miguel Pino,juancarabina@fb.com,2019-02-12 11:23:13-08:00,184629a721adf84a4fbd1fd5db8306c49da7685d,https://github.com/pytorch/fairseq/commit/184629a721adf84a4fbd1fd5db8306c49da7685d,"Add onnx_trace argument for learned embeddings (#492)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/492

This argument was missing so we cannot export Transformer if we use learned positional embeddings. See also https://github.com/pytorch/translate/pull/335

Reviewed By: jhcross

Differential Revision: D13984781

fbshipit-source-id: 2187377e952ff587e07237de312c5b68f7d68891",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
489,Myle Ott,myleott@fb.com,2019-02-15 16:34:51-08:00,9998bbfab7267d0bc82c8eb10fb2d9310139c689,https://github.com/pytorch/fairseq/commit/9998bbfab7267d0bc82c8eb10fb2d9310139c689,"Merge internal changes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/505

Differential Revision: D14110201

Pulled By: myleott

fbshipit-source-id: 099ce61fa386c016f3a1d1815c6fe1a9a6c9005d",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
490,Ruty Rinott,ruty@fb.com,2019-02-19 10:59:53-08:00,08e866f977b8bc98b5ec6939f5394a9cd2a277c0,https://github.com/pytorch/fairseq/commit/08e866f977b8bc98b5ec6939f5394a9cd2a277c0,"moving masking logic to collate

Summary: Move masking logic to data_utils

Reviewed By: kartikayk, jingfeidu

Differential Revision: D14098403

fbshipit-source-id: c7b7e811ab48b9c5a12662dc1e2f2ed694724176",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
491,Myle Ott,myleott@fb.com,2019-02-22 10:06:22-08:00,b65c579bed003ec5111dc31aeaaac3bb36784a5a,https://github.com/pytorch/fairseq/commit/b65c579bed003ec5111dc31aeaaac3bb36784a5a,"Modularize generate.py (#351)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/351

This makes it easier for tasks to plugin to generate.py/interactive.py
Pull Request resolved: https://github.com/pytorch/fairseq/pull/520

Differential Revision: D14183881

Pulled By: myleott

fbshipit-source-id: ede5e53ddc1215ed3b12b8f1eba048c946913c33",11,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('HypoTokens', '(hypos_id[0], data[id][])'), ('HypoScore', '(hypos_id[0], expected_scores[id])')]",[],[],[],[],[],[],[],[],[],[],"[('HypoTokens', '(hypos[0], data[id][])'), ('HypoScore', '(hypos[0], expected_scores[id])')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
492,Myle Ott,myleott@fb.com,2019-02-22 13:11:22-08:00,4294c4f6d72e6ac2b0315133adb26e83e99f0e62,https://github.com/pytorch/fairseq/commit/4294c4f6d72e6ac2b0315133adb26e83e99f0e62,"Add code for mixture of experts (#521)

Summary:
Code for the paper: [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](https://arxiv.org/abs/1902.07816).
Pull Request resolved: https://github.com/pytorch/fairseq/pull/521

Differential Revision: D14188021

Pulled By: myleott

fbshipit-source-id: ed5b1ed5ad9a582359bd5215fa2ea26dc76c673e",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
493,Myle Ott,myleott@fb.com,2019-02-22 16:17:14-08:00,392bdd6ce0deb107d7b30ac14bdd7b4ac27aca01,https://github.com/pytorch/fairseq/commit/392bdd6ce0deb107d7b30ac14bdd7b4ac27aca01,"Update README for Mixture of Experts paper

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/522

Differential Revision: D14194672

Pulled By: myleott

fbshipit-source-id: 4ff669826c4313de6f12076915cfb1bd15289ef0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
494,Myle Ott,myleott@fb.com,2019-02-23 19:59:07-08:00,94fedf0026ee0bdc652bc7c5706568ae27b78375,https://github.com/pytorch/fairseq/commit/94fedf0026ee0bdc652bc7c5706568ae27b78375,"Add scoring script for Mixture of Experts

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/523

Differential Revision: D14200060

Pulled By: myleott

fbshipit-source-id: a2e3d6ec7c6b9cacc9f44565d2b91e65b580b084",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
495,Myle Ott,myleott@fb.com,2019-02-25 18:33:12-08:00,65c1903e4e17840884a6ac787cbd250f71ea8715,https://github.com/pytorch/fairseq/commit/65c1903e4e17840884a6ac787cbd250f71ea8715,"Misc fixes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/529

Differential Revision: D14218384

Pulled By: myleott

fbshipit-source-id: 5d2cbb1f56ea42e9929785aff4a5ae5f44d13724",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
496,Myle Ott,myleott@fb.com,2019-02-25 18:34:10-08:00,44d27e645b5260098ff59454f4ae484defe01db0,https://github.com/pytorch/fairseq/commit/44d27e645b5260098ff59454f4ae484defe01db0,"Add Tensorboard support (#530)

Summary:
Enable with the `--tensorboard-logdir` option.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/530

Differential Revision: D14218430

Pulled By: myleott

fbshipit-source-id: e7a54f66f928e3bb02ae03fda09b22fa4fa7d053",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
497,Myle Ott,myleott@fb.com,2019-02-25 18:40:37-08:00,00493490badb8dbf11a2e3e14ffd9a05db7f7d08,https://github.com/pytorch/fairseq/commit/00493490badb8dbf11a2e3e14ffd9a05db7f7d08,"Multilingual training example (#527)

Summary:
* Add example for multilingual translation on IWSLT'17
* Match dataset ordering for multilingual_translation and translation
* Fix bug with LegacyDistributedDataParallel when calling forward of sub-modules
Pull Request resolved: https://github.com/pytorch/fairseq/pull/527

Differential Revision: D14218372

Pulled By: myleott

fbshipit-source-id: 2e3fe24aa39476bcc5c9af68ef9a40192db34a3b",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
498,Myle Ott,myleott@fb.com,2019-02-25 19:03:01-08:00,98daf039b666ed0e3f49a76df6d99a1d3c2e2655,https://github.com/pytorch/fairseq/commit/98daf039b666ed0e3f49a76df6d99a1d3c2e2655,"Support LM generation from interactive.py (fixes #526)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/528

Differential Revision: D14218377

Pulled By: myleott

fbshipit-source-id: facb0a32f6aebf56a4fea7259080394ad2d2d846",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
499,Jo Chuang,josephch405@gmail.com,2019-02-28 07:13:22-08:00,19b6e8bf1eef5e27f96bc47fd9b26c6ac09c6642,https://github.com/pytorch/fairseq/commit/19b6e8bf1eef5e27f96bc47fd9b26c6ac09c6642,"Extract after skipping download for LM example script

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/538

Differential Revision: D14258736

Pulled By: myleott

fbshipit-source-id: ca16355e4c4700fc8eecf2c9374ec170bca826a4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
500,Myle Ott,myleott@fb.com,2019-02-28 07:51:55-08:00,139e3a3c4089f31f1f43f7664249d5ec9d63d370,https://github.com/pytorch/fairseq/commit/139e3a3c4089f31f1f43f7664249d5ec9d63d370,"Add sacrebleu to requirements

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/542

Differential Revision: D14258895

Pulled By: myleott

fbshipit-source-id: 950a840e1d001a472be8d4737c9e4de5224137b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
501,Myle Ott,myleott@fb.com,2019-02-28 08:53:05-08:00,bc919276a1540c121b7c9af4982507584772ba3f,https://github.com/pytorch/fairseq/commit/bc919276a1540c121b7c9af4982507584772ba3f,"Add test for mixture of experts

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/543

Differential Revision: D14259481

Pulled By: myleott

fbshipit-source-id: fcb0a150b8e851cf86ea5ed1f083f56e1600588e",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
502,Vladimir Karpukhin,vladk@fb.com,2019-02-28 09:15:35-08:00,f296824f4013dc28b471c47d7779547460afc7f0,https://github.com/pytorch/fairseq/commit/f296824f4013dc28b471c47d7779547460afc7f0,"Move string line encoding logic from tokenizer to Dictionary (unified diff). (#541)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/541

Just a combo of a stacked pair D14057943 & D14176011,
Made this as a separete diff cause there seems to be some issue with porting a stacked change into github repo

Differential Revision: D14251048

fbshipit-source-id: 0a47f534a69d6ab2ebe035fba40fd51748cccfb8",13,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
503,Myle Ott,myleott@fb.com,2019-02-28 15:20:37-08:00,8a8df81dee1e7c1e213d92e6255357175c00ecc2,https://github.com/pytorch/fairseq/commit/8a8df81dee1e7c1e213d92e6255357175c00ecc2,"Deprecate _aggregate_logging_outputs

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/498

Differential Revision: D14024524

Pulled By: myleott

fbshipit-source-id: 1b0be4bb212dbab41ea0959ac34020832ff00645",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
504,JingboWang1997,wjbjimmy@gmail.com,2019-02-28 17:44:04-08:00,4d59517fdc0003fb62b8283e7f9d6131bff87723,https://github.com/pytorch/fairseq/commit/4d59517fdc0003fb62b8283e7f9d6131bff87723,"ignore data files in .gitignore

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/546

Differential Revision: D14272808

Pulled By: myleott

fbshipit-source-id: e993450354e7d7561b14b56c12d4859a8ee7121b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
505,Kartikay Khandelwal,kartikayk@fb.com,2019-02-28 19:15:39-08:00,92a6c5481ad46f62f764ab0eadd791ce7b5512d6,https://github.com/pytorch/fairseq/commit/92a6c5481ad46f62f764ab0eadd791ce7b5512d6,"Refactor BERTDataset to the more general MaskedLMDataset

Summary: The current BERTDataset has a lot of components needed for generic MaskedLM training but is too restrictive in terms of the assumptions it makes - two blocks being masked, the special tokens used for the sentence embedding as well as the separator etc. In this diff I refactor this dataset and at the same time add make some of the parameters including the probabilities associated with masking configurable.

Reviewed By: rutyrinott

Differential Revision: D14222467

fbshipit-source-id: e9f78788dfe7f56646ba09c62967c4c0bd30aed8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
506,Myle Ott,myleott@fb.com,2019-03-01 13:09:31-08:00,66262a387975530cf422198f41408b65409a5043,https://github.com/pytorch/fairseq/commit/66262a387975530cf422198f41408b65409a5043,"Use --workers for validation sets in preprocess.py

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/550

Differential Revision: D14286008

Pulled By: myleott

fbshipit-source-id: 6055acf98023fdd01f85ac3d7c4e7fb786e54389",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
507,James King,lfthwjx@gmail.com,2019-03-01 13:49:45-08:00,88bf8b56d055288bbd4cff9b998c9e2d7f8c68c7,https://github.com/pytorch/fairseq/commit/88bf8b56d055288bbd4cff9b998c9e2d7f8c68c7,"Fixed the issue that no space in string converted from tensor

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/548

Differential Revision: D14286021

Pulled By: myleott

fbshipit-source-id: 7c725304185e63787220371a812ec860e178872c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
508,Myle Ott,myleott@fb.com,2019-03-02 07:18:45-08:00,1fd0a6f6b436debbf482de6bbc5ebe65f2d42d66,https://github.com/pytorch/fairseq/commit/1fd0a6f6b436debbf482de6bbc5ebe65f2d42d66,"Fix Pdb

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/551

Differential Revision: D14295227

Pulled By: myleott

fbshipit-source-id: 404f2a2697a62ce0dbf22e5ab2e1cf932acc83ac",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
509,Myle Ott,myleott@fb.com,2019-03-04 07:52:49-08:00,2ad1178e6526a56d58b4e999c9ff7fdf08cc9267,https://github.com/pytorch/fairseq/commit/2ad1178e6526a56d58b4e999c9ff7fdf08cc9267,"Add --curriculum (fixes #533)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/554

Differential Revision: D14300596

Pulled By: myleott

fbshipit-source-id: f38c8e58daef99d5e4b97dd423e4142e4294a4f0",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
510,Louis MARTIN,louisrtm@gmail.com,2019-03-04 14:31:46-08:00,5869385c4badbacdca6d6fd110f9f2af137cebf4,https://github.com/pytorch/fairseq/commit/5869385c4badbacdca6d6fd110f9f2af137cebf4,"Try to access sys.stdin.fileno() only at runtime and not during import (#553)

Summary:
Accessing sys.stdin.fileno() raises an error in multiple contexts
(pytest, joblib, jupyter...).
Thus accessing it at the top level of the file can cause other scripts
to crash when they import fairseq.
This is why it is moved inside the method of MultiprocessingPdb to only
be accessed at runtime if needed.

See  Issue #517
Pull Request resolved: https://github.com/pytorch/fairseq/pull/553

Differential Revision: D14309284

Pulled By: myleott

fbshipit-source-id: 6ca36f2053a86ebc02e2d6f025459c6a78c592e7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
511,Jose Fonollosa,jarfo@yahoo.com,2019-03-11 07:27:26-07:00,fef4e002783816631d84791e7d454e60c9d8bb8a,https://github.com/pytorch/fairseq/commit/fef4e002783816631d84791e7d454e60c9d8bb8a,"Add missing parentheses in regex expression (#567)

Summary:
The regex pattern without parentheses is not correct. The checkpoints are not sorted in descending order
Pull Request resolved: https://github.com/pytorch/fairseq/pull/567

Differential Revision: D14404380

Pulled By: myleott

fbshipit-source-id: 98cd0cfa8c92b78a03ffbb94840bc0f7a118eca1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
512,Matt Le,mattle@fb.com,2019-03-11 14:15:10-07:00,7fc9a3be80e8417bc177df9f8717aec4ae53aacb,https://github.com/pytorch/fairseq/commit/7fc9a3be80e8417bc177df9f8717aec4ae53aacb,"Create fairseq_cli_lib

Summary: This allows one to call fairseq_cli functions from within python without dispatching to bash.

Reviewed By: myleott

Differential Revision: D14404719

fbshipit-source-id: 044eb652045bb15fc40e72ecbaf6fb10df9f8c61",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
513,Dmytro Okhonko,oxo@fb.com,2019-03-12 15:08:47-07:00,9e1c880fbe539e65b928ea8dde0d4059b3bf7edc,https://github.com/pytorch/fairseq/commit/9e1c880fbe539e65b928ea8dde0d4059b3bf7edc,"FairseqEncoderModel

Summary: Base class for encoder-only models. Some models doesn't have decoder part.

Reviewed By: myleott

Differential Revision: D14413406

fbshipit-source-id: f36473b91dcf3c835fd6d50e2eb6002afa75f11a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
514,Dmytro Okhonko,oxo@fb.com,2019-03-12 15:08:47-07:00,d17fa8513547cbbaee206f56c9c8e80f9bdb9196,https://github.com/pytorch/fairseq/commit/d17fa8513547cbbaee206f56c9c8e80f9bdb9196,"Adadelta optimizer

Summary: Adding Adadelta optimizer to fairseq as wrapper around torch.optim.Adadelta

Reviewed By: myleott

Differential Revision: D14418635

fbshipit-source-id: 6bf5ec008e905a4a2cbf7415e9492f5eea3ff07f",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestCommonOptions(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
515,Dmytro Okhonko,oxo@fb.com,2019-03-12 15:08:47-07:00,860010e9074518dd9434ce7e03661f8a10e45ad7,https://github.com/pytorch/fairseq/commit/860010e9074518dd9434ce7e03661f8a10e45ad7,"Handle 3+ dimensional input in sequence_generator + nits

Summary: sequence_generator assumes that model input is 2d tensor of longs. But it can be something like 3d tensor of floats and we should be able to handle this as long as first dimension is batch size followed by source lengths.

Reviewed By: myleott

Differential Revision: D14420044

fbshipit-source-id: bf8b1e42ad1873f7b803c1a377b0af21648db015",6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
516,Qing Sun,qingsun@fb.com,2019-03-12 22:04:13-07:00,4d3401b09f155995cd81fd394dfa50bf65ee8e5f,https://github.com/pytorch/fairseq/commit/4d3401b09f155995cd81fd394dfa50bf65ee8e5f,"Enable sampling (#571)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/571

Enable sampling from Fairseq

Reviewed By: akinh

Differential Revision: D13981666

fbshipit-source-id: 2af1bd67701a73a2c76a9255bd8381d6a7518876",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
517,Wen-Ding Li,kvlxu3@gmail.com,2019-03-14 09:50:40-07:00,a24880bd10f3c3101b3bc993947ef92a83f1ad4f,https://github.com/pytorch/fairseq/commit/a24880bd10f3c3101b3bc993947ef92a83f1ad4f,"Minor fix for multilingual example shell command (#561)

Summary:
Add `\` to fix for the shell command.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/561

Differential Revision: D14460091

Pulled By: myleott

fbshipit-source-id: 3658ca41e69bcd00d4ad8ec2d79ddcc6a8de586e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
518,Myle Ott,myleott@fb.com,2019-03-14 11:39:03-07:00,48d9afbeb37584586411c3d8e53d91228528dec4,https://github.com/pytorch/fairseq/commit/48d9afbeb37584586411c3d8e53d91228528dec4,"Speed improvements (#531)

Summary:
* Add FusedLayerNorm and FusedAdam
* Softmax and zero grad optimizations
Pull Request resolved: https://github.com/pytorch/fairseq/pull/531

Differential Revision: D14218457

Pulled By: myleott

fbshipit-source-id: 5656b2d0152cd85f77dc21ec0e1439ec04b9fa89",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
519,Myle Ott,myleott@fb.com,2019-03-15 10:22:10-07:00,e6422528dae0b899848469efe2dc404c1e639ce9,https://github.com/pytorch/fairseq/commit/e6422528dae0b899848469efe2dc404c1e639ce9,"0.6.1 -> 0.6.2 (#577)

Summary:
Changelog:
- 998ba4f: Add language models from Baevski & Auli (2018)
- 4294c4f: Add mixture of experts code from Shen et al. (2019)
- 0049349: Add example for multilingual training
- 48d9afb: Speed improvements, including fused operators from apex
- 44d27e6: Add Tensorboard support
- d17fa85: Add Adadelta optimizer
- 9e1c880: Add `FairseqEncoderModel`
- b65c579: Add `FairseqTask.inference_step` to modularize generate.py
- 2ad1178: Add back `--curriculum`
- Misc bug fixes and other features

Pull Request resolved: https://github.com/pytorch/fairseq/pull/577

Differential Revision: D14481233

Pulled By: myleott

fbshipit-source-id: 4ff8625ef1c0b24273fc65df7c5658e3c932e8b7",20,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
520,Myle Ott,myleott@fb.com,2019-03-15 21:27:03-07:00,66f033e6a2b1742a8ab907ff7ca2930858736922,https://github.com/pytorch/fairseq/commit/66f033e6a2b1742a8ab907ff7ca2930858736922,"Update setup.py

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/580

Differential Revision: D14494390

Pulled By: myleott

fbshipit-source-id: 524cc16a106f2af630357e2ebdf7dde35fa7d494",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
521,Myle Ott,myleott@fb.com,2019-03-19 06:19:02-07:00,f305086031fc31a838d5b6c06eec35fd5cd16038,https://github.com/pytorch/fairseq/commit/f305086031fc31a838d5b6c06eec35fd5cd16038,"Update scoring script for MoE paper

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/586

Differential Revision: D14517550

Pulled By: myleott

fbshipit-source-id: fab68a8f597a98cf28d812d89eff845c5776b65b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
522,Myle Ott,myleott@fb.com,2019-03-19 06:20:13-07:00,b26d6b58b0bd55c1b59aa89f1b03d79011dbe7aa,https://github.com/pytorch/fairseq/commit/b26d6b58b0bd55c1b59aa89f1b03d79011dbe7aa,"Remove unused import

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/587

Differential Revision: D14517597

Pulled By: myleott

fbshipit-source-id: 4831ea5a9da1c2e207529a4ab3c4d0b070f5f34e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
523,Haoran Li,aimeeli@fb.com,2019-03-26 12:45:12-07:00,8e66a12f62cdabf9e81ddb02be1fadf554672389,https://github.com/pytorch/fairseq/commit/8e66a12f62cdabf9e81ddb02be1fadf554672389,"fixes for exporter issue of bi-transformer model (#597)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/597

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/424

Fixes two issues:
1. the new Layernorm has issues in exporting
2. fix tensorboard writing by using the ""RAW"" operator_export_type

Differential Revision: D14610694

fbshipit-source-id: 1b859f54c571a90766128ab28539a9901375c3e6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
524,Felix Wu,felixgwu@gmail.com,2019-03-28 22:16:51-07:00,34c9ebf07327875013b5fa309e548493e65ea175,https://github.com/pytorch/fairseq/commit/34c9ebf07327875013b5fa309e548493e65ea175,"Fixing a bug of DynamicConv in the unfolding mode (#593)

Summary:
The unfold1d.py has the same name as the function `unfold1d` function, which will cause an error when using DynamicConv1dTBC with `unfold=True`.
This doesn't affect the NMT models which don't use the unfolding mode though.

I rename `unfold1d.py` as `unfold.py` to fix this bug.

Originally we would get `TypeError` when running this code:
```
import torch
from fairseq.modules import LightweightConv1dTBC, DynamicConv1dTBC

x = torch.rand(4, 10, 8)
m = LightweightConv1dTBC(8, 4, 3)
o = m(x, unfold=True)

m = DynamicConv1dTBC(8, 4, 3)
o = m(x, unfold=True)
```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/593

Differential Revision: D14597117

Pulled By: myleott

fbshipit-source-id: 59752fd7ff62c53a4aba8b56b83155291e5f5792",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
525,Facebook Community Bot,facebook-github-bot@users.noreply.github.com,2019-03-28 22:30:42-07:00,c7e9e4582c91382b182fc9091d4993eed6ae05d7,https://github.com/pytorch/fairseq/commit/c7e9e4582c91382b182fc9091d4993eed6ae05d7,"Adding Code of Conduct file (#603)

Summary:
This is pull request was created automatically because we noticed your project was missing a Code of Conduct file.

Code of Conduct files facilitate respectful and constructive communities by establishing expected behaviors for project contributors.

This PR was crafted with love by Facebook's Open Source Team.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/603

Differential Revision: D14680981

Pulled By: myleott

fbshipit-source-id: 653262641554735d89f96c392c72fb311e53a451",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
526,Stefan Schweter,stefan@schweter.it,2019-03-28 22:39:04-07:00,8ab27e6e1e264ae5efa900dd4343d1f02f40bdcc,https://github.com/pytorch/fairseq/commit/8ab27e6e1e264ae5efa900dd4343d1f02f40bdcc,"Documentation: fix link to language model readme (#600)

Summary:
Hi,

currently, the link to the language model readme is broken on the `examples/language_model/transformer_lm` page.

This PR fixes the link :)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/600

Differential Revision: D14680985

Pulled By: myleott

fbshipit-source-id: 62291efbf4ece2af54fae45c408c2759863f9847",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
527,Myle Ott,myleott@fb.com,2019-03-28 23:00:19-07:00,a78ad1ac7017628a083bb7f4f3d16c6ccbda4d33,https://github.com/pytorch/fairseq/commit/a78ad1ac7017628a083bb7f4f3d16c6ccbda4d33,"Add utils.deprecation_warning

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/607

Differential Revision: D14681031

Pulled By: myleott

fbshipit-source-id: 466ee526a30543218e2b7138fb651db866ae5ab3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
528,Myle Ott,myleott@fb.com,2019-03-28 23:13:18-07:00,2340832fdd7acaaaf07626daa6a0cef6fda06cd1,https://github.com/pytorch/fairseq/commit/2340832fdd7acaaaf07626daa6a0cef6fda06cd1,"Output original IDs in interactive.py (in case some rows are filtered; fixes #591)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/606

Differential Revision: D14680968

Pulled By: myleott

fbshipit-source-id: 8044d828a8167199c10f2aee24f7e611feb91802",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
529,Yash Kumar Atri,yashkumaratri@gmail.com,2019-04-01 20:27:50-07:00,3efc39ee0a80049752aaaf64ce3868bc5244eeb6,https://github.com/pytorch/fairseq/commit/3efc39ee0a80049752aaaf64ce3868bc5244eeb6,"Update data_utils.py for (#598)

Summary:
Correcting the syntax error in assert function cause of a character before error message.

Assertion and the code is working fine now, Tested with wmt-ende task.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/598

Differential Revision: D14712846

Pulled By: myleott

fbshipit-source-id: 3f708aa2362ceecba19174750f9ffc9238537512",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
530,Myle Ott,myleott@fb.com,2019-04-02 06:46:28-07:00,e88ad84b26118caa4070c10cf3f93f191486a7b9,https://github.com/pytorch/fairseq/commit/e88ad84b26118caa4070c10cf3f93f191486a7b9,"Use --train-subset and --valid-subset properly

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/614

Differential Revision: D14712321

Pulled By: myleott

fbshipit-source-id: 8ef973c5d30ebccf0df0f1cabdddd590248a8f8d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
531,Myle Ott,myleott@fb.com,2019-04-02 08:33:20-07:00,eef6663cdbe837d0cc713b13d0909501c2b8621a,https://github.com/pytorch/fairseq/commit/eef6663cdbe837d0cc713b13d0909501c2b8621a,"Add checkpoint write timer

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/613

Differential Revision: D14712311

Pulled By: myleott

fbshipit-source-id: 3e7646629b539c10b6af89dece2c0c564f31125f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
532,Paco Guzman,fguzman@fb.com,2019-04-03 01:13:48-07:00,10ad74953f671168c474d0da5700d91661e2b5a2,https://github.com/pytorch/fairseq/commit/10ad74953f671168c474d0da5700d91661e2b5a2,"sort dictionary items lexicographically for consistency

Summary: Sorts dictionaries lexicographically before creating counter. This makes distributed preprocessing deterministic

Reviewed By: myleott

Differential Revision: D14678214

fbshipit-source-id: 7a9e2f0cb367e8fb76da01e108dda4c6c5aab505",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
533,James Cross,jcross@fb.com,2019-04-03 14:41:56-07:00,3a64acedf06c366582437db27e39979bb0e90e2b,https://github.com/pytorch/fairseq/commit/3a64acedf06c366582437db27e39979bb0e90e2b,"work around lack of optional output for forks (#429)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/429

Pull Request resolved: https://github.com/pytorch/fairseq/pull/618

PyTorch export for transformer models was broken because as written, they used a placeholder `None` value during inference for the variable `key_padding_mask` to indicate no padding, but PyTorch is unable trace such values. This diff adds a minor hack to allow the use of an empty tensor for the same purpose.

Reviewed By: jmp84

Differential Revision: D14581730

fbshipit-source-id: 2ea4664c20ecab8478c578b2182a85319140036c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
534,Jay Mahadeokar,jaym@fb.com,2019-04-03 18:55:40-07:00,3658fa329b8cb987d951b2e38ec86c44b9e1fea5,https://github.com/pytorch/fairseq/commit/3658fa329b8cb987d951b2e38ec86c44b9e1fea5,"aligned training task and CE related changes

Summary:
This diff adds:

1. Aligned training task specifically for doing cross entropy criterion training using prod data and prod like models
2. Few changes to correctly register the task and criterions.
3. Changes to trainer code for propogating accuracy metrics which we care about for training.

Couple of things are hacky right now:
- The reporting is not modular (this needs to be thought about in general for fairseq).

- The get dummy batch could be specific to task instead of specific for dataset.

Reviewed By: myleott

Differential Revision: D14670482

fbshipit-source-id: dc077247b2ae9d26a8e842a386ec5faa5771e836",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
535,Kartikay Khandelwal,kartikayk@fb.com,2019-04-04 17:25:10-07:00,f040158aa91e0180da0ba273f8058d986804baf9,https://github.com/pytorch/fairseq/commit/f040158aa91e0180da0ba273f8058d986804baf9,"Add Transformer Sentence Encoder for BERT and XLM Pre-training in PyText (#621)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/621

In this commit, I add some modules to Fairseq needed to set up Bert/XLM style pretraining.

Reviewed By: borguz

Differential Revision: D14719663

fbshipit-source-id: 1c5c36b6b2cde1c9bcd3c9e9ac853d2b7ae64102",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
536,Kartikay Khandelwal,kartikayk@fb.com,2019-04-04 17:52:35-07:00,f492db25aa331d738c9fd5b0b64c66fb7eaa3c11,https://github.com/pytorch/fairseq/commit/f492db25aa331d738c9fd5b0b64c66fb7eaa3c11,"Refactor Fairseq models for BERT and XLM to use TransformerSentenceEncoder (#622)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/622

Updating some defaults to more meaningful values

Reviewed By: rutyrinott

Differential Revision: D14761263

fbshipit-source-id: 7ac670aa370f315ddfb511c63273583a6062c569",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
537,Liezl Puzon,lie@fb.com,2019-04-05 11:42:35-07:00,40ac340b3c3e76ab9869652df58f4193513e32fb,https://github.com/pytorch/fairseq/commit/40ac340b3c3e76ab9869652df58f4193513e32fb,"Eval and log on a subset of directions for multimodel training (#605)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/605

Eval and log on a subset of directions for multimodel training

This reduces code duplication in PyTorch Translate's semi_supervised task and will enable clean multitask setups in the future.

Reviewed By: pipibjc, dpacgopinath

Differential Revision: D14672779

fbshipit-source-id: 1342c71781f0824cc56a38ad1c1822e34eaef337",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
538,Haoran Li,aimeeli@fb.com,2019-04-07 14:49:08-07:00,34028c63b0fb6a21f57b6b546eeee77e2e87c21e,https://github.com/pytorch/fairseq/commit/34028c63b0fb6a21f57b6b546eeee77e2e87c21e,"move distributed_init after get_batch_iterator

Summary: There are constantly wait timeout issue for using multiple nodes, even setting copylocallytempdir:/ doesn't help, eg f105637629. It seems to be working after I moved distributed_init after get_batch_iterator, eg f106520580

Reviewed By: myleott

Differential Revision: D14817769

fbshipit-source-id: edbb101a28d8082241c7bdd8c5500c9dad27647c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
539,Kartikay Khandelwal,kartikayk@fb.com,2019-04-09 12:03:04-07:00,94e9d77ca23b88a50c6845ee54c3602fa4bad501,https://github.com/pytorch/fairseq/commit/94e9d77ca23b88a50c6845ee54c3602fa4bad501,"Fix save_dir creation while training on multiple nodes (#626)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/626

While training a model on multiple GPUs, the current fairseq train workflow fails while creating the directory from which to load a checkpoint. This seems to be happening because multiple nodes attempt to create the same directory thus causing some weird interaction with os.makedirs option ""exist_ok=True"". Fixing this by making sure only rank 0 creates this directory.

Reviewed By: myleott

Differential Revision: D14841304

fbshipit-source-id: c9b73ba804de97e2cb19a616189fefce476d8c74",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
540,Kartikay Khandelwal,kartikayk@fb.com,2019-04-09 12:15:45-07:00,c2820af002ed841ef4a4828b7b8059500dee7d8d,https://github.com/pytorch/fairseq/commit/c2820af002ed841ef4a4828b7b8059500dee7d8d,"Rename embedding layers to be the same as NMT (#628)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/628

Updating embedding layers in TransformerSentenceEncoder to be compatible with the transformer model.

Reviewed By: liezl200

Differential Revision: D14836883

fbshipit-source-id: 2240f61bf40b191d01b4efdaac4dd7562b4166c6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
541,Peng-Jen Chen,pipibjc@devfair0209.h2.fair,2019-04-10 10:53:07-07:00,d7e19573fa8674a04421ecc77a07f38f121af4df,https://github.com/pytorch/fairseq/commit/d7e19573fa8674a04421ecc77a07f38f121af4df,"Back translation + denoising in MultilingualTranslation task (#620)

Summary:
- Add language token to MultilingualTranslation task
- Add back translation and denoising loss to MultilingualTranslation task
Pull Request resolved: https://github.com/pytorch/fairseq/pull/620

Reviewed By: liezl200

Differential Revision: D14756873

Pulled By: pipibjc

fbshipit-source-id: 89d668db26848fd95f446edf5923bab2113636f7",12,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
542,Kritika Singh,skritika@fb.com,2019-04-10 11:47:52-07:00,309f25110605e31bb8aa2bd6d202ace73285e990,https://github.com/pytorch/fairseq/commit/309f25110605e31bb8aa2bd6d202ace73285e990,"Add anneal-eps argument

Summary: Used in fairspeq/train.py

Reviewed By: myleott, yqwangustc

Differential Revision: D14841512

fbshipit-source-id: 02fd7b58841c32e2797e3159e65f2bef36f02da1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
543,Liezl Puzon,lie@fb.com,2019-04-10 12:13:25-07:00,e5ba94ab9277f947dce99dccf68a0cf505a5baf4,https://github.com/pytorch/fairseq/commit/e5ba94ab9277f947dce99dccf68a0cf505a5baf4,"Make TransformerEncoderLayer layer norm names more descriptive

Summary:
I added an upgrade_state_dict function so that loading old models will still work

layer_norms[0] --> self_attn_layer_norm
layer_norms[1] --> final_layer_norm

Reviewed By: pipibjc

Differential Revision: D14689849

fbshipit-source-id: b2809262c11fe9d083e571fa31044798aefd48ce",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
544,Xian Li,xianl@fb.com,2019-04-10 16:04:38-07:00,58b912f6177d1dff0dd8bc90aa4ed5d2a0a25cd7,https://github.com/pytorch/fairseq/commit/58b912f6177d1dff0dd8bc90aa4ed5d2a0a25cd7,"Fix sacrebleu (#630)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/630

sacrebleu scorer has stopped working in pytorch_translate (maybe
fairseq too) probably due to  a recent api change.

Reviewed By: jmp84

Differential Revision: D14792797

fbshipit-source-id: c2a00246e08bc913c41e60c5fbf8ab4ab5e80d18",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
545,Liezl Puzon,lie@fb.com,2019-04-12 11:25:38-07:00,a47630e127db2a4af19a3a4625807823b9c6068b,https://github.com/pytorch/fairseq/commit/a47630e127db2a4af19a3a4625807823b9c6068b,"Fix hybrid transformer state dict update after encoder layernorm rename (#633)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/633

Pull Request resolved: https://github.com/pytorch/translate/pull/456

This diff makes it easier to upgrade the state dict for components that use TransformerEncoderLayer

Reviewed By: jhcross

Differential Revision: D14916941

fbshipit-source-id: 6d0258c8a9492a720684dadce59c90fc87cbf5cf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
546,Myle Ott,myleott@fb.com,2019-04-15 07:29:14-07:00,e12e1d254c77b363d3876893cc4943277dcc5f1f,https://github.com/pytorch/fairseq/commit/e12e1d254c77b363d3876893cc4943277dcc5f1f,"Simplify and generalize utils.make_positions

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/625

Differential Revision: D14822123

Pulled By: myleott

fbshipit-source-id: 8a263d30020588577ee02fb8c6959ff918705103",9,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
547,freewym,freewym@gmail.com,2019-04-15 07:30:02-07:00,de8aeab5c6e3c18a2d110fcdaa36754825a481ea,https://github.com/pytorch/fairseq/commit/de8aeab5c6e3c18a2d110fcdaa36754825a481ea,"fix checkpoint timer (#634)

Summary:
If arg.keep_interval_updates or args.keep_last_epochs > 0, `checkpoints` would refer to a list of checkpoint files to be removed, which can be empty. So moved the logging code to the right position.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/634

Differential Revision: D14933655

Pulled By: myleott

fbshipit-source-id: 68182ee99d9701e1536833d31e0a7c5d2eb2d679",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
548,Myle Ott,myleott@fb.com,2019-04-15 07:31:51-07:00,303b95ce80b85a1ec222373d6a8f5cf5842b2362,https://github.com/pytorch/fairseq/commit/303b95ce80b85a1ec222373d6a8f5cf5842b2362,"Better distributed init

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/615

Differential Revision: D14933742

Pulled By: myleott

fbshipit-source-id: c2c20425875743c89bbc2ac564a2fbb6ff4958b2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
549,Kartikay Khandelwal,kartikayk@fb.com,2019-04-16 16:12:53-07:00,8776928c75738087ea6b62331f2a9cc8feac601a,https://github.com/pytorch/fairseq/commit/8776928c75738087ea6b62331f2a9cc8feac601a,"Open Source MLM Implementation in Fairseq (#635)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/635

Adding a task and relevant models, datasets and criteria needed for training Cross-lingual Language Models similar to Masked Language Model used in XLM (Lample and Conneau, 2019 - https://arxiv.org/abs/1901.07291).

Reviewed By: liezl200

Differential Revision: D14943776

fbshipit-source-id: 3e416a730303d1dd4f5b92550c78db989be27073",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
550,Ning Dong,dnn@fb.com,2019-04-16 23:26:04-07:00,17cef3f60c738ab97a081eda317902b7c6b3224f,https://github.com/pytorch/fairseq/commit/17cef3f60c738ab97a081eda317902b7c6b3224f,"Black formatting for multi_corpus_sampled_dataset.py (#638)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/638

RT

Reviewed By: liezl200

Differential Revision: D14967268

fbshipit-source-id: 2da361497743d90a841fdbf2a50085136c70b468",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
551,Ning Dong,dnn@fb.com,2019-04-16 23:26:04-07:00,90d6eac2b313968f4210a229efd3a88f243b3438,https://github.com/pytorch/fairseq/commit/90d6eac2b313968f4210a229efd3a88f243b3438,"Enable custom sampling strategy in MultiCorpusSampledDataset (#639)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/639

Add argument sampling_func in the constructor to enable custom sampling over a list of dataset keys. The default strategy is to sample uniformly as it did previously.

Reviewed By: liezl200

Differential Revision: D14965774

fbshipit-source-id: f3285688a9ae3729c0ba12c22254c1144d0eea9e",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMultiCorpusSampledDataset(unittest.TestCase):'],"[('Less', '(')]",['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
552,Kartikay Khandelwal,kartikayk@fb.com,2019-04-17 13:49:01-07:00,d2f3007ce9ebbfc455195e5db26426416877abbf,https://github.com/pytorch/fairseq/commit/d2f3007ce9ebbfc455195e5db26426416877abbf,"Open BlockPairDataset for MaskedLMData to work (#641)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/641

Fix breaking import

Reviewed By: pipibjc

Differential Revision: D14978454

fbshipit-source-id: 7b43152cb30100881e9991ead871531ee3f60e07",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
553,Yongqiang Wang,yqw@fb.com,2019-04-21 18:09:47-07:00,d63477e1edae10fce56d51fa51b350da737c283a,https://github.com/pytorch/fairseq/commit/d63477e1edae10fce56d51fa51b350da737c283a,"reduce memory footprint for average_checkpoints (#647)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/647

the current implementation of average_checkpoints requires loading all
the model parameters into memory and then do the averaging. To average large
models (e.g., transformer) over a large number of checkpoints (e.g., >50),
it may require over 100GB memory.

Loading all the parameters is not necessary, as we know the number of models in advance.

Reviewed By: skritika

Differential Revision: D15027513

fbshipit-source-id: 0afe37c9a031a9ab0f1e78844a37be49ec5f76f1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
554,Max Ryabinin,mryabinin0@gmail.com,2019-04-22 16:30:27-07:00,fa52d20277246a59c13260fdfb7d71101e521478,https://github.com/pytorch/fairseq/commit/fa52d20277246a59c13260fdfb7d71101e521478,"Fix generation with --no-early-stop (#627)

Summary:
Because the size of `unfinalized_scores` is equal to current `bsz` and not initial batch size, we need to index it by `unfin_idx` instead of `sent` in `is_finished`.
Fixes #588.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/627

Differential Revision: D15034641

Pulled By: myleott

fbshipit-source-id: 2638e68e877ae01256cac7d8e69b5b7fec8f7017",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
555,Myle Ott,myleott@fb.com,2019-04-24 13:16:06-07:00,0020477a6a68dc798d38475cd7dee83e9d2bca6c,https://github.com/pytorch/fairseq/commit/0020477a6a68dc798d38475cd7dee83e9d2bca6c,"Don't reload best validation loss when using --reset-optimizer

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/661

Differential Revision: D15068312

Pulled By: myleott

fbshipit-source-id: 1216835fd4c7f83ea5e350bff83901c93ac57447",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
556,Angela Fan,angela.h.fan@gmail.com,2019-04-24 22:40:02-07:00,5ecedd696acbedcf5c843caba784c08ecf4e8f66,https://github.com/pytorch/fairseq/commit/5ecedd696acbedcf5c843caba784c08ecf4e8f66,"added link to sample stories

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/665

Differential Revision: D15077853

Pulled By: huihuifan

fbshipit-source-id: 2a0d3f6236ae002579f1ee72735d6d8000b8e6b6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
557,ankur6ue,ankur6ue@gmail.com,2019-04-24 22:49:03-07:00,d8d03745b343e455fe288debc69092be2496e47a,https://github.com/pytorch/fairseq/commit/d8d03745b343e455fe288debc69092be2496e47a,"Added link to blog post (#662)

Summary:
Added link to blog post about incremental decoder in the FairseqIncrementalDecoder class description.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/662

Differential Revision: D15077845

Pulled By: myleott

fbshipit-source-id: f23294721739600e14feb2cca4ece95f2b968f44",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
558,Liezl Puzon,lie@fb.com,2019-04-25 05:52:36-07:00,8500bdd0c8babc09c13b6ce556957e777bd90adc,https://github.com/pytorch/fairseq/commit/8500bdd0c8babc09c13b6ce556957e777bd90adc,"Add gelu and gelu_fast as possible activation functions (#653)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/653

After this diff, you can train a transformer model with --activation-fn 'relu', 'gelu', or 'gelu_fast'

gelu_fast is the default implementation in https://github.com/hendrycks/GELUs/blob/master/mnist_fcn.py#L72-L77
gelu is the alternate implementation in https://github.com/hendrycks/GELUs/blob/master/mnist_fcn.py#L72-L77 and the default implementation in https://github.com/facebookresearch/XLM

Reviewed By: pipibjc

Differential Revision: D14966006

fbshipit-source-id: 94e95fb99bd548ba47cf23b4999265c7b6833fc1",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
559,Liezl Puzon,lie@fb.com,2019-04-25 05:52:36-07:00,8da9b1c530de274cd0099dd73dea793235e84755,https://github.com/pytorch/fairseq/commit/8da9b1c530de274cd0099dd73dea793235e84755,"Load a XLM model into transformer encoder / decoder for MT training (#629)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/629

Use GeLU as an alternate activation layer for ReLU.

Reviewed By: lematt1991

Differential Revision: D14689851

fbshipit-source-id: 7ec81fa34bc7bd0e1e43b337847ae932dcbf8b15",3,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMaskedLanguageModel(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
560,Liezl Puzon,lie@fb.com,2019-04-25 05:52:36-07:00,5008fd4e5a6a08636327a2ff25f48102c04850b9,https://github.com/pytorch/fairseq/commit/5008fd4e5a6a08636327a2ff25f48102c04850b9,"XLM for NMT: option to only load encoder or decoder (#666)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/666

Option to load the XLM weights into only the encoder or the decoder

Reviewed By: pipibjc

Differential Revision: D14881004

fbshipit-source-id: 6d0d598ea9c445ec468f71b8e855712de89a5dac",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
561,Liezl Puzon,lie@fb.com,2019-04-25 08:31:57-07:00,57b6a6dbfb290718eaa25040551d9db8a6b68a9b,https://github.com/pytorch/fairseq/commit/57b6a6dbfb290718eaa25040551d9db8a6b68a9b,"Fix fairseq unittest timeouts (#667)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/667

Use smaller models so that unittests won't timeout

Reviewed By: pipibjc

Differential Revision: D15056894

fbshipit-source-id: af9fbda6ea6e56cf82d52555620121b189e2f013",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
562,Mohammad Sadegh Rasooli,rasooli@fb.com,2019-04-26 09:06:02-07:00,f701aa8c7885f73dea6d3373c5be41d65832c377,https://github.com/pytorch/fairseq/commit/f701aa8c7885f73dea6d3373c5be41d65832c377,"Passing kwargs in setup_task in fairseq_task (#670)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/670

Pytorch-translate task needs to use extra arguments (such as vocabulary objects). By passing kwargs, we are able to have the ability to have extra arguments in setup_task

Reviewed By: akinh, pipibjc

Differential Revision: D15086810

fbshipit-source-id: 555f7976020eaac1febb8226f5a0055af0407ea6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
563,Myle Ott,myleott@fb.com,2019-04-27 11:33:30-07:00,8bf8399d3c0c303174ce7db6e50df84d8d4c122b,https://github.com/pytorch/fairseq/commit/8bf8399d3c0c303174ce7db6e50df84d8d4c122b,"Add small comments for MonolingualDataset and TokenBlockDataset

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/669

Differential Revision: D15114160

Pulled By: myleott

fbshipit-source-id: 64f4a8154c8931ddbbe459d4d4a54c46680ad6b6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
564,Noe Casas,noe.casas@gmail.com,2019-04-27 11:38:06-07:00,257a3b89edf359c1a2603ff7a841db6b926460bd,https://github.com/pytorch/fairseq/commit/257a3b89edf359c1a2603ff7a841db6b926460bd,"Add args and sys.argv to tensorboard (#673)

Summary:
Log fairseq's `args` and `sys.argv` in tensorboard to easily identify run hyperparameters from within tensorboard.

The idea was suggested in https://twitter.com/Thom_Wolf/status/1106300583835766786
Pull Request resolved: https://github.com/pytorch/fairseq/pull/673

Differential Revision: D15114159

Pulled By: myleott

fbshipit-source-id: d48133a7f629dffe984836712390c317916cf413",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
565,Myle Ott,myleott@fb.com,2019-04-29 07:58:19-07:00,849605a0fd025c4f7fccf13307e7cfb50871d62e,https://github.com/pytorch/fairseq/commit/849605a0fd025c4f7fccf13307e7cfb50871d62e,"Update comments and citations

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/676

Differential Revision: D15114128

Pulled By: myleott

fbshipit-source-id: b11dde77b2f2610d33649101aea03fb5a3eeb56a",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
566,Myle Ott,myleott@fb.com,2019-04-29 09:09:56-07:00,ace8f72418c9a893b66b8a722122d7ac6a060f19,https://github.com/pytorch/fairseq/commit/ace8f72418c9a893b66b8a722122d7ac6a060f19,"Update README.md (#679)

Summary:
Add missing backslash.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/679

Differential Revision: D15122270

Pulled By: myleott

fbshipit-source-id: fbdfde648051294eaa9f7a4e0c4cfbc57491a718",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
567,Liezl Puzon,lie@fb.com,2019-04-29 17:22:46-07:00,121877f5fcd7b594b801e86be002c73fcaa6de13,https://github.com/pytorch/fairseq/commit/121877f5fcd7b594b801e86be002c73fcaa6de13,"Fix upgrade_state_dict for XLM Transformer sentence encoder (#680)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/680

Some embedding names were renamed but this one was missed

So far I've only seen this affect our runs during continuing training. If you encountered any errors when continuing training from an XLM save_dir, rebasing past this diff (or patching this and canarying) should fix the problem

Reviewed By: pipibjc

Differential Revision: D15137463

fbshipit-source-id: c72067f16aaf1ba2b8286938bd25a19b70ae8712",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
568,Liezl Puzon,lie@fb.com,2019-04-29 19:12:24-07:00,89a696161b3c0f682449107b6e779d80b24341b8,https://github.com/pytorch/fairseq/commit/89a696161b3c0f682449107b6e779d80b24341b8,"Add more details in error message when sentence length > max tokens (#672)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/672

title

Reviewed By: jmp84, pipibjc

Differential Revision: D15094977

fbshipit-source-id: c24e4ec9355b53e1585ac4da32809f1c339c7364",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
569,Myle Ott,myleott@fb.com,2019-04-29 19:45:01-07:00,d45db804314d2c05a3e350ebdce420e513993b01,https://github.com/pytorch/fairseq/commit/d45db804314d2c05a3e350ebdce420e513993b01,"Merge internal changes (#654)

Summary:
- Add --add-bos-token option to LM task
- Cleanup utils.py and options.py
Pull Request resolved: https://github.com/pytorch/fairseq/pull/654

Differential Revision: D15041794

Pulled By: myleott

fbshipit-source-id: 3ad00007769d5f48308052cfd40de39c5ffa1a6e",34,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
570,Myle Ott,myleott@fb.com,2019-04-30 05:55:00-07:00,f5e52c1928ccd9d596ec0fd82219b6a5082f5b35,https://github.com/pytorch/fairseq/commit/f5e52c1928ccd9d596ec0fd82219b6a5082f5b35,"Add rm_pt.py helper script for removing checkpoint files

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/681

Differential Revision: D15147107

fbshipit-source-id: 4452c98059586a4d748868a7659329285a76d5ef",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
571,Myle Ott,myleott@fb.com,2019-04-30 09:38:27-07:00,6b8cb7dbc7e3d4a40949d858f6765cc7c7bdb7c9,https://github.com/pytorch/fairseq/commit/6b8cb7dbc7e3d4a40949d858f6765cc7c7bdb7c9,"Merge internal changes

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/682

Differential Revision: D15147735

Pulled By: myleott

fbshipit-source-id: 4a5f12c0b24591f964fe1f465be3775a67578e79",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
572,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-04-30 11:04:19-07:00,9421e978b6b841d38bc125f4fafe9d468b39f99d,https://github.com/pytorch/fairseq/commit/9421e978b6b841d38bc125f4fafe9d468b39f99d,"addding polynomial lr scheduler (#683)

Summary:
Co-authored-by: jingfeidu <jingfeidu@fb.com>

The implementation is by Jingfei Du from branch ""bigbert"". Copied over to this CR to get it merged in isolation since other changes seem to be already in master.

**Small changes from original:**
Added following line in `__init__` as discovered by myleott :

```
self.optimizer.set_lr(self.warmup_factor * self.lr)
```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/683

Reviewed By: myleott

Differential Revision: D15149628

Pulled By: myleott

fbshipit-source-id: 5f715611182cdd111e636c66d5f24aa88fa03e29",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
573,Ning Dong,dnn@fb.com,2019-04-30 19:58:01-07:00,3742085504a86edc1dedfdcd02addc9cbeae15c4,https://github.com/pytorch/fairseq/commit/3742085504a86edc1dedfdcd02addc9cbeae15c4,"Add default noising argument in WordNoiser initialization (#664)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/664

Previously arguments for noising (dropout_prob for WordDropout and max_shuffle_distance for WordShuffle) are only passed in noising() so it could not be customized in NoisingDataset.

Now add default argument in initializer so the value could be specified at construction.

Reviewed By: liezl200

Differential Revision: D15071632

fbshipit-source-id: 59a9bf5a5e6d03c1e74f1b31c1927e221cb11dfa",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
574,Myle Ott,myleott@fb.com,2019-05-01 06:12:26-07:00,da9e493ef8c123de9daca6cf4221c8d4ba1de9d8,https://github.com/pytorch/fairseq/commit/da9e493ef8c123de9daca6cf4221c8d4ba1de9d8,"Better OOM recovery

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/685

Differential Revision: D15154647

Pulled By: myleott

fbshipit-source-id: 36c72359755192a4a53367e19f8dd006791d483c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
575,Ning Dong,dnn@fb.com,2019-05-01 10:19:27-07:00,ff74ca94ce1d01d136c11b24c6ba66dc66bae54f,https://github.com/pytorch/fairseq/commit/ff74ca94ce1d01d136c11b24c6ba66dc66bae54f,"Support dataset upsampling / relative ratio in PytorchTranslateTask (#494)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/494

Pull Request resolved: https://github.com/pytorch/fairseq/pull/657

Library side change split from D14924942

Added 2 arguments for load_dataset in PytorchTranslateTask
1. dataset_upsampling. A nested dictionary {direction:{dataset: upsampling_ratio}}. Upsampling_ratio larger than one mean that the bitext is ob- served more often than actually present in the combined bitext and synthetic training corpus.

2. dataset_relative_ratio. A tuple (dataset, ratio). The ratio represents the frequency certain dataset gets sampled to the rest of corpora map.

At most one of them could be specified.

Reviewed By: liezl200

Differential Revision: D15041293

fbshipit-source-id: 92daad29895c234e26d1b19f121106118a3957ad",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
576,taineleau,taineleau@gmail.com,2019-05-01 14:15:49-07:00,91c78477d14cfb369933ba20e430980d669010d5,https://github.com/pytorch/fairseq/commit/91c78477d14cfb369933ba20e430980d669010d5,"add ConcatDataset support for XLM

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/684

Differential Revision: D15154631

Pulled By: myleott

fbshipit-source-id: 5e7dd9651d9ed239b60c51b9a11d08c80307d3ba",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
577,Myle Ott,myleott@fb.com,2019-05-01 15:27:00-07:00,e112d501c50f5acdbafcb6babc404c8262656885,https://github.com/pytorch/fairseq/commit/e112d501c50f5acdbafcb6babc404c8262656885,"Make MultiCorpusSampledDataset and IndexedCachedDataset Picklable

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/691

Differential Revision: D15172543

Pulled By: myleott

fbshipit-source-id: f2b626ff7f5e95f0ddc83c105af7ab9d092a135e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
578,Kritika Singh,skritika@fb.com,2019-05-01 18:21:52-07:00,ffc9c8cc9f98389cfc9e76f0a37744e006d7286a,https://github.com/pytorch/fairseq/commit/ffc9c8cc9f98389cfc9e76f0a37744e006d7286a,"Make CTC work with more encoder-only models

Summary:
Changes include:
1. Added get_normalized_probabilities to the encoder-only base class FairseqEncoderModel
2. Made CTCCriterion work for both batch_first (LSTMSubsampleEncoderModel) and batch_second (LSTMEncoderOnly) encoder types
3. Added tests for different encoder and CTC combinations.

TODO:
CTC still doesn't work for VGGLSTMEncoderModel so I have disabled that. Will debug and send out fix in another diff.

Reviewed By: jay-mahadeokar

Differential Revision: D15158818

fbshipit-source-id: acb484bad705c937d676d2c3dcde3e3562d68ed9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
579,Myle Ott,myleott@fb.com,2019-05-02 06:34:37-07:00,4a30a5f6c5592a6546a4f5b494436b873b87170a,https://github.com/pytorch/fairseq/commit/4a30a5f6c5592a6546a4f5b494436b873b87170a,"Fix inconsistent gradient check

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/692

Differential Revision: D15174954

fbshipit-source-id: 1a7bff9aeed3e2cc658577be9d79e8c9f72314c2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
580,Myle Ott,myleott@fb.com,2019-05-02 06:35:35-07:00,fb18be00f7c89b7173879784c4d6995a52025ffc,https://github.com/pytorch/fairseq/commit/fb18be00f7c89b7173879784c4d6995a52025ffc,"Validate on all sets based on --save-interval-updates

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/693

Differential Revision: D15174831

fbshipit-source-id: 98688b1269ead5694e5116659ff64507d3c0d1c0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
581,Myle Ott,myleott@fb.com,2019-05-02 08:02:46-07:00,34726d5612322b79b621e52f7f6fe47a6716eb65,https://github.com/pytorch/fairseq/commit/34726d5612322b79b621e52f7f6fe47a6716eb65,"Move distributed_init into DistributedFairseqModel (#687)

Summary:
This should make rendezvous happen as lazily as possible.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/687

Differential Revision: D15151145

Pulled By: myleott

fbshipit-source-id: d70816a85414c5d509a6b12e2b339b4736db2c88",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
582,Peng-Jen Chen,pipibjc@fb.com,2019-05-02 13:49:42-07:00,39264559f64f581098e298b45674f40e6c4a5ee9,https://github.com/pytorch/fairseq/commit/39264559f64f581098e298b45674f40e6c4a5ee9,"Make learned positional embedding optional

Summary:
- Add learned positional embedding binary flag to masked LM model.
- Add base arch config for masked LM model which sets all the binary parameters to False. Otherwise some of the binary flag parameters will always be override by config in `xlm_architecture` (e.g. encoder_learned_pos)

Reviewed By: liezl200

Differential Revision: D15054487

fbshipit-source-id: d78827f352b9160a89c9dc4f45b9fce15a2f234d",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
583,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-03 08:15:42-07:00,f5fbcaaf4d1c0975f908c5da16661e4f4a3a952f,https://github.com/pytorch/fairseq/commit/f5fbcaaf4d1c0975f908c5da16661e4f4a3a952f,"added bert large architecture (#698)

Summary:
Added bert_large architecture
Pull Request resolved: https://github.com/pytorch/fairseq/pull/698

Differential Revision: D15198698

Pulled By: myleott

fbshipit-source-id: 1dc9e8d4c8c877d15afffe5fe581b4b93eefbc66",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
584,Yongqiang Wang,yqw@fb.com,2019-05-03 12:28:07-07:00,a2901f985720547a73abdaa66a24d7f23078be48,https://github.com/pytorch/fairseq/commit/a2901f985720547a73abdaa66a24d7f23078be48,"an option to raise exception if oom happens during fairseq.trainer.train_step (#2)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairspeq/pull/2

Pull Request resolved: https://github.com/pytorch/fairseq/pull/689

We found not raising OOM during trainer.train_step causes various
issue, including NCCL hangs / gloo sync errors because gradient is not synced
properly. Before we found the root cause, let's give users an option to raise
OOMs.

Reviewed By: jmp84

Differential Revision: D15170357

fbshipit-source-id: 3e15e4e111a8380612157955509c39821a216ec4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
585,Kritika Singh,skritika@fb.com,2019-05-03 17:57:12-07:00,657a883620703c841d4e35472d83449f09f24393,https://github.com/pytorch/fairseq/commit/657a883620703c841d4e35472d83449f09f24393,"Bugfix in size of multi-corpus dataset

Summary: See comment

Reviewed By: jay-mahadeokar

Differential Revision: D15070187

fbshipit-source-id: ffefca0effb2cc866ce6fa22a59d5419b592fb7b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
586,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-04 11:46:01-07:00,7a5996fdc75dc9325646fd00e8e69e3f55cbb05e,https://github.com/pytorch/fairseq/commit/7a5996fdc75dc9325646fd00e8e69e3f55cbb05e,"use fused layer norm in transformer sentence encoder (#702)

Summary:
We can later get rid off `BertLayerNorm` also, as I think the implementation of that is exactly same as `LayerNorm`. (will confirm with jingfeidu on that).
But this should be drop and replace.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/702

Differential Revision: D15213116

Pulled By: myleott

fbshipit-source-id: ba5c00e1129a4443ef5d3d8bebd0bb6c6ee3b188",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
587,Myle Ott,myleott@fb.com,2019-05-04 16:31:00-07:00,fc1a19a38d6bd0a48873c5091afb4320b9aceb2e,https://github.com/pytorch/fairseq/commit/fc1a19a38d6bd0a48873c5091afb4320b9aceb2e,"Deprecate dummy_batch (#699)

Summary:
It was tedious defining these, let's try just taking the first batch lazily instead.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/699

Differential Revision: D15188266

Pulled By: myleott

fbshipit-source-id: a4c9f7ee3111278faaffa8a22ba91ed5f50e143d",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
588,Myle Ott,myleott@fb.com,2019-05-04 16:33:48-07:00,96ac28d33d248bee66bab9a8bffdaa2a69d25213,https://github.com/pytorch/fairseq/commit/96ac28d33d248bee66bab9a8bffdaa2a69d25213,"Fix and generalize --temperature option (#508)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/508

The previous version applied the temperature after the softmax. Fix that, and
also generalize so it works with other search approaches.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/694

Differential Revision: D15175160

Pulled By: myleott

fbshipit-source-id: cc87ff0e97a8a1dd37f9983163f58a8641155ab0",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
589,Myle Ott,myleott@fb.com,2019-05-04 17:08:05-07:00,cf17068aad5af49eb9a106e314f617b15438dc9e,https://github.com/pytorch/fairseq/commit/cf17068aad5af49eb9a106e314f617b15438dc9e,"Initialize distributed using multiproc with all visible GPUs

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/695

Differential Revision: D15182613

Pulled By: myleott

fbshipit-source-id: 4196346517d8e75ed9e903e9e01ab943d086f6f1",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
590,Myle Ott,myleott@fb.com,2019-05-04 17:54:11-07:00,437c2386dec1d8f9cfba7c9b9555ab669ed4c538,https://github.com/pytorch/fairseq/commit/437c2386dec1d8f9cfba7c9b9555ab669ed4c538,"Speed up saving checkpoints (#703)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/703

It's better to write one checkpoint and copy it, rather than repeatedly pickling the model via torch.save.

Differential Revision: D15213778

fbshipit-source-id: 27dad39853b09dab7f0e11c030313019f035dbb0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
591,Myle Ott,myleott@fb.com,2019-05-04 19:45:07-07:00,7176667d1d58f806e07770f406d51d800dfd1b15,https://github.com/pytorch/fairseq/commit/7176667d1d58f806e07770f406d51d800dfd1b15,"Fix apex Adam to not break CPU mode

Reviewed By: chenyangyu1988

Differential Revision: D14784219

fbshipit-source-id: 273888d6e3d22a01d5e7edfbc786195e7b78efef",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
592,Liezl Puzon,lie@fb.com,2019-05-06 00:14:51-07:00,39cd4ce2723d550dd54f6b14b0ed2878e10427f8,https://github.com/pytorch/fairseq/commit/39cd4ce2723d550dd54f6b14b0ed2878e10427f8,"Load pretrained encoder or decoder (#705)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/705

This adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.

Reviewed By: jmp84

Differential Revision: D15207084

fbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
593,Maksym Del,max.del.edu@gmail.com,2019-05-06 08:25:00-07:00,817fccf5b95a5653c6e1e53c87bf4218520eb5b4,https://github.com/pytorch/fairseq/commit/817fccf5b95a5653c6e1e53c87bf4218520eb5b4,"Fix semisupervised_translation task (#706)

Summary:
Pass required ""sample_key"" argument to forward-backward call in semi-supervised task.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/706

Differential Revision: D15217957

Pulled By: pipibjc

fbshipit-source-id: bf943d566c5caa67682dfb16ff8b7c432323cdba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
594,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-06 11:48:47-07:00,e1ffea873d572f31b3766585ea765ca06ae7092f,https://github.com/pytorch/fairseq/commit/e1ffea873d572f31b3766585ea765ca06ae7092f,"added masked_lm task (#697)

Summary:
Co-authored-by: jingfeidu <jingfeidu@fb.com>

1) Adding `masked_lm` task for BERT like training. Code mostly taken from jingfeidu 's implementation.

2) Added `has_eos` option to `block_pair_dataset` for working with dataset that has been preprocessed with having `eos`.

Depends on: https://github.com/pytorch/fairseq/pull/696
Pull Request resolved: https://github.com/pytorch/fairseq/pull/697

Differential Revision: D15214050

fbshipit-source-id: c179ce2d70e59d2ddc941b13ceda99d929878931",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
595,Myle Ott,myleott@fb.com,2019-05-06 11:55:21-07:00,57da383c9a954ac82910411238fdd92d558d35f6,https://github.com/pytorch/fairseq/commit/57da383c9a954ac82910411238fdd92d558d35f6,"Remove redundant distributed init

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/707

Differential Revision: D15219014

Pulled By: myleott

fbshipit-source-id: f38f2cf817d05e0871ff9084a810d109848e827c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
596,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-06 14:56:13-07:00,0add50c2e0b5dfaeb0900df08131b0cb87cba273,https://github.com/pytorch/fairseq/commit/0add50c2e0b5dfaeb0900df08131b0cb87cba273,"allowing sharded dataset (#696)

Summary:
Co-authored-by: myleott <myleott@fb.com>

Changing `data` to be `str` with colon separated list for loading sharded datasets. This change is useful for loading large datasets that cannot fit into, memory. The large dataset can be sharded and then each shard is loaded in one epoch in roudrobin manner.

For example, if there are `5` shards of data and `10` epochs then the shards will be iterated upon `[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]`.

myleott We need to look into `translation.py` as it currently already expects a list and then concats the datasets.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/696

Differential Revision: D15214049

fbshipit-source-id: 03e43a7b69c7aefada2ca668abf1eac1969fe013",9,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
597,Kartikay Khandelwal,kartikayk@fb.com,2019-05-06 18:27:00-07:00,8d9063fe0d3bfdbca286b3aee2ef69efebab1809,https://github.com/pytorch/fairseq/commit/8d9063fe0d3bfdbca286b3aee2ef69efebab1809,"Mask out embeddings associated with padding (#710)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/710

Previously there was a bug in how we dealt with padding when computing the input representation from the segment and position embedding. D15144912 fixed this by adding an offset based on the padding id. However this makes assumptions about the padding id which may not hold true for vocabularies built outside of pyText and fairseq. Based on a discussion with barlaso, this diff 0's out all the embeddings associated with the padding.

Reviewed By: borguz

Differential Revision: D15209395

fbshipit-source-id: 5573020e610f5466e673fe3845c3ed34ebb5c44d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
598,Myle Ott,myleott@fb.com,2019-05-07 07:05:49-07:00,e4edf27a97a8580cf50b636ad80a20455c26ed75,https://github.com/pytorch/fairseq/commit/e4edf27a97a8580cf50b636ad80a20455c26ed75,"Improve init speed of TokenBlockDataset and EpochBatchIterator

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/704

Differential Revision: D15221549

Pulled By: myleott

fbshipit-source-id: b0021acdc2d7792ce51421f1432e1f2bd8218f7b",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(epoch_itr.iterations_in_epoch, 149)'), ('True', '(itr.has_next())'), ('False', '(itr.has_next())'), ('True', '(itr.has_next())'), ('Equal', '(epoch_itr.epoch, 3)'), ('Equal', '(epoch_itr.iterations_in_epoch, 0)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
599,Davide Caroselli,davide@modernmt.eu,2019-05-07 07:06:16-07:00,a1c997bd9a4e626b0b75aa2dd8d08d8d2beb0c71,https://github.com/pytorch/fairseq/commit/a1c997bd9a4e626b0b75aa2dd8d08d8d2beb0c71,"Memory-Mapped IndexedDataset implementation (#589)

Summary:
Following discussion in https://github.com/pytorch/fairseq/issues/574:

 - Implemented MMapIndexedDataset and MMapIndexedDatasetBuilder compatible with IndexedDataset/IndexedDatasetBuilder
- Update scripts/read_binarized.py to support new MMapIndexedDataset
- Option '--raw-text' and '--lazy-load' replaced with '--dataset-impl' and moved the option definition custom task args to more high-level options.add_dataset_args() (more appropriate)
- Implemented also utils functions in indexed_dataset: make_dataset(), dataset_exists()
Pull Request resolved: https://github.com/pytorch/fairseq/pull/589

Differential Revision: D14597128

Pulled By: myleott

fbshipit-source-id: 4e92d99920cbaa52cfe5a0f1f5d9ae5c92d4268e",11,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
600,taineleau,taineleau@gmail.com,2019-05-07 07:09:10-07:00,e37bd94856d680ca7111321ad24845d70cb726cc,https://github.com/pytorch/fairseq/commit/e37bd94856d680ca7111321ad24845d70cb726cc,"bugfix: passing args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/711

Differential Revision: D15239618

Pulled By: myleott

fbshipit-source-id: 82f3f79501a13a967324b8a66281cd134bf1ef23",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
601,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-07 08:07:02-07:00,20e7836e83aa871c7fc13c73f43acd90f8f2e827,https://github.com/pytorch/fairseq/commit/20e7836e83aa871c7fc13c73f43acd90f8f2e827,"fixed arg passing in masked_lm_dataset

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/715

Differential Revision: D15240723

fbshipit-source-id: 11d7280cb187d68f107902822e878f2a04b840c7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
602,Jay Mahadeokar,jaym@fb.com,2019-05-07 20:10:11-07:00,6a7eb6ce05678d8cab1bea260830f2d1eb766cfe,https://github.com/pytorch/fairseq/commit/6a7eb6ce05678d8cab1bea260830f2d1eb766cfe,"bugfix data not in args

Summary:
D15214049 introduced a bug such that if a tasks args does not contain data, then it will give error
```
File ""/data/users/jaym/fbsource/fbcode/buck-out/dev/gen/deeplearning/projects/fairspeq/train#link-tree/train.py"", line 119, in reload_train
   if len(args.data.split("":"")) == 1:
AttributeError: 'Namespace' object has no attribute 'data'
```

This diff checks if data is in args to avoid above error.

Reviewed By: myleott, jmp84

Differential Revision: D15253373

fbshipit-source-id: 14fb9ad878ee50f1b7583349bb17e29c03c40815",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
603,Myle Ott,myleott@fb.com,2019-05-08 04:53:26-07:00,0cb45bcb124fc9255285ed48938e3d46f79e96b1,https://github.com/pytorch/fairseq/commit/0cb45bcb124fc9255285ed48938e3d46f79e96b1,"Bugfix

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/717

Differential Revision: D15254560

Pulled By: myleott

fbshipit-source-id: 2a07614e8d294636f706939e60f0091c73115494",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
604,Myle Ott,myleott@fb.com,2019-05-08 06:11:05-07:00,eddcdf08e11aa96226e5631f08dc3f46250ed301,https://github.com/pytorch/fairseq/commit/eddcdf08e11aa96226e5631f08dc3f46250ed301,"Fix indexing in TokenBlockDataset

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/719

Differential Revision: D15258483

Pulled By: myleott

fbshipit-source-id: dd00daa6f1c87264c1196a77dfffc8c876ebde7f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
605,Myle Ott,myleott@fb.com,2019-05-08 08:57:56-07:00,f2563c21e15e0991b87eb49398234e2f4e809d88,https://github.com/pytorch/fairseq/commit/f2563c21e15e0991b87eb49398234e2f4e809d88,"Cleanup LM + Flake8

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/720

Differential Revision: D15259091

Pulled By: myleott

fbshipit-source-id: 06a35996c06ccddb49fdc9e01e348ff3c9da334e",34,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
606,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-08 11:36:22-07:00,bd6e5c4f1dd5d751036b283b051af0ae03b18cfc,https://github.com/pytorch/fairseq/commit/bd6e5c4f1dd5d751036b283b051af0ae03b18cfc,"bug_fixes and small changes to masked lm (#721)

Summary:
1) Made the model compatible with using either `masked_lm_dataset` or `monolingual_dataset`.
2) fixed default args setting task. (`bert` vs `masked_lm`) myleott should we keep both?
3) bug in setting default value of `sentence_class_num`
4) bug for padding mask in `fp16`.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/721

Differential Revision: D15259885

fbshipit-source-id: 9dbf7fb8192992c1251670287bed719e41c08fcc",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
607,Myle Ott,myleott@fb.com,2019-05-08 11:49:59-07:00,61f29f7fb077c74c7b4a86888001044cd490e5af,https://github.com/pytorch/fairseq/commit/61f29f7fb077c74c7b4a86888001044cd490e5af,"Better error message for incorrect --dataset-impl

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/723

Differential Revision: D15260870

Pulled By: myleott

fbshipit-source-id: 73d9b138b9ab44f96824076258f1a6319193d0f7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
608,Myle Ott,myleott@fb.com,2019-05-08 14:30:16-07:00,acb9ab32a5a754455517f6581519db3f886d895c,https://github.com/pytorch/fairseq/commit/acb9ab32a5a754455517f6581519db3f886d895c,"Don't allow abbreviated argument options

Reviewed By: jmp84

Differential Revision: D15264847

fbshipit-source-id: 4ba9224d1b35c3de0d26c9b4c1ee6d641d3d8535",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
609,Jingfei Du,jingfeidu@fb.com,2019-05-08 17:40:33-07:00,93ec8d0bc692e9162e424e7d070f861e5efd2429,https://github.com/pytorch/fairseq/commit/93ec8d0bc692e9162e424e7d070f861e5efd2429,"expose arguments for bias_kv and zero_attn for masked_lm

Summary: the old no_bias_kv argument for masked_lm models are not used. Split it into 2 arguments and expose them.

Reviewed By: myleott

Differential Revision: D15266154

fbshipit-source-id: 60b041f8370ca1d8869ed3402fb9a67d1cd8e0e8",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
610,Myle Ott,myleott@fb.com,2019-05-09 06:34:46-07:00,8e8e1afcffe4b7201f38b09721f6d238f5d53b11,https://github.com/pytorch/fairseq/commit/8e8e1afcffe4b7201f38b09721f6d238f5d53b11,"Add sweep scripts

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/564

Differential Revision: D15278017

Pulled By: myleott

fbshipit-source-id: b6fba1b62145ea533b40f5eb9b134e6aa122e546",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
611,Myle Ott,myleott@fb.com,2019-05-09 06:46:45-07:00,2af922f141dd02d73bf8631165498c40a07ece25,https://github.com/pytorch/fairseq/commit/2af922f141dd02d73bf8631165498c40a07ece25,"Revert ""Add sweep scripts""

This reverts commit 8e8e1afcffe4b7201f38b09721f6d238f5d53b11.",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
612,Myle Ott,myleott@fb.com,2019-05-09 06:55:51-07:00,219cbf6e7d40083f1ff865b083fdeea54e6724af,https://github.com/pytorch/fairseq/commit/219cbf6e7d40083f1ff865b083fdeea54e6724af,Set initial learning rate in LR schedulers by calling step_update(0) at init,1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
613,myleott,myleott@fb.com,2019-05-10 06:06:15-07:00,47fbc4918f44fd152bc909d47c0815ca4cec2ac8,https://github.com/pytorch/fairseq/commit/47fbc4918f44fd152bc909d47c0815ca4cec2ac8,fbshipit-source-id: 682b375c6e7535f12faaf9ca32811051f9e874da,5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
614,Jay Mahadeokar,jaym@fb.com,2019-05-10 14:14:19-07:00,8a2e6e812309926b9d88f95f9d83c9a35bd98321,https://github.com/pytorch/fairseq/commit/8a2e6e812309926b9d88f95f9d83c9a35bd98321,"add option to specify lr-threshold while using lr-on-plateau strategy

Summary: As in title.

Reviewed By: skritika

Differential Revision: D15299135

fbshipit-source-id: 2fd513b32c0ab41911cdf0b0186f6c3bb5256285",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
615,Myle Ott,myleott@fb.com,2019-05-11 07:54:00-07:00,5dcc855ab298a79c22b1071bb8c2d4e1af7aa2b8,https://github.com/pytorch/fairseq/commit/5dcc855ab298a79c22b1071bb8c2d4e1af7aa2b8,"Add missing options to TransformerDecoderLayer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560

Differential Revision: D15260838

Pulled By: myleott

fbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
616,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-11 09:12:30-07:00,43722c5e2b3f9bc687c9cd19564d5159db767007,https://github.com/pytorch/fairseq/commit/43722c5e2b3f9bc687c9cd19564d5159db767007,"convert logits to fp32 for calculating loss in masked_lm_loss criterion

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/568

Differential Revision: D15308483

Pulled By: myleott

fbshipit-source-id: 9d898ce523e46e6b6fb444274f478da0b577b603",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
617,Myle Ott,myleott@fb.com,2019-05-12 16:28:51-07:00,287d31e210bea92ad09aa30db6243f08245c51f6,https://github.com/pytorch/fairseq/commit/287d31e210bea92ad09aa30db6243f08245c51f6,"Add scripts for working with txt files containing document boundaries

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/736

Differential Revision: D15314626

Pulled By: myleott

fbshipit-source-id: 1e0c32529afee57e43fe5d6c7991cd13eb8a52c4",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
618,zhiqiang,zhiqwang@outlook.com,2019-05-12 16:35:11-07:00,d0577ba7a544c52adb280c7c2adf8ea83b88c3c0,https://github.com/pytorch/fairseq/commit/d0577ba7a544c52adb280c7c2adf8ea83b88c3c0,"Fix option in docs (#735)

Summary:
`--output-format` -> `--dataset-impl` in Tutorial: Classifying Names with a Character-Level RNN
Pull Request resolved: https://github.com/pytorch/fairseq/pull/735

Differential Revision: D15314625

Pulled By: myleott

fbshipit-source-id: 65b8efd1a367ca754e5b9dca088aefbc648864dd",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
619,Myle Ott,myleott@fb.com,2019-05-13 07:42:58-07:00,b95f1b5dd0c3058229b5fd0d0741008a2f3c1e9e,https://github.com/pytorch/fairseq/commit/b95f1b5dd0c3058229b5fd0d0741008a2f3c1e9e,"Add LAMB optimizer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/572

Differential Revision: D15317928

Pulled By: myleott

fbshipit-source-id: b3f0e9229737a63b49937e7c5b918470f18ddc45",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
620,Myle Ott,myleott@fb.com,2019-05-13 07:48:11-07:00,72291287c8bedd868eaeb2cc9bb6a15134d1cdb5,https://github.com/pytorch/fairseq/commit/72291287c8bedd868eaeb2cc9bb6a15134d1cdb5,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/574

Differential Revision: D15317984

Pulled By: myleott

fbshipit-source-id: 09a66229cc6b4c95678ca1ca13c9e0da25b203de",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
621,Myle Ott,myleott@fb.com,2019-05-13 13:34:17-07:00,939ab6ae8688f1bec560a20e8bf7bac4712a3afc,https://github.com/pytorch/fairseq/commit/939ab6ae8688f1bec560a20e8bf7bac4712a3afc,"gelu_fast -> gelu_accurate (#571)

Summary:
This was named gelu_fast after the original implementation: https://github.com/hendrycks/GELUs/blob/master/mnist_ae.py#L62-L63

But in practice it's actually slower and uses more memory. Rename to gelu_accurate.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/571

Differential Revision: D15317874

Pulled By: myleott

fbshipit-source-id: c96fbc89bf91b27ced1ab8d5b25a8f23f922ec24",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
622,Myle Ott,myleott@fb.com,2019-05-13 13:34:26-07:00,c124d27248c7d3944f7db657b09675dd17911f94,https://github.com/pytorch/fairseq/commit/c124d27248c7d3944f7db657b09675dd17911f94,"Transition smoothly after warmup in polynomial LR decay schedule

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/576

Differential Revision: D15318086

Pulled By: myleott

fbshipit-source-id: c6587737ca7b97edc97ad4aef5c5c9ac7e92b2f2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
623,Dmytro Okhonko,oxo@fb.com,2019-05-14 12:53:05-07:00,cd1e5c09faf64ee835bd310bc7072861e5bb6db5,https://github.com/pytorch/fairseq/commit/cd1e5c09faf64ee835bd310bc7072861e5bb6db5,"Move save/load checkpoint functions to utils

Summary:
Move `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py
Move `get_perplexity` from train.py to utils.py.
This will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.

Reviewed By: myleott

Differential Revision: D15289607

fbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
624,Nayan Singhal,naysing@fb.com,2019-05-14 15:56:29-07:00,2c278ff0b2e485511022c4b14422c2abb3c464ff,https://github.com/pytorch/fairseq/commit/2c278ff0b2e485511022c4b14422c2abb3c464ff,"Alignment Training task using minibatch

Summary:
1. Define a EpochMinibatchIterator which extends the EpochBatchIterator. It has same functionality as EpochBatchIterator except two major changes: use static batching and use MiniBatchIterator for getting the indices.

2. SplitSeqCollater is used instead of Seq2SeqCollater.
3. LSTM_subsample started storing the previous states and reset it once the sample is over.

Reviewed By: jay-mahadeokar

Differential Revision: D15209023

fbshipit-source-id: 900b8bd1f25159ffc77f8106e26729a3e7422a1f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
625,Myle Ott,myleott@fb.com,2019-05-14 16:41:37-07:00,7432130eb0123f4e05bc781a2bbfaadac0198bf5,https://github.com/pytorch/fairseq/commit/7432130eb0123f4e05bc781a2bbfaadac0198bf5,"rm default_key from MultiCorpusSampledDataset

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/575

Differential Revision: D15318004

Pulled By: myleott

fbshipit-source-id: ad918d71b1bd8074decf5ec3463dd9bc9487bbe9",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
626,Myle Ott,myleott@fb.com,2019-05-14 17:56:44-07:00,bf106796399dac38a7d6da1870bb70f4998c9a5e,https://github.com/pytorch/fairseq/commit/bf106796399dac38a7d6da1870bb70f4998c9a5e,"Various fixes for Masked LM (#573)

Summary:
Various fixes for Masked LM

- use --activation-fn instead of --gelu
- use --dataset-impl instead of --lazy-load
- add embed_scale option to TransformerSentenceEncoder
- fix encoder_normalize_before to include a final layer norm
- delete BertLayerNorm
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/573

Reviewed By: borguz

Differential Revision: D15317933

Pulled By: myleott

fbshipit-source-id: 8ecb46556ad43e76e92d41ed8f5a62e8516fd375",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
627,Myle Ott,myleott@fb.com,2019-05-15 04:21:42-07:00,5277882750c8f574ca280c569a420a88cd91dcd6,https://github.com/pytorch/fairseq/commit/5277882750c8f574ca280c569a420a88cd91dcd6,"Add missing imports

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/579

Differential Revision: D15352058

Pulled By: myleott

fbshipit-source-id: cebef02edcfcb203ef2e32c64f7f28e08c4e46b0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
628,Myle Ott,myleott@fb.com,2019-05-15 04:39:26-07:00,a0c5f9b860a3c1dbfe35be2a6c8c8918900ca338,https://github.com/pytorch/fairseq/commit/a0c5f9b860a3c1dbfe35be2a6c8c8918900ca338,"Allow TransformerSentenceEncoder to return only last state

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/578

Differential Revision: D15352060

Pulled By: myleott

fbshipit-source-id: 7dc2fceca37ec96c89356662831b0d82f28bef6f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
629,Myle Ott,myleott@fb.com,2019-05-15 07:09:48-07:00,dffb167449cf0e3ead73b4a67e84f222311b0ff5,https://github.com/pytorch/fairseq/commit/dffb167449cf0e3ead73b4a67e84f222311b0ff5,"Updates to model API (#561)

Summary:
- `FairseqModel` -> `FairseqEncoderDecoderModel`
- add `FairseqDecoder.extract_features` and `FairseqDecoder.output_layer`
- `encoder_out_dict` -> `encoder_out`
- rm unused `remove_head` functions
- update docs
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/561

Differential Revision: D15271142

Pulled By: myleott

fbshipit-source-id: 8e8864e399336020f0271c780598e968ff51a264",16,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
630,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-15 11:16:00-07:00,d1d3a581a5c1d0e530c934137af89fdc832d637e,https://github.com/pytorch/fairseq/commit/d1d3a581a5c1d0e530c934137af89fdc832d637e,"added missing dense layers in masked lm model (#581)

Summary:
1) Added pooled_output for sentence classification as `Tanh(Linear())`.
2) Added lm_head_transform as `LayerNorm(GeLU(Linear(x)))`
3) `act_dropout = 0.0`
4) added `lm_output_learned_bias`
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/581

Reviewed By: borguz

Differential Revision: D15353575

Pulled By: borguz

fbshipit-source-id: 4ff64c6ceed23f3e99348f73d189546f1d84452e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
631,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-15 11:28:12-07:00,74c936dc28fc6e01b98ee91fb1a47b22ab8e8a86,https://github.com/pytorch/fairseq/commit/74c936dc28fc6e01b98ee91fb1a47b22ab8e8a86,"added shuffle as arg for masked_lm for experimenting with pad effecie… (#582)

Summary:
added shuffle as arg for masked_lm for experimenting with pad effecient batching
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/582

Reviewed By: jingfeidu

Differential Revision: D15355105

Pulled By: jingfeidu

fbshipit-source-id: 9925271a0bc2f9d283f354d158bd4b5ec8788b39",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
632,Ruty Rinott,ruty@fb.com,2019-05-15 14:21:00-07:00,2a3adcdc033998a0ae1b84acd0f2a4f5c34ee93c,https://github.com/pytorch/fairseq/commit/2a3adcdc033998a0ae1b84acd0f2a4f5c34ee93c,"Fix biTransformer export (#583)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/583

D14610694 fixed issues in layerNorm exporting by making it conditional.  D15260838 changed the implementation of TransformerDecoderLayer to the one under transformer, thus losing the fix. Bringing it back here.

Reviewed By: myleott, geof90, liaimi

Differential Revision: D15357119

fbshipit-source-id: e29e053ca5beca0008d7a8dad9880a483a14c7b9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
633,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-05-15 18:10:03-07:00,861dd2b7aab577295acd904e9ac27e53afbc1034,https://github.com/pytorch/fairseq/commit/861dd2b7aab577295acd904e9ac27e53afbc1034,"fixed cmd arg for shuffle dataset masked lm task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/584

Reviewed By: myleott

Differential Revision: D15360774

Pulled By: myleott

fbshipit-source-id: b18efbb6ff5a8832c61b689f3d87c958cbd908e9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
634,Peng-Jen Chen,pipibjc@fb.com,2019-05-15 19:14:02-07:00,0863ea68b88ae72d2d0e2098d7da4f75d709ab0a,https://github.com/pytorch/fairseq/commit/0863ea68b88ae72d2d0e2098d7da4f75d709ab0a,"Add multi-dataset loading to multilingual_translation

Summary: Similar to TranslationTask, we want to enable multilingual translation task to be able to load 'train{k}' datasets from data-bin folder.

Reviewed By: lematt1991

Differential Revision: D15363481

fbshipit-source-id: 5fed7be19383023b792ed2fd38e655cbcecc8b90",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
635,Myle Ott,myleott@fb.com,2019-05-16 06:37:29-07:00,e797f63316a699aceb47f896da2f89588ba15667,https://github.com/pytorch/fairseq/commit/e797f63316a699aceb47f896da2f89588ba15667,"Cleanup rm_pt.py script

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/585

Differential Revision: D15372416

fbshipit-source-id: add226a4558ae4d84dd261e9317b80c43970f771",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
636,Myle Ott,myleott@fb.com,2019-05-16 07:47:23-07:00,e2a0b87d0c16e4f0dcd40d69b9ab748632e6ae22,https://github.com/pytorch/fairseq/commit/e2a0b87d0c16e4f0dcd40d69b9ab748632e6ae22,"Back out ""reduce memory footprint for average_checkpoints"" (#743)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/743

Original commit changeset: 0afe37c9a031

According to edunov: ""We need to be careful here with shared parameters, I believe right now it is broken if you have shared encoder/decoder input embeddings (encoder.embed_tokens.weight and decoder.embed_tokens.weight) as they get updated several times""

We also have OSS issues that look related, e.g., https://github.com/pytorch/fairseq/issues/732.

Backing this out until we can confirm the correct behavior for shared params.

Differential Revision: D15372673

fbshipit-source-id: 8683c0f2514e21fa1e9d2fe6dfc48d98957a2831",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
637,Jingfei Du,jingfeidu@fb.com,2019-05-16 14:45:43-07:00,fca32e0565dd488a05f8957d2ba0f4c76c9db8c6,https://github.com/pytorch/fairseq/commit/fca32e0565dd488a05f8957d2ba0f4c76c9db8c6,"fixed bugs of masked_lm for fine-tuning (#744)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/744

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/587

After we added additional prediciton layers for language model predictions. The fine-tuning is broken because of 2 reasons.
1. checkpoint cannot be loaded since we didn't update state_dict names
2. lm_output_learned_bias is not initialize if load_softmax is false

Reviewed By: myleott

Differential Revision: D15377380

fbshipit-source-id: d58544b1d2c549586abef42fec19ec8bf27a994a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
638,Myle Ott,myleott@fb.com,2019-05-16 21:00:17-07:00,3bfbb49ba5a8cba98a34496f81b70248a33664f0,https://github.com/pytorch/fairseq/commit/3bfbb49ba5a8cba98a34496f81b70248a33664f0,"Clean up sharded train iterator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586

Differential Revision: D15372949

Pulled By: myleott

fbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da",6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
639,Myle Ott,myleott@fb.com,2019-05-17 14:23:02-07:00,ba989ed136487427e4eaea8e2707476ea95cc676,https://github.com/pytorch/fairseq/commit/ba989ed136487427e4eaea8e2707476ea95cc676,"Small features + lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588

Differential Revision: D15389638

Pulled By: myleott

fbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
640,Kartikay Khandelwal,kartikayk@fb.com,2019-05-18 16:59:47-07:00,e265c2396f6865e2aa888cecc4e69c5068237550,https://github.com/pytorch/fairseq/commit/e265c2396f6865e2aa888cecc4e69c5068237550,"Make Fairseq compatible with pre-computed position tensors (#570)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/570

Pull Request resolved: https://github.com/pytorch/fairseq/pull/731

Currently the LearnedPositionalEmbedding module computes the position tensor based on the input data. However this really doesnt work for XLM where we have different behavior based on the Masked LM and Translation LM. In this diff I keep the same default behavior for LearnedPositionalEmbedding as before but add the ability for these models to work with pre-computed position tensors.

Reviewed By: myleott

Differential Revision: D15305474

fbshipit-source-id: de7d908245a2a620b58d36055211600a08f2d1dc",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
641,Myle Ott,myleott@fb.com,2019-05-20 10:41:30-07:00,5aebd0963546410be09ecf89a19eb05827f031d6,https://github.com/pytorch/fairseq/commit/5aebd0963546410be09ecf89a19eb05827f031d6,"Fix for tasks that don't define args.data

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/591

Differential Revision: D15415490

Pulled By: myleott

fbshipit-source-id: c45df5f3b5327911e2c9b11642e7da2e8bb835dc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
642,Ning Dong,dnn@fb.com,2019-05-20 11:24:41-07:00,ee28411f768fc1a654ce9a5b15cf19227d40b5cc,https://github.com/pytorch/fairseq/commit/ee28411f768fc1a654ce9a5b15cf19227d40b5cc,"Make ConcatDataset work in PytorchTranslateTask multi-path dataset loading (#730)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/730

Pull Request resolved: https://github.com/pytorch/translate/pull/528

Add/modify necessary functions for ConcatDataset to work in PytorchTranslateTask and replace MultiCorpusSampledDataset which doesn't support mixed batch.

Any idea on how to implement collater here for mixed batch? Now I'm just using the collater of the first dataset.

Reviewed By: liezl200

Differential Revision: D15260872

fbshipit-source-id: 14b148c506e9f8ebf4fe60a49f95444d4123d76f",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestConcatDataset(unittest.TestCase):'],[],['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
643,Jingfei Du,jingfeidu@fb.com,2019-05-20 12:05:48-07:00,4fac3b60f9800b78036cbf69c9748da56e68610e,https://github.com/pytorch/fairseq/commit/4fac3b60f9800b78036cbf69c9748da56e68610e,"fix bug for masking (#752)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/752

previously we sample masked tokens with replace=True (default). Because of this, we would mask same tokens multiple times, which will make us mask less tokens finally

Reviewed By: liaimi

Differential Revision: D15403556

fbshipit-source-id: cf12eeb13f9610431136a345de9199ad0292984b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
644,Myle Ott,myleott@fb.com,2019-05-20 15:05:49-07:00,b71f8f450e68e3eb18c71f204d21201c7c3087aa,https://github.com/pytorch/fairseq/commit/b71f8f450e68e3eb18c71f204d21201c7c3087aa,"Add --disable-validation

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/592

Differential Revision: D15415499

Pulled By: myleott

fbshipit-source-id: 87ba09b9b38501daebd95bbf28815e048c78f9a3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
645,Myle Ott,myleott@fb.com,2019-05-21 12:16:45-07:00,d10fe896b6801628f765c6e53c357572f58ed98e,https://github.com/pytorch/fairseq/commit/d10fe896b6801628f765c6e53c357572f58ed98e,"Add compare_namespaces.py helper

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/597

Differential Revision: D15432965

Pulled By: myleott

fbshipit-source-id: 4471a2a8bb468bb639a80f977ab4c20480acb461",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
646,Myle Ott,myleott@fb.com,2019-05-21 15:38:35-07:00,ef62ec0a3180fa0f977d003ce851549607fa8253,https://github.com/pytorch/fairseq/commit/ef62ec0a3180fa0f977d003ce851549607fa8253,"Add missing LM options

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/596

Differential Revision: D15432359

Pulled By: myleott

fbshipit-source-id: ebfdf0031864c3c88357543c0202ba0bd65a7b90",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
647,Myle Ott,myleott@fb.com,2019-05-21 15:40:52-07:00,4604b4a5303fa38df0b2006025be32b758a0a7f1,https://github.com/pytorch/fairseq/commit/4604b4a5303fa38df0b2006025be32b758a0a7f1,"Don't load training set twice

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/595

Differential Revision: D15428242

Pulled By: myleott

fbshipit-source-id: 3cec83a2353498a4802398eba8bcb1aefaf6d5c4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
648,zhiqiang,zhiqwang@outlook.com,2019-05-22 10:35:03-07:00,886ef6bc7f027faa0ca0e1e7f9446341ed448b77,https://github.com/pytorch/fairseq/commit/886ef6bc7f027faa0ca0e1e7f9446341ed448b77,"Remove duplicate code (#754)

Summary:
Remove duplicate definition of PositionalEmbedding in `lightconv.py`
Pull Request resolved: https://github.com/pytorch/fairseq/pull/754

Differential Revision: D15451443

Pulled By: myleott

fbshipit-source-id: a3d82ab2c1335d66be3c5d67a07893162d138c7a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
649,Matt Le,mattle@fb.com,2019-05-22 13:14:04-07:00,c11aaf1485f01a97ba7a4667940793a0824c3100,https://github.com/pytorch/fairseq/commit/c11aaf1485f01a97ba7a4667940793a0824c3100,"Fix semisupervised translation

Summary: Fixes semisupervised translation task to deal with change in order of data loading and model creation (D15428242).  When we build the model, we create the backtranslation function, which we can then pass in to the constructor of BacktranslationDataset

Reviewed By: myleott

Differential Revision: D15455420

fbshipit-source-id: 95101ca92f8af33702be3416147edd98da135a20",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
650,Kritika Singh,skritika@fb.com,2019-05-22 23:15:44-07:00,72a5487c1a75a470d19023535936e80b12dbe40e,https://github.com/pytorch/fairseq/commit/72a5487c1a75a470d19023535936e80b12dbe40e,"Allow unused params in distributed training

Summary:
Context from https://fb.workplace.com/groups/1405155842844877/permalink/2785095451517569/:

I am adding a model to pyspeech (formerly fairspeq) with the following `forward`:
```
def forward(self, src_tokens, src_lengths, prev_output_tokens, name):
    encoder_out = self.encoder(src_tokens, src_lengths)
    if name == Dataset.d1:
        decoder_out = self.decoder1(prev_output_tokens, encoder_out)
    elif name == Dataset.d2:
        decoder_out = self.decoder2(encoder_out)
    return decoder_out
```
When I run distributed training on this model, I get the following error:

```
RuntimeError: Expected to have finished reduction in the prior iteration before starting a
new one. This error indicates that your module has parameters that were not used in
producing loss. You can enable unused parameter detection by (1) passing the keyword
argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2)
making sure all `forward` function outputs participate in calculating loss. If you already have
done the above two steps, then the distributed data parallel module wasn't able to locate the
output tensors in the return value of your module's `forward` function. Please include the loss
function and the structure of the return value of `forward` of your module when reporting this
issue (e.g. list, dict, iterable). (prepare_for_backward at
caffe2/torch/csrc/distributed/c10d/reducer.cpp:410)
```

The recommended fix is to pass find_unused_parameters=True to DistributedDataParallel's initialization

Reviewed By: myleott

Differential Revision: D15439726

fbshipit-source-id: 7fd80d4a3f49ac90182dec723b49b14e6689406a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
651,Myle Ott,myleott@fb.com,2019-05-23 07:02:51-07:00,128f4beafdc255dd7190e7cbb5efd19b56dcb4fe,https://github.com/pytorch/fairseq/commit/128f4beafdc255dd7190e7cbb5efd19b56dcb4fe,"Fix gating for find_unused_parameters

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/600

Differential Revision: D15469322

Pulled By: myleott

fbshipit-source-id: fdefa8efbb10e48b2a04a6bc10404fd2f3f21ecf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
652,Jason Fried,fried@fb.com,2019-05-23 10:59:57-07:00,6b3a516f508383b75b0c69c12b7aaff2edf25386,https://github.com/pytorch/fairseq/commit/6b3a516f508383b75b0c69c12b7aaff2edf25386,"collections.abc python 3.8

Summary:
In python 3.7 collections.abc warns when importing abc classes from `collections`
In 3.8 this will not work at all.

This changes all code using abc's from collections to attempt to import from `collections.abc`

I am not fixing existing lint's don't ask, if `arc lint` auto-fixed I accepted, except for spelling in code.

Reviewed By: lisroach

Differential Revision: D15461049

fbshipit-source-id: ac2bf2ec8cffacd8ba5572882b0832bbf99a1646",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
653,Jingfei Du,jingfeidu@fb.com,2019-05-24 10:55:37-07:00,6b0cce849a2d7bdbb940f86087c4d7c740a92c1e,https://github.com/pytorch/fairseq/commit/6b0cce849a2d7bdbb940f86087c4d7c740a92c1e,"fix bug for masking prob (#758)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/758

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/603

fixed a typo for _mask_block of mlm. This typo will make we never set masked token as random token, which should take 10% of the masked tokens.

Reviewed By: akinh

Differential Revision: D15492315

fbshipit-source-id: 1e03dc862e23a6543e51d7401c74608d366ba62d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
654,Yongqiang Wang,yqw@fb.com,2019-05-24 12:09:18-07:00,8ce2c35d8e2dfb2b6dd220058710f81df5eb5729,https://github.com/pytorch/fairseq/commit/8ce2c35d8e2dfb2b6dd220058710f81df5eb5729,"Implement reducing footprint of average checkpoint correctly (#747)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/747

In https://github.com/pytorch/fairseq/pull/647, checkpoint averaging
is not Implemented correctly when it comes to shared parameters. This diff
has the right Implementation and a test case to guard future change.

Reviewed By: myleott

Differential Revision: D15402943

fbshipit-source-id: 8004836d5c2571814ea54844650618008a9ee522",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('True', '('), ('True', '('), ('True', '(')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
655,Myle Ott,myleott@fb.com,2019-05-28 11:49:58-07:00,65f464739dbb610a3e731de4a5d7c0e8bc26ffc3,https://github.com/pytorch/fairseq/commit/65f464739dbb610a3e731de4a5d7c0e8bc26ffc3,"Add --sentence-bleu option to score.py

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/605

Differential Revision: D15518167

Pulled By: myleott

fbshipit-source-id: 8b0e6b32adff018136d0d251b7fde3818e373d6f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
656,Kartikay Khandelwal,kartikayk@fb.com,2019-05-29 12:42:05-07:00,4e9ecb800f74e0e72a5986af41b89e45e3213484,https://github.com/pytorch/fairseq/commit/4e9ecb800f74e0e72a5986af41b89e45e3213484,"Make XLM torchscipt Export-able (#765)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/765

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/614

This diff has changes needed to make XLM torchscript exportable.

Reviewed By: bethebunny

Differential Revision: D15497208

fbshipit-source-id: fd9645119e154e3c397f147acf9144d661d9a5c8",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
657,Spencer Poff,spoff@fb.com,2019-05-29 13:28:11-07:00,ed592ab5347fab3826893aef10edddc461932ace,https://github.com/pytorch/fairseq/commit/ed592ab5347fab3826893aef10edddc461932ace,"making it easier to use transformer_lm model with new tasks

Summary:
There were two non-obvious errors I ran into while creating a new language modeling task:
- `transformer_lm` implicitly required the `tokens_per_sample` arg
- `transformer_lm` assumed the task had a `dictionary` and `output_dictionary` property, neither of which are specified in the FairseqTask interface

Reviewed By: myleott

Differential Revision: D15532345

fbshipit-source-id: 200d7d3b542c35f17cc2d6bca4219c4a4d17cb6b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
658,Myle Ott,myleott@fb.com,2019-05-29 14:03:20-07:00,3e472b2268fdee10461f7eafbeb6d07a1d313b5e,https://github.com/pytorch/fairseq/commit/3e472b2268fdee10461f7eafbeb6d07a1d313b5e,"rm BertLayerNorm

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/608

Differential Revision: D15541220

Pulled By: myleott

fbshipit-source-id: 52a8e4da72cc6e3e25cf98c989d34a269d614c9d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
659,Myle Ott,myleott@fb.com,2019-05-29 14:03:28-07:00,977e36e56aa9be4b114a85de90df52378df2c3dd,https://github.com/pytorch/fairseq/commit/977e36e56aa9be4b114a85de90df52378df2c3dd,"Support multiple seeds in data_utils.numpy_seed

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/610

Differential Revision: D15541261

Pulled By: myleott

fbshipit-source-id: f0b823cf4f04c5ef3205f6d259c6dcad4cc329b1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
660,Myle Ott,myleott@fb.com,2019-05-29 14:04:36-07:00,c97978a24f2feba21f1e10fa807ff6ee371a2221,https://github.com/pytorch/fairseq/commit/c97978a24f2feba21f1e10fa807ff6ee371a2221,"Fix warmup for polynomial decay schedule

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/611

Differential Revision: D15541303

Pulled By: myleott

fbshipit-source-id: 279ca813437c834fca49576a48b75cbf1fdf0e76",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
661,Myle Ott,myleott@fb.com,2019-05-29 14:06:02-07:00,b18a3126dfe3be65d7ef111fda11a78c015854bc,https://github.com/pytorch/fairseq/commit/b18a3126dfe3be65d7ef111fda11a78c015854bc,"Faster masking in MultiheadAttention

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/612

Differential Revision: D15541377

Pulled By: myleott

fbshipit-source-id: 4762516a3b545d03bc81d3660f47827e15466dce",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
662,Zhanghao Wu,zhanghao.wu@outlook.com,2019-05-29 16:27:17-07:00,497d972ed2db73bd9931adcac40cecc2d14bfe4f,https://github.com/pytorch/fairseq/commit/497d972ed2db73bd9931adcac40cecc2d14bfe4f,"Fix Tensorboard Init (#763)

Summary:
Fix the mismatching between the parameter fed into `SummaryWriter` and the API of the latest [tensorboardX](https://github.com/lanpa/tensorboardX/blob/3e35c9b5f85e8ceb0294532d9eb772341a04c097/tensorboardX/writer.py#L192), i.e. ""log_dir"" -> ""logdir"".
Pull Request resolved: https://github.com/pytorch/fairseq/pull/763

Differential Revision: D15547192

Pulled By: myleott

fbshipit-source-id: c51b88da5ec589fb8ca5b4876bc229efeb7bf494",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
663,lukovnikov,lukovnikov@outlook.com,2019-05-29 17:23:54-07:00,dd0dc54ce6f26d1e312a3270039387a8a7e755c9,https://github.com/pytorch/fairseq/commit/dd0dc54ce6f26d1e312a3270039387a8a7e755c9,"device error in SinusoidalPositionalEmbedding (#746)

Summary:
Not sure if I'm doing something wrong elsewhere, but I had a device error in `SinusoidalPositionalEmbedding` when running on GPU > 0 because the weights were on a different device than the input.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/746

Differential Revision: D15547217

Pulled By: myleott

fbshipit-source-id: 37849d895ce483c14615fdb4ace8a8c4fb05b568",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
664,Sujit Verma,sujitv@devfair020.maas,2019-05-29 19:32:44-07:00,47313d85e8433ee5478a4c58fbb9df9c8882592e,https://github.com/pytorch/fairseq/commit/47313d85e8433ee5478a4c58fbb9df9c8882592e,"Added support for plotting scalars through palaas tbwriter interface. (#580)

Summary: Changes for supporting tensorboard scalar plotting.

Reviewed By: myleott

Differential Revision: D15456534

Pulled By: myleott

fbshipit-source-id: a012a4eea028aae764ce11786570b7d96841c4a5",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
665,Myle Ott,myleott@fb.com,2019-05-30 05:37:12-07:00,9770f36772a0edea3614a88dbbf4bc23a94088de,https://github.com/pytorch/fairseq/commit/9770f36772a0edea3614a88dbbf4bc23a94088de,"Fix PyTorch deprecation warnings

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/618

Differential Revision: D15552599

Pulled By: myleott

fbshipit-source-id: 2192a30a9c5af31b954a3a1716166dd6ba27b23a",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
666,Myle Ott,myleott@fb.com,2019-05-30 11:38:20-07:00,ffc3bb5806c8b06ff299c85063f7728f6ec3c733,https://github.com/pytorch/fairseq/commit/ffc3bb5806c8b06ff299c85063f7728f6ec3c733,"Add --reset-dataloader

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613

Differential Revision: D15541384

Pulled By: myleott

fbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
667,Khoa Ho,25312735+khoa-ho@users.noreply.github.com,2019-05-30 12:02:58-07:00,d5f76d7446537fc7d6252c4b1ea5836e02647948,https://github.com/pytorch/fairseq/commit/d5f76d7446537fc7d6252c4b1ea5836e02647948,"Clarify mixed precision training support (#766)

Summary:
Change the wording to avoid confusion. Mixed precision ensures both higher arithmetic throughput and numerical stability, not exactly synonymous to pure half-precision/FP16 training. Also add mentioning of tensor cores since older generation GPUs without tensor cores don't support true mixed precision training.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/766

Differential Revision: D15559565

Pulled By: myleott

fbshipit-source-id: c71e720772657bb3e8ad330b58bf69e23beb614e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
668,Myle Ott,myleott@fb.com,2019-05-30 13:33:08-07:00,75cc8821949430e893e0783bf4d392fa4ef6b889,https://github.com/pytorch/fairseq/commit/75cc8821949430e893e0783bf4d392fa4ef6b889,"Update MoE README

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/619

Differential Revision: D15562983

Pulled By: myleott

fbshipit-source-id: 9240f56f18c87120b7d38e0db374d24a55999395",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
669,Myle Ott,myleott@fb.com,2019-05-30 16:43:51-07:00,38e82904d8149b27a9961539b204544aa354abb3,https://github.com/pytorch/fairseq/commit/38e82904d8149b27a9961539b204544aa354abb3,"Update --memory-efficient-fp16 to work with c10d DDP

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/617

Differential Revision: D15555328

Pulled By: myleott

fbshipit-source-id: 35d1f329f887cb0b867c7a22f17a16f3c9c66815",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
670,Myle Ott,myleott@fb.com,2019-05-30 21:17:42-07:00,8ca05802d084954bb9df9af91bd602b49d99f68a,https://github.com/pytorch/fairseq/commit/8ca05802d084954bb9df9af91bd602b49d99f68a,"Replace --decoder-final-norm with --no-decoder-final-norm

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/620

Differential Revision: D15569440

Pulled By: myleott

fbshipit-source-id: c4681f1c72467c04cd2654e87bc724c94b76e3fb",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
671,Myle Ott,myleott@fb.com,2019-05-31 18:27:45-07:00,8c03ff2ddc0df95094cf716146d3fcdbd8a17b00,https://github.com/pytorch/fairseq/commit/8c03ff2ddc0df95094cf716146d3fcdbd8a17b00,"Fix positions for LM

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/622

Differential Revision: D15572555

Pulled By: myleott

fbshipit-source-id: 2b81f22207b4c894ffe645af0b45c70ac0a80612",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
672,Myle Ott,myleott@fb.com,2019-06-02 07:26:44-07:00,6a21b232a71d943ce67732982a4fb085cb55c3cb,https://github.com/pytorch/fairseq/commit/6a21b232a71d943ce67732982a4fb085cb55c3cb,"Backward compatibility + updated links for pretrained language models

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/624

Differential Revision: D15595746

Pulled By: myleott

fbshipit-source-id: b79e489de9ff37ee7cbf939092a6e5ec0dbebbf5",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
673,Myle Ott,myleott@fb.com,2019-06-02 07:51:46-07:00,b35d9bcac47516a99e7cf34a2062ba28cdbf76d3,https://github.com/pytorch/fairseq/commit/b35d9bcac47516a99e7cf34a2062ba28cdbf76d3,"Fix rearranging of encoder_out in SequenceGenerator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/625

Differential Revision: D15595787

Pulled By: myleott

fbshipit-source-id: ba6edf305ed41be392194f492e034dd66d1743fe",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
674,Nathan Ng,nng@devfair0226.h2.fair,2019-06-03 08:39:19-07:00,a2aed890324284e7ee53f042282529453f745b95,https://github.com/pytorch/fairseq/commit/a2aed890324284e7ee53f042282529453f745b95,"Torch hub

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/621

Differential Revision: D15571435

Pulled By: myleott

fbshipit-source-id: 67d25b00c8c1bc69dbffd8521da56f7cc14eb75e",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
675,Haoran Li,aimeeli@fb.com,2019-06-03 12:33:25-07:00,dc028c5289ff33c31a6c05cbfa4199dc2409c3d9,https://github.com/pytorch/fairseq/commit/dc028c5289ff33c31a6c05cbfa4199dc2409c3d9,"fix masked_lm for loading in pytext

Summary: lm_output_learned_bias doesn't exist when loading the model for fine-tuning

Reviewed By: jingfeidu

Differential Revision: D15579190

fbshipit-source-id: 45e8e193399943c89b77cc553d3d6d49b056e55a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
676,Biao Lu,biaolu@fb.com,2019-06-03 23:57:57-07:00,4ed5abc9e355ff22a971372d365f0bc27f2d941f,https://github.com/pytorch/fairseq/commit/4ed5abc9e355ff22a971372d365f0bc27f2d941f,"Adding masked_lm_dictionary to pytorch_translate (#630)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/630

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/629

Pull Request resolved: https://github.com/pytorch/translate/pull/562

Pull Request resolved: https://github.com/pytorch/fairseq/pull/774

forked masked_lm_dictionary from fairseq
changed import in pytorch_translate to use the new masked_lm_dictionary
registered cooresponding tasks

Reviewed By: liezl200

Differential Revision: D15410352

fbshipit-source-id: 06516caabdd4dc5cdee9ad1d8025978f4eea6c4b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
677,lematt1991,lematt1991@gmail.com,2019-06-04 07:13:17-07:00,b1dd40cf970fa43680b7fe594bf7ec8361749512,https://github.com/pytorch/fairseq/commit/b1dd40cf970fa43680b7fe594bf7ec8361749512,"Remove overridden inverse_sqrt lr scheduler in dynamic conv example (#769)

Summary:
Resolves #768
Pull Request resolved: https://github.com/pytorch/fairseq/pull/769

Differential Revision: D15621841

Pulled By: lematt1991

fbshipit-source-id: 694effe3788ff7d04864217d673608ec31da589e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
678,lematt1991,lematt1991@gmail.com,2019-06-04 14:24:50-07:00,0d636744d0ec484c715944e9acb5d0824fb90ca6,https://github.com/pytorch/fairseq/commit/0d636744d0ec484c715944e9acb5d0824fb90ca6,"Fixing xlm example docts (#776)

Summary:
Resolves #762
Pull Request resolved: https://github.com/pytorch/fairseq/pull/776

Differential Revision: D15631503

Pulled By: lematt1991

fbshipit-source-id: 103f77d553476917b8b0f8001767217fb311d920",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
679,Matt Le,mattle@fb.com,2019-06-04 15:30:49-07:00,5408bc0821144235e15f59ed7de1ca8db0e297f9,https://github.com/pytorch/fairseq/commit/5408bc0821144235e15f59ed7de1ca8db0e297f9,"Fix loading XLM pretraining

Summary: We never actually load the model parameters from an XLM model when using tranformer_from_pretrained_xlm.  Also, change encoder_learned_pos from True -> False

Reviewed By: liezl200

Differential Revision: D15629061

fbshipit-source-id: 759eadc88041eae94505477960de57dd78a99dcb",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
680,Matt Le,mattle@fb.com,2019-06-06 07:35:25-07:00,fa7791df9af91b43ee8a485da85066376ee720c8,https://github.com/pytorch/fairseq/commit/fa7791df9af91b43ee8a485da85066376ee720c8,"Change encoder_learned_pos default back to True for xlm_base

Reviewed By: pipibjc

Differential Revision: D15635402

fbshipit-source-id: e92fab914de40775d7bad851420355240d822bde",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
681,Ning Dong,dnn@fb.com,2019-06-06 19:39:41-07:00,1ca075a27f46dd0e06f2c1433ef1acebac086d45,https://github.com/pytorch/fairseq/commit/1ca075a27f46dd0e06f2c1433ef1acebac086d45,"Replace unknown word by original source word when empty string is given (#770)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/770

Without this change comment here https://fburl.com/w1cejgw9 is inconsistent with the implementation.

Reviewed By: xianxl

Differential Revision: D15582826

fbshipit-source-id: 16d8368560153b251beed8b290f51fcdd8a8faee",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
682,freewym,freewym@gmail.com,2019-06-10 10:48:05-07:00,a58c1127f1d609dd9dc0b29a1efc0c517fcf3a16,https://github.com/pytorch/fairseq/commit/a58c1127f1d609dd9dc0b29a1efc0c517fcf3a16,"fix log printing in progress bar (#778)

Summary:
In the current progress bar, the counter for log_interval will always start from 0, which is not correct if  reloading from a checkpoint in the middle of an epoch. This fix obtains the offset from the iterator to set the counter correctly.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/778

Differential Revision: D15739953

Pulled By: myleott

fbshipit-source-id: a1d13403ec5783b22e01d7cb63874fd8dea7f8b0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
683,Myle Ott,myleott@fb.com,2019-06-10 10:50:03-07:00,4868c1826a720dace6c95f29a526712d06d7d8d9,https://github.com/pytorch/fairseq/commit/4868c1826a720dace6c95f29a526712d06d7d8d9,"More generator features for demo (#791)

Summary:
- make it possible to load file_utils.py without the dependencies
- add some more demo features
Pull Request resolved: https://github.com/pytorch/fairseq/pull/791

Differential Revision: D15739950

Pulled By: myleott

fbshipit-source-id: 38df5209973a6fe2e3651575b97134e096aaf5bf",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
684,Sergey Edunov,edunov@fb.com,2019-06-10 20:51:29-07:00,ee8bcb17144773b78366858b707ac5421dc2830d,https://github.com/pytorch/fairseq/commit/ee8bcb17144773b78366858b707ac5421dc2830d,"Fix of MHA for TPUs (#636)

Summary:
Multi-Head attention is currently not TPU-friendly, specifically .data_ptr() is not supported and should not be used. Also there are potential issues with correctness of existing code (e.g. data_ptr() can point to the same storage for different tensors).  Rather than rely on data_ptr() we should explicitly set self_attention or encoder_decoder_attention flags.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/636

Reviewed By: myleott

Differential Revision: D15709898

Pulled By: edunov

fbshipit-source-id: f931713193c51be848a5de20da730ac3a3ce0187",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
685,yilinyang7,yilinyang721@gmail.com,2019-06-11 03:20:04-07:00,9dc9a486c5936464a73d9fae4f3b8ad46d2813ba,https://github.com/pytorch/fairseq/commit/9dc9a486c5936464a73d9fae4f3b8ad46d2813ba,"when given prefix_tokens, sequence generator would generate (exactly) same finished candidates (#713)

Summary:
https://github.com/pytorch/fairseq/issues/712
Pull Request resolved: https://github.com/pytorch/fairseq/pull/713

Differential Revision: D15242432

Pulled By: myleott

fbshipit-source-id: a230ee48f4bf891c805609c428d7233a0ad21179",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
686,Myle Ott,myleott@fb.com,2019-06-11 03:21:25-07:00,9b40999eece2201a7855307c1dc85e1f4cf07c0d,https://github.com/pytorch/fairseq/commit/9b40999eece2201a7855307c1dc85e1f4cf07c0d,"Add generic registry mechanism

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/792

Differential Revision: D15741781

Pulled By: myleott

fbshipit-source-id: c256c7900c307d485904e69b1526b9acbe08fec9",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
687,Bairen Yi,yibairen.byron@bytedance.com,2019-06-11 04:06:52-07:00,a8f28ecb63ee01c33ea9f6986102136743d47ec2,https://github.com/pytorch/fairseq/commit/a8f28ecb63ee01c33ea9f6986102136743d47ec2,"Python3.5 compat (#794)

Summary:
See #467. Ping myleott to review.

This is a work-related contribution. Ping lark to review.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/794

Differential Revision: D15756816

Pulled By: myleott

fbshipit-source-id: 6dce3ff3a713bf5f60e5782bc260b2ca9d2c0a9b",9,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
688,Myle Ott,myleott@fb.com,2019-06-11 09:41:50-07:00,1b937bb2fe0e9ba3857d498aad28c6f10b47942b,https://github.com/pytorch/fairseq/commit/1b937bb2fe0e9ba3857d498aad28c6f10b47942b,"Add exception for bsz=1 with prefix generation (#796)

Summary:
This is a temporary workaround to support sampling after https://github.com/pytorch/fairseq/issues/713. We'll need to revisit this to support sampling and beam more generally.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/796

Differential Revision: D15760808

Pulled By: myleott

fbshipit-source-id: ecaf4f161b0c30de037f32007e4610a559a49230",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
689,Myle Ott,myleott@fb.com,2019-06-11 11:16:48-07:00,eea4d20b65bd619670549712a4036a02f3cd43c8,https://github.com/pytorch/fairseq/commit/eea4d20b65bd619670549712a4036a02f3cd43c8,"Automatically fill in default values from add_args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797

Differential Revision: D15761071

Pulled By: myleott

fbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
690,Myle Ott,myleott@fb.com,2019-06-11 14:57:39-07:00,5bdee18e968ae6ceff079a65f77fcba2d5128663,https://github.com/pytorch/fairseq/commit/5bdee18e968ae6ceff079a65f77fcba2d5128663,"Iterate on torch.hub interface

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/793

Differential Revision: D15758755

Pulled By: myleott

fbshipit-source-id: b93e4ac11bde36a0b59b4d6d1c84d31c3124d767",21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
691,Myle Ott,myleott@fb.com,2019-06-11 18:18:41-07:00,37df862ea5b99d1097005293b1bda8b8d710c868,https://github.com/pytorch/fairseq/commit/37df862ea5b99d1097005293b1bda8b8d710c868,"Add missing dependencies to hubconf

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/799

Differential Revision: D15773932

Pulled By: myleott

fbshipit-source-id: 650c0621bedb3b7ecebc0654d8e10d7692c50994",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
692,Myle Ott,myleott@fb.com,2019-06-12 10:20:33-07:00,78c2fcf0879cede3e61c75babf4b1396dd349aff,https://github.com/pytorch/fairseq/commit/78c2fcf0879cede3e61c75babf4b1396dd349aff,"Add more torch.hub deps

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/801

Differential Revision: D15781975

Pulled By: myleott

fbshipit-source-id: b86276cd3a40138c09494637c43ce52a56c4aced",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
693,Nayan Singhal,naysing@fb.com,2019-06-12 15:24:47-07:00,6982c404c2e2f30566a6304bf40d8033bf29a645,https://github.com/pytorch/fairseq/commit/6982c404c2e2f30566a6304bf40d8033bf29a645,"Add Model Averaging

Summary:
Implemented model averaging for fairseq.
Removed the ddp wrapper if global optimizer is provided.
Syncing all the models based on the iteration provide in the input

TODO:
1) Fix throughput and wps meter. Need to check other meters too.
2) Replace Model average code with BMUF algorithm implementation.

Reviewed By: myleott

Differential Revision: D15711044

fbshipit-source-id: 58a4af74db2a61d06762597b95836cbeb1ed82cc",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
694,Myle Ott,myleott@fb.com,2019-06-12 19:17:36-07:00,6d1233fa2f0711cb715a3f1466a5efc19732c9f1,https://github.com/pytorch/fairseq/commit/6d1233fa2f0711cb715a3f1466a5efc19732c9f1,"Switch to gzip for large WMT'18 ensemble (#803)

Summary:
It's so much faster to extract (3 minutes instead of 20).
Pull Request resolved: https://github.com/pytorch/fairseq/pull/803

Differential Revision: D15795810

Pulled By: myleott

fbshipit-source-id: 3b2ae8bd7924a77ac8e795f5e1a7da0c4ae27374",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
695,Myle Ott,myleott@fb.com,2019-06-15 14:46:01-07:00,1c1fd73052c0f71f6e6c9bc9268cb1a949b25c8d,https://github.com/pytorch/fairseq/commit/1c1fd73052c0f71f6e6c9bc9268cb1a949b25c8d,"Close memory maps

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/655

Differential Revision: D15816573

fbshipit-source-id: ac0118a1d407dc132cc7d82e029eac6c8ec76d2a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
696,Arya McCarthy,aryamc@fb.com,2019-06-18 17:53:29-07:00,14282ff3db54464d290a98223a204b6a73102540,https://github.com/pytorch/fairseq/commit/14282ff3db54464d290a98223a204b6a73102540,"Add fairspeq task to train ASR model with auxiliary data. (#813)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/813

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/663

Pull Request resolved: https://github.com/fairinternal/fairspeq/pull/4

Introduce new training for speech models which accept additional training data.

Reviewed By: liezl200

Differential Revision: D15846661

fbshipit-source-id: 8b2cbfd56a86cf03c0b34c4a025bebdd5db7204e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
697,freewym,freewym@gmail.com,2019-06-19 08:19:29-07:00,00ac823e3e95b457a611021a933170108341d4f9,https://github.com/pytorch/fairseq/commit/00ac823e3e95b457a611021a933170108341d4f9,"Replace the use of the deprecated torch.distributed.reduce_op with to… (#804)

Summary:
…rch.distributed.ReduceOp
Pull Request resolved: https://github.com/pytorch/fairseq/pull/804

Differential Revision: D15877033

Pulled By: myleott

fbshipit-source-id: 58e7c39a88b67345a55b761fee4d9f211a5ee82c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
698,Myle Ott,myleott@fb.com,2019-06-19 08:55:23-07:00,461a366d7cb5df6b9290e546c6075f699d7cebc5,https://github.com/pytorch/fairseq/commit/461a366d7cb5df6b9290e546c6075f699d7cebc5,"Support different embed dim in Transformer decoder

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/811

Differential Revision: D15880880

Pulled By: myleott

fbshipit-source-id: c47e09a90c945aca82b26edb4a8af93e063d5b00",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
699,Michael Wu,mikaell@fb.com,2019-06-19 12:44:00-07:00,af9500dc320dfa7ed36f4abd8009aaa6e34b539d,https://github.com/pytorch/fairseq/commit/af9500dc320dfa7ed36f4abd8009aaa6e34b539d,"Add option to freeze transformer params for fine-tuning

Summary: add flags to freeze embedding parameters and transformer layer parameters in `TransformerSentenceEncoder`.

Reviewed By: myleott

Differential Revision: D15866135

fbshipit-source-id: e634d7adfd5e81eacccf2b9cf6bc15bad30bd1fe",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
700,Myle Ott,myleott@fb.com,2019-06-19 19:05:48-07:00,bd710e75aecb5f1dd8fdd6e2a947d6aeebe45258,https://github.com/pytorch/fairseq/commit/bd710e75aecb5f1dd8fdd6e2a947d6aeebe45258,"v0.7.0 (#817)

Summary:
Notable (possibly breaking) changes:
- d45db80: Remove checkpoint utility functions from utils.py into checkpoint_utils.py
- f2563c2: Move LM definitions into separate files
- dffb167: Updates to model API:
  - `FairseqModel` -> `FairseqEncoderDecoderModel`
  - add `FairseqDecoder.extract_features` and `FairseqDecoder.output_layer`
  - `encoder_out_dict` -> `encoder_out`
  - rm unused `remove_head` functions
- 34726d5: Move `distributed_init` into `DistributedFairseqModel`
- cf17068: Simplify distributed launch by automatically launching multiprocessing on each node for all visible GPUs (allows launching just one job per node instead of one per GPU)
- d45db80: Change default LR scheduler from `reduce_lr_on_plateau` to `fixed`
- 96ac28d: Rename `--sampling-temperature` -> `--temperature`
- fc1a19a: Deprecate dummy batches
- a1c997b: Add memory mapped datasets
- 0add50c: Allow cycling over multiple datasets, where each one becomes an ""epoch""

Plus many additional features and bugfixes
Pull Request resolved: https://github.com/pytorch/fairseq/pull/817

Differential Revision: D15913844

Pulled By: myleott

fbshipit-source-id: d5b5d678efdd9dd3e4d7ca848ddcf1ec2b21bf6b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
701,alexeib,alexei.b@gmail.com,2019-06-19 19:20:46-07:00,392fce8a9873e54eca71cfca9d98f2685fdf6238,https://github.com/pytorch/fairseq/commit/392fce8a9873e54eca71cfca9d98f2685fdf6238,"wav2vec model (#654)

Summary:
Merging wav2vec to master. Includes renames (Cpc -> wav2vec) and some light example files.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/654

Differential Revision: D15913409

Pulled By: alexeib

fbshipit-source-id: f723e6f211706cd9431c7d76dc12c4e80c9cfc80",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
702,Peng-Jen Chen,pipibjc@fb.com,2019-06-19 19:42:28-07:00,9c3bb5c6d6c7d6442a28ccb8a81b2fc4e5782ace,https://github.com/pytorch/fairseq/commit/9c3bb5c6d6c7d6442a28ccb8a81b2fc4e5782ace,"Better explain the inference argument format of multilingual translation

Summary:
In https://github.com/pytorch/fairseq/issues/656, people are often confused about how to set multilingual translation parameters at inference time.

This diff add more checks to ensure the arguments (`--lang-pairs`, `--encoder-langtok`, `--decoder-langtok`) load from checkpoint are consistent with arguments specified in generate/interactive command line.
We also add a section in example page to explain how to set the arguments

Reviewed By: myleott

Differential Revision: D15682169

fbshipit-source-id: 64e6db94cd72ea7ce2d0aa1067c9c2dcd3b8a2ac",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
703,davidecaroselli,davide@modernmt.eu,2019-06-19 20:21:10-07:00,9462a819e4915a3da767cf70a9db23881b60800a,https://github.com/pytorch/fairseq/commit/9462a819e4915a3da767cf70a9db23881b60800a,"Enhanced MMapIndexedDataset: less memory, higher speed (#816)

Summary:
I have made an upgrade to my previous implementation of MMapIndexedDataset, now:
- It uses up to **4 times less memory and disk space**
- Words per second is slightly improved thanks to less memory access
Pull Request resolved: https://github.com/pytorch/fairseq/pull/816

Differential Revision: D15899848

Pulled By: myleott

fbshipit-source-id: 9ddeb4809729ef69cc6b0867b33ee71184d845e6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
704,Myle Ott,myleott@fb.com,2019-06-20 06:23:49-07:00,881381cfc7cf5006712181138ff5b0811052cc57,https://github.com/pytorch/fairseq/commit/881381cfc7cf5006712181138ff5b0811052cc57,"v0.7.1: fix PyPI setup and tests

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/818

Differential Revision: D15916265

Pulled By: myleott

fbshipit-source-id: c66c0bd988d3472c4150226952f34ee8d4c3db86",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
705,Matt Le,mattle@fb.com,2019-06-20 08:49:23-07:00,6be5f07c20acad41b4cdb6cecdeb01f4f7deffcf,https://github.com/pytorch/fairseq/commit/6be5f07c20acad41b4cdb6cecdeb01f4f7deffcf,"Use bert init for xlm_base

Summary:
Use bert init for xlm_base.  This seems to be much closer to what is done in the [XLM](https://github.com/facebookresearch/XLM/blob/master/src/model/transformer.py#L44) repo.

At update 10 with BERT init (f121471600), loss starts at 14.234

At update 10 without BERT init (f121471612), loss starts at 154.423

Reviewed By: liezl200, pipibjc

Differential Revision: D15874836

fbshipit-source-id: f81bf83a078992d7476ba7fdf263b731a9f5b66d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
706,Myle Ott,myleott@fb.com,2019-06-20 17:46:32-07:00,b625d53d029afbd52a508bfcf8f7cd6d45aa0f79,https://github.com/pytorch/fairseq/commit/b625d53d029afbd52a508bfcf8f7cd6d45aa0f79,"Support DDP.no_sync context manager

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/671

Differential Revision: D15925248

fbshipit-source-id: 9eeea8a257929347e2458afdfc1def8dbb925a72",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
707,James Cross,jcross@fb.com,2019-06-21 08:05:28-07:00,7b4f55173e47827dadeff711e0bd810a73fb9f75,https://github.com/pytorch/fairseq/commit/7b4f55173e47827dadeff711e0bd810a73fb9f75,"get_batch_iterator: allow max_positions=None (#673)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/673

This function breaks when leaving the argument `max_positions` with the default value `None`, which is presumably not the intended behavior.

Reviewed By: theweiho, myleott

Differential Revision: D15937221

fbshipit-source-id: 1f5dc1c27ad9b6a89501d2dc015de12181059349",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
708,Qian Wang,wangqian5730@gmail.com,2019-06-23 13:08:05-07:00,4340b34ef26172a3e91fa59663c1ecafea6a2456,https://github.com/pytorch/fairseq/commit/4340b34ef26172a3e91fa59663c1ecafea6a2456,"Stringlize 2-d tensors with Dictionary.

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/828

Differential Revision: D15960629

Pulled By: myleott

fbshipit-source-id: ca631651e9a90ce8ed90ca23987519001fea3656",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
709,Alex Mathai,alexmathai98@gmail.com,2019-06-23 13:09:33-07:00,39a60b844aad67aa59267d873edeb4948f6f0af9,https://github.com/pytorch/fairseq/commit/39a60b844aad67aa59267d873edeb4948f6f0af9,"Fixed argument for Adaptive Softmax Instantiation

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/830

Differential Revision: D15960624

Pulled By: myleott

fbshipit-source-id: ecfef5c51b886e3162bb8e07d232c6e9ea1169b0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
710,Myle Ott,myleott@fb.com,2019-06-23 14:16:27-07:00,efb4345042b9b3cf1b5397d10d0c92b1441fe103,https://github.com/pytorch/fairseq/commit/efb4345042b9b3cf1b5397d10d0c92b1441fe103,"Fix resuming training when using --memory-efficient-fp16

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/678

Differential Revision: D15956712

Pulled By: myleott

fbshipit-source-id: 5048d06ddfbec0045558a22c777a966cca1ec396",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMemoryEfficientFP16(unittest.TestCase):'],"[('True', '(k.dtype == torch.float16)'), ('True', '(v_i.dtype == torch.float32)')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
711,Myle Ott,myleott@fb.com,2019-06-24 04:50:45-07:00,d9c79133ffb4bdbe4abdb7777d1fc61910e236e1,https://github.com/pytorch/fairseq/commit/d9c79133ffb4bdbe4abdb7777d1fc61910e236e1,"Add 'doc' break mode to TokenBlockDataset

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/679

Test Plan: https://our.intern.facebook.com/intern/chronos/jobinstance/?jobinstanceid=5191319216&smc=chronos_gp_admin_client&log_type=stdout&offset=0&pretty_logs=false

Differential Revision: D15961008

Pulled By: myleott

fbshipit-source-id: cf214de96665b33887ef64cfcb45a51f81002ed1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
712,freewym,freewym@gmail.com,2019-06-25 07:53:29-07:00,b3864b28b1878888d0ec1c7de69a29714d96a286,https://github.com/pytorch/fairseq/commit/b3864b28b1878888d0ec1c7de69a29714d96a286,"avoid ""divided by zero error"" in logging_outputs when --use-bmuf is e… (#812)

Summary:
… enabled.

When doing multi-gpu training with --use-bmuf turned on and --global-sync-iter > 1, each replica may not sync with other replicas at each iteration. So logging_outputs only has stats of their own.  On the other hand, logging_outputs may be empty at the end of an epoch after ""a dummy iteration"" because the number of replicas does not divide the number of batches of the training data. If this happens, sample_size and ntokens would be 0 for some replica  and cause ""divided by 0"" error. This fix sets *loss to 0 if sample_size/ntokens is 0.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/812

Reviewed By: myleott, yqwangustc

Differential Revision: D15908614

Pulled By: nayansinghal

fbshipit-source-id: c92e8e095f012bdb4ef753a3c627fd215afa215d",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
713,Myle Ott,myleott@fb.com,2019-06-26 08:08:53-07:00,ab2fa185edca69b433d56bcc920e0ea26db0347a,https://github.com/pytorch/fairseq/commit/ab2fa185edca69b433d56bcc920e0ea26db0347a,"Move task import in MultilingualTransformer to fix circular dependencies

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/687

Differential Revision: D16005399

Pulled By: myleott

fbshipit-source-id: bf099c17e2095394acc452e9abcb4ee04afd0426",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
714,Liang Wang,wangliangpeking@gmail.com,2019-06-26 08:56:53-07:00,8b514b9f53f3017070925fc4999a550ea8cda1bd,https://github.com/pytorch/fairseq/commit/8b514b9f53f3017070925fc4999a550ea8cda1bd,"FIx dataset loading when there are multiple valid subsets (#835)

Summary:
When we have multiple valid subsets, say `valid`, `valid1` and `valid2`, if `combine=True` holds, when loading `valid` subset, it will try to locate and load `valid`, `valid1`, `valid2`... and then combine them into one dataset. Set `combine` to `False` solves this issue.

In my experiment, I have 3 valid subsets with 3000, 5000 and 8701 examples, with argument `--valid-subset valid,valid1,valid2`, the log is as follows:

```
......
| ./mix_data/bin valid src-trg 3000 examples
| ./mix_data/bin valid1 src-trg 5000 examples
| ./mix_data/bin valid2 src-trg 7801 examples
| ./mix_data/bin valid1 src-trg 5000 examples
| ./mix_data/bin valid2 src-trg 7801 examples
......
```

As shown above, `valid1` and `valid2` subsets are incorrectly loaded twice.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/835

Differential Revision: D16006343

Pulled By: myleott

fbshipit-source-id: ece7fee3a00f97a6b3409defbf7f7ffaf0a54fdc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
715,Alexander Rives,alexrives@users.noreply.github.com,2019-06-26 09:04:30-07:00,1328bc090499fd96aeac5ec52bca332c9875bc2f,https://github.com/pytorch/fairseq/commit/1328bc090499fd96aeac5ec52bca332c9875bc2f,"add missing condition for first moment type as statement

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/684

Differential Revision: D16006333

Pulled By: myleott

fbshipit-source-id: 95bd4215734281194008fa029e81407d63b335ac",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
716,Nayan Singhal,naysing@fb.com,2019-06-26 23:07:47-07:00,c246df424be18ac5ededbabc680c18650fb7f69b,https://github.com/pytorch/fairseq/commit/c246df424be18ac5ededbabc680c18650fb7f69b,"2/N bmuf

Summary:
Added BMUF implementation.

Todo:
1) Add unit test case for testing model averaging and bmuf
2) Add warm before actually start training the model

Reviewed By: jay-mahadeokar

Differential Revision: D15871477

fbshipit-source-id: 866b0aba2d5bea5b65b4438acb49c886c4a87924",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
717,Bao-Yu,baoy@smail.nju.edu.cn,2019-06-27 16:08:38-07:00,c86d70cc2f1c2276725497519c69e5e97f80c36c,https://github.com/pytorch/fairseq/commit/c86d70cc2f1c2276725497519c69e5e97f80c36c,"Update generate.py (#831)

Summary:
Repeated use of 'i' in evaluate may cause some problems.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/831

Differential Revision: D15980227

Pulled By: myleott

fbshipit-source-id: 7b6b54a6b54938ad63ed1720d930505b56e5c84b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
718,Myle Ott,myleott@fb.com,2019-06-30 11:28:24-07:00,89e077c36bdd37fe0b0649b0232550bb670f672a,https://github.com/pytorch/fairseq/commit/89e077c36bdd37fe0b0649b0232550bb670f672a,"Add additional options for configuring writing of checkpoints

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697

Differential Revision: D16068465

Pulled By: myleott

fbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
719,Myle Ott,myleott@fb.com,2019-06-30 11:32:09-07:00,0bac688c1561580bb347899d8407de55ab370fa3,https://github.com/pytorch/fairseq/commit/0bac688c1561580bb347899d8407de55ab370fa3,"Fix --end-learning-rate in polynomial LR schedule

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/699

Differential Revision: D16068551

Pulled By: myleott

fbshipit-source-id: dddd8768b531032af7c4598af9dae3c6c00ff9ac",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
720,Myle Ott,myleott@fb.com,2019-06-30 12:45:47-07:00,ca5b1da579edf9e6c0d1df9630996f0156189ac9,https://github.com/pytorch/fairseq/commit/ca5b1da579edf9e6c0d1df9630996f0156189ac9,"Add linear activation

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/698

Differential Revision: D16068477

Pulled By: myleott

fbshipit-source-id: a68f6f519dc5481f857d8e10cc443249eccb2545",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
721,Myle Ott,myleott@fb.com,2019-06-30 16:54:08-07:00,b5949373543f4bf03aaaa281f91265e3b99c8e7a,https://github.com/pytorch/fairseq/commit/b5949373543f4bf03aaaa281f91265e3b99c8e7a,"Fix docs (fixes #843)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/844

Differential Revision: D16069358

Pulled By: myleott

fbshipit-source-id: 5ca4ab392dbdc4dfdaa27b63e8ff1c3940c91a26",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
722,Myle Ott,myleott@fb.com,2019-07-01 05:54:45-07:00,1757ef695c50f9f06ce86e58c729eda47a61aafd,https://github.com/pytorch/fairseq/commit/1757ef695c50f9f06ce86e58c729eda47a61aafd,"Better distributed init for SLURM

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/696

Differential Revision: D16068394

Pulled By: myleott

fbshipit-source-id: 92b44470ab8aeb9f99838cf74e34176104eb2b87",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
723,Myle Ott,myleott@fb.com,2019-07-01 07:11:34-07:00,b6d420c25207ffca4457f297a293dc5991d34951,https://github.com/pytorch/fairseq/commit/b6d420c25207ffca4457f297a293dc5991d34951,"Make segment_labels optional

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/703

Differential Revision: D16072305

Pulled By: myleott

fbshipit-source-id: b77019bdcfbfb95f2817a29a74515bc8f5b682bf",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
724,Myle Ott,myleott@fb.com,2019-07-01 14:06:45-07:00,39bbc9a50c9b717b2e7992ff2cc80b330da85a57,https://github.com/pytorch/fairseq/commit/39bbc9a50c9b717b2e7992ff2cc80b330da85a57,"Fixes checkpointing bug introduced in 89e077c

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847

Differential Revision: D16075498

Pulled By: myleott

fbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
725,Xutai Ma,xutaima@fb.com,2019-07-01 18:21:08-07:00,bccfddbb58bcca41d47955cecb5386af4705590a,https://github.com/pytorch/fairseq/commit/bccfddbb58bcca41d47955cecb5386af4705590a,"add --max-tokens-valid option for validation

Summary: Add the max-token-valid option. Sometime a separate max batch tokens for validation may be helpful, for example when there is a long sequence in validation set thats larger than max_tokens (it's rare in MT but could happen in ASR or AST).

Reviewed By: myleott

Differential Revision: D16076951

fbshipit-source-id: ae7f4218594580b9450a8196d7afa1e7e2018aee",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
726,Spencer Poff,spoff@fb.com,2019-07-03 18:45:38-07:00,5c241c8c5f7770c95b4c3ef67bc58788e348c7af,https://github.com/pytorch/fairseq/commit/5c241c8c5f7770c95b4c3ef67bc58788e348c7af,"support streaming iterator

Summary:
For tasks that involve streaming data directly from an API, we need a simpler epoch iterator.

Also included in this change is support for initializing a dictionary with an arbitrary list of special symbols.

Reviewed By: myleott

Differential Revision: D16110603

fbshipit-source-id: be6d9f680292dec1512614871f9269c95ac84861",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
727,Louis MARTIN,louisrtm@gmail.com,2019-07-06 12:27:58-07:00,cc292afaedd01bc2641f515190917627b437bde7,https://github.com/pytorch/fairseq/commit/cc292afaedd01bc2641f515190917627b437bde7,"Add specific compile flags for macOS (#862)

Summary:
Fairseq wouldn't install on macOS.
A workaround was found here: https://github.com/pytorch/fairseq/issues/289
This is now automatic in setup.py, maybe be there's a cleaner way to do it.

I checked that it compiles fine on Linux and macOS.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/862

Differential Revision: D16142105

Pulled By: myleott

fbshipit-source-id: 998ac7781d7a1ac047f4f9239c1fe16eab4be0dd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
728,vineetk1,vineetchandhok@gmail.com,2019-07-06 12:45:49-07:00,417ecb4b60028e741736bc527183c65fb0683e20,https://github.com/pytorch/fairseq/commit/417ecb4b60028e741736bc527183c65fb0683e20,"Added a comment to inform coders that pack_padded_sequence requires that padding must be on the right (#860)

Summary:
The PyTorch document on pack_padded_sequence has no information regarding a requirement that padding must be on the right. Therefore, this information is added as a comment on Line 212 of [https://github.com/vineetk1/fairseq/blob/master/fairseq/models/lstm.py](url)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/860

Differential Revision: D16142102

Pulled By: myleott

fbshipit-source-id: 7cb6d4df64b17b54b223de03bd966ca16077c3fe",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
729,Guanheng Zhang,zhangguanheng@devfair0435.h2.fair,2019-07-08 12:36:22-07:00,6d2e08314c0a13e58eb9518cbee84b93b4a8e097,https://github.com/pytorch/fairseq/commit/6d2e08314c0a13e58eb9518cbee84b93b4a8e097,"Integrate torch.nn and fairseq MultiheadAttention (#772)

Summary:
Integrate torch.nn and fairseq MultiheadAttention modules. In the future, both libraries will be benefited from performance optimization together.

Under the following circumstances, the calculation of the MultiheadAttention will still remain in fairseq, including:
1. onnx trace
2. incremental state
3. static kv

We plan to gradually mitigate those capabilities to PyTorch's core library.

Faieseq users can user the attribute self.enable_torch_version to force the calculations in either torch or fairseq. We use the following script to ensure both versions yield the same results.

------------------------------------------------------------------------------------
```
import torch
from fairseq.modules import MultiheadAttention
import time

embed_dim = 64
kv_embed_dim = 1208
num_heads = 16
src_len = 20
tgt_len = 30
bsz = 10

model = MultiheadAttention(embed_dim, num_heads, kdim=kv_embed_dim, vdim=kv_embed_dim,
                           bias=True, add_bias_kv=True, add_zero_attn=True)

query = torch.rand((src_len, bsz, embed_dim))
key = torch.rand((src_len, bsz, kv_embed_dim))
value = torch.rand((src_len, bsz, kv_embed_dim))

attn_mask = torch.randint(0, 2, (src_len, src_len)).float()
attn_mask.masked_fill_(attn_mask == 0, float('-inf'))
attn_mask.masked_fill_(attn_mask > 0, float('0.0'))

seq_mask = torch.randint(0, 2, (1, src_len))
key_padding_mask = seq_mask
for i in range(bsz-1):
    key_padding_mask = torch.cat([key_padding_mask, seq_mask], axis=0)
key_padding_mask = key_padding_mask == 1

# Apply torch.nn version
model.enable_torch_version = True
torch_output, torch_weight = model(query, key, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask)

# Apply fairseq version
model.enable_torch_version = False
fairseq_output, fairseq_weight = model(query, key, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask)

print(""torch and fairseq generate same results: outputs are same ? "",
      torch.allclose(torch_output, fairseq_output, atol=5e-6, rtol=1e-6),
      "", weights are same ? "",
      torch.allclose(torch_weight, fairseq_weight, atol=5e-6, rtol=1e-6)
)
```
------------------------------------------------------------------------------------
Expected results:
torch and fairseq generate same results: outputs are same ?  True , weights are same ?  True

------------------------------------------------------------------------------------
Similar performance is expected for both two versions. Using the following setup and have the initial performance benchmark results:

#########################
embed_dim = 32
kv_embed_dim = 32
num_heads = 4
src_len = 3
tgt_len = 2
bsz = 4
num_samples = 50000

#########################
torch-version MultiheadAttention cpu time: 0.46589  ms per iteration.
fairseq-version MultiheadAttention cpu time: 0.47861  ms per iteration.
torch-version MultiheadAttention gpu time: 0.82330  ms per iteration.
fairseq-version MultiheadAttention gpu time: 0.79410  ms per iteration.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/772

Reviewed By: myleott

Differential Revision: D16108450

Pulled By: zhangguanheng66

fbshipit-source-id: cd2eb5a6eeeab6c274999b7928c2af14fc211565",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
730,Peng-Jen Chen,pipibjc@fb.com,2019-07-09 11:34:05-07:00,e4335001d829ae806167af09aeac2b42c95ee764,https://github.com/pytorch/fairseq/commit/e4335001d829ae806167af09aeac2b42c95ee764,"Fix multilingual evaluation bug (#592)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/592

Fix bug reported at
https://github.com/pytorch/fairseq/commit/9c3bb5c6d6c7d6442a28ccb8a81b2fc4e5782ace#r34181600

D15682169 breaks the multilingual translation generation.

Reviewed By: dpacgopinath

Differential Revision: D16147454

fbshipit-source-id: e0cf4d32f362190a0542fa0160f65a2a207ca3fa",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
731,Taylan Bilal,taylanbil@google.com,2019-07-10 07:20:52-07:00,2bf7e75df91754a2dc1bb856b37a5f8b74df01b4,https://github.com/pytorch/fairseq/commit/2bf7e75df91754a2dc1bb856b37a5f8b74df01b4,"add __len__ for progress_bar (#871)

Summary:
We need this so that `progress_bar`s work with pytorch/xla i.e. TPUs. See [here](https://github.com/pytorch/xla/blob/master/torch_xla_py/data_parallel.py#L130).
Pull Request resolved: https://github.com/pytorch/fairseq/pull/871

Differential Revision: D16181062

Pulled By: myleott

fbshipit-source-id: 02c65033260396c2a243fbb66e31ffc2965f2376",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
732,Myle Ott,myleott@fb.com,2019-07-11 06:20:17-07:00,0a4911f6f05855775cbd48e2ecccfb74333b100f,https://github.com/pytorch/fairseq/commit/0a4911f6f05855775cbd48e2ecccfb74333b100f,"Small fix for trainer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/711

Differential Revision: D16192752

Pulled By: myleott

fbshipit-source-id: 102ed337a3d31e2047be7c033e9007c04223a684",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
733,Taylan Bilal,taylanbil@google.com,2019-07-11 16:15:20-07:00,397ba265ae45cd5a9d7a3f6f765e090dc6d89124,https://github.com/pytorch/fairseq/commit/397ba265ae45cd5a9d7a3f6f765e090dc6d89124,"adding __getstate__ to fairseq_optimizer (#872)

Summary:
self._optimizer has __getstate__
We need this so that fairseq_optimizer's work with pytorch/xla

```
% find . | xargs grep -s -i __getstate__
./third_party/tensorflow/tensorflow/python/util/deprecation_wrapper.py:  def __getstate__(self):
./torch_xla_py/xla_model.py:  for param_group in optimizer.__getstate__()['param_groups']:
```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/872

Differential Revision: D16211062

Pulled By: alexeib

fbshipit-source-id: 1b5575c85d34b7b021d719a03fd58d1c2ee453ee",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
734,Taylan Bilal,taylanbil@google.com,2019-07-14 12:45:21-07:00,c38b1f9135189cdcc9c0b0fd0d6a65b38770efb5,https://github.com/pytorch/fairseq/commit/c38b1f9135189cdcc9c0b0fd0d6a65b38770efb5,"removing tensor resizing in future_mask (#877)

Summary:
tensor resizing doesn't work well with tpus, this change is equivalent
to the base and works better w/ tpus.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/877

Differential Revision: D16241620

Pulled By: myleott

fbshipit-source-id: 402c7d5eb6175a66a0420d10e74eb0a9e085790e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
735,Taylan Bilal,taylanbil@google.com,2019-07-17 04:36:58-07:00,8db7b1c7f8af2d8158584f1ea7f2aaf8f4116f11,https://github.com/pytorch/fairseq/commit/8db7b1c7f8af2d8158584f1ea7f2aaf8f4116f11,"Forcing static shapes in loss computation (LSCE) (#876)

Summary:
applying non_pad_mask results in dynamic shapes = bad for tpus
This is an equivalent loss computation (tested), but tensor shapes are
constant (in the case of reduce=True)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/876

Differential Revision: D16241621

Pulled By: myleott

fbshipit-source-id: 973254b7e0842f2b55817afd66b2a110a566f149",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
736,Myle Ott,myleott@fb.com,2019-07-17 04:58:47-07:00,61e328cc3c7c74c867a3992b3ed39226605d7b03,https://github.com/pytorch/fairseq/commit/61e328cc3c7c74c867a3992b3ed39226605d7b03,"Add suppress_defaults functionality to options parser (#723)

Summary:
This is useful for standalone scripts that want to load a model and inherit most of the args from the model (e.g., eval_lm.py).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/723

Differential Revision: D16255751

Pulled By: myleott

fbshipit-source-id: 562b61511d5d7113e805c9644c877ebb8a3a1889",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
737,Jiajun Shen,shenjiajun90@gmail.com,2019-07-17 05:28:36-07:00,473389a34d51f2f9ea96751394074646b1e9773c,https://github.com/pytorch/fairseq/commit/473389a34d51f2f9ea96751394074646b1e9773c,"Small bug fix for generation when batch_size is small

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/727

Differential Revision: D16332742

Pulled By: myleott

fbshipit-source-id: becedd573c2c071fd21fcb5e55fead554c9bd9d1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
738,Xing Zhou,xingz@devfair0162.h2.fair,2019-07-17 06:18:06-07:00,e46b924dea823407a1822099580b0be667a6e9a4,https://github.com/pytorch/fairseq/commit/e46b924dea823407a1822099580b0be667a6e9a4,"Nucleus (top-P) sampling (#710)

Summary:
Implement Nucleus (top-P) sampling: sample among the smallest set of elements whose cumulative probability mass exceeds p.

To test it:
python generate.py   ~myleott/data/data-bin/wmt17_zh_en_full/   --path ~myleott/zh_en/model.pt   --remove-bpe   --nbest 5   --beam 5 --sampling --sampling-topp 0.3
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/710

Test Plan:
python generate.py   ~myleott/data/data-bin/wmt17_zh_en_full/   --path ~myleott/zh_en/model.pt   --remove-bpe   --nbest 5   --beam 5 --sampling --sampling-topp 0.3

python tests/test_sequence_generator.py

python tests/test_binaries.py

Reviewed By: myleott

Differential Revision: D16286688

Pulled By: xingz9

fbshipit-source-id: 1776d21e17c4532a3d24ac75bb7e75da9acad58f",6,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,27,1,0,0,0,0,0,0,0,0,2,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestSequenceGeneratorBase(unittest.TestCase):'],"[('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)'), ('HypoTokens', '(hypos[0][0], [w1, w1, eos])'), ('HypoScore', '(hypos[0][0], [1.0, 0.4, 1.0])'), ('HypoTokens', '(hypos[0][1], [w1, w1, eos])'), ('HypoScore', '(hypos[0][1], [1.0, 0.4, 1.0])'), ('HypoTokens', '(hypos[1][0], [w1, w1, eos])'), ('HypoScore', '(hypos[1][0], [1.0, 0.4, 1.0])'), ('HypoTokens', '(hypos[1][1], [w1, w1, eos])'), ('HypoScore', '(hypos[1][1], [1.0, 0.4, 1.0])'), ('True', '(self.hypoTokens(hypos[0][0], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[0][0], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[0][1], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[0][1], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[1][0], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[1][0], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[1][1], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[1][1], [1.0, 0.4, 1.0]) or')]",['def setUp(self):'],[],[],[],[],[],[],[],[],"['class TestSequenceGenerator(unittest.TestCase):', 'class TestDiverseBeamSearch(unittest.TestCase):']","[('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
739,Ning Dong,dnn@fb.com,2019-07-17 14:07:56-07:00,1f5b414f0204b643244d0d8740aba32f21075395,https://github.com/pytorch/fairseq/commit/1f5b414f0204b643244d0d8740aba32f21075395,"Support Latent Variable Model in base training (#879)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/879

Pull Request resolved: https://github.com/pytorch/translate/pull/598

Details in https://fb.workplace.com/notes/ning-dong/closing-research-to-production-gap-a-story-of-latent-variable-model-migration/443418839813586/

Reviewed By: xianxl

Differential Revision: D15742439

fbshipit-source-id: 168c84bd30a5da3c2fb404fcca74266deef1f964",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
740,Myle Ott,myleott@fb.com,2019-07-19 06:30:30-07:00,b002d0096ea82c77be7027b205d0b29de82edde4,https://github.com/pytorch/fairseq/commit/b002d0096ea82c77be7027b205d0b29de82edde4,"v0.7.1 -> v0.7.2 (#891)

Summary:
No major API changes since the last release. Cutting a new release since we'll be merging significant (possibly breaking) changes to logging, data loading and the masked LM implementation soon.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/891

Differential Revision: D16377132

Pulled By: myleott

fbshipit-source-id: f1cb88e671ccd510e53334d0f449fe18585268c7",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
741,Myle Ott,myleott@fb.com,2019-07-19 06:34:20-07:00,be5821b82bcb5f700693d805c31ba8d20c41cd01,https://github.com/pytorch/fairseq/commit/be5821b82bcb5f700693d805c31ba8d20c41cd01,"Switch to torch.nn.functional.gelu when available

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/735

Differential Revision: D16377046

Pulled By: myleott

fbshipit-source-id: 9725d4a3ce6b2fc8cee0b1d1cb8921f9d59c551a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
742,Myle Ott,myleott@fb.com,2019-07-19 06:34:46-07:00,8af555426980b775b9804ad2172fd34e4e818c9c,https://github.com/pytorch/fairseq/commit/8af555426980b775b9804ad2172fd34e4e818c9c,"Improve interactive generation (support --tokenizer and --bpe)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/734

Differential Revision: D16377044

Pulled By: myleott

fbshipit-source-id: 37d5553d76aa7c653113fec089f59710281c31d7",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
743,Myle Ott,myleott@fb.com,2019-07-19 08:08:16-07:00,c811e0e02d06f8d5fd6a0b738546b0e200c706cd,https://github.com/pytorch/fairseq/commit/c811e0e02d06f8d5fd6a0b738546b0e200c706cd,"Store task in the criterion base class

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/737

Differential Revision: D16377805

Pulled By: myleott

fbshipit-source-id: 1e090a02ff4fbba8695173f57d3cc5b88ae98bbf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
744,Myle Ott,myleott@fb.com,2019-07-19 08:08:22-07:00,ffe53d6fbc9b36668cf3c9bdf1cc786730fdee79,https://github.com/pytorch/fairseq/commit/ffe53d6fbc9b36668cf3c9bdf1cc786730fdee79,"Create standalone label_smoothed_nll_loss

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/739

Differential Revision: D16377798

Pulled By: myleott

fbshipit-source-id: 20047c80de2e6f108269ace4ae3eec906a5920dd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
745,Myle Ott,myleott@fb.com,2019-07-19 08:10:22-07:00,7efde2261f78e9e8d20e637e252bdd9977ec9290,https://github.com/pytorch/fairseq/commit/7efde2261f78e9e8d20e637e252bdd9977ec9290,"Allow not specifying --warmup-init-lr

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/736

Differential Revision: D16378001

Pulled By: myleott

fbshipit-source-id: 2907f63bcbf7068ceaa48b00096040fa2639e569",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
746,Myle Ott,myleott@fb.com,2019-07-19 13:10:23-07:00,69d0f7f826e58a7e07bc431afd74ce2c54c63eb6,https://github.com/pytorch/fairseq/commit/69d0f7f826e58a7e07bc431afd74ce2c54c63eb6,"Rename _load_model_ensemble -> load_model_ensemble_and_task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738

Differential Revision: D16377803

Pulled By: myleott

fbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
747,Myle Ott,myleott@fb.com,2019-07-21 03:48:34-07:00,f812e5295610d8a6467fe7212d8a47ce16d8d081,https://github.com/pytorch/fairseq/commit/f812e5295610d8a6467fe7212d8a47ce16d8d081,"Rename data.transforms -> data.encoders

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/747

Differential Revision: D16403464

Pulled By: myleott

fbshipit-source-id: ee3b4184f129a02be833c7bdc00685978b4de883",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
748,Liang Wang,wangliang01@fenbi.com,2019-07-21 07:17:50-07:00,1f96d284bea7dac46f3781e95e32a54117238f3a,https://github.com/pytorch/fairseq/commit/1f96d284bea7dac46f3781e95e32a54117238f3a,"Fix topp sampling issues (#882)

Summary:
Two issues here:

1. `last_included` should be the last included index `cumsum_mask[:, :, -1:]` instead of `cumsum_mask[:, :, :1]`  (which is either 0 or 1);

2. If `--no-repeat-ngram-size` is set, the sum of `probs` may less than 1, we need to re-normalize to make it a valid probability distribution

The following code can reproduce this issues:

```
import torch
import numpy as np

def _sample_topp(probs):

    # =====  Code from  fairseq/search.py _sample_topp ======

    # sort the last dimension (vocab dimension) in descending order
    sorted_probs, sorted_indices = probs.sort(descending=True)

    # compute a mask to indicate the words to be included in the top-P set.
    cumsum_probs = sorted_probs.cumsum(dim=2)
    mask = cumsum_probs.lt(sampling_topp)

    # note that mask was computed by 'lt'. One more word needs to be included
    # so that the cumulative probability mass can exceed p.
    cumsum_mask = mask.cumsum(dim=2)
    last_included = cumsum_mask[:, :, :1]
    mask = mask.scatter_(2, last_included, 1)

    # truncate unnecessary dims.
    max_dim = last_included.max()
    truncated_mask = mask[:, :, :max_dim + 1]
    truncated_probs = sorted_probs[:, :, :max_dim + 1]
    truncated_indices = sorted_indices[:, :, :max_dim + 1]

    # trim the words that are not in top-P by setting their probabilities
    # to 0, so that they would not be sampled later.
    trim_mask = 1 - truncated_mask
    trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)
    return trimed_probs, truncated_indices

    # ========================================================

if __name__ == '__main__':
    np.random.seed(1234)
    torch.manual_seed(1234)

    sampling_topp = 0.9
    probs = torch.softmax(torch.randn(1, 1, 10), dim=-1)
    # probs = tensor([0.0545, 0.0779, 0.0189, 0.0647, 0.0282, 0.0862, 0.0656, 0.1041, 0.0399, 0.4600])
    print('probs =', probs[0][0])

    trimed_probs, truncated_indices = _sample_topp(probs)

    cum_probs = trimed_probs.cumsum(dim=-1)[0][0]
    # cumsum = tensor([0.4600, 0.5641])
    print('cumsum =', cum_probs)
    # Will throw AssertionError
    assert float(cum_probs[-1]) >= sampling_topp

```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/882

Differential Revision: D16409269

Pulled By: xingz9

fbshipit-source-id: 94b1122eed50c656057b64e22af6f4a6ea7a68af",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
749,Myle Ott,myleott@fb.com,2019-07-21 12:42:50-07:00,5f78106a2d0fd91f6a977d350d999f07b45d5396,https://github.com/pytorch/fairseq/commit/5f78106a2d0fd91f6a977d350d999f07b45d5396,"Default to mmap and infer dataset implementations automatically

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/751

Differential Revision: D16410989

Pulled By: myleott

fbshipit-source-id: ddbbee49756f9ff6c4487977a3f5d2259b7abafe",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
750,Myle Ott,myleott@fb.com,2019-07-21 12:44:30-07:00,62b5498bebfc7825b277e488ec6eca2558e92295,https://github.com/pytorch/fairseq/commit/62b5498bebfc7825b277e488ec6eca2558e92295,"Update GPT-2 BPE

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/749

Differential Revision: D16410984

Pulled By: myleott

fbshipit-source-id: 7698df46b8a179afccb287990f9705358690454a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
751,Myle Ott,myleott@fb.com,2019-07-21 17:10:49-07:00,9c89e8821ee8df810c8b46e45002202a89106612,https://github.com/pytorch/fairseq/commit/9c89e8821ee8df810c8b46e45002202a89106612,"Misc improvements to torch hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/750

Differential Revision: D16410986

Pulled By: myleott

fbshipit-source-id: 8ee6b4371d6ae5b041b00a54a6039a422345795e",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
752,Myle Ott,myleott@fb.com,2019-07-21 19:30:03-07:00,47fd985269e92735826c05d9160d68dc8e8a9807,https://github.com/pytorch/fairseq/commit/47fd985269e92735826c05d9160d68dc8e8a9807,"Move Masked LM components to legacy/ -- new ones are coming

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/740

Differential Revision: D16377797

Pulled By: myleott

fbshipit-source-id: f7d6c8b00a77e279ea94376b1f0fcd15087eaf5f",12,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
753,Myle Ott,myleott@fb.com,2019-07-22 06:10:03-07:00,bccfa7d0067c05dbdce6b54f915480c78ddb896d,https://github.com/pytorch/fairseq/commit/bccfa7d0067c05dbdce6b54f915480c78ddb896d,"Add fallback for SLURM config

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/752

Differential Revision: D16417582

Pulled By: myleott

fbshipit-source-id: 6b4289febcf9290452bb91f1f2181a02c09c82a7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
754,Myle Ott,myleott@fb.com,2019-07-22 07:29:46-07:00,906411da12dfaf4c2b37d46b48540a89bca7fa31,https://github.com/pytorch/fairseq/commit/906411da12dfaf4c2b37d46b48540a89bca7fa31,"Fix --reset-meters

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/756

Differential Revision: D16418302

Pulled By: myleott

fbshipit-source-id: 62495a0bff41d1741e2b09807a3b43ff2c66c8fb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
755,Myle Ott,myleott@fb.com,2019-07-22 08:11:58-07:00,51ba35217debfbf34cdfbae14e09cb9df9c3be5b,https://github.com/pytorch/fairseq/commit/51ba35217debfbf34cdfbae14e09cb9df9c3be5b,"Simplify hubconf

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/758

Differential Revision: D16418932

Pulled By: myleott

fbshipit-source-id: 59f005164b61b9fa712922eeb23525f7eec38f38",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
756,Myle Ott,myleott@fb.com,2019-07-22 08:55:12-07:00,654affc03dd78b356c8dc6941609ca76a342c881,https://github.com/pytorch/fairseq/commit/654affc03dd78b356c8dc6941609ca76a342c881,"Add new Datasets

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/757

Differential Revision: D16418305

Pulled By: myleott

fbshipit-source-id: 25f293a2792509f7a75c688e4bf8cff02e6bba2e",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
757,Myle Ott,myleott@fb.com,2019-07-22 11:43:48-07:00,e8d609a80ddb6524baf978a15ceb75cdcfe5ac60,https://github.com/pytorch/fairseq/commit/e8d609a80ddb6524baf978a15ceb75cdcfe5ac60,"Add new Masked LM task + criterion

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/761

Differential Revision: D16421335

Pulled By: myleott

fbshipit-source-id: 257d92c2b90361147642e2baa38486b4d18f6297",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
758,Sara Hanson,sarahanson@fb.com,2019-07-22 16:36:08-07:00,a03fe6faf3b0fc9415d14b1cecf5598d4672b85d,https://github.com/pytorch/fairseq/commit/a03fe6faf3b0fc9415d14b1cecf5598d4672b85d,"Implement sparse transformer fixed attention pattern (#804)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/804

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/746

Pull Request resolved: https://github.com/pytorch/fairseq/pull/894

Adding an implementation of the sparse transformer to multi-head attention using the fixed attention pattern specified https://arxiv.org/pdf/1904.10509.pdf. The sparse_mask masks out words using -inf; after softmax, -inf becomes 0. Thus, a mask does not need to be re-calculated and re-applied when multiplying attn_weights and values.

Four inputs are added to the config: sparse, is_bidirectional, stride, expressivity. If we are using the sparse transformer, is_bidirectional, stride, and expressivity must be specified (there are defaults). If is_bidirectional is False, the mask values using the fixed attention pattern described in the paper. If is_bidirectional is True, subset one includes all values in the current stride window and a summary from every stride window--all other values are masked. Stride (L in the paper) controls the window size and expressivity (c in the paper) controls the size of the summary.

Reviewed By: borguz

Differential Revision: D16042988

fbshipit-source-id: c59166dc7cfe89187a256e4076000c2458842fd5",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestSparseMultiheadAttention(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
759,Myle Ott,myleott@fb.com,2019-07-22 19:38:23-07:00,30123e2caac819f5ccdd04c00f36d6b9303af111,https://github.com/pytorch/fairseq/commit/30123e2caac819f5ccdd04c00f36d6b9303af111,"Fix read_binarized.py script

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/762

Differential Revision: D16427266

Pulled By: myleott

fbshipit-source-id: 9bd9b8c6b4994ae98a62a37b34d03265bd365453",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
760,Taylan Bilal,taylanbil@google.com,2019-07-23 14:51:20-07:00,af6b361c92178458e1fe938bf8247080f38e44b4,https://github.com/pytorch/fairseq/commit/af6b361c92178458e1fe938bf8247080f38e44b4,"Initializing mask as a tensor of ints (not long) (#875)

Summary:
Since mask really is a tensor of ints, this change should be mathematically
equivalent to the base.

On the other hand, this has performance implications for xla, hence the
pull request.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/875

Differential Revision: D16232877

Pulled By: myleott

fbshipit-source-id: e63175ee0016dcf0dfe10e2fd22570b8bbfbde84",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
761,Myle Ott,myleott@fb.com,2019-07-23 15:39:44-07:00,208295dfc76492748500f97a4f9a808d8053a184,https://github.com/pytorch/fairseq/commit/208295dfc76492748500f97a4f9a808d8053a184,"Update README.md

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/899

Differential Revision: D16448602

Pulled By: myleott

fbshipit-source-id: afd1a1b713274b6328150cd85d7f8a81833597aa",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
762,Spencer Poff,spoff@fb.com,2019-07-24 16:54:51-07:00,b49ea81c556fef9375a94e18b8f95fb97459becc,https://github.com/pytorch/fairseq/commit/b49ea81c556fef9375a94e18b8f95fb97459becc,"check save_dir before beginning training

Summary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!

Reviewed By: myleott

Differential Revision: D16455394

fbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
763,Myle Ott,myleott@fb.com,2019-07-25 06:25:45-07:00,3d764a3dc6f0d1ae3968870645fe800debb12ad6,https://github.com/pytorch/fairseq/commit/3d764a3dc6f0d1ae3968870645fe800debb12ad6,"Update torch.hub usage

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/770

Differential Revision: D16491911

Pulled By: myleott

fbshipit-source-id: 8dd2b76f8fa24183640ae9d1129ea47ded77d43d",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
764,Myle Ott,myleott@fb.com,2019-07-25 07:19:44-07:00,8835d93cf08e429b073d7adabd077099a2cb7602,https://github.com/pytorch/fairseq/commit/8835d93cf08e429b073d7adabd077099a2cb7602,"Standardize on 'teacher forcing' rather than 'input feeding' which is… (#769)

Summary:
Input feeding generally refers to a slightly different concept
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/769

Differential Revision: D16491898

Pulled By: myleott

fbshipit-source-id: 68573584e820f11f199db4e7e37e9ee7a69a3287",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
765,Myle Ott,myleott@fb.com,2019-07-26 17:58:27-07:00,17fcc72a641e6994bea0b14356a611a0dd6cd1a1,https://github.com/pytorch/fairseq/commit/17fcc72a641e6994bea0b14356a611a0dd6cd1a1,"Add RoBERTa README

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/778

Differential Revision: D16525447

Pulled By: myleott

fbshipit-source-id: e721e3a10e243a2408a04f89f06b5adbbe2fdff2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
766,Myle Ott,myleott@fb.com,2019-07-27 07:19:05-07:00,40f16872c749d5b5cbf26218b3bd33c6a6788582,https://github.com/pytorch/fairseq/commit/40f16872c749d5b5cbf26218b3bd33c6a6788582,"Add return_all_hiddens flag to hub interface

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/909

Differential Revision: D16532919

Pulled By: myleott

fbshipit-source-id: 16ce884cf3d84579026e4406a75ba3c01a128dbd",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
767,Myle Ott,myleott@fb.com,2019-07-28 06:46:44-07:00,5218a7c970308a7b807548c05d0f20cb1ab37bdd,https://github.com/pytorch/fairseq/commit/5218a7c970308a7b807548c05d0f20cb1ab37bdd,"Fix compatibility with PyTorch 1.0.x (Fixes #906)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/910

Differential Revision: D16536532

Pulled By: myleott

fbshipit-source-id: 56bb5570e70b5670ad87c64d9dd20c64c1fa9f5c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
768,Myle Ott,myleott@fb.com,2019-07-28 06:51:16-07:00,abc13e28711ba228bb0c2065071887417ea254d0,https://github.com/pytorch/fairseq/commit/abc13e28711ba228bb0c2065071887417ea254d0,"Make hub_utils.generator inherit from nn.Module

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/913

Differential Revision: D16536562

Pulled By: myleott

fbshipit-source-id: ce28642da6868ec884e3e416388a652977a062df",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
769,Myle Ott,myleott@fb.com,2019-07-28 06:53:19-07:00,8207f26347f523b8fb655e4f248ade28ed9231db,https://github.com/pytorch/fairseq/commit/8207f26347f523b8fb655e4f248ade28ed9231db,"Misc dataset improvements

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/911

Differential Revision: D16536559

Pulled By: myleott

fbshipit-source-id: 7fe495054ce5b7658b1d3a43eca38c5858360236",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
770,Myle Ott,myleott@fb.com,2019-07-28 06:53:20-07:00,1362b21bc3e1debe442b4162adc14eb30df5fd23,https://github.com/pytorch/fairseq/commit/1362b21bc3e1debe442b4162adc14eb30df5fd23,"Correctly zero padding index in TransformerSentenceEncoder

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/912

Differential Revision: D16536561

Pulled By: myleott

fbshipit-source-id: 54c5c20a826a14f4e690770e027bcb282acdf911",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
771,Myle Ott,myleott@fb.com,2019-07-28 08:22:28-07:00,c446c44b1f2023808d48609dbeb48c58fdba1cf3,https://github.com/pytorch/fairseq/commit/c446c44b1f2023808d48609dbeb48c58fdba1cf3,"Add Adamax optimizer

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/914

Differential Revision: D16536670

Pulled By: myleott

fbshipit-source-id: 8a41c98f0fb87af6c384cdade756e3eae2978a88",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
772,Myle Ott,myleott@fb.com,2019-07-28 16:01:16-07:00,76ff39f56655e764488f757fe92e5c564fbc7c91,https://github.com/pytorch/fairseq/commit/76ff39f56655e764488f757fe92e5c564fbc7c91,"Change default --num-workers to 1

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/779

Differential Revision: D16536673

Pulled By: myleott

fbshipit-source-id: bf56e9a81d3086f3d95a3273391dc5e04ed2dbc4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
773,Myle Ott,myleott@fb.com,2019-07-28 17:22:44-07:00,a80cade964704a7e100fcd1943219432ce0e6009,https://github.com/pytorch/fairseq/commit/a80cade964704a7e100fcd1943219432ce0e6009,"Update BPE library code

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/780

Differential Revision: D16537567

Pulled By: myleott

fbshipit-source-id: 4e18c529959935e82ea122c3a2ee477308ffcbe3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
774,Myle Ott,myleott@fb.com,2019-07-28 18:40:05-07:00,8d036c2fe01be5158c3ae5265d32c619131d8783,https://github.com/pytorch/fairseq/commit/8d036c2fe01be5158c3ae5265d32c619131d8783,"Add RoBERTa

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/916

Differential Revision: D16537774

Pulled By: myleott

fbshipit-source-id: 86bb7b1913a428ee4a21674cc3fc7b39264067ec",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
775,Myle Ott,myleott@fb.com,2019-07-29 06:02:19-07:00,ce7f044bb100aeec6b3c524a654ce8c177403c0b,https://github.com/pytorch/fairseq/commit/ce7f044bb100aeec6b3c524a654ce8c177403c0b,"Add instructions to load RoBERTa models on PyTorch 1.0

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/921

Differential Revision: D16541025

Pulled By: myleott

fbshipit-source-id: bb78d30fe285da2adfc7c4e5897ee01fa413b2e4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
776,Myle Ott,myleott@fb.com,2019-07-29 06:20:49-07:00,36df0dadb9bc22b1f3432a7586e1be11dfd0270e,https://github.com/pytorch/fairseq/commit/36df0dadb9bc22b1f3432a7586e1be11dfd0270e,"Fix RoBERTa model import (fixes #918)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/920

Differential Revision: D16540932

Pulled By: myleott

fbshipit-source-id: b64438ad8651ecc8fe8904c5f69fa6111b4bed64",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
777,Myle Ott,myleott@fb.com,2019-07-29 07:46:42-07:00,2f6d8b352a142424bef1858ef62e24180e8fbd0b,https://github.com/pytorch/fairseq/commit/2f6d8b352a142424bef1858ef62e24180e8fbd0b,"Add missing files for RoBERTa hub interface

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/923

Differential Revision: D16541289

Pulled By: myleott

fbshipit-source-id: b3563a9d61507d4864ac6ecf0648672eaa40b5f3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
778,Xing Zhou,51722896+xingz9@users.noreply.github.com,2019-07-29 10:34:35-07:00,2fe45f09a1e775cfc1093103054aa3a554eb53e0,https://github.com/pytorch/fairseq/commit/2fe45f09a1e775cfc1093103054aa3a554eb53e0,"Update README.md to add top-p sampling (#783)

Summary:
Update README.md to include the recently implemented top-p/nucleus sampling.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/783

Differential Revision: D16543974

Pulled By: myleott

fbshipit-source-id: 27c502af10ee390d29607038118a99ff0067aec4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
779,Myle Ott,myleott@fb.com,2019-07-29 14:47:43-07:00,33597e5a65db7ebb3c3d9caf3851a4dc8e239bc5,https://github.com/pytorch/fairseq/commit/33597e5a65db7ebb3c3d9caf3851a4dc8e239bc5,"Support different --max-positions and --tokens-per-sample

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/924

Differential Revision: D16548165

Pulled By: myleott

fbshipit-source-id: 49569ece3e54fad7b4f0dfb201ac99123bfdd4f2",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
780,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-07-29 16:03:11-07:00,138dc8e4fd02f81074842731d1bdd9401aa59489,https://github.com/pytorch/fairseq/commit/138dc8e4fd02f81074842731d1bdd9401aa59489,"adding glue data preprocessing scripts (#771)

Summary:
1) Added glue data pre-processing script.
2) updated README with usage.

TODO:
1) releasing fairseq dictionary and remove hardcoded path.
2) remove hard-coded path for bpe-encoding,

myleott what do you recommend for above TODOs?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771

Reviewed By: myleott

Differential Revision: D16547679

Pulled By: myleott

fbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
781,Myle Ott,myleott@fb.com,2019-07-30 05:30:22-07:00,c132b9b931893257ecb0ac6351969ab4142d6782,https://github.com/pytorch/fairseq/commit/c132b9b931893257ecb0ac6351969ab4142d6782,"Fix tokenization (fixes #926) (#929)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/926
Pull Request resolved: https://github.com/pytorch/fairseq/pull/929

Differential Revision: D16560281

Pulled By: myleott

fbshipit-source-id: 751051bcdbf25207315bb05f5bee0235d21be627",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
782,Myle Ott,myleott@fb.com,2019-07-30 07:45:13-07:00,e75cff5f2c1d62f12dc911e0bf420025eb1a4e33,https://github.com/pytorch/fairseq/commit/e75cff5f2c1d62f12dc911e0bf420025eb1a4e33,"Relicense fairseq under MIT license (#786)

Summary:
The previous BSD+PATENTS license was controversial. We have been
approved to relicense fairseq under the MIT license.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786

Differential Revision: D16560654

Pulled By: myleott

fbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034",199,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
783,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-07-30 09:20:15-07:00,3b2cecda72bbdd9a59f85e44c92becaeb123715f,https://github.com/pytorch/fairseq/commit/3b2cecda72bbdd9a59f85e44c92becaeb123715f,"1) replaced fstring 2) fixed error from max-positions arg

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/787

Differential Revision: D16562052

fbshipit-source-id: 640e30b2378ec917d60092558d3088a77f9741cb",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
784,Myle Ott,myleott@fb.com,2019-07-30 11:32:50-07:00,d82517e9553ab690b8a264f7ca5be46a41eee700,https://github.com/pytorch/fairseq/commit/d82517e9553ab690b8a264f7ca5be46a41eee700,"Add roberta.decode to hub interface to decode BPE (#931)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/930.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/931

Differential Revision: D16562511

Pulled By: myleott

fbshipit-source-id: c4c07e2f067326b79daa547dcb3db84aeddbd555",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
785,Nathan Ng,n.ng555@gmail.com,2019-07-31 03:41:41-07:00,b651b000033fd8ff51d1c3bea76f4fd1897bdf9c,https://github.com/pytorch/fairseq/commit/b651b000033fd8ff51d1c3bea76f4fd1897bdf9c,"Wmt19 models (#767)

Summary:
Release of the WMT 19 pretrained models
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/767

Reviewed By: edunov

Differential Revision: D16472717

Pulled By: nng555

fbshipit-source-id: acf0fa3548c33f2bf2b5f71e551c782ad8c31a42",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
786,Johannes Villmow,johannes.villmow@hs-rm.de,2019-07-31 05:54:52-07:00,37eb9f2b3c36d29ac557524fb4b9dbb644ef4f8b,https://github.com/pytorch/fairseq/commit/37eb9f2b3c36d29ac557524fb4b9dbb644ef4f8b,"Use commandline interface in preprocess_GLUE_tasks.sh (#937)

Summary:
Just a small fix for issue https://github.com/pytorch/fairseq/issues/936 .
Pull Request resolved: https://github.com/pytorch/fairseq/pull/937

Differential Revision: D16580263

Pulled By: myleott

fbshipit-source-id: 1777e782491c63697726e95bd555892da3fed4ec",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
787,Dongjin Na,dongjinna@gmail.com,2019-07-31 08:30:37-07:00,c5650bfc0a41ebcbb8940dc0430bb6c095ca09c7,https://github.com/pytorch/fairseq/commit/c5650bfc0a41ebcbb8940dc0430bb6c095ca09c7,"Update language_model README.md (#941)

Summary:
Adding a backslash in the convolutional language model training usage.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/941

Differential Revision: D16581388

Pulled By: myleott

fbshipit-source-id: 7e2e05ecf13e86cb844dc5200d49f560c63b12ff",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
788,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-07-31 14:57:55-07:00,fe8a163986672bbbec1a922231be229cc79dafe6,https://github.com/pytorch/fairseq/commit/fe8a163986672bbbec1a922231be229cc79dafe6,"Roberta add classification finetuning example readme (#790)

Summary:
Added readme for IMDB classification as tutorial for custm finetuning of roberta
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/790

Reviewed By: myleott

Differential Revision: D16587877

Pulled By: myleott

fbshipit-source-id: ed265b7254e6fa2fc8a899ba04c0d2bb45a7f5c4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
789,Nathan Ng,n.ng555@gmail.com,2019-07-31 15:29:51-07:00,94722a9fb87f4a36d56d9e1888fb54ea010c7a91,https://github.com/pytorch/fairseq/commit/94722a9fb87f4a36d56d9e1888fb54ea010c7a91,"Fix citation errors (#791)

Summary:
Fixing booktitle in wmt19 citation
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/791

Reviewed By: myleott

Differential Revision: D16589372

Pulled By: nng555

fbshipit-source-id: 28402784bb6ef0615e46b8d8383bfa52d79e46de",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
790,Myle Ott,myleott@fb.com,2019-07-31 18:17:10-07:00,3e0e5becff16580c4b6758ceb0a2ce0e837da422,https://github.com/pytorch/fairseq/commit/3e0e5becff16580c4b6758ceb0a2ce0e837da422,"Fix small syntax error in hub_utils.py (fixes #942)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/944

Differential Revision: D16593568

Pulled By: myleott

fbshipit-source-id: 611bccae2ad0b8dc704c47a8a3343161010c2356",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
791,Myle Ott,myleott@fb.com,2019-08-01 05:51:19-07:00,5b2be870f4008f54ccd145e10d4de24d2db9c1de,https://github.com/pytorch/fairseq/commit/5b2be870f4008f54ccd145e10d4de24d2db9c1de,"Update PyTorch Hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782

Differential Revision: D16542256

Pulled By: myleott

fbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
792,Myle Ott,myleott@fb.com,2019-08-01 07:28:55-07:00,4abadbdf775e8e1a0088da68677842a2330d36d9,https://github.com/pytorch/fairseq/commit/4abadbdf775e8e1a0088da68677842a2330d36d9,"Fix sampling with beam>1

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/792

Differential Revision: D16591987

Pulled By: myleott

fbshipit-source-id: d27c490ae75f80ded19226b8384f4776485dd694",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],"[('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0])'), ('HypoTokens', '(hypos[0][1], [w2, w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.1, 0.9, 0.9, 1.0])'), ('HypoTokens', '(hypos[1][0], [w2, w2, w2, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.3, 0.9, 0.99, 0.4, 1.0])'), ('HypoTokens', '(hypos[1][1], [w1, w2, w1, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.4, 0.4, 1.0])')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
793,Iurii Zdebskyi,iuriiz@fb.com,2019-08-01 07:47:44-07:00,430905d7fae0bdfcc9d969934df35a354e71c4ec,https://github.com/pytorch/fairseq/commit/430905d7fae0bdfcc9d969934df35a354e71c4ec,"Changed tensor comparison return type from uint8 to bool (#21113)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21113
ghimport-source-id: 9c4ba63457a72bfc41894387e0b01be3fd9a9baf

Test Plan: Imported from OSS

Differential Revision: D15552204

Pulled By: izdeby

fbshipit-source-id: a608213668649d058e22b510d7755cb99e7d0037",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
794,Myle Ott,myleott@fb.com,2019-08-01 11:56:12-07:00,45f23f66bec3063cb20ffae8c6a9084e45669aba,https://github.com/pytorch/fairseq/commit/45f23f66bec3063cb20ffae8c6a9084e45669aba,"Add more details for bulk BPE encoding

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/793

Differential Revision: D16603930

Pulled By: myleott

fbshipit-source-id: b302db3743db4f36c14fb0dc7f3456fe8a0079dd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
795,Christian Clauss,cclauss@me.com,2019-08-01 15:20:53-07:00,ea6cc1da11db821e24ca461b570ae01784e19358,https://github.com/pytorch/fairseq/commit/ea6cc1da11db821e24ca461b570ae01784e19358,"Use ==/!= to compare str, bytes, and int literals (#948)

Summary:
Identity is not the same thing as equality in Python.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/948

Differential Revision: D16608269

Pulled By: myleott

fbshipit-source-id: be203d62e7824c96c59400d1b342196adb89a839",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
796,Nathan Ng,n.ng555@gmail.com,2019-08-01 16:43:09-07:00,ccb5dea58dc556549df00453a144d382ffb727b6,https://github.com/pytorch/fairseq/commit/ccb5dea58dc556549df00453a144d382ffb727b6,"Fix wmt19 links (#796)

Summary:
fix links to .tar.gz vs .tar.bz2
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/796

Reviewed By: myleott

Differential Revision: D16611740

Pulled By: nng555

fbshipit-source-id: 76210484225ed917ff14ef626845680d918948f5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
797,Myle Ott,myleott@fb.com,2019-08-02 05:19:26-07:00,5f34252767261dca6056dc27249e241f06cc9700,https://github.com/pytorch/fairseq/commit/5f34252767261dca6056dc27249e241f06cc9700,"Update beam search code to support torch.bool change

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/797

Differential Revision: D16617067

Pulled By: myleott

fbshipit-source-id: 52e3aeb98d6e3b55ff9154b784028bf13eabfe38",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
798,Myle Ott,myleott@fb.com,2019-08-02 06:20:46-07:00,abb7ed4c91b55b1b714021d8163f0a8c73f82f46,https://github.com/pytorch/fairseq/commit/abb7ed4c91b55b1b714021d8163f0a8c73f82f46,"Update READMEs for torch.hub

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/795

Differential Revision: D16620488

Pulled By: myleott

fbshipit-source-id: 1998a9ccd8816fc7f590861fb4898f910a36bc1e",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
799,Myle Ott,myleott@fb.com,2019-08-02 08:40:59-07:00,f02f70cce2dc1126c4250170eabfd3a95d8bb378,https://github.com/pytorch/fairseq/commit/f02f70cce2dc1126c4250170eabfd3a95d8bb378,"Add single-models for WMT'19 for hub tutorial

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/800

Differential Revision: D16621509

Pulled By: myleott

fbshipit-source-id: d3e8e97d30bcafbc35c3f67cd8bbc657b6fa5fe7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
800,Myle Ott,myleott@fb.com,2019-08-02 10:54:16-07:00,3903f46904f8b2ca0fa229cf6131675858997a30,https://github.com/pytorch/fairseq/commit/3903f46904f8b2ca0fa229cf6131675858997a30,"Fewer torch.hub requirements (#959)

Summary:
We will raise exceptions if these are needed and aren't available. Only keep minimum set of reqs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/959

Differential Revision: D16623304

Pulled By: myleott

fbshipit-source-id: 8e65253742e393b527e8396a9433e64ebec9bb55",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
801,Ning Dong,dnn@fb.com,2019-08-02 13:17:26-07:00,9012e87d36a5f88cc3894508688d4328bbfd3abe,https://github.com/pytorch/fairseq/commit/9012e87d36a5f88cc3894508688d4328bbfd3abe,"Avoid cast in PositionalEmbeddings to fix BLEU drop in pytorch native export

Summary:
Tracing mode doesn't generalize correctly in positional embedding calculation, which caused -5 BLEU at transformer export when using pytorch native.

Details: The original issue was that in ensemble_export, _to_tensor(x) in scripting mode turns integer x into 1-d tensor torch.tensor([x]), not 0-d tensor (scalar x) which is expected in the embedding. So the return value in embedding forward() is actually of wrong shape. When self.weights is of size [x,y], the return value should be (bsz, y, 1) but it was (bsz, 1, y), which caused problem in downstream computation. Tracing only becomes an issue when I used pos = timestep.view(-1)[0] to fix the shape. Then casting the scalar to primary int, to be used as index is not generalizable by tracing mode. Thus I need to convert everything to tensor and replace the advanced indexing with index_select operator.

In summary, less understood features in both scripting&tracing sides caused the bleu drop. :)

Reviewed By: myleott

Differential Revision: D16623025

fbshipit-source-id: 0c7a2c3eafbd774760a5c880c6034009ee084abb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
802,Myle Ott,myleott@fb.com,2019-08-03 04:39:04-07:00,12258e5798a7b89d46443c1c80dc6f281637807e,https://github.com/pytorch/fairseq/commit/12258e5798a7b89d46443c1c80dc6f281637807e,"Fix generating with a fixed prefix

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/801

Differential Revision: D16628318

Pulled By: myleott

fbshipit-source-id: 50e93bb9108afd2ba90f1edd4f34306a7c9964a4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
803,alexeib,alexei.b@gmail.com,2019-08-03 08:29:09-07:00,c728b864247ff85e968d3138a4412385722c4b7b,https://github.com/pytorch/fairseq/commit/c728b864247ff85e968d3138a4412385722c4b7b,"remove default params from args so architecture works properly

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/798

Reviewed By: myleott

Differential Revision: D16619502

Pulled By: alexeib

fbshipit-source-id: af20c90c4522458850d8f42cab001259ef4293cc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
804,Myle Ott,myleott@fb.com,2019-08-04 06:22:44-07:00,1684e166e3da03f5b600dbb7855cb98ddfcd0805,https://github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805,"Add doc string for Roberta.encode function

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/969

Differential Revision: D16642388

Pulled By: myleott

fbshipit-source-id: c5b1655dbddb697822feefa433f33f6bb08253ab",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
805,Naman Goyal,namangoyal@learnfair1287.h2.fair,2019-08-05 12:39:42-07:00,5d543f9b19e76772386903d4eeebdceaeb3d1b69,https://github.com/pytorch/fairseq/commit/5d543f9b19e76772386903d4eeebdceaeb3d1b69,"fixed roberta finetuning with --find-unused-parameters on multiGPU

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/806

Differential Revision: D16649933

fbshipit-source-id: 6eeda6e2caf8019228e3efc0c27ddfcc3c4d8674",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
806,Myle Ott,myleott@fb.com,2019-08-06 09:11:54-07:00,e40e4b21cb620408e99ab8c6a45e314584ef3508,https://github.com/pytorch/fairseq/commit/e40e4b21cb620408e99ab8c6a45e314584ef3508,"Add back set_epoch functionality lost in RoBERTa merge

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/982

Differential Revision: D16668353

Pulled By: myleott

fbshipit-source-id: 699243d6c028c47cd0e3f801d89051b3f919b17e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
807,Myle Ott,myleott@fb.com,2019-08-07 06:31:34-07:00,2b7843daf85bad39b634b7963604771d3528e671,https://github.com/pytorch/fairseq/commit/2b7843daf85bad39b634b7963604771d3528e671,"Add code to realign RoBERTa features to word-level tokenizers

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/805

Differential Revision: D16670825

Pulled By: myleott

fbshipit-source-id: 872a1a0274681a34d54bda00bfcfcda2e94144c6",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
808,Myle Ott,myleott@fb.com,2019-08-07 07:25:45-07:00,1e55bbdb385cccc72c7ac0d305ffd120ded7e1b6,https://github.com/pytorch/fairseq/commit/1e55bbdb385cccc72c7ac0d305ffd120ded7e1b6,"Fix tests and GLUE finetuning (fixes #989)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/991

Differential Revision: D16687970

Pulled By: myleott

fbshipit-source-id: d877fc16891a8ab97aec47a8d440baa56c2b5f46",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
809,Naman Goyal,namangoyal@learnfair1287.h2.fair,2019-08-07 09:25:17-07:00,a9eda736ec6295118dfa2b46f614519e0c191cbb,https://github.com/pytorch/fairseq/commit/a9eda736ec6295118dfa2b46f614519e0c191cbb,"Added mask_fill api and some examples in README (#807)

Summary:
1) This currently works only for single `<mask>` token as multi mask, we might have to look more into order of factorization.
2) This is currently only for single BPE token
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/807

Differential Revision: D16674509

fbshipit-source-id: 0a020030ee5df6a5115e5f85d5a9ef52b1ad9e1c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
810,Naman Goyal,namangoyal@learnfair1287.h2.fair,2019-08-07 10:44:13-07:00,9a1038f68a92444a8f9cd2f0ca42a362b90fed20,https://github.com/pytorch/fairseq/commit/9a1038f68a92444a8f9cd2f0ca42a362b90fed20,"fixed reloading from checkpoint (#811)

Summary:
Tested by starting training from (a) `roberta.large`, (b) `roberta.large.mnli`, (c) `checkpoints/checkpoint_last.pt`
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/811

Reviewed By: myleott

Differential Revision: D16689528

Pulled By: myleott

fbshipit-source-id: 849d72ede9d526c34b4753c1bffd689554d1f837",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
811,Dmytro Okhonko,oxo@fb.com,2019-08-08 02:42:38-07:00,72f9364cc6aa28380c5453476e1cc25e22f4f869,https://github.com/pytorch/fairseq/commit/72f9364cc6aa28380c5453476e1cc25e22f4f869,"Asr initial push (#810)

Summary:
Initial code for speech recognition task.
Right now only one ASR model added - https://arxiv.org/abs/1904.11660

unit test testing:
python -m unittest discover tests

also run model training with this code and obtained
5.0 test_clean | 13.4 test_other
on librispeech with pytorch/audio features
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/810

Reviewed By: cpuhrsch

Differential Revision: D16706659

Pulled By: okhonko

fbshipit-source-id: 89a5f9883e50bc0e548234287aa0ea73f7402514",24,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],5,26,12,17,0,0,0,6,0,0,2,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestBaseFairseqModelBase(unittest.TestCase):', 'class TestFairseqEncoderBase(unittest.TestCase):', 'class TestFairseqDecoderBase(unittest.TestCase):', 'class CrossEntropyCriterionTestBase(unittest.TestCase):', 'class TestSeq2SeqCollator(unittest.TestCase):']","[('True', '(isinstance(model, BaseFairseqModel))'), ('True', '('), ('True', '(succ, msg=msg)'), ('True', '(hasattr(logprob, ))'), ('True', '(hasattr(prob, ))'), ('True', '(torch.is_tensor(logprob))'), ('True', '(torch.is_tensor(prob))'), ('True', '('), ('True', '(succ, msg=msg)'), ('True', '(hasattr(logprob, ))'), ('True', '(hasattr(prob, ))'), ('True', '(torch.is_tensor(logprob))'), ('True', '(torch.is_tensor(prob))'), ('True', '('), ('True', '(succ, msg=msg)'), ('True', '('), ('True', '(succ, msg=msg)'), ('TensorEqual', '(batch[], torch.tensor([1, 0]))'), ('Equal', '(batch[], 7)'), ('TensorEqual', '('), ('TensorEqual', '('), ('TensorEqual', '(batch[], torch.tensor([3, 2]))'), ('TensorEqual', '('), ('Equal', '(batch[], 2)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)')]","['def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):']","[('Class', '(cls)'), ('Model', '(self, model)'), ('Class', '(cls)'), ('Model', '(self, model_cls, extra_args_setters=None)'), ('Input', '(self, input=None)'), ('Class', '(cls)'), ('Model', '(self, model_cls, extra_args_setters=None)'), ('Input', '(self, input=None)'), ('Class', '(cls)'), ('Encoder', '(self, encoder)'), ('Input', '(self, input=None)'), ('Class', '(cls)'), ('Decoder', '(self, decoder)'), ('Input', '(self, input=None)'), ('PrevOutputTokens', '(self, tokens=None)'), ('Class', '(cls)'), ('Args', '(self)')]",[],[],[],"['()', '()', '()', '()', '()', '()']",[],[],"['import unittest', 'import unittest']",[],[],[],[],[],[],[],[],[],[],[],"['logging_output[] == 20', 'logging_output[] == 20', 'logging_output[] == 20', 'logging_output[] == 20', 'logging_output[] == 0', 'logging_output[] == 20', 'logging_output[] == 20', 'logging_output[] == 20']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
812,Myle Ott,myleott@fb.com,2019-08-08 08:23:22-07:00,439ead5a7738bc5080d1d4643ae4bf6dfc78b8ca,https://github.com/pytorch/fairseq/commit/439ead5a7738bc5080d1d4643ae4bf6dfc78b8ca,"Integrate with Apache Arrow/Plasma in-memory store for large datasets (#995)

Summary:
Datasets with many examples can generate very large indexes in TokenBlockDataset (and possibly elsewhere). When using `--num-workers>0` these indexes are pickled and transferred via a multiprocessing pipe, which is slow and can fail if the index grows beyond 4GB (~0.5B examples). Apache Arrow has an in-memory store called Plasma that will offload these arrays to shared memory, which both reduces duplication of the data and avoids needing to pickle.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/995

Differential Revision: D16697219

Pulled By: myleott

fbshipit-source-id: 1b679ee5b3d2726af54ff418f6159a3671173fb8",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
813,Hafiz Shafruddin,gmhafiz@gmail.com,2019-08-08 10:04:57-07:00,6398aa9e5be57c89785e059462d73b108513238d,https://github.com/pytorch/fairseq/commit/6398aa9e5be57c89785e059462d73b108513238d,"replace 'mkdir' with 'mkdir -p' (#997)

Summary:
Allow shell script to create sub directories with -p flag. Amends readme file too.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/997

Differential Revision: D16710813

Pulled By: myleott

fbshipit-source-id: 89abefa27e8fac99d212fc9b7b0dbc3690043ba0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
814,Naman Goyal,namangoyal@learnfair1180.h2.fair,2019-08-09 09:43:16-07:00,3563e59abe6366981ae5eb7c3333efcd64225d04,https://github.com/pytorch/fairseq/commit/3563e59abe6366981ae5eb7c3333efcd64225d04,"added superglue dev set results to readme

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/815

Differential Revision: D16733633

fbshipit-source-id: 0a5029e41b6dbb9fb28e9703ad057d939d489d90",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
815,Vincent Quenneville-Belair,vincentqb@gmail.com,2019-08-09 10:03:53-07:00,838e108a917ba2dda5ed552d91ec308d7be49a8c,https://github.com/pytorch/fairseq/commit/838e108a917ba2dda5ed552d91ec308d7be49a8c,"MacOS requires c++ flag (#1000)

Summary:
To install on MacOS, `-stdlib=libc++` needs to be specified.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1000

Differential Revision: D16733819

Pulled By: myleott

fbshipit-source-id: 7a1ed11e2b4e1071e61c64c379c84f72e02ad2b5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
816,Jingfei Du,jingfeidu@fb.com,2019-08-09 11:03:51-07:00,b6c55b62d72c0df3a80c2b326679bea6fa26a0cf,https://github.com/pytorch/fairseq/commit/b6c55b62d72c0df3a80c2b326679bea6fa26a0cf,"added sentence ranking task and loss (#809)

Summary:
This task and loss are used for sentence ranking and multiple choice tasks such as RACE
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/809

Reviewed By: myleott

Differential Revision: D16715745

Pulled By: jingfeidu

fbshipit-source-id: cb4d1c7b26ebb3e2382449ba51af5745ef56f30f",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
817,Myle Ott,myleott@fb.com,2019-08-09 22:35:41-07:00,a00ce132445703291626918bd1d6b7c05c9f7144,https://github.com/pytorch/fairseq/commit/a00ce132445703291626918bd1d6b7c05c9f7144,"Fix Python 3.5 compat

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1005

Differential Revision: D16751489

Pulled By: myleott

fbshipit-source-id: 6e372ac23643e32a3791044c13f4466bdc28f049",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
818,Myle Ott,myleott@fb.com,2019-08-10 08:13:26-07:00,832491962b30fb2164bed696e1489685a885402f,https://github.com/pytorch/fairseq/commit/832491962b30fb2164bed696e1489685a885402f,"Add WSC task and criterion

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004

Differential Revision: D16751443

Pulled By: myleott

fbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab",17,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
819,Myle Ott,myleott@fb.com,2019-08-10 10:43:10-07:00,c0a5d29e59e9b2d999298e06d0e46f96487eb024,https://github.com/pytorch/fairseq/commit/c0a5d29e59e9b2d999298e06d0e46f96487eb024,"Fix torch.hub for MNLI

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1006

Differential Revision: D16753078

Pulled By: myleott

fbshipit-source-id: 970055632edffcce4e75931ed93b42a249120a4a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
820,Myle Ott,myleott@fb.com,2019-08-12 07:13:55-07:00,3bbdc5543c4ec56a7b60a91a2cb9683fa15e5208,https://github.com/pytorch/fairseq/commit/3bbdc5543c4ec56a7b60a91a2cb9683fa15e5208,"Update --restore-file logic (partially fixes #999)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007

Differential Revision: D16762490

Pulled By: myleott

fbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
821,Myle Ott,myleott@fb.com,2019-08-12 07:41:49-07:00,969f4474bce2ebc850db012408dd394e426d3655,https://github.com/pytorch/fairseq/commit/969f4474bce2ebc850db012408dd394e426d3655,"Remove LAMB optimizer (at least until we can test it more)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1008

Differential Revision: D16763315

Pulled By: myleott

fbshipit-source-id: d4bad8384eec273f2d5de4ed29fb8d158ab9187c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
822,Myle Ott,myleott@fb.com,2019-08-12 09:05:01-07:00,2b68e91f231a2b7997664e1418f30b808d889963,https://github.com/pytorch/fairseq/commit/2b68e91f231a2b7997664e1418f30b808d889963,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817

Differential Revision: D16762905

Pulled By: myleott

fbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56",27,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
823,Myle Ott,myleott@fb.com,2019-08-12 14:29:32-07:00,d00366405b774671d2a4ca59a8dcf2ed0bb59119,https://github.com/pytorch/fairseq/commit/d00366405b774671d2a4ca59a8dcf2ed0bb59119,"Minor fixes for RACE finetuning (#818)

Summary:
- remove unnecessary extra spaces in RACE data in preprocessing
- fix finetuning instructions (add `--truncate-sequence` and add `--dropout` params)
- close file handle in SentenceRankingTask
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/818

Differential Revision: D16770055

Pulled By: myleott

fbshipit-source-id: 2c80084e92cdf8692f2ea7e43f7c344c402b9e61",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
824,Ilia Kulikov,kulikov@fb.com,2019-08-12 15:36:41-07:00,0563d879245d3c6bb04f2302782c935794635924,https://github.com/pytorch/fairseq/commit/0563d879245d3c6bb04f2302782c935794635924,"ignore files starting with . e.g. .ipynb_checkpoints (#819)

Summary:
.ipynb_checkpoints folder in models folders crashed the importlib
now there is a check for this
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/819

Differential Revision: D16772192

Pulled By: myleott

fbshipit-source-id: 01c956aef4ed312bc7645c31c83dbf98af89d931",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
825,Siddharth Shah,siddshah@fb.com,2019-08-12 17:30:55-07:00,577e4fa78a295fd7cd3ee7e9fd4b936ca800ebea,https://github.com/pytorch/fairseq/commit/577e4fa78a295fd7cd3ee7e9fd4b936ca800ebea,"fix cosine scheduler docstring

Summary: as title

Reviewed By: myleott

Differential Revision: D16773845

fbshipit-source-id: 2d10e197c31f94d894430559327289a4d03e33f7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
826,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-08-13 06:43:14-07:00,a171c2dda06f54ec404da9c496d396856fb1a5fc,https://github.com/pytorch/fairseq/commit/a171c2dda06f54ec404da9c496d396856fb1a5fc,"added readme code for inference with GLUE finetuned model

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/820

Differential Revision: D16783469

fbshipit-source-id: d5af8ba6a6685608d67b72d584952b8e43eabf9f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
827,Myle Ott,myleott@fb.com,2019-08-13 07:49:25-07:00,a33ac060de722d921d33dbc6e0c7c93bbff9ee9d,https://github.com/pytorch/fairseq/commit/a33ac060de722d921d33dbc6e0c7c93bbff9ee9d,"Add Commonsense QA task

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1014

Differential Revision: D16784120

Pulled By: myleott

fbshipit-source-id: 946c0e33b594f8378e4ab6482ce49efcb36e1743",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
828,Myle Ott,myleott@fb.com,2019-08-13 13:03:40-07:00,d015d23a1f3fbe01a12d9ee6d07ae0a59b6241f8,https://github.com/pytorch/fairseq/commit/d015d23a1f3fbe01a12d9ee6d07ae0a59b6241f8,"Add fairseq-validate

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/765

Differential Revision: D16763357

Pulled By: myleott

fbshipit-source-id: 758b03158e486ee82786e2d5bf4e46073b50c503",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
829,Myle Ott,myleott@fb.com,2019-08-13 17:34:29-07:00,baa8ce119d56939c751ca81f6d0ddbab6feefb66,https://github.com/pytorch/fairseq/commit/baa8ce119d56939c751ca81f6d0ddbab6feefb66,"Updates for PyTorch 1.2 masking/bool behavior

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/821

Differential Revision: D16790120

Pulled By: myleott

fbshipit-source-id: 2fb5070172636561d08596a29f08c93df07548bf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
830,Myle Ott,myleott@fb.com,2019-08-13 20:33:07-07:00,7c89e13f64897b4caf24c83faec72fb82711e418,https://github.com/pytorch/fairseq/commit/7c89e13f64897b4caf24c83faec72fb82711e418,"Fix tests

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/822

Differential Revision: D16800078

Pulled By: myleott

fbshipit-source-id: b86e08e01f2fe13c64b77f1d23a5f6800f252bf7",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
831,Myle Ott,myleott@fb.com,2019-08-14 05:00:05-07:00,ffffe04ea12679bdc12fbbafdc6406dd3e6c9943,https://github.com/pytorch/fairseq/commit/ffffe04ea12679bdc12fbbafdc6406dd3e6c9943,"v0.7.2 -> v0.8.0 (#1017)

Summary:
Changelog:
- Relicensed under MIT license
- Add RoBERTa
- Add wav2vec
- Add WMT'19 models
- Add initial ASR code
- Changed torch.hub interface (`generate` renamed to `translate`)
- Add `--tokenizer` and `--bpe`
- f812e52: Renamed data.transforms -> data.encoders
- 654affc: New Dataset API (optional)
- `47fd985`: Deprecate old Masked LM components
- `5f78106`: Set mmap as default dataset format and infer format automatically
- Misc fixes for sampling
- Misc fixes to support PyTorch 1.2
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1017

Differential Revision: D16799880

Pulled By: myleott

fbshipit-source-id: 45ad8bc531724a53063cbc24ca1c93f715cdc5a7",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
832,Myle Ott,myleott@fb.com,2019-08-14 08:24:36-07:00,b870468689a6d903fb6c2464ca18e07ce73444fb,https://github.com/pytorch/fairseq/commit/b870468689a6d903fb6c2464ca18e07ce73444fb,"Update READMEs

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/823

Differential Revision: D16804995

Pulled By: myleott

fbshipit-source-id: abac5dc0ed6b7bfe2309ba273456e54b37340b2c",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
833,Nathan Ng,n.ng555@gmail.com,2019-08-14 10:45:52-07:00,f840564da943f3f95bf80e93fd49be6f93d98348,https://github.com/pytorch/fairseq/commit/f840564da943f3f95bf80e93fd49be6f93d98348,"initial light and dynamic convolution kernels (#547)

Summary:
CUDA code for light/dynamicconv kernels, including pytorch modules. Modules can be built by running setup.py in each respective folder, and can then be imported and used like any other module.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/547

Reviewed By: myleott, shubho

Differential Revision: D15703660

Pulled By: nng555

fbshipit-source-id: e9c913753be3a1cd571965f7200df6678b644520",23,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
834,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-08-15 06:50:44-07:00,1d44cc8520fc7d2fb4957fcab32d102de6b30626,https://github.com/pytorch/fairseq/commit/1d44cc8520fc7d2fb4957fcab32d102de6b30626,"added effcient wsc task/criterion for winogrande (#825)

Summary:
1) So far getting `78%`  on winogrande validation dataset comapred to `63.5%` in the paper.
2) Will upgrade readme once everything is finalized.

Questions:

1) Should I just call `binary_wsc_task` instead of `winogrande` to be less specific to dataset and be generic?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/825

Differential Revision: D16810159

fbshipit-source-id: cfde73561fa4caaaa63a4773c0aecd12ce1fa518",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
835,Myle Ott,myleott@fb.com,2019-08-15 09:45:46-07:00,ac66df47b5394e730aa05efa50ed7ec6103388bb,https://github.com/pytorch/fairseq/commit/ac66df47b5394e730aa05efa50ed7ec6103388bb,"Update README

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/826

Differential Revision: D16830402

Pulled By: myleott

fbshipit-source-id: 25afaa6d9de7b51cc884e3f417c8e6b349f5a7bc",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
836,Nathan Ng,n.ng555@gmail.com,2019-08-15 10:03:44-07:00,49177c99c45f7d6e99a8f1500d16396e2d7b4519,https://github.com/pytorch/fairseq/commit/49177c99c45f7d6e99a8f1500d16396e2d7b4519,"Backward reranking public (#667)

Summary:
Implementation of noisy channel model reranking for release with paper
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/667

Reviewed By: michaelauli

Differential Revision: D15901665

Pulled By: nng555

fbshipit-source-id: 2de2c518be8e5828ffad72db3e741b0940623373",13,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
837,Myle Ott,myleott@fb.com,2019-08-15 12:02:25-07:00,a8e321116b2b64d9530413c9add9622955fe2498,https://github.com/pytorch/fairseq/commit/a8e321116b2b64d9530413c9add9622955fe2498,"Update README

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/827

Differential Revision: D16833252

Pulled By: myleott

fbshipit-source-id: 8eded8cc651002dfd60869fc2383d305ed335d3a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
838,Nayan Singhal,naysing@fb.com,2019-08-15 14:24:43-07:00,ed27ed8bacc2e1837f18942230b109f6815c73b6,https://github.com/pytorch/fairseq/commit/ed27ed8bacc2e1837f18942230b109f6815c73b6,"BMUF Resetting local state param

Summary:
BMUF
1) Resetting BMUF parameters after warmup.
2) Resetting local param state after warmup.
3) Allowing user to pass block momentum value instead of gpu derived Block Momentum.

Reviewed By: skritika, mrshenli

Differential Revision: D16692026

fbshipit-source-id: d02eaf29d0e4b37007418166ec937d4bf5fe6aca",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
839,Naman Goyal,namangoyal@learnfair0757.h2.fair,2019-08-16 07:46:08-07:00,a3cfd51dd92c60381c13435f8761ca66691bea6b,https://github.com/pytorch/fairseq/commit/a3cfd51dd92c60381c13435f8761ca66691bea6b,"added hf bert bpe

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/829

Differential Revision: D16856693

fbshipit-source-id: 545bbf4815f5c40e72a6ed241312a51dc90e34a1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
840,Naman Goyal,namangoyal@learnfair0757.h2.fair,2019-08-16 12:08:09-07:00,851c022610b27da3beaa4e40a6834b5fb3b44f44,https://github.com/pytorch/fairseq/commit/851c022610b27da3beaa4e40a6834b5fb3b44f44,"added check in token block dataset for multiple consecutive blank lines

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/830

Differential Revision: D16861799

fbshipit-source-id: d85deaf78ec5b9c23eafd4145a96252e3901fa22",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
841,Yongqiang Wang,yqw@fb.com,2019-08-16 20:59:48-07:00,732d15a98ac7435c2f391b00e54f6959595d8dd3,https://github.com/pytorch/fairseq/commit/732d15a98ac7435c2f391b00e54f6959595d8dd3,"implement tri-stage lr_scheduler (#1028)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1028

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/831

tri-stage lr-scheduler consisted of 3 stages: 1. warmup; 2. hold; 3.
(exponentially) decay; used in https://arxiv.org/pdf/1904.08779.pdf

Reviewed By: myleott

Differential Revision: D16806206

fbshipit-source-id: 40e472ec382449a0fb711f8ee980f14d27d2114a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
842,Chunting Zhou,chuntinz@fb.com,2019-08-19 07:28:41-07:00,0c75c7603185a1de8309ac91d496f88c7858624f,https://github.com/pytorch/fairseq/commit/0c75c7603185a1de8309ac91d496f88c7858624f,"Fix bug (the returned value has a dimension mismatch) in label-smoothed-cross-entropy for MoE (#1037)

Summary:
MoE will encounter a dimension mismatch bug when using label-smoothed cross entropy as the criterion, which occurs at [https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/translation_moe.py#L125](url). This is a fix to the bug.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1037

Differential Revision: D16892674

Pulled By: myleott

fbshipit-source-id: a73bc03d2280356667d02422d22ad11d968d0c65",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
843,freewym,freewym@gmail.com,2019-08-19 07:33:18-07:00,02cb5a43dbf33ac81aa3ba2f28e69b3b98adc223,https://github.com/pytorch/fairseq/commit/02cb5a43dbf33ac81aa3ba2f28e69b3b98adc223,"remove shlex.quote in scripts/spm_train.py (#972)

Summary:
to resolve the issue https://github.com/pytorch/fairseq/issues/971
Pull Request resolved: https://github.com/pytorch/fairseq/pull/972

Differential Revision: D16892827

Pulled By: myleott

fbshipit-source-id: baf277961f1e292f4593eefe31e3541aa9d0d8c4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
844,Trinkle23897,463003665@qq.com,2019-08-19 07:38:48-07:00,79460d34da272354c4f4ae0036245623287006ff,https://github.com/pytorch/fairseq/commit/79460d34da272354c4f4ae0036245623287006ff,"add constrains when checking multiple consecutive blank lines (#1031)

Summary:
It will cause runtime error on some standard datasets (e.g. wikitext-103).

Details:
After preprocessing to wikitext-103 folder with current master branch, I use fairseq-train and get the following Error:
```bash
Traceback (most recent call last):
  File ""/home/trinkle/.local/bin/fairseq-train"", line 11, in <module>
    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()
  File ""/data/git/Transformer/fairseq/fairseq_cli/train.py"", line 321, in cli_main
    main(args)
  File ""/data/git/Transformer/fairseq/fairseq_cli/train.py"", line 46, in main
    task.load_dataset(valid_sub_split, combine=False, epoch=0)
  File ""/data/git/Transformer/fairseq/fairseq/tasks/language_modeling.py"", line 167, in load_dataset
    break_mode=self.args.sample_break_mode, include_targets=True,
  File ""/data/git/Transformer/fairseq/fairseq/data/token_block_dataset.py"", line 54, in init
    ""Found multiple blank lines in the dataset, please remove them""
AssertionError: Found multiple blank lines in the dataset, please remove them (eg. cat -s raw.txt) and preprocess the data again.
```

It's because these datasets have multiple blank lines. The assertion is added in https://github.com/pytorch/fairseq/commit/851c022610b27da3beaa4e40a6834b5fb3b44f44, however, adding this assertion is not a good way.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1031

Differential Revision: D16892942

Pulled By: myleott

fbshipit-source-id: 90c41b7d98a7b78f506bb57320f9f6b901e05d5b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
845,Myle Ott,myleott@fb.com,2019-08-19 15:03:43-07:00,2eb53b8ef1c9f5033c669f4ebec41106a29368f9,https://github.com/pytorch/fairseq/commit/2eb53b8ef1c9f5033c669f4ebec41106a29368f9,"Add instructions to resume training from released RoBERTa models (fixes #1034)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1041

Differential Revision: D16904073

Pulled By: myleott

fbshipit-source-id: 22e5e25a15f7a0b6f2d827d98c953a6cec07610e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
846,Myle Ott,myleott@fb.com,2019-08-19 15:04:41-07:00,6ce55e4b011275e43404034832b40648b1483ff6,https://github.com/pytorch/fairseq/commit/6ce55e4b011275e43404034832b40648b1483ff6,"Small fixes

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/835

Differential Revision: D16904038

Pulled By: myleott

fbshipit-source-id: 2c9d0b913f8d688297ac80fcabd905bd1397f66a",41,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
847,Myle Ott,myleott@fb.com,2019-08-19 15:19:24-07:00,c81fed46ac7868c6d80206ff71c6f6cfe93aee22,https://github.com/pytorch/fairseq/commit/c81fed46ac7868c6d80206ff71c6f6cfe93aee22,"Back out ""[fairseq][PR] Fix bug (the returned value has a dimension mismatch) in label-smoothed-cross-entropy for MoE"" (#837)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/837

Original commit changeset: a73bc03d2280

Differential Revision: D16904372

fbshipit-source-id: b4c4047b2686ba47258cdf0783059726134c920a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
848,Dmytro Okhonko,oxo@fb.com,2019-08-20 12:35:57-07:00,4812f64b651ab64881510d38d4e35ce4ce22b04f,https://github.com/pytorch/fairseq/commit/4812f64b651ab64881510d38d4e35ce4ce22b04f,"Fix method has same name as property

Summary:
Training is failing sometimes because `self.collater` can be both method and property for AsrDataset
https://github.com/pytorch/fairseq/issues/1036

Reviewed By: jcai1

Differential Revision: D16919945

fbshipit-source-id: b34ba54e4dae315b7c723996610a348a8e3031af",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
849,Arya McCarthy,aryamc@fb.com,2019-08-20 14:24:28-07:00,9e5edc104a40708912f4cf463cadbaf912c1a263,https://github.com/pytorch/fairseq/commit/9e5edc104a40708912f4cf463cadbaf912c1a263,"Give path when checkpoint can't be found (#1040)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1040

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/836

Reviewed By: myleott, liezl200

Differential Revision: D16889252

fbshipit-source-id: 45a1b6c1217fb099f0350096e38e1c7d83ea0a64",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
850,Siddharth Dalmia,sdalmia@fb.com,2019-08-20 21:07:19-07:00,7a31fe068c8015e4ee9f8d45887efcd722b53fc0,https://github.com/pytorch/fairseq/commit/7a31fe068c8015e4ee9f8d45887efcd722b53fc0,"vggblock support without pooling and pooling_kernel_size missing self (#839)

Summary:
1) VggBlock was not supported if pooling kernel size was None.
2) Since we modify pooling kernel size by using _pair. We should use self.pooling_kernel_size. But I agree it doesn't matter as pytorch is robust to this.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/839

Differential Revision: D16934112

Pulled By: okhonko

fbshipit-source-id: b6b95163b0e7f7203d76d535f01a41912382bdc3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
851,alexeib,alexei.b@gmail.com,2019-08-21 09:53:50-07:00,a2f5361d7021ca49184545170d6dddc032647f3d,https://github.com/pytorch/fairseq/commit/a2f5361d7021ca49184545170d6dddc032647f3d,"Multiset (#838)

Summary:
Adds ability to tag individual examples with the names of their datasets, along with some minor miscellaneous fixes and improvements
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/838

Differential Revision: D16919175

Pulled By: alexeib

fbshipit-source-id: 4bf493299645bae63f3ee6382e15f18a9f73666c",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
852,Jeff Cai,jcai@fb.com,2019-08-21 13:41:41-07:00,ba5f829f64d2adf9de4502dab94abf0a1a54106b,https://github.com/pytorch/fairseq/commit/ba5f829f64d2adf9de4502dab94abf0a1a54106b,"Parameterized criterions (#808)

Summary:
Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.

With this diff, parameters in criterions will be:
(1) updated by optimizers, with a configurable learning rate
(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters
(3) synchronized across nodes in distributed training.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808

Reviewed By: jcai1

Differential Revision: D16934097

Pulled By: okhonko

fbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038",15,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
853,Trinkle23897,463003665@qq.com,2019-08-21 15:19:34-07:00,93057cc099041665f53744cda297a6935ebd9f5c,https://github.com/pytorch/fairseq/commit/93057cc099041665f53744cda297a6935ebd9f5c,"fix string format to work in python 3.5 (#1050)

Summary:
change string fromat in fairseq/data/subsample_dataset.py#20
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1050

Differential Revision: D16946060

Pulled By: okhonko

fbshipit-source-id: 0eabf22e7ffd4f658b6d18c87dc6e59c81a355c7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
854,Myle Ott,myleott@fb.com,2019-08-21 17:41:23-07:00,3c2cf3b02a29f44e540648bcecd5ff663ad6f2b5,https://github.com/pytorch/fairseq/commit/3c2cf3b02a29f44e540648bcecd5ff663ad6f2b5,"Misc changes

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/840

Differential Revision: D16947645

Pulled By: myleott

fbshipit-source-id: e869789bc22bbf5cb08d9adfa44f9fc09b3805af",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
855,Nathan Ng,n.ng555@gmail.com,2019-08-22 12:35:14-07:00,8c509a94faf84effa8cf652977bcd4f004cce6a1,https://github.com/pytorch/fairseq/commit/8c509a94faf84effa8cf652977bcd4f004cce6a1,"Add links to cuda models (#828)

Summary:
Add links to pre-trained cuda models in pay less attention
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/828

Reviewed By: michaelauli

Differential Revision: D16833577

Pulled By: nng555

fbshipit-source-id: 1556aa77fd87ea259812de8ef65963257c370f9b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
856,Nathan Ng,n.ng555@gmail.com,2019-08-22 16:23:02-07:00,d4c9136ce0d6101f52307098595addfcc6a53db5,https://github.com/pytorch/fairseq/commit/d4c9136ce0d6101f52307098595addfcc6a53db5,"Fix year in noisy channel citation (#842)

Summary:
2018->2019
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/842

Differential Revision: D16973530

Pulled By: nng555

fbshipit-source-id: 00207b79821ac0257a53a0581a84582130e1bff5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
857,Alexei Baevski,abaevski@fb.com,2019-08-22 18:33:34-07:00,6e2bd794e02d61e07fbf2173e7d172600f9cc276,https://github.com/pytorch/fairseq/commit/6e2bd794e02d61e07fbf2173e7d172600f9cc276,"wav2vec everstore support

Summary: changes for internal support

Differential Revision: D16646887

fbshipit-source-id: ac5bf6c32901819726249422324eae32a0a6e148",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
858,Naman Goyal,namangoyal@learnfair0356.h2.fair,2019-08-23 07:31:18-07:00,4fc39538aec5141aa41f5d6d7dc0097e7c0f7b48,https://github.com/pytorch/fairseq/commit/4fc39538aec5141aa41f5d6d7dc0097e7c0f7b48,"Cythonize token block dataset (#834)

Summary:
Cythonized token block dataset code, it's `> 100x` faster. Token block for entire `bookwiki+CC+stories+openweb` is just ~`39.9` seconds.

TODO:
1) I think, I can make it 2x more faster.
2) cleanup.

EDIT History:
~~First pass at parellelizing `token_block_dataset`. The code feels somewhat complicated and cluttered.
This is 2-3x faster though on my tests on `bookwiki` dataset with both `complete` and `complete_doc` modes.
myleott Can you take a look for correctness as I am still not 100% sure that I am not missing corner cases.~~
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/834

Test Plan:
Imported from GitHub, without a `Test Plan:` line.

Test workflow: f133816198

Reviewed By: myleott

Differential Revision: D16970257

Pulled By: myleott

fbshipit-source-id: ec45a308193c9e9f3e7075336c15df4723228d6f",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
859,Myle Ott,myleott@fb.com,2019-08-23 07:33:38-07:00,833f053dd73e1f9ff5f898b2c2aabf4e5b0ae865,https://github.com/pytorch/fairseq/commit/833f053dd73e1f9ff5f898b2c2aabf4e5b0ae865,"Suppress leaked semaphore warnings

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/844

Differential Revision: D16985131

Pulled By: myleott

fbshipit-source-id: 66ba3b9aa0cdf329a1e38fc09786f34906afdb43",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
860,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-08-26 07:18:01-07:00,8a8c0691baf1f9b626f99ce5e326bc710e004b71,https://github.com/pytorch/fairseq/commit/8a8c0691baf1f9b626f99ce5e326bc710e004b71,"fix cython dependency in the setup (#847)

Summary:
Fixes broken build for `pytext` https://github.com/pytorch/fairseq/commit/4fc39538aec5141aa41f5d6d7dc0097e7c0f7b48

Earlier version of setup tools required `cython` to be installed before even starting setup.py. This one fixes it.
More details: https://github.com/pypa/setuptools/blob/master/CHANGES.rst#180
and https://stackoverflow.com/questions/37471313/setup-requires-with-cython
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/847

Differential Revision: D16997450

fbshipit-source-id: 5f65026c228a1b94280ca73937078ee3e21ce4f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
861,Alexei Baevski,abaevski@fb.com,2019-08-26 18:30:17-07:00,3ab8e0fd73dc85eb62acbf2e0f25a350d33fd2d7,https://github.com/pytorch/fairseq/commit/3ab8e0fd73dc85eb62acbf2e0f25a350d33fd2d7,"wav2vec everstore support fix

Summary: fixes some merge issues that prevented wav2vec from training properly

Reviewed By: myleott

Differential Revision: D16981120

fbshipit-source-id: cad39aaf2f44daabcbafe7b4e8735d055b3842a7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
862,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-08-27 07:10:35-07:00,396ff7f59f027a98d2df9951ed15045bdd91554b,https://github.com/pytorch/fairseq/commit/396ff7f59f027a98d2df9951ed15045bdd91554b,"installing numpy headers for cython

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/848

Differential Revision: D17060283

fbshipit-source-id: c7e61cae76a0566cc3e2ddc3ab4d48f8dec9d777",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
863,Sosuke Kobayashi,soskek@users.noreply.github.com,2019-08-27 08:39:33-07:00,920b85d4bd39e181229db5639c701c854c83ec5c,https://github.com/pytorch/fairseq/commit/920b85d4bd39e181229db5639c701c854c83ec5c,"Minor update of README.md of language model example (#1063)

Summary:
With this white space, the command might fail.
```
fairseq-preprocess: error: unrecognized arguments:
zsh: command not found: --destdir
```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1063

Differential Revision: D17072516

Pulled By: myleott

fbshipit-source-id: 68bb9d05b40b215b18aceac2bff3f5ec1ef2f537",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
864,Myle Ott,myleott@fb.com,2019-08-27 10:06:26-07:00,d2410c4207b3a32cd1147236982abec2273a3d69,https://github.com/pytorch/fairseq/commit/d2410c4207b3a32cd1147236982abec2273a3d69,"Minor cleanup for setup.py

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1078

Differential Revision: D17072514

Pulled By: myleott

fbshipit-source-id: 69a8c8c9cc7caa7e04c414329a5d79e6e1a6621c",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
865,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-08-28 11:35:15-07:00,108f94bc2aaf32ba1882dc9fd8f014496fe8f0c5,https://github.com/pytorch/fairseq/commit/108f94bc2aaf32ba1882dc9fd8f014496fe8f0c5,"use numpy function for filter by size when possible (#845)

Summary:
For general Masked language modeling use-case, this is much faster, (`3 minutes vs 1 sec`).

Let me know what you think about it myleott, if you don't like all the special case checking, we can think of reorganizing the dataset APIs to always have `sizes` as property calculated in `__init__`.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/845

Reviewed By: myleott

Differential Revision: D16993769

Pulled By: myleott

fbshipit-source-id: 161bba62af2965190c07c47e838ee967cb886e88",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
866,Myle Ott,myleott@fb.com,2019-08-28 21:42:30-07:00,0a96d22f2ea7b1158e8da2d348e93ddb91ef1f7d,https://github.com/pytorch/fairseq/commit/0a96d22f2ea7b1158e8da2d348e93ddb91ef1f7d,"Fix multi-gpu training (fixes #1088)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1089

Differential Revision: D17108918

Pulled By: myleott

fbshipit-source-id: 818c77a5bbf3b146028991aca64d79b93f144b28",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
867,Paul O'Shannessy,poshannessy@fb.com,2019-08-29 23:19:10-07:00,8777465b8fdceeacb1d2fc8535b46c4971527f42,https://github.com/pytorch/fairseq/commit/8777465b8fdceeacb1d2fc8535b46c4971527f42,"Adopt Contributor Covenant

Summary:
In order to foster healthy open source communities, we're adopting the
[Contributor Covenant](https://www.contributor-covenant.org/). It has been
built by open source community members and represents a shared understanding of
what is expected from a healthy community.

Reviewed By: josephsavona, danobi, rdzhabarov

Differential Revision: D17104640

fbshipit-source-id: d210000de686c5f0d97d602b50472d5869bc6a49",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
868,alexeib,alexei.b@gmail.com,2019-08-30 16:23:40-07:00,4a7cd5825717b0b0c96a6463e0dd2d7b18dd4331,https://github.com/pytorch/fairseq/commit/4a7cd5825717b0b0c96a6463e0dd2d7b18dd4331,"set numpy seed explicitly + other minor fixes (#850)

Summary:
not setting the numpy seed explicitly at the beginning was an extremely annoying bug to find. it it caused different gpus to have a different view of data if some randomization was used in the dataset (e.g. subsample dataset)
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/850

Differential Revision: D17085006

Pulled By: alexeib

fbshipit-source-id: 62bb2116369fb703df878e6bc24c06f1ea4e75a0",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
869,alexeib,alexei.b@gmail.com,2019-08-31 01:01:00-07:00,c1951aa277618848bb007cd9459e153a4d84a1d1,https://github.com/pytorch/fairseq/commit/c1951aa277618848bb007cd9459e153a4d84a1d1,"add missing colorize dataset

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/851

Differential Revision: D17145769

Pulled By: alexeib

fbshipit-source-id: 9dd26799d044ae5386e8204a129b5e3fc66d6e85",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
870,Myle Ott,myleott@fb.com,2019-08-31 13:43:00-07:00,746e59a26239a7cb96e42b879b81b887eeeb7454,https://github.com/pytorch/fairseq/commit/746e59a26239a7cb96e42b879b81b887eeeb7454,"Improve support for `python setup.py build_ext --inplace`

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/852

Differential Revision: D17147452

Pulled By: myleott

fbshipit-source-id: 5fd9c7da3cc019c7beec98d41db1aef1329ee57a",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
871,Myle Ott,myleott@fb.com,2019-08-31 16:52:03-07:00,8d4588b1bacc10e9df70ba28bd3a308b2ca894d6,https://github.com/pytorch/fairseq/commit/8d4588b1bacc10e9df70ba28bd3a308b2ca894d6,"Cleaner handling of numpy-based extensions in setup.py

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/853

Differential Revision: D17147879

Pulled By: myleott

fbshipit-source-id: b1f5e838533de62ade52fa82112ea5308734c70f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
872,Naman Goyal,namangoyal@learnfair0753.h2.fair,2019-09-01 12:14:08-07:00,20dfba73b9993864b24b456b4e46e6e3994114c0,https://github.com/pytorch/fairseq/commit/20dfba73b9993864b24b456b4e46e6e3994114c0,"fixed numpy based size filtering (#854)

Summary:
This bug got introduced in my [commit](https://github.com/fairinternal/fairseq-py/commit/9624f9651478bcb88022decf7e1b0685b410133b) for fast numpy based size filtering.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/854

Differential Revision: D17150350

fbshipit-source-id: cb564119543e116d6a17784d1c22e9bce7059a0c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
873,altale,578340750@qq.com,2019-09-03 07:44:02-07:00,6c00b338c54b17bfd5343a4eabcb4d0df160764e,https://github.com/pytorch/fairseq/commit/6c00b338c54b17bfd5343a4eabcb4d0df160764e,"Fix an error in the command about Hierarchical Neural Story Generation (#1099)

Summary:
When I try to reproduce the experiment in  _Hierarchical Neural Story Generation_, I found the command about generation cannot be executed.

It said that **fairseq-generate: error: unrecognized arguments: --sampling-temperature 0.8**
In the document, I find:
```
--temperature   temperature for generation
Default: 1.0
```
And I don't find a parameter named `--sampling-temperature`, so I think the parameter `--sampling-temperature` should be changed to `--temperature`
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1099

Differential Revision: D17163065

Pulled By: myleott

fbshipit-source-id: 25c430eeee4703f8ec30353825ffec4bb973da0d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
874,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-09-03 09:06:44-07:00,1f0f7cd82ce88ef1c074c2e30ac5417f8d104cb1,https://github.com/pytorch/fairseq/commit/1f0f7cd82ce88ef1c074c2e30ac5417f8d104cb1,"added cython to install_requires

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/856

Reviewed By: myleott

Differential Revision: D17162411

Pulled By: myleott

fbshipit-source-id: e70ecc802398bbba2b5326e9700f2121c422fd18",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
875,Peng-Jen Chen,pipibjc@fb.com,2019-09-03 20:18:03-07:00,1566cfb9634fd06c551df510ac24f36c4bfca5a6,https://github.com/pytorch/fairseq/commit/1566cfb9634fd06c551df510ac24f36c4bfca5a6,"Fix multilingual translation bug for to-many case

Summary:
The logic for adding decoder side language token was wrongly implemented.
The way we inject the language token is by replacing the eos symbol with language token symbol. However, the parameter for source / target eos symbol was not set correctly.

Reviewed By: tangyuq

Differential Revision: D17129108

fbshipit-source-id: 6fae385b787370656fd7ca7ab74e6bb91fe5463b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
876,Roman Rädle,raedle@fb.com,2019-09-05 15:33:53-07:00,3e3fe72299980f53262880e24e372ed7d785093c,https://github.com/pytorch/fairseq/commit/3e3fe72299980f53262880e24e372ed7d785093c,"Return predicted token for RoBERTa filling mask

Summary:
Added the `predicted_token` to each `topk` filled output item

Updated RoBERTa filling mask example in README.md

Reviewed By: myleott

Differential Revision: D17188810

fbshipit-source-id: 5fdc57ff2c13239dabf13a8dad43ae9a55e8931c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
877,Nayan Singhal,naysing@fb.com,2019-09-12 10:56:16-07:00,1fd8943e94cd303b8bbdc64f6aeb064732ecaf9b,https://github.com/pytorch/fairseq/commit/1fd8943e94cd303b8bbdc64f6aeb064732ecaf9b,"Average local optimizer param after warmup and during bmuf sync

Summary: We have seen that averaging the local param instead of doing reset or broadcast after warmup improves the WER.

Reviewed By: skritika

Differential Revision: D16739278

fbshipit-source-id: 75033d2d25f9a88fd6dd325d0d9d4c856d22d947",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
878,Naman Goyal,namangoyal@learnfair0685.h2.fair,2019-09-16 12:52:05-07:00,e1ba32aae224897bde5dd421f6be04f1e93f72eb,https://github.com/pytorch/fairseq/commit/e1ba32aae224897bde5dd421f6be04f1e93f72eb,"added fast stats sync option (#858)

Summary:
Added `--fast-stat-sync` option.
This avoids pickle and achieves `~7%` more `wps` on 16 nodes.
It is less flexible as it just aggregates only basic stats and it ignores the aggregate function defined by criterion.

Let me know what you think myleott
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/858

Differential Revision: D17398770

fbshipit-source-id: 36261a1d970e67deeda8211af8f009ef9b4f9c14",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
879,Myle Ott,myleott@fb.com,2019-09-17 13:39:15-07:00,a3882abfbd5e68c2d5d667be63fb670f3783a65b,https://github.com/pytorch/fairseq/commit/a3882abfbd5e68c2d5d667be63fb670f3783a65b,"Update README.md

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1140

Differential Revision: D17431506

Pulled By: myleott

fbshipit-source-id: b47dae303d7e76daa5b49795476b5e48d7b090ad",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
880,Nelson Liu,nelson-liu@users.noreply.github.com,2019-09-17 13:59:46-07:00,31dd13fa651cb6a604c8f1f513394792b6678354,https://github.com/pytorch/fairseq/commit/31dd13fa651cb6a604c8f1f513394792b6678354,"Fix link to RACE fine-tuning instructions.

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1125

Differential Revision: D17431557

Pulled By: myleott

fbshipit-source-id: f712e5355d8dbb0a8f1170674d62e2b6880295b4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
881,Naman Goyal,namangoyal@learnfair0893.h2.fair,2019-09-18 10:05:01-07:00,718677ebb044e27aaf1a30640c2f7ab6b8fa8509,https://github.com/pytorch/fairseq/commit/718677ebb044e27aaf1a30640c2f7ab6b8fa8509,"dont project maske tokens for mlm loss (#859)

Summary:
This saves ~4-5gb gpu memory while training roberta large with `seq_len=512`.

I am able to fit `--max-sentences=16` on `volta32gb` for `roberta-large`
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/859

Differential Revision: D17435814

fbshipit-source-id: 2663909768fac0ef0102107613770ee01b1f8c00",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
882,Akhilesh Gotmare,dg.akhilesh@gmail.com,2019-09-18 15:20:27-07:00,8dbee4ab8fdce42be0ac72dab100d8e1f8434979,https://github.com/pytorch/fairseq/commit/8dbee4ab8fdce42be0ac72dab100d8e1f8434979,"Minor fix to make adafactor work for >2d conv kernels (#1122)

Summary:
missing .unsqueeze(-1) in line 124,
without this change we'll encounter runtime error for >2d convolutional kernels, with this fix, we're applying adafactor's 2d logic to the two final dimensions.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1122

Differential Revision: D17431662

Pulled By: myleott

fbshipit-source-id: e7435e77270a9252f75f01b2457ef0048f5bcf36",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
883,Jerry Ma,noreplyspamblackhole@gmail.com,2019-09-18 15:57:15-07:00,f994c9b8b5655b889bc58d9b20d51e538a470c3c,https://github.com/pytorch/fairseq/commit/f994c9b8b5655b889bc58d9b20d51e538a470c3c,"Add autogenerated cython files to gitignore (#860)

Summary:
`python setup.py build_ext --inplace` generates C++ source files directly in the Python source tree. They should most likely be ignored by git.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/860

Differential Revision: D17460597

Pulled By: jma127

fbshipit-source-id: 72a29d438ebb57627b68ec7e9a2a77c8a36f1c21",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
884,Myle Ott,myleott@fb.com,2019-09-18 18:25:18-07:00,0eaaf3551659de40dcb273b7a0a3272e50a6e38a,https://github.com/pytorch/fairseq/commit/0eaaf3551659de40dcb273b7a0a3272e50a6e38a,"Add cython language_level hints

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1147

Differential Revision: D17468447

Pulled By: myleott

fbshipit-source-id: 0dbac04b92c8df74ad991d5e92cd02036d662369",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
885,Jerry Ma,noreplyspamblackhole@gmail.com,2019-09-19 10:34:23-07:00,a8a85c267693f450c8039f4cfec217df8390edde,https://github.com/pytorch/fairseq/commit/a8a85c267693f450c8039f4cfec217df8390edde,"Add dataset class for weighted sampling with replacement. (#861)

Summary:
As discussed with Naman earlier today. Weighted sampling with
replacement can be done on a per-epoch basis using `set_epoch()`
functionality, which generates the samples as a function of random seed
and epoch.

Additionally, `FairseqTask` needs to set the starting epoch for the
dataset at the very beginning of iterator construction.

Not yet implemented is the per-epoch iterator construction, which
is necessary to actually regenerate the batches for each epoch.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/861

Differential Revision: D17460687

Pulled By: jma127

fbshipit-source-id: 1c2a54f04ac96b3561c100a6fd66a9fccbe3c658",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestResamplingDataset(unittest.TestCase):'],[],['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['len(self.dataset) == len(self.strings) == len(self.weights)', 'len(resampling_dataset) == self.size_ratio * len(self.strings)', 'len(indices) == len(resampling_dataset)', 'resampling_dataset[i] == resampling_dataset[i]', 'cur_size == len(resampling_dataset[i])', 'set(freqs.keys()) == set(self.strings)', 'not results[]', 'results[] < 0.02', 'results[]', 'results[] < 0.02']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
886,Naman Goyal,namangoyal@learnfair1595.h2.fair,2019-09-20 09:33:29-07:00,32335404f09c47cccbfbf731abc4c510d0eef043,https://github.com/pytorch/fairseq/commit/32335404f09c47cccbfbf731abc4c510d0eef043,"added multilingual masked LM training (#849)

Summary:
The multilingual-RoBERTa training is working with aconneau XLM data.

Two pieces remaining:

1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.

2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849

Differential Revision: D17162460

fbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
887,Myle Ott,myleott@fb.com,2019-09-20 14:51:32-07:00,e869c80df6bc13163e9a120ce1ea7fecc2e4702c,https://github.com/pytorch/fairseq/commit/e869c80df6bc13163e9a120ce1ea7fecc2e4702c,"Update README.race.md

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1155

Differential Revision: D17509762

Pulled By: myleott

fbshipit-source-id: 4de535289c1f35abff0d8142d8580f3ede039f47",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
888,Myle Ott,myleott@fb.com,2019-09-20 15:11:03-07:00,10f9349e8a1f624255166aeef1c9c721de93041c,https://github.com/pytorch/fairseq/commit/10f9349e8a1f624255166aeef1c9c721de93041c,"Remove extraneous call to RNG in multi-GPU code path

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/865

Differential Revision: D17510276

Pulled By: myleott

fbshipit-source-id: 24119402ad5fe95a1312fadb77bafe49a9197c6b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
889,Naman Goyal,namangoyal@learnfair0339.h2.fair,2019-09-23 11:32:20-07:00,3b09b98b662d4bbde50ebd067f7a14268b2eab1e,https://github.com/pytorch/fairseq/commit/3b09b98b662d4bbde50ebd067f7a14268b2eab1e,"fixed train valid epoch iter

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/866

Differential Revision: D17517115

fbshipit-source-id: fd6921e642c99e37fce6ad58b24c93e70a5364e5",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
890,Jerry Ma,noreplyspamblackhole@gmail.com,2019-09-23 12:25:44-07:00,3f4fc5016334255d6908b20202267ca0b0287335,https://github.com/pytorch/fairseq/commit/3f4fc5016334255d6908b20202267ca0b0287335,"Miscellaneous documentation improvements: (#868)

Summary:
- More clearly document the correspondence between FairseqAdam and torch.optim.AdamW
- Add ResamplingDataset to Sphinx docs
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/868

Differential Revision: D17523244

Pulled By: jma127

fbshipit-source-id: 8e7b34b24889b2c8f70b09a52a625d2af135734b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
891,Naman Goyal,namangoyal@learnfair1193.h2.fair,2019-09-23 16:13:00-07:00,2ed65b68e06cae536ec7687286829033fbf241d7,https://github.com/pytorch/fairseq/commit/2ed65b68e06cae536ec7687286829033fbf241d7,"fixed corner case in mlm criterion when all tokens get masked

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/869

Reviewed By: myleott

Differential Revision: D17531776

Pulled By: myleott

fbshipit-source-id: 349c9449a0a7db5d3bb8449561302d4220cfa60c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
892,Jamie Morton,jmorton@eng.ucsd.edu,2019-09-24 06:14:24-07:00,fa7dea6bf847404f24b9591aeef8bc96e6bf90c5,https://github.com/pytorch/fairseq/commit/fa7dea6bf847404f24b9591aeef8bc96e6bf90c5,"Issue 1146: Minor fix to roberta pre-training readme (#1165)

Summary:
This is to make this instructions a little more generalizable, since in some systems, bash will parse the spaces within quotes

Addressing https://github.com/pytorch/fairseq/issues/1146
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1165

Differential Revision: D17547810

Pulled By: myleott

fbshipit-source-id: 5a026d42f678126b5ca8bc4477ba8f26ea549dcd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
893,vineetk1,vineetchandhok@gmail.com,2019-09-26 08:18:36-07:00,e073ddfe46d71f80340c2600a7bf9aed2696c692,https://github.com/pytorch/fairseq/commit/e073ddfe46d71f80340c2600a7bf9aed2696c692,"PR for Issue #1154: Two comments in lstm.py seem to be incorrect

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1185

Differential Revision: D17602249

Pulled By: lematt1991

fbshipit-source-id: bd515b7d2ebce8181a80684f45223a8db7c7e3cd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
894,Zhanghao Wu,zhanghao.wu@outlook.com,2019-09-27 07:26:12-07:00,2314979ea59965caa1b1c566fd0e0e662ab0d212,https://github.com/pytorch/fairseq/commit/2314979ea59965caa1b1c566fd0e0e662ab0d212,"Update getting_started.rst (#1188)

Summary:
Hi,

I think there is a minor mistake in the doc. `--distributed-no-spawn` argument is needed for distributed training on multiple machines without `slurm`. Otherwise, the program will start 8 jobs on each GPU, when `nproc_per_node=8`.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1188

Differential Revision: D17627778

Pulled By: myleott

fbshipit-source-id: 35ab6b650dc1132d7cb2d150e80d2ebf0caf3e69",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
895,Louis Martin,louisrtm@gmail.com,2019-09-27 07:34:11-07:00,62e65c418b1914ccf448783f66eed3d2f4a41525,https://github.com/pytorch/fairseq/commit/62e65c418b1914ccf448783f66eed3d2f4a41525,"Explain the language modelling format in RoBERTa pretraining readme

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1174

Differential Revision: D17627767

Pulled By: myleott

fbshipit-source-id: 7b5f77146b8776a5967699e430136039c066c851",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
896,Nayan Singhal,naysing@fb.com,2019-09-27 10:44:56-07:00,6c1da0f74b1187060715219da306cc1af6505e1b,https://github.com/pytorch/fairseq/commit/6c1da0f74b1187060715219da306cc1af6505e1b,"Fixing BMUF warmup and sync strategy

Summary:
Bmuf sync started happening even before warmup is done.
This diff fixes the behavior and do bmuf sync once warmup is done or if it's zero.

TODO: write a unit test case so that these problems can be figure out faster.

Reviewed By: jay-mahadeokar

Differential Revision: D17356277

fbshipit-source-id: 21500e6ed1225b97794e4ee203e5d7d04a2840f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
897,Changhan Wang,changhan@fb.com,2019-09-27 13:56:47-07:00,86857a58bf2919c7bec3c29c58234aa4c434d566,https://github.com/pytorch/fairseq/commit/86857a58bf2919c7bec3c29c58234aa4c434d566,"Levenshtein Transformer paper code

Summary:
Code for our NeurIPS paper [Levenshtein Transformer](https://arxiv.org/abs/1905.11006)
* Added Levenshtein Transformer model, task and criterion class
* Added iterative NAT Transformer, insertion Transformer and CMLM Transformer model class for baselines
* Add an option for prepending BOS to dictionary class and translation task class

Reviewed By: myleott

Differential Revision: D17297372

fbshipit-source-id: 54eca60831ae95dc721c2c34e882e1810ee575c7",25,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
898,Aditya Chetan,achetan40@gmail.com,2019-09-27 16:06:03-07:00,1cb267ed9412e9f86b27e0ebd0cd0ebae3f5bc58,https://github.com/pytorch/fairseq/commit/1cb267ed9412e9f86b27e0ebd0cd0ebae3f5bc58,"Fixing example of batched predictions for Roberta (#1195)

Summary:
For batched predictions in Roberta, the README was giving an example that was pretty unclear. After a thorough discussion with ngoyal2707 in issue https://github.com/pytorch/fairseq/issues/1167 he gave a clear example of how batched predictions were supposed to be done. Since I spent a lot of time on this inconsistency, I thought that it might benefit the community if his solution was in the official README 😄 !

For for details, see issue https://github.com/pytorch/fairseq/issues/1167
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1195

Differential Revision: D17639354

Pulled By: myleott

fbshipit-source-id: 3eb60c5804a6481f533b19073da7880dfd0d522d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
899,Myle Ott,myleott@fb.com,2019-09-28 08:56:03-07:00,ea1a410d590e63e6fd24942ab8376600c12e2194,https://github.com/pytorch/fairseq/commit/ea1a410d590e63e6fd24942ab8376600c12e2194,"RoBERTa now supported on TPU and TensorFlow via transformers library

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1197

Differential Revision: D17651374

Pulled By: myleott

fbshipit-source-id: 5feb986de1e682eb83c4479f419ad51325718572",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
900,Stephan Peitz,speitz@apple.com,2019-09-29 05:08:24-07:00,4ac2c5f2cc8a8b1f221f1e8e9b7839f07c25d997,https://github.com/pytorch/fairseq/commit/4ac2c5f2cc8a8b1f221f1e8e9b7839f07c25d997,"Implementation of the WeCNLP abstract ""Cross+Self-Attention for Transformer Models"" (#1097)

Summary:
This PR implements a new attention module which combines cross-attention (encoder-decoder attention) and the decoder self-attention. This work was accepted as an abstract at WeCNLP 2019 (https://www.wecnlp.ai/wecnlp-2019).

Cross+Self-Attention reduces the amount of parameter and increases the inference speed without any degradation in translation quality.
More details can be found in the attached [abstract](https://github.com/pytorch/fairseq/files/3561282/paper.pdf)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1097

Differential Revision: D17653168

Pulled By: myleott

fbshipit-source-id: deb834c2c78a229d7418ffbfea20ba3ce252991c",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
901,Guntupalli Venkata Sai Kalyan,kalyan-6941@kalyan-6941.csez.zohocorpin.com,2019-09-29 14:51:52-07:00,13519720f3132c38d1d8f2145b73216d42a62a67,https://github.com/pytorch/fairseq/commit/13519720f3132c38d1d8f2145b73216d42a62a67,"fix typo in README of examples/translation

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1200

Differential Revision: D17659658

Pulled By: myleott

fbshipit-source-id: 1863e6d60a439dbb7e71e5da68817c9d53649737",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
902,Myle Ott,myleott@fb.com,2019-09-30 02:09:13-07:00,acb6fba005f45e363a6da98d7ce79c36c011d473,https://github.com/pytorch/fairseq/commit/acb6fba005f45e363a6da98d7ce79c36c011d473,"Fix torch.hub to not depend on libnat

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/878

Differential Revision: D17661768

Pulled By: myleott

fbshipit-source-id: 1e4c5f09eb14c40d491ca2459fd2adb8382fb6d2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
903,Sarthak Garg,sarthak_garg@apple.com,2019-09-30 06:56:15-07:00,1c6679294848f303a361cba7b306b760e299bd9c,https://github.com/pytorch/fairseq/commit/1c6679294848f303a361cba7b306b760e299bd9c,"Implementation of the paper ""Jointly Learning to Align and Translate with Transformer Models"" (#877)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/877

This PR implements guided alignment training described in  ""Jointly Learning to Align and Translate with Transformer Models (https://arxiv.org/abs/1909.02074)"".

In summary, it allows for training selected heads of the Transformer Model with external alignments computed by Statistical Alignment Toolkits. During inference, attention probabilities from the trained heads can be used to extract reliable alignments. In our work, we did not see any regressions in the translation performance because of guided alignment training.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1095

Differential Revision: D17170337

Pulled By: myleott

fbshipit-source-id: daa418bef70324d7088dbb30aa2adf9f95774859",20,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
904,Chenyang Yu,chenyangyu@instagram.com,2019-10-01 11:12:06-07:00,58e43cb3ff18f1f47fd62926f00c70cb5920a66f,https://github.com/pytorch/fairseq/commit/58e43cb3ff18f1f47fd62926f00c70cb5920a66f,"extract FP16OptimizerMixin for share the same logic in PyText (#1180)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1180

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/874

extract FP16OptimizerMixin for share the same logic in PyText

Reviewed By: hudeven

Differential Revision: D17594102

fbshipit-source-id: 8625a4e4f3e09cbaba6ae92599c1121b86ed4e78",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
905,Debojeet Chatterjee,debo@fb.com,2019-10-04 09:59:33-07:00,de348d1f0a6862b50e2f9ca6ba821d6fd26d5a59,https://github.com/pytorch/fairseq/commit/de348d1f0a6862b50e2f9ca6ba821d6fd26d5a59,"Native Torchscript Wordpiece Tokenizer Op for BERTSquadQA, Torchscriptify BertSQUADQAModel (#879)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/879

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1023

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1211

Added a new native op that does wordpiece tokenization while additionally returning token start and end indices in the raw text as required by BertSquadQA. Includes Unit Tests for the native op and also to check its parity with the PyText Wordpiece Tokenizer.

Also combined is a torchscript implementation of the Bert SQUAD QA Model.

There are scripts for evaluation and testing of the torchscript code as well.

Reviewed By: borguz, hikushalhere

Differential Revision: D17455985

fbshipit-source-id: c2617c7ecbce0f733b31d04558da965d0b62637b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
906,Jerry Ma,noreplyspamblackhole@gmail.com,2019-10-04 13:35:25-07:00,315c463d4546037bb6698fcd504f647a03d795cc,https://github.com/pytorch/fairseq/commit/315c463d4546037bb6698fcd504f647a03d795cc,"Add periodic CUDA cache cleanup (#882)

Summary:
This adds a periodic call to `torch.cuda.empty_cache()` in order to
mitigate memory fragmentation in the PyTorch CUDA cached allocator
that can cause OOMs on models approaching GPU memory limit.
By default, this will occur every 64 updates.

Performance considerations:

- I've benchmarked this on a reasonably large model with memory
  footprint 16 GB, and the overhead with the default setting is <0.2%.
  With `update-freq > 1`, the cost is mitigated even further.
- This behavior can be disabled with a value of zero.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/882

Differential Revision: D17742386

Pulled By: jma127

fbshipit-source-id: 68d8f93f798d6818b5efc3d67d43b52dfb8b2865",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
907,alexeib,alexei.b@gmail.com,2019-10-04 17:09:27-07:00,4cb895b6f6b3e16ceeefb579432bddb1a73c1e39,https://github.com/pytorch/fairseq/commit/4cb895b6f6b3e16ceeefb579432bddb1a73c1e39,"add pre-trained wav2vec model

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/884

Differential Revision: D17774515

Pulled By: alexeib

fbshipit-source-id: d1ffe8ab723fa284c69b067bbd43d699eaa2f02f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
908,Nayan Singhal,naysing@fb.com,2019-10-07 11:13:34-07:00,6f58e15e240007bb58fa9364d9bd35295548f1d7,https://github.com/pytorch/fairseq/commit/6f58e15e240007bb58fa9364d9bd35295548f1d7,"Setting Global sync to 50 in BMUF

Summary:
In all our final settings, we are using global_sync = 50 and we get comparable results with DDP and caffe2.

Setting the default global-sync-iter = 50
and users can just define --use-bmuf to enable it for training.

Reviewed By: skritika

Differential Revision: D17765094

fbshipit-source-id: 369591eeff266d757f89e1fc8dda01711146fdbc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
909,Changhan Wang,changhan@fb.com,2019-10-08 14:08:05-07:00,c2165224d198450a3b4329ae099a772aa65d51c5,https://github.com/pytorch/fairseq/commit/c2165224d198450a3b4329ae099a772aa65d51c5,"fix max lengths in Levenshtein Tramsformer

Summary: Fix the max length calculation in Levenshtein Transformer

Reviewed By: jhcross

Differential Revision: D17672946

fbshipit-source-id: e5efbe7e56cf879d3e822864e4398f99f45b04d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
910,Jungo Kasai,jkasai@fb.com,2019-10-08 14:55:25-07:00,34e79c58d3cd71c8cd3e5604b4faa8df964a4763,https://github.com/pytorch/fairseq/commit/34e79c58d3cd71c8cd3e5604b4faa8df964a4763,"ensemble levts

Summary:
Add ensemble wrappers to the levenshtein NAT.
Levenshtein
Final softmax ensemble over the pipeline of three steps: deletion, placeholder insertion, and word selection.
1. Deletion
2. Placeholder Insertion
3. Word Selection

Each step involves scoring, averaging the scores over the ensemble, and then make hard decisions with argmax. Then next step follows. We cannot do the three steps in parallel by design.

Reviewed By: kahne

Differential Revision: D17723202

fbshipit-source-id: 05f7a4fcd922a972cc4796ca397e8220f0b4d53e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
911,Jerry Ma,noreplyspamblackhole@gmail.com,2019-10-08 16:34:51-07:00,63b6b3f411fd037d97f452df0417171ba5aa4f5d,https://github.com/pytorch/fairseq/commit/63b6b3f411fd037d97f452df0417171ba5aa4f5d,"Add printing of PyTorch memory summary on OOM (#885)

Summary:
PyTorch now has more comprehensive memory instrumentation, added in https://github.com/pytorch/pytorch/pull/27361 . This PR makes fairseq print a summary table of the memory state when an OOM occurs.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/885

Differential Revision: D17820445

Pulled By: jma127

fbshipit-source-id: 1887417c7648d703f78e1cff9f2a5b89901f49d0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
912,Alex Xiao,axiao@fb.com,2019-10-09 14:51:16-07:00,b6e001f634a13ee216d4b496a2e39031e22ddecb,https://github.com/pytorch/fairseq/commit/b6e001f634a13ee216d4b496a2e39031e22ddecb,"Fix data loading memory issue in pyspeech

Summary:
We currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.

3 changes:

1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.

2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.

2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.

Reviewed By: yqwangustc

Differential Revision: D17750715

fbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
913,Jeff Cai,jcai@fb.com,2019-10-09 18:14:39-07:00,33646ac9b7b720444416e2c18d1120d03b37e156,https://github.com/pytorch/fairseq/commit/33646ac9b7b720444416e2c18d1120d03b37e156,"wav2letter integration

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/846

Reviewed By: jcai1

Differential Revision: D17845996

Pulled By: okhonko

fbshipit-source-id: 3826fd9a4418496916bf1835c319dd85c89945cc",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
914,Dmytro Okhonko,oxo@fb.com,2019-10-10 11:12:38-07:00,c4893ca6eaefda846088f27df1fd38471a9c8640,https://github.com/pytorch/fairseq/commit/c4893ca6eaefda846088f27df1fd38471a9c8640,"Add ctc loss to ASR task (#1233)

Summary:
Adds CTC loss and corresponding transformer ctc based models.

Tested with
`CUDA_VISIBLE_DEVICES=0 python train.py $DATA_PATH --save-dir $SAVE_DIR --max-epoch 30 --task speech_recognition --arch vggtransformer_enc_1 --optimizer adadelta --lr 1.0 --adadelta-eps 1e-8 --adadelta-rho 0.95 --clip-norm 10.0  --max-tokens 10000 --log-format json --log-interval 1 --criterion ctc_loss --user-dir examples/speech_recognition/ --validate-interval=10`
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1233

Reviewed By: jcai1

Differential Revision: D17856824

Pulled By: okhonko

fbshipit-source-id: f3eac64d3fdd0c37cf8c539dd360cfb610d8a6ef",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
915,Jiatao Gu,jgu@fb.com,2019-10-11 07:37:13-07:00,cce92bdd5a53b7bb45524c70d03d3ba13eab5412,https://github.com/pytorch/fairseq/commit/cce92bdd5a53b7bb45524c70d03d3ba13eab5412,"add new_arange function + FIX BUGS of returning attn values

Summary:
Implementation of Levenshtein Transformer paper.
Add a new helper function ""new_arange"" to create arange tensor easily.
Fix bugs of returning attn values for NAT models
Delete files which are not necessary or experimental.

Reviewed By: kahne

Differential Revision: D17652009

fbshipit-source-id: 436bbb5d45de2f8067003232de4f2bd51e87719c",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
916,Jiatao Gu,jgu@fb.com,2019-10-11 11:23:19-07:00,02b74c58b9de9ccc0f280657533eb1bd757bdb4a,https://github.com/pytorch/fairseq/commit/02b74c58b9de9ccc0f280657533eb1bd757bdb4a,"fix the random mask function for CMLM model

Summary: The original implementation of the random mask is different from what the paper was stated.

Reviewed By: kahne

Differential Revision: D17652564

fbshipit-source-id: 238a9158041b3ff2482ee50ce6151c3f77f0b2c1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
917,Sujit Verma,sujit.verma@oculus.com,2019-10-11 21:51:50-07:00,d80ad54f75186adf9b597ef0bcef005c98381b9e,https://github.com/pytorch/fairseq/commit/d80ad54f75186adf9b597ef0bcef005c98381b9e,"Added option to save checkpoints using Path Manager.

Summary: Added option to save checkpoints using Path Manager.

Reviewed By: hudeven

Differential Revision: D17392754

fbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
918,Changhan Wang,changhan@fb.com,2019-10-14 18:21:35-07:00,e3a40d9d34ec9669c40c2f38a5de962cb0b54e88,https://github.com/pytorch/fairseq/commit/e3a40d9d34ec9669c40c2f38a5de962cb0b54e88,"fix libnat imports

Summary: Bring back the changes in D17661768

Reviewed By: ailzhang

Differential Revision: D17920299

fbshipit-source-id: be3f93a044a8710c8b475012c39e36a3e6507fad",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
919,Nayan Singhal,naysing@fb.com,2019-10-15 09:58:06-07:00,b5f41f828b0ec9b67fa60aceb0778073d1b368b2,https://github.com/pytorch/fairseq/commit/b5f41f828b0ec9b67fa60aceb0778073d1b368b2,"Add Unit test cases for BMUF

Summary:
This unit test guards the bmuf code.

change:
1. distributed_init assumes we are always using cuda device which is not the case if you are using ""gloo"" backend on CPU machine.

Reviewed By: jay-mahadeokar

Differential Revision: D17821391

fbshipit-source-id: 28e1bb39f7a4889b1dc6bd636b7c499e55bfc69a",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,3,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestBMUF(unittest.TestCase):'],"[('AlmostEqual', '(results[0], results[1])'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],['len(results) == 2'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
920,Changhan Wang,changhan@fb.com,2019-10-18 12:43:13-07:00,3dcb5c77165c1a0c33a35a7831182f1aa2e8ad73,https://github.com/pytorch/fairseq/commit/3dcb5c77165c1a0c33a35a7831182f1aa2e8ad73,"fix levenshtein transfromer attn

Summary: When the `if` statements in the levenshtein transformer decoder forward are removed, `attn` may get inconsistent batch sizes with output tokens. This is a fix.

Reviewed By: cndn

Differential Revision: D17936411

fbshipit-source-id: a1583f3806dc9f41caeb783c043429e247035803",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
921,dikshameghwal,dikshameghwal@gmail.com,2019-10-18 13:09:00-07:00,c8a7b627527ba2d54d93a8d19ac15414a83e858e,https://github.com/pytorch/fairseq/commit/c8a7b627527ba2d54d93a8d19ac15414a83e858e,"fixed a bug in preprocess glue dataset dev filename (#1270)

Summary:
removed redundant quotes in the filename assigned for dev dataset for GLUE tasks
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1270

Differential Revision: D18013071

fbshipit-source-id: 35f00162e117c6584dc859f760503ca32dcb706e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
922,Spencer Poff,spoff@fb.com,2019-10-18 14:29:49-07:00,b8d024e9b8c20058dd7282f1418ebef00bfb8974,https://github.com/pytorch/fairseq/commit/b8d024e9b8c20058dd7282f1418ebef00bfb8974,"add missing function to FairseqLanguageModel

Summary: In https://github.com/fairinternal/fairseq-py/pull/877, sequence_generator began calling `model.forward_decoder`, but not all decoder models were given an implementation of that function.

Reviewed By: okhonko

Differential Revision: D17863751

fbshipit-source-id: ea70b636c9dafcf87f5d5e49631d0c4b7cf14984",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
923,Jiatao Gu,jgu@fb.com,2019-10-19 18:01:59-07:00,a3c629b5cef188b064c543cbdd0fa9128d1d353e,https://github.com/pytorch/fairseq/commit/a3c629b5cef188b064c543cbdd0fa9128d1d353e,"Fix typos on Examples for Nonautoregressive translation

Summary: Fix typos in the examples

Reviewed By: kahne

Differential Revision: D18030097

fbshipit-source-id: 84f0cbafd85e50ffd5033738835373935e3b83d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
924,Jiatao Gu,jgu@fb.com,2019-10-20 12:42:05-07:00,66d24dc2ae5ed2bd631bfeccdb09983a34abc818,https://github.com/pytorch/fairseq/commit/66d24dc2ae5ed2bd631bfeccdb09983a34abc818,"Enable separate models for insertion and deletion;

Summary:
The Diff conatins two fixes:
(1) enabling non-shared decoder layers for deletion/insertion
(2) adding options to perform sampling instead of argmax when learning the deletion

Reviewed By: kahne

Differential Revision: D18011220

fbshipit-source-id: c60815fb7bc3a0004c81249504f7a641536ae2d8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
925,Louis MARTIN,louisrtm@gmail.com,2019-10-21 19:57:22-07:00,34e6a5e8edc7f297286161d9034e2bcba2c7b8c5,https://github.com/pytorch/fairseq/commit/34e6a5e8edc7f297286161d9034e2bcba2c7b8c5,"Fix load_dataset signature (#1281)

Summary:
Fix for https://github.com/pytorch/fairseq/issues/1240
Tested with MaskedLMTask.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1281

Differential Revision: D18051472

fbshipit-source-id: 0aeff60c71489655f5e621349f780ba9cd8c027a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
926,Louis MARTIN,louisrtm@gmail.com,2019-10-21 19:59:18-07:00,2d51e04d93295ab40fb787fabae9f3b53cc1266e,https://github.com/pytorch/fairseq/commit/2d51e04d93295ab40fb787fabae9f3b53cc1266e,"Rename ""loaded {} batches"" to ""loaded {} blocks"" (#1279)

Summary:
Very small change.
The previous message was misleading, the length of TokenBlocksDataset is a number of ""blocks"" or ""streams"" but not the number of batches strictly speaking if I am not mistaken. I use the notion of batch from roberta https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md.
It took me some time to understand what was going on, I hope it saves some time for others.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1279

Differential Revision: D18051476

fbshipit-source-id: 71fa35f21b9dbc8d6bde28cd3a487723690aadee",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
927,Changhan Wang,changhan@fb.com,2019-10-22 13:08:41-07:00,e49b302a635f06301cd3eb0afe25fad2742bd4b1,https://github.com/pytorch/fairseq/commit/e49b302a635f06301cd3eb0afe25fad2742bd4b1,"fix score

Summary: Bugfix for inconsistent scores on the same input sentences. This only affects the displayed scores in `generate.py` and does not affect the model outputs.

Reviewed By: MultiPath

Differential Revision: D17799343

fbshipit-source-id: 2b868ac03097a4db27db736e126a61d50958acc5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
928,Yilei Li,yileil@fb.com,2019-10-22 19:46:38-07:00,8defa9d9a47bf99d07090efd35da78f99fb3e48d,https://github.com/pytorch/fairseq/commit/8defa9d9a47bf99d07090efd35da78f99fb3e48d,"Add warmup support in reduce_on_plateau lr schedule

Summary:
Enables reduce_on_plateau schedule with optional warmup phase, where we linearly increase the learning rate from some initial learning rate (``--warmup-init-lr``) until the configured learning rate (``--lr``). Thereafter the lr is adjusted according to original reduce_on_plateau scheme
During warmup::

      lrs = torch.linspace(args.warmup_init_lr, args.lr, args.warmup_updates)
      lr = lrs[update_num]

Reviewed By: yqwangustc

Differential Revision: D17779925

fbshipit-source-id: c3bfb3321c76850824fc42df4fac4e5dcf73fbf8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
929,Ning Dong,dnn@fb.com,2019-10-23 17:44:55-07:00,5a2f76ede044b4904af9461e18253f2929cfc5a4,https://github.com/pytorch/fairseq/commit/5a2f76ede044b4904af9461e18253f2929cfc5a4,"NAT productionization

Summary:
NAT productionization diff

(1) Integrate NAT model training / Evaluation in LATTE base training workflow.
(2) Make NAT tracing compliant. Since it calls into Fairseq transformer, we need to refactor the code and I created a ~copy of it named fb_tracing_transformer.
(3) Decoder side C++ code is landed in the diff earlier.

Reviewed By: xianxl

Differential Revision: D17888324

fbshipit-source-id: ef4ef195fddd360da921502adcef82b087e46ce6",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
930,Jerry Ma,noreplyspamblackhole@gmail.com,2019-10-23 20:15:03-07:00,39faa0a419a8051837ee26c433c8ba863a2b51f3,https://github.com/pytorch/fairseq/commit/39faa0a419a8051837ee26c433c8ba863a2b51f3,"Reset both WPS and UPS on first minibatch (#891)

Summary:
Makes more sense to reset either both meters or neither of them.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/891

Differential Revision: D18109027

Pulled By: jma127

fbshipit-source-id: f63baed9a6b928a6f591a76e69ef6e9c524e4398",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
931,Jerry Ma,noreplyspamblackhole@gmail.com,2019-10-23 20:50:42-07:00,d0358bb38e4a40d8faaa155900ef7859c9b867b5,https://github.com/pytorch/fairseq/commit/d0358bb38e4a40d8faaa155900ef7859c9b867b5,"fix inconsistency w/ recent pytorch cuda device logic

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/892

Differential Revision: D18109685

Pulled By: jma127

fbshipit-source-id: f96e1080a5577b8ee0748dfdd956bf72bed47474",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
932,Ning Dong,dnn@fb.com,2019-10-24 09:54:33-07:00,5b086a0c17a16ed84285ce78017637d47fa50caa,https://github.com/pytorch/fairseq/commit/5b086a0c17a16ed84285ce78017637d47fa50caa,"OSS tracing compliant transformer to unbreak master (#1299)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1299

 LevT calls into tracing compliant transformer we didn't plan to OSS earlier. This is a workaround to unbreak the master. Will revisit and simplify the code later.

Reviewed By: pipibjc

Differential Revision: D18110339

fbshipit-source-id: 3bb51c56c2c20f45db1d5786d030b374b412eab1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
933,Halil Akin,halilakin@fb.com,2019-10-25 09:02:19-07:00,fdf4c3e9002ec1ee01a281779a095512ada90e40,https://github.com/pytorch/fairseq/commit/fdf4c3e9002ec1ee01a281779a095512ada90e40,"Simplify fairseq multihead attention (#888)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/888

We want to simplify multihead attention and get rid of the dynamic in_proj_weight logic. Sending the diff early for feedback, will have further changes as I try to fix breaking tests

Reviewed By: edunov

Differential Revision: D17912661

fbshipit-source-id: 0e6319fc694d8ec5187d1c2fefe5839d9d522186",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
934,Halil Akin,halilakin@fb.com,2019-10-25 09:02:19-07:00,c07362c675975ad7eb70afc941c6fee705c21642,https://github.com/pytorch/fairseq/commit/c07362c675975ad7eb70afc941c6fee705c21642,"Convert matmuls to quantizable nn.Linear modules (#1304)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1304

Pull Request resolved: https://github.com/pytorch/translate/pull/657

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1065

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/889

We are converting matmuls to quantizable nn.Linear modules in this diff. First let's test profile after the diff to see how low level operations are changing.

Reviewed By: jmp84, edunov, lly-zero-one, jhcross

Differential Revision: D17964796

fbshipit-source-id: 3ddd3ff81fa1ea5864dded98e993f4fe3b71fe5e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
935,Xian Li,xianl@fb.com,2019-10-25 17:19:52-07:00,eb68afca0208a040d4e91eceae86f5f22ca24b04,https://github.com/pytorch/fairseq/commit/eb68afca0208a040d4e91eceae86f5f22ca24b04,"fix a type mismatch in NAT quantization run

Summary:
Fix a type mismatch which was found after patching NAT on top of quantization.
Ning suggested this fix. Need to further understand: why this only appears after patching quantization diff?

Reviewed By: kahne, jhcross

Differential Revision: D18147726

fbshipit-source-id: a51becc9ad58a637a0180074eaa2b46990ab9f84",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
936,Angela Fan,angela.h.fan@gmail.com,2019-10-27 12:09:29-07:00,dabbef467692ef4ffb7de8a01235876bd7320a93,https://github.com/pytorch/fairseq/commit/dabbef467692ef4ffb7de8a01235876bd7320a93,"adding layerdrop code for training, pruning, and readme (#890)

Summary:
TEST 1: EVALUATION TIME WORKS
checked
achieves correct model perplexity: 18.68

TEST 2: TRAINING NEW MODEL WORKS
checked

without layerdrop:
--decoder-layerdrop 0 OR no flag at all
| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30
| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57
| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84
| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112
| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140

with layerdrop (regularization effect should be seen in PPL):
--decoder-layerdrop 0.2

| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24
| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45
| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68
| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90
| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112

TEST 3: PICKING UP TRAINING FROM EXISTING MODEL
checked

| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train

TEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789
achieves correct accuracy on SST2 for this model

TEST 5: TRAINING NEW BERT MODEL WORKS
checked and works

TEST 6: NMT

without layerdrop
--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified

| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3
| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6
| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9
| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12
| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15

with layerdrop (regularization effect should be seen in PPL)

A) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2
B) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5
C) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0

| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3
| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6
| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8
| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11
| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14

TEST 7: PRUNING TESTCASES

A) after adding the pruning flags, model can evaluate as a full model
checked, reaches correct PPL
num. model params: 246933504
| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)
| Loss: 2.9275, Perplexity: 18.68

B) after adding pruning flags, model can be pruned. this works with multiple flag settings
checked three cases:
num. model params: 146163712
| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)
| Loss: 3.0932, Perplexity: 22.05

num. model params: 209144832
| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)
| Loss: 2.9526, Perplexity: 19.16

C) model can pick up training if you want to finetune the pruned model
checked:
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396

D) works with BERT
checked:
without specifying any flags, reproduces the correct standard accuracy
with flags, produces the correct pruned accuracy

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop
| Accuracy:  0.9220183486238532
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890

Reviewed By: edunov

Differential Revision: D18094657

Pulled By: huihuifan

fbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
937,Ning Dong,dnn@fb.com,2019-10-27 22:30:43-07:00,50cf3bb596abba70a0770ad2308fed0f4d32a002,https://github.com/pytorch/fairseq/commit/50cf3bb596abba70a0770ad2308fed0f4d32a002,"Fix LevT generator interface

Summary: Revert the interface change for iterative_refinement_generator

Reviewed By: kahne

Differential Revision: D18165103

fbshipit-source-id: 075c276746eb90d7c359b6ad92e1ef25e8452bcc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
938,Xian Li,xianl@fb.com,2019-10-30 12:54:21-07:00,856d8b8262e47a310b71e276c1e8e089d840f206,https://github.com/pytorch/fairseq/commit/856d8b8262e47a310b71e276c1e8e089d840f206,"layer drop

Summary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.

Reviewed By: jhcross

Differential Revision: D18165586

fbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
939,Myle Ott,myleott@fb.com,2019-10-31 10:56:58-07:00,f30fc7d71c93ecc28115b9eca2c2d680bd061d09,https://github.com/pytorch/fairseq/commit/f30fc7d71c93ecc28115b9eca2c2d680bd061d09,"Fix MultiheadAttention and torch hub

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/895

Reviewed By: akinh

Differential Revision: D18246479

Pulled By: myleott

fbshipit-source-id: a610f1e4943619d32a523601a572fb09cdc5638d",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
940,Myle Ott,myleott@fb.com,2019-10-31 12:53:48-07:00,99c524c5463b65c3e9c49a9ecbd53c39d0573c86,https://github.com/pytorch/fairseq/commit/99c524c5463b65c3e9c49a9ecbd53c39d0573c86,"Fix fairspeq unit test

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/897

Differential Revision: D18250587

Pulled By: myleott

fbshipit-source-id: b9cef376bc014b68766229aab7b6e454480757d3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
941,Halil Akin,halilakin@fb.com,2019-11-01 09:38:27-07:00,4c6b689eebe66a53717dacf28cba7a11b6ffa64f,https://github.com/pytorch/fairseq/commit/4c6b689eebe66a53717dacf28cba7a11b6ffa64f,"Remove in_proj_weight/in_proj_bias in multihead attention and fix the failing tests instead (#898)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/898

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1333

Pull Request resolved: https://github.com/fairinternal/fairspeq/pull/11

This in_proj_weight and in_proj_bias properties are not the right way of providing backward compatibility, and it's causing other incompatibilities with the new Dynamic Quantization API. So, let's remove this, and properly fix the failing tests.

Reviewed By: myleott

Differential Revision: D18264129

fbshipit-source-id: fc1838657a60d914ca83c4e0f6add5ed8206ac54",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
942,Chau Tran,chau@fb.com,2019-11-01 12:31:58-07:00,828c1ca7522278aa6c1eaf91fe0425b8f40dd832,https://github.com/pytorch/fairseq/commit/828c1ca7522278aa6c1eaf91fe0425b8f40dd832,"Fix BPE for dual learning

Summary: Fix integration test

Reviewed By: xianxl

Differential Revision: D18040440

fbshipit-source-id: 98c8ab7970d081f17deb54c69aa35669de12d767",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
943,Myle Ott,myleott@fb.com,2019-11-02 16:51:32-07:00,a0f75996b1e25c97149bcbc6aad7eed5601daab0,https://github.com/pytorch/fairseq/commit/a0f75996b1e25c97149bcbc6aad7eed5601daab0,"Fix building of docs

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1340

Differential Revision: D18289455

Pulled By: myleott

fbshipit-source-id: a1c8163a35273b6c646d300142701e8a317d7378",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
944,Spencer Poff,spoff@fb.com,2019-11-05 06:49:31-08:00,68dd3e171bff81e02eb201cb749fc91c8f7e9f7f,https://github.com/pytorch/fairseq/commit/68dd3e171bff81e02eb201cb749fc91c8f7e9f7f,"Fixing key padding mask during transformer generation

Summary:
https://github.com/pytorch/fairseq/pull/1097 added key padding mask history in TransformerDecoderLayer, but during an edge case where only the current or only the previous key_padding_mask exists, the resulting key_padding_mask is the wrong size.

This diff adds empty columns in such a case to ensure key_padding_mask is a usable size.

Reviewed By: myleott

Differential Revision: D18224313

fbshipit-source-id: c9fb7266baf0a2d79a66704e00a5ea8bd2987ff6",3,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMultiheadAttention(unittest.TestCase):'],"[('True', '('), ('Equal', '(key_padding_mask.size(0), bsz)'), ('Equal', '(key_padding_mask.size(1), src_len)'), ('IsNone', '(c[2])')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
945,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-11-05 15:01:26-08:00,e23e5eaa321e2aa18330f1d13b70a9851e125901,https://github.com/pytorch/fairseq/commit/e23e5eaa321e2aa18330f1d13b70a9851e125901,"XLM-R code and model release (#900)

Summary:
TODO:
1) Need to update bibtex entry
2) Need to upload models, spm_vocab and dict.txt to public s3 location.

For Future:

1) I will probably add instructions to finetune on XNLI and NER, POS etc. but currently no timeline for that.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/900

Reviewed By: myleott

Differential Revision: D18333076

Pulled By: myleott

fbshipit-source-id: 3f3d3716fcc41c78d2dd4525f60b519abbd0459c",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
946,Jerry Ma,noreplyspamblackhole@gmail.com,2019-11-06 02:36:36-08:00,bafeed46fb76fa337a771ebb41d65bb95039565a,https://github.com/pytorch/fairseq/commit/bafeed46fb76fa337a771ebb41d65bb95039565a,"log more OOM sites (#893)

Summary:
- Adds memory summary logging to validation and optimization steps.
- Clarifies in the logging that optimization OOMs are not recoverable.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/893

Differential Revision: D18110763

Pulled By: jma127

fbshipit-source-id: 49340e611169c606ab9c991265167a79f51846e6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
947,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-11-06 08:49:08-08:00,1d1e460d42486207f423ecfd8eabb26de1f09abf,https://github.com/pytorch/fairseq/commit/1d1e460d42486207f423ecfd8eabb26de1f09abf,"Xlmr update readme

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/901

Differential Revision: D18349686

fbshipit-source-id: ba0a378e3fb98a35b3ef2e2103c2f921c4729e40",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
948,Liam,liangding.liam@gmail.com,2019-11-07 08:13:01-08:00,f03392d11faf1588cb571d19835d6a61ab0d9ca6,https://github.com/pytorch/fairseq/commit/f03392d11faf1588cb571d19835d6a61ab0d9ca6,"fix typos (#1310)

Summary:
""pytorch.fairseq"" -> ""pytorch/fairseq"" to avoid following error:
```
ValueError: not enough values to unpack (expected 2, got 1)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1310

Differential Revision: D18338223

Pulled By: myleott

fbshipit-source-id: c95fcc3bb814c7f980a22996dc7923d6d487810b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
949,freewym,freewym@gmail.com,2019-11-07 08:50:51-08:00,7ca56cb8cf24f8701739ecdcbf346cde9a6e0881,https://github.com/pytorch/fairseq/commit/7ca56cb8cf24f8701739ecdcbf346cde9a6e0881,"add set_epoch() to class ConcatDataset and ConcatSentenceDataset to c… (#1272)

Summary:
…all set_epoch() for each sub dataset
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1272

Differential Revision: D18338300

Pulled By: myleott

fbshipit-source-id: 973d57f52c5cf4ad40122d4a625942281c7983b7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
950,Louis MARTIN,louisrtm@gmail.com,2019-11-07 08:51:05-08:00,37c9d96f3706aa409ed55698d4b438c8765273c5,https://github.com/pytorch/fairseq/commit/37c9d96f3706aa409ed55698d4b438c8765273c5,"Add whole word masking for SentencepieceBPE (#1292)

Summary:
Models seem to train fine with this modification. I checked that the mask for beginning of words is correct but didn't check if the actual masking worked correctly.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1292

Differential Revision: D18338307

Pulled By: myleott

fbshipit-source-id: eae9e29d6ab648e768d70921694a898554496704",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
951,Kevin,b03902086@ntu.edu.tw,2019-11-07 09:06:52-08:00,13d9e2baf8a67339bcdb25af18538630f7fa128f,https://github.com/pytorch/fairseq/commit/13d9e2baf8a67339bcdb25af18538630f7fa128f,"Fix changes of file locations of subword-nmt (#1219)

Summary:
Solves https://github.com/pytorch/fairseq/issues/1218.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1219

Differential Revision: D18339541

Pulled By: myleott

fbshipit-source-id: 6d5bd7b60fa7fd30c038fdad54591343a01f228b",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
952,Xian Li,xianl@fb.com,2019-11-07 17:40:00-08:00,e9171ce19bf06dbb5e9716ca9e2584ef7da7b1de,https://github.com/pytorch/fairseq/commit/e9171ce19bf06dbb5e9716ca9e2584ef7da7b1de,"Fix LevT edge cases

Summary:
To avoid the case where can_ins_mask has all False so max_lengths has size [0, 1] which failed expand_as operator. Move it back into the skipping branch in script.

The same for deletion and ins_word.

Reviewed By: kahne

Differential Revision: D18365340

fbshipit-source-id: 509ac21d7d6fd9083d0710697288203977314c52",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
953,Myle Ott,myleott@fb.com,2019-11-08 04:36:43-08:00,e98bf7e64f0a224e72b7b0ae0bf5733e8c7fea3e,https://github.com/pytorch/fairseq/commit/e98bf7e64f0a224e72b7b0ae0bf5733e8c7fea3e,"Move fb_pathmgr registration out of train.py

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/903

Reviewed By: sujitoc

Differential Revision: D18327653

fbshipit-source-id: 739ddbaf54862acdf7b4f1bc3ad538bde5ae00fd",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
954,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-11-08 21:00:06-08:00,a92bcdad5a0dea6a440cc92976e4166811b16671,https://github.com/pytorch/fairseq/commit/a92bcdad5a0dea6a440cc92976e4166811b16671,"adding first version of bart code release (#902)

Summary:
This is the first version of BART code / model release.

It still requires lot of clean up, instructions, making sure results are reproducible before we can release it.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/902

Differential Revision: D18389535

fbshipit-source-id: 77f16800307ce831bd29538fdd34800793210f46",18,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
955,Louis Martin,louisrtm@gmail.com,2019-11-10 11:27:48-08:00,b31849aa9282755bbb9eecd9384b2e0fc2b9c0a1,https://github.com/pytorch/fairseq/commit/b31849aa9282755bbb9eecd9384b2e0fc2b9c0a1,"Camembert model and code (#904)

Summary:
Check locally that everything works fine.
Model is uploaded to fbaipublicfiles.

I fixed a few inconsistencies in the bpe encoding along the way, e.g. related to https://github.com/pytorch/fairseq/issues/1306..
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/904

Reviewed By: ngoyal2707

Differential Revision: D18418345

Pulled By: louismartin

fbshipit-source-id: 53acb4d021581968d70430ee9babee07d6573c17",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
956,Spencer Poff,spoff@fb.com,2019-11-11 17:17:37-08:00,2a9b4ec2374574cd0315f95e126788e4fe795f0d,https://github.com/pytorch/fairseq/commit/2a9b4ec2374574cd0315f95e126788e4fe795f0d,"More thorough support for iterable datasets

Summary: Using PyTorch IterableDataset for streaming iterators. Such that there is a clean differentiation in interface between datasets that are streaming data and those that support indexed access.

Reviewed By: myleott

Differential Revision: D18438694

fbshipit-source-id: 482857d8357091ea2a6bf819535b09ba7f1a5b7d",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
957,Myle Ott,myleott@fb.com,2019-11-13 09:10:52-08:00,27568a7ebed1a35f08ac0390f35b3de9b8dad0dd,https://github.com/pytorch/fairseq/commit/27568a7ebed1a35f08ac0390f35b3de9b8dad0dd,"Merge TracingCompliantTransformer and regular Transformer, fix NAT tests

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/899

Differential Revision: D18373060

Pulled By: myleott

fbshipit-source-id: bb5510ec15799a0a10a7c0669e76d8200e1ba479",14,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
958,Myle Ott,myleott@fb.com,2019-11-13 10:47:05-08:00,4d21c157ad798c1dc6cdfda073e41eb9dad6c992,https://github.com/pytorch/fairseq/commit/4d21c157ad798c1dc6cdfda073e41eb9dad6c992,"Have `setup.py clean` remove compiled Cython files

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/907

Differential Revision: D18480215

Pulled By: myleott

fbshipit-source-id: b02002f631f6d47380f309d4f464bd135d623280",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
959,Zhanghao Wu,zhanghao.wu@outlook.com,2019-11-13 11:27:28-08:00,aaa37f05f1bfe6576be39de93e9711026ccb5a69,https://github.com/pytorch/fairseq/commit/aaa37f05f1bfe6576be39de93e9711026ccb5a69,"Add 'ppl' to tensorboard (#1212)

Summary:
Originally, the 'ppl' is calculated but returned as a string, which will not be printed to the tensorboard.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1212

Differential Revision: D18339553

Pulled By: myleott

fbshipit-source-id: 52e64d5d173bfd79836a72ee103cb25c8bb2a4c2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
960,zheng,simon.gaozheng@gmail.com,2019-11-13 11:34:48-08:00,096d7d301e2ad6687673f4abfad0a008b9f426ea,https://github.com/pytorch/fairseq/commit/096d7d301e2ad6687673f4abfad0a008b9f426ea,"Fix the type annotations of three parameters found in two constructors (#1268)

Summary:
As their names suggest, the parameters `embedding_dim`, `ffn_embedding_dim`, and `num_attention_heads` should have type `int`, not `float`.

Also validated by https://github.com/pytorch/fairseq/blob/b5f41f828b0ec9b67fa60aceb0778073d1b368b2/fairseq/modules/sparse_transformer_sentence_encoder.py#L22#L24.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1268

Differential Revision: D18372518

Pulled By: myleott

fbshipit-source-id: 666739b6270a975536785886068a975e07312bb0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
961,Myle Ott,myleott@fb.com,2019-11-13 14:35:39-08:00,e26ee47a8c16a299d2304226dc4e5a6a43827db2,https://github.com/pytorch/fairseq/commit/e26ee47a8c16a299d2304226dc4e5a6a43827db2,"Fix LM generation and add unit test

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/896

Differential Revision: D18250948

Pulled By: myleott

fbshipit-source-id: 7a515311e18795670b29f5e24eeba7619a625da7",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
962,Jiatao Gu,jgu@fb.com,2019-11-13 23:47:15-08:00,b85fb035063eb065a6204c520890eb1847c28ecf,https://github.com/pytorch/fairseq/commit/b85fb035063eb065a6204c520890eb1847c28ecf,"Enable to print the history of NAT; fix LevT decoding bug (#908)

Summary:
(1) Enable to print the iterative refinement history for all NAT models by setting --retain-iter-history during decoding;
(2) Fix a small bug in the decoding process in Levenshtein Transformer.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/908

Differential Revision: D18493234

Pulled By: MultiPath

fbshipit-source-id: 9e7702adcea49f39d3c10b5349b5a9ae66399a24",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
963,Abhimanyu Sharma,asharm@fb.com,2019-11-14 11:24:19-08:00,d9836217480846f64008d6f9af4753f21eb72e70,https://github.com/pytorch/fairseq/commit/d9836217480846f64008d6f9af4753f21eb72e70,"Adopt Fairseq MemoryEfficientFP16Optimizer in PyText (#910)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/910

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1124

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1362

Split the Fariseq MemoryEfficientFP16Optimizer class into 2 classes so that it can be easily imported in pytext through a wrapper class.

Iter 2 - fixed some issues to ensure code runs correctly on fblearner.

Iter 3 - fixed review comments, incorrect import and lints.

Iter 4 - fixed pytext test breaks.

Iter 5 - fix pytext test breaks.

Iter 6 - fix comments and refactor based on conversation with chenyang.

Reviewed By: chenyangyu1988

Differential Revision: D18410916

fbshipit-source-id: 5238ee553cd2811ed0573825e1c29000980cc489",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
964,freewym,freewym@gmail.com,2019-11-14 11:48:34-08:00,0d03aa889cf6f8363067a854cd40cbe5bd2bfb29,https://github.com/pytorch/fairseq/commit/0d03aa889cf6f8363067a854cd40cbe5bd2bfb29,"fix a bug when resuming training from the last epoch (#1275)

Summary:
If the training stopped in the middle of the last epoch, and then it was resumed from checkpoint, it will not continue the training because `epoch_itr.epoch < max_epoch` is not satisfied. This PR fixed the issue.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1275

Differential Revision: D18483945

Pulled By: myleott

fbshipit-source-id: 80df6f73fa17606a79a28e8328bb4c577f504683",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
965,Myle Ott,myleott@fb.com,2019-11-14 15:13:29-08:00,e404785250d0b96c1e6ef2213e29ada1a97b073b,https://github.com/pytorch/fairseq/commit/e404785250d0b96c1e6ef2213e29ada1a97b073b,"Add internal tests for torch hub

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/911

Differential Revision: D18511627

Pulled By: myleott

fbshipit-source-id: 37d7606ae629f9acf84715dbc9045fb683075db4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
966,Ilia Cherniavskii,iliacher@fb.com,2019-11-15 08:45:22-08:00,8446cb6385b4f9aec422a469029ed9c900867955,https://github.com/pytorch/fairseq/commit/8446cb6385b4f9aec422a469029ed9c900867955,"TorchScript-ify BERT training (#887)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/887

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1052

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1250

Adding config parameter ""use_torchscript"" that enables use of TS for BERT
training

Reviewed By: chenyangyu1988

Differential Revision: D17872083

fbshipit-source-id: 00ac4b04e7f26aa56fe84fe9feaded676d6deb71",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
967,Angela Fan,angela.h.fan@gmail.com,2019-11-17 14:39:46-08:00,7fb1df536132e32c1947c9d908e82ab4e5b6c6c7,https://github.com/pytorch/fairseq/commit/7fb1df536132e32c1947c9d908e82ab4e5b6c6c7,"added prompt file

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1385

Differential Revision: D18565188

Pulled By: huihuifan

fbshipit-source-id: 9580663b208f286a249bbfa2bacd71f34a01ca9f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
968,Louis Martin,louisrtm@gmail.com,2019-11-17 16:43:38-08:00,6fc03d3cf4a482083240b436741a56c32c8cf09b,https://github.com/pytorch/fairseq/commit/6fc03d3cf4a482083240b436741a56c32c8cf09b,"Update README.md

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/913

Differential Revision: D18565866

Pulled By: myleott

fbshipit-source-id: e845759dafe915805c2e38f53c6835cbcef5db2f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
969,Myle Ott,myleott@fb.com,2019-11-17 17:42:36-08:00,6f7b7d202199dc1534a77a247da1ad4604c21baa,https://github.com/pytorch/fairseq/commit/6f7b7d202199dc1534a77a247da1ad4604c21baa,"Build Cython components when loading hub (#1386)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/1376
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1386

Differential Revision: D18566839

Pulled By: myleott

fbshipit-source-id: 71805f58fab90f53f757bf4ef69eb914195af38a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
970,alexeib,alexei.b@gmail.com,2019-11-18 15:03:45-08:00,9bf0f107ccec52b2994562e945ef89f4e690f2d1,https://github.com/pytorch/fairseq/commit/9bf0f107ccec52b2994562e945ef89f4e690f2d1,"fix defaults for layer drop things (#918)

Summary:
recent layerdrop related changes break existing models because they assume presence of certain args
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/918

Reviewed By: huihuifan

Differential Revision: D18578572

Pulled By: alexeib

fbshipit-source-id: 368c2d5b3add55864bf59516820807303aac6001",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
971,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-11-18 19:27:10-08:00,4fd2a16bc676ffe196c9aabfd1bb8dbe7270b8e0,https://github.com/pytorch/fairseq/commit/4fd2a16bc676ffe196c9aabfd1bb8dbe7270b8e0,"Bart push cnn eval

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/915

Differential Revision: D18580996

fbshipit-source-id: 9505a81892ba8ad997c03465d6a2d488c379c762",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
972,freewym,freewym@gmail.com,2019-11-19 06:08:03-08:00,534eaa2c5f5298aa1220515f1e99cb33f92c8ef5,https://github.com/pytorch/fairseq/commit/534eaa2c5f5298aa1220515f1e99cb33f92c8ef5,"add and set the missing state ""shuffle"" properly to EpochBatchIterato… (#1375)

Summary:
…r to correctly recover the training from a ""non-shuffle"" checkpoint
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1375

Differential Revision: D18566535

Pulled By: myleott

fbshipit-source-id: ff7b1a6ead708801f537ec7885e30e37168cd34b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
973,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-11-19 09:00:17-08:00,831b6b6e7f16a14ca8cd5a4d998a05a974a1d2b2,https://github.com/pytorch/fairseq/commit/831b6b6e7f16a14ca8cd5a4d998a05a974a1d2b2,"Bart fix prev tokens collate

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/920

Differential Revision: D18593088

fbshipit-source-id: d4479ee8dae2ca623e62e12bd145165a116fb70a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
974,Jiatao Gu,jgu@jgu-fedora-mj0a98z5.dhcp.thefacebook.com,2019-11-19 21:04:48-08:00,51eb980227d5f4916d7da3221dc7460e94f304dc,https://github.com/pytorch/fairseq/commit/51eb980227d5f4916d7da3221dc7460e94f304dc,"clean up the NAT loss (#921)

Summary:
Clean up the original NAT loss and make it more general to adapt new losses used in NAT models.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/921

Differential Revision: D18610145

Pulled By: MultiPath

fbshipit-source-id: d04dd0fc4047b5f8e332cfe66b1e28cbf39494af",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
975,Alex Xiao,axiao@fb.com,2019-11-20 16:47:53-08:00,99fbd317f6b3256a39868d6568e70672f0f512b9,https://github.com/pytorch/fairseq/commit/99fbd317f6b3256a39868d6568e70672f0f512b9,"Refactor data sharding to be specified via caller of task rather than task itself

Summary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.

Reviewed By: myleott

Differential Revision: D18456424

fbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
976,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-11-21 09:05:15-08:00,226c1f48afb8cc5af7e9f9283778bc9d71d6a111,https://github.com/pytorch/fairseq/commit/226c1f48afb8cc5af7e9f9283778bc9d71d6a111,"added instructions to FT bart on cnn-dm

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/922

Differential Revision: D18617322

fbshipit-source-id: 50645197cb7f075b5f878818a97358653077c3e0",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
977,Tatiana Likhomanenko,antares@fb.com,2019-11-21 09:41:32-08:00,60e16a35032cfdab214457a35369c65c2261ccf5,https://github.com/pytorch/fairseq/commit/60e16a35032cfdab214457a35369c65c2261ccf5,"Fix warmup for fixed_schedule in case of first update (#1408)

Summary:
I faced the error while using warmup for fixed lr schedule

```
Traceback (most recent call last):
  File ""/private/home/antares/.conda/envs/fairseq-20190809/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
    fn(i, *args)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py"", line 291, in distributed_main
    main(args, init_distributed=True)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py"", line 81, in main
    train(args, trainer, task, epoch_itr)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py"", line 122, in train
    log_output = trainer.train_step(samples)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/trainer.py"", line 409, in train_step
    self.optimizer.step()
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/fp16_optimizer.py"", line 153, in step
    self.fp32_optimizer.step(closure)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/fairseq_optimizer.py"", line 98, in step
    self.optimizer.step(closure)
  File ""/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/nag.py"", line 68, in step
    lr_correct = lr / lr_old
ZeroDivisionError: float division by zero
```
which is due to `num_updates=0` for the first iteration and thus `lr` we set to the optimizer is zero.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1408

Differential Revision: D18637526

Pulled By: myleott

fbshipit-source-id: fdd81dd69b1b38bc21a4fa315b4e25cee03af6bf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
978,mingruimingrui,mingrui.wang@shopee.com,2019-11-21 17:49:29-08:00,5349052aae4ec1350822c894fbb6be350dff61a0,https://github.com/pytorch/fairseq/commit/5349052aae4ec1350822c894fbb6be350dff61a0,"Quick fix for Structured Dropout checkpointing (#1406)

Summary:
Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.

To keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1406

Differential Revision: D18637540

Pulled By: myleott

fbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
979,Changhan Wang,changhan@fb.com,2019-11-25 11:55:16-08:00,fb3e1e36d2c72eae38535193dd55ed9d7edd3f33,https://github.com/pytorch/fairseq/commit/fb3e1e36d2c72eae38535193dd55ed9d7edd3f33,"update LevT ensemble

Summary: Update LevT ensemble class with the recent API changes in LevT and iterative decoder classes.

Reviewed By: jhcross

Differential Revision: D18689292

fbshipit-source-id: 64d4cdb6513a32a32d49e0ebf57886ae576722d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
980,Myle Ott,myleott@fb.com,2019-11-25 13:38:42-08:00,cb6c67bcdba257c228a7771f58e2fb895fea6b16,https://github.com/pytorch/fairseq/commit/cb6c67bcdba257c228a7771f58e2fb895fea6b16,"Make torch.hub interface automatically apply tokenization and BPE

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/926

Differential Revision: D18685772

Pulled By: myleott

fbshipit-source-id: 0f99d79ed6ee72e9d3ced786d75ab9504d0dfcf0",11,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
981,Myle Ott,myleott@fb.com,2019-11-25 16:40:11-08:00,5d9392df979553827082afee836c1c386fc7cbda,https://github.com/pytorch/fairseq/commit/5d9392df979553827082afee836c1c386fc7cbda,"Better handling of unspecified max_tokens and max_sentences (fixes #1427)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/928

Differential Revision: D18691525

Pulled By: myleott

fbshipit-source-id: e787c17434d4cb0c4621e9858e0ebec4f9951630",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
982,Myle Ott,myleott@fb.com,2019-11-25 18:11:42-08:00,181dc58e98f4e1c31693595bbc0cd34389c4d015,https://github.com/pytorch/fairseq/commit/181dc58e98f4e1c31693595bbc0cd34389c4d015,"Documentation fixes

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/927

Differential Revision: D18691521

Pulled By: myleott

fbshipit-source-id: a79cb0a7614a30be765e741761819263d9fb5047",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
983,Wei Ho,weiho@fb.com,2019-11-26 07:43:11-08:00,9f6b552d685711738aae21dbdf3868f4ca280cda,https://github.com/pytorch/fairseq/commit/9f6b552d685711738aae21dbdf3868f4ca280cda,"Remove unused ignore_utf_errors option from Dictionary

Reviewed By: donhusa

Differential Revision: D18703314

fbshipit-source-id: 93a6b25a7de5e8a29879302ba23b9d6f60660b40",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
984,Xilun Chen,xilun@fb.com,2019-11-26 10:13:55-08:00,9398a2829596393b73f5c5f1b99edf4c2d8f9316,https://github.com/pytorch/fairseq/commit/9398a2829596393b73f5c5f1b99edf4c2d8f9316,"Add an extract_features option for LSTMDecoder

Summary:
This diff adds a new option to the LSTMDecoder to obtain unprojected decoder outputs (before the final output projection layer).

The original forward() method remains unchanged, but is divided into two parts: extract_features() and output_layer().

extract_features() outputs the hidden states of the decoder, which offers more flexibility to the model.

For instance, the unprojected decoder outputs are needed to implement a copy pointer attention that uses the decoder output to determine whether to copy certain tokens from the source sequence.

Reviewed By: myleott

Differential Revision: D18650255

fbshipit-source-id: 321c3085676d98b8b4f4ad6102917c94800643a5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
985,Naman Goyal,namangoyal@learnfair0329.h2.fair,2019-12-02 09:00:44-08:00,cfc4b3037d4014fcb7fcd1dbf2f6c7d10a0f9ed3,https://github.com/pytorch/fairseq/commit/cfc4b3037d4014fcb7fcd1dbf2f6c7d10a0f9ed3,"added missing cmd arg for bart cnn ft

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/930

Differential Revision: D18763707

fbshipit-source-id: 453a877f5bb39c5afcf7f9bc101019b1be4a0a60",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
986,Wei Ho,weiho@fb.com,2019-12-02 13:25:24-08:00,10c2e8c1fa876f7b1c63bf36ac4fd7e3fdeeaab0,https://github.com/pytorch/fairseq/commit/10c2e8c1fa876f7b1c63bf36ac4fd7e3fdeeaab0,"Apply Black auto-formatting

Reviewed By: sujitoc

Differential Revision: D18738392

fbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
987,Myle Ott,myleott@fb.com,2019-12-03 08:23:34-08:00,5be1cf3053b5ca47ac9fe22f06d7abec4501f745,https://github.com/pytorch/fairseq/commit/5be1cf3053b5ca47ac9fe22f06d7abec4501f745,"Fix bug in forward_embedding (#931)

Summary:
See: https://twitter.com/nymwa/status/1200684169115734016
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/931

Differential Revision: D18782345

Pulled By: myleott

fbshipit-source-id: 9e74287a8ce677237042c5fdbe0bdbd4774b5ce6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
988,Myle Ott,myleott@fb.com,2019-12-03 09:24:29-08:00,1c565940012c81e3c5f24ef0913002418beb7afa,https://github.com/pytorch/fairseq/commit/1c565940012c81e3c5f24ef0913002418beb7afa,"Fix lightconv_lm and add test (#932)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/536
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/932

Differential Revision: D18783032

Pulled By: myleott

fbshipit-source-id: a520faccc20be78296a228214923ee1495fb536f",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
989,Naman Goyal,namangoyal@devfair0110.h2.fair,2019-12-03 12:59:30-08:00,d48895bd53ffc90099beb76cf154c50a1ba23742,https://github.com/pytorch/fairseq/commit/d48895bd53ffc90099beb76cf154c50a1ba23742,"fixed word level extract features for roberta-xlmr

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/933

Differential Revision: D18783780

fbshipit-source-id: fa0a27fab886a5fa5be8d5f49151d1d9dd9775f1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
990,Myle Ott,myleott@fb.com,2019-12-03 15:18:07-08:00,df2f84ce619edddb80e720a56abc74d5490fed99,https://github.com/pytorch/fairseq/commit/df2f84ce619edddb80e720a56abc74d5490fed99,"v0.8.0 -> v0.9.0 (#1452)

Summary:
Possibly breaking changes:
- Set global numpy seed (4a7cd58)
- Split `in_proj_weight` into separate k, v, q projections in MultiheadAttention (fdf4c3e)
- TransformerEncoder returns namedtuples instead of dict (27568a7)

New features:
- Add `--fast-stat-sync` option (e1ba32a)
- Add `--empty-cache-freq` option (315c463)
- Support criterions with parameters (ba5f829)

New papers:
- Simple and Effective Noisy Channel Modeling for Neural Machine Translation (49177c9)
- Levenshtein Transformer (86857a5, ...)
- Cross+Self-Attention for Transformer Models (4ac2c5f)
- Jointly Learning to Align and Translate with Transformer Models (1c66792)
- Reducing Transformer Depth on Demand with Structured Dropout (dabbef4)
- Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa) (e23e5ea)
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (a92bcda)
- CamemBERT: a French BERT (b31849a)

Speed improvements:
- Add CUDA kernels for LightConv and DynamicConv (f840564)
- Cythonization of various dataloading components (4fc3953, ...)
- Don't project mask tokens for MLM training (718677e)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1452

Differential Revision: D18798409

Pulled By: myleott

fbshipit-source-id: 860a0d5aaf7377c8c9bd63cdb3b33d464f0e1727",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
991,Nayan Singhal,naysing@fb.com,2019-12-03 15:22:32-08:00,3c8e2461b873ec91f939eb12a3303d34a7ba4475,https://github.com/pytorch/fairseq/commit/3c8e2461b873ec91f939eb12a3303d34a7ba4475,"Fixing Gradient calculation in BMUF

Summary:
Gradient calculation:
1. Accumulate graidents from all the machines.
2. Divide the gradient by the total samples seen by all the machines.

In DDP: gradients are calculated:
1. They are accumulated from all the machines and divide by the total number of GPUs.
2. Later, they are multiplied by #GPU / #total_sample_size

For BMUF case: graidents are calulated:
1. During non-sync, gradients should be divide by samples seen by the local machine.
2. During bmuf-sync, gradients are accumiulated by bmuf sync which is divided by #GPUs. And, later they are multiplied by #GPU / #total_sample_size

Other small fix:
1. Set average local momentum sync to False.
2. Set nesterov block sync to False.

Reviewed By: jay-mahadeokar

Differential Revision: D18773892

fbshipit-source-id: f7d01713d57a3faeb8670654ec55c99845bab037",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
992,Jiatao Gu,thomagram@gmail.com,2019-12-03 18:37:47-08:00,b0fb74f143ec17e01da9cf18a852aebc559ccaef,https://github.com/pytorch/fairseq/commit/b0fb74f143ec17e01da9cf18a852aebc559ccaef,"REFACTOR: NAT Implementation (#925)

Summary:
This diff mainly first contains the implementation for NAT-CRF model:
- Fast Structured Decoding for Sequence Models (NAT-CRF, Sun et al., 2019)

We implemented a dynamic CRF module and incorporate it into the implementation of vanilla NAT model. In order to reproduce the performance on paper.

We implemented the length beam as well as reranking from a learned autoregressive model in the iterative-refinement-generator;
We also implemented a new ensemble code which enables to do ensemble for all NAT models, not only Levenshtein Transformer itself. We refactor all the codes and move the models into ``fairseq/models/nat``.

Finally, we updated the README.md for NAT models.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/925

Differential Revision: D18738085

Pulled By: MultiPath

fbshipit-source-id: 4e421c5d52d2456fbe99e7863d715c756b1fd49b",19,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
993,Bryan Eikema,bryan.eikema@student.uva.nl,2019-12-04 07:18:04-08:00,5b8236fb7de9f2b7da572c50b5f5c8acdcadc719,https://github.com/pytorch/fairseq/commit/5b8236fb7de9f2b7da572c50b5f5c8acdcadc719,"Make use of args.skip_invalid_size_inputs_valid_test in fairseq-interactive (#1444)

Summary:
`fairseq-interactive` currently ignores the `--skip_invalid_size_inputs_valid_test` parameter. I think this is the intended use of the parameter.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1444

Differential Revision: D18809521

Pulled By: myleott

fbshipit-source-id: ceb70eca89b807b94588f13e705b6f8d4f79458d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
994,zmleok,zaemyung@gmail.com,2019-12-04 07:18:33-08:00,424fffa649998e279e4f8abad045685862930f55,https://github.com/pytorch/fairseq/commit/424fffa649998e279e4f8abad045685862930f55,"Use the HTTPS URL for cloning fastBPE (#1441)

Summary:
Cloning with SSH URL raises permission error.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1441

Differential Revision: D18809516

Pulled By: myleott

fbshipit-source-id: 1c296d3c369179e337fc9f7fe0ef6bc1efcce749",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
995,Cola,43774355+colanim@users.noreply.github.com,2019-12-04 13:08:16-08:00,fb2af4b1df2f75962d3f26be079ff34486925639,https://github.com/pytorch/fairseq/commit/fb2af4b1df2f75962d3f26be079ff34486925639,"Fix typo in BART CNN/DM finetuning (#1436)

Summary:
Small typo in the inference script, in the README for finetuning BART on CNN/DM dataset.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1436

Differential Revision: D18809455

Pulled By: myleott

fbshipit-source-id: 272ccda9e6e189123fdf2d516c2659d76f2659a3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
996,Yutong Pang,yutongpang@fb.com,2019-12-04 14:10:02-08:00,72bcb9dcdb15bdf04c1198e3711ebd05be56f3a3,https://github.com/pytorch/fairseq/commit/72bcb9dcdb15bdf04c1198e3711ebd05be56f3a3,"data augmentation pipeline

Summary:
Seq2Seq Data augmentation pipeline based on prefix with the following workflow

{F207337357}

The pipeline takes about 5 hours to process music domain, the music domain output is about 100k augmented data.

Example:
f136633622

fix 2 bugs:

* sequence_generator max_len assertion
* lengths in interactive.py should be python list instead of tensor

Reviewed By: myleott

Differential Revision: D17138089

fbshipit-source-id: eaeeadd5ba81e02930a45f8873069137469925b6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
997,Haoming Jiang,jianghm.ustc@gmail.com,2019-12-04 15:37:25-08:00,f8180af3555ba8b185af95f71b14863ea9146cc3,https://github.com/pytorch/fairseq/commit/f8180af3555ba8b185af95f71b14863ea9146cc3,"Fix shape issue of `return_all_hiddens` in roberta (#1438)

Summary:
By default `return_all_hiddens` is False, the shape of `features` will be BxTxC.
If use `return_all_hiddens`, the shape of `features` will be TxBxC.
See
https://github.com/pytorch/fairseq/blob/9398a2829596393b73f5c5f1b99edf4c2d8f9316/fairseq/modules/transformer_sentence_encoder.py#L227
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1438

Differential Revision: D18809509

Pulled By: myleott

fbshipit-source-id: 696b395934e2b7e5807387069fe1da49a4df98c7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
998,Wei Ho,weiho@fb.com,2019-12-05 18:12:58-08:00,51d2fe2503b3f6cd3e07689edcfa160d27a829a5,https://github.com/pytorch/fairseq/commit/51d2fe2503b3f6cd3e07689edcfa160d27a829a5,"Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility

Reviewed By: myleott

Differential Revision: D18736914

fbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
999,Myle Ott,myleott@fb.com,2019-12-06 07:09:14-08:00,3f4faafbafbbebb6296de2a00a965f0d0dc251bc,https://github.com/pytorch/fairseq/commit/3f4faafbafbbebb6296de2a00a965f0d0dc251bc,"Simplify example LM config

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/935

Differential Revision: D18852262

Pulled By: myleott

fbshipit-source-id: affe54703148c30105415efb1bdf050132936354",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1000,Myle Ott,myleott@fb.com,2019-12-06 08:17:37-08:00,af3050285d646125e7c76a245188b23a0b043185,https://github.com/pytorch/fairseq/commit/af3050285d646125e7c76a245188b23a0b043185,"Fix non-fvcore code path in PathManager

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/938

Differential Revision: D18852704

Pulled By: myleott

fbshipit-source-id: 23e2c143362af1f005b598a9803b2e38612ab018",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1001,Andrew Gallagher,andrewjcg@fb.com,2019-12-08 00:18:12-08:00,30f23b92390c291eadff1722cadca24873a7f28f,https://github.com/pytorch/fairseq/commit/30f23b92390c291eadff1722cadca24873a7f28f,"multifeed/aggregator: add-only run of IWYU on dependents (3/5) (5/10)

Summary:
Run IWYU (https://fburl.com/fbcode-iwyu) in ""add-only"" mode on all transitive
dependents of multifeed sources used in multifeed/aggregator builds:

```
$ buck query 'kind(cxx_.*, rdeps(set(//admarket/... //insights/... //multifeed/...), kind(cxx_.*,deps(fbcode//multifeed/aggregator:aggregator_link) intersect set(//multifeed/...))))'  | sort > /var/tmp/targets.txt
$ buck run tools/build/iwyu:iwyu -- -j 24 --add-only $(split -n l/3/5 /var/tmp/targets.txt)
```

Differential Revision: D18856262

fbshipit-source-id: 2d4aeb07a7eb99693f10d0a38db2aaacda86f1d6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1002,Changhan Wang,changhan@fb.com,2019-12-08 11:07:10-08:00,7612eefc6d8232380f861c1212fdcf45bc9a3f69,https://github.com/pytorch/fairseq/commit/7612eefc6d8232380f861c1212fdcf45bc9a3f69,"add VizSeq to README

Summary: add VizSeq to README

Reviewed By: MultiPath

Differential Revision: D18877679

fbshipit-source-id: f1de226e37b19ec967dfcec91216521d4e5b6e22",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1003,Sujit Verma,sujitv@fb.com,2019-12-09 14:18:02-08:00,28b131359b7638cc609c3c537d6692170a0e0d1d,https://github.com/pytorch/fairseq/commit/28b131359b7638cc609c3c537d6692170a0e0d1d,"Added unit test for PathManager file io (with or without fvcore).

Summary: Added unit test for PathManager file io (with or without fvcore).

Reviewed By: theweiho

Differential Revision: D18880067

fbshipit-source-id: 969c2be90415d22041b8276b7a5ff264571561d0",1,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestFileIO(unittest.TestCase):'],"[('Equal', '(s, self._tmpfile_contents)'), ('Equal', '(s, self._tmpfile_contents)')]",[],[],[],[],[],[],[],[' unittest.mock '],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1004,Naman Goyal,namangoyal@learnfair1944.h2.fair,2019-12-10 15:45:36-08:00,4c0d432ec74e0737c5132cde18182ce3f94599c0,https://github.com/pytorch/fairseq/commit/4c0d432ec74e0737c5132cde18182ce3f94599c0,"releas final xlmr models

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/940

Differential Revision: D18912528

fbshipit-source-id: 697ba6842f90c955fdd7792ec3322609f4d438db",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1005,Wei Ho,weiho@fb.com,2019-12-11 00:11:21-08:00,742ce49bfcc9b4b6e57ccd5c0d8cf62c40df49fa,https://github.com/pytorch/fairseq/commit/742ce49bfcc9b4b6e57ccd5c0d8cf62c40df49fa,"Add Manifold support for PyTorch Translate and fairseq vocab Dictionary

Reviewed By: sujitoc

Differential Revision: D18735421

fbshipit-source-id: 914b3cb4991f4c82b05a3ee0c3f6c764c217fa8d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1006,Juan Miguel Pino,juancarabina@fb.com,2019-12-11 09:28:53-08:00,0b2129922fd1a999ec996d860877b96f9298efe2,https://github.com/pytorch/fairseq/commit/0b2129922fd1a999ec996d860877b96f9298efe2,"Filter training data by max_tokens

Summary:
https://github.com/pytorch/fairseq/blob/master/fairseq/data/data_utils_fast.pyx#L50 makes training fail when there is an example with length > max_tokens.
Using max_positions requires specifying it in the task or model and have the user think about specifying it, so this is probably a useful default behavior.

Reviewed By: myleott

Differential Revision: D18868398

fbshipit-source-id: 8146eb64c374d0d18223bcca789dce8b69a23f0d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1007,Sharad,sharad.duwal@gmail.com,2019-12-11 10:34:45-08:00,a7cc3693538467ad94b1956967a8ccb1f57f2753,https://github.com/pytorch/fairseq/commit/a7cc3693538467ad94b1956967a8ccb1f57f2753,"Add piece decoding in spm_decode.py

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1473

Differential Revision: D18933898

Pulled By: myleott

fbshipit-source-id: d05dd2af450135b22720fa10eaa3b95d8f9e485f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1008,Myle Ott,myleott@fb.com,2019-12-11 10:57:16-08:00,15bd9bebbb6d34c70cacdc69ac21df2bbd6c1afd,https://github.com/pytorch/fairseq/commit/15bd9bebbb6d34c70cacdc69ac21df2bbd6c1afd,"Misc fixes

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/939

Differential Revision: D18874807

Pulled By: myleott

fbshipit-source-id: 3c97b8315042fc499a0d47b778ba0aee2a318b34",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1009,Li Dong,donglixp@gmail.com,2019-12-11 11:16:25-08:00,fd547dd23f615d91f52ee2fcc6a0709367ccd9c9,https://github.com/pytorch/fairseq/commit/fd547dd23f615d91f52ee2fcc6a0709367ccd9c9,"ensure self.scale_window is an integer (#1457)

Summary:
The scale window condition is `elif (self._iter - self._last_overflow_iter) % self.scale_window == 0`, which requires self.scale_window is an integer. If self.scale_window is float, the result of `(self._iter - self._last_overflow_iter) % self.scale_window` will never be 0.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1457

Differential Revision: D18810052

Pulled By: lematt1991

fbshipit-source-id: 3a527a157d2b2228d3e3baca3f4f2ce6b2a0f95b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1010,ngoyal2707,ngoyal2707@users.noreply.github.com,2019-12-12 10:46:05-08:00,0e2e5f1fc475c2b87f19c183f65351e13e38ffde,https://github.com/pytorch/fairseq/commit/0e2e5f1fc475c2b87f19c183f65351e13e38ffde,"xlmr updated readme with list of languages

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/943

Differential Revision: D18955467

fbshipit-source-id: a1f458e405f3c91485611191808e11ef056008c0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1011,Yun Wang,yunwang@fb.com,2019-12-13 14:03:49-08:00,29d7182447c4120057c116eb22c06d1d909eb3a1,https://github.com/pytorch/fairseq/commit/29d7182447c4120057c116eb22c06d1d909eb3a1,"Fairseq: Save predictions in logging output for evaluating MAP and MAUC (#1499)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1499

In sound event detection, the evaluation metrics are mean average precision (MAP) and mean area under the curve (MAUC).
These metrics evaluate how well the system *sorts* instances.
They are not decomposable across instances, so one needs to collect the predictions and truths of all instances of a validation set.

This diff adds a switch `log_pred` to the `forward` function of the binary cross entropy.
When this switch is turned on, it saves the predictions and truths to the `logging_output` dictionary.

This dictionary needs to be synced across workers by the `all_gather_list` function in `distributed_utils.py`.
The existing implementation restricts the size of the dictionary to 64KB, because it only allocates 2 bytes for the size during serialization. The predictions and truths often exceed this limit.
This diff uses 4 bytes for the size during serialization, and increases the limit.

Reviewed By: myleott

Differential Revision: D18922980

fbshipit-source-id: 1210b85c62dafab729de1fe0eced9a0cc56814a4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1012,lematt1991,lematt1991@gmail.com,2019-12-13 15:11:36-08:00,73bacfb22dc4efc33219c590142ead36c23c338d,https://github.com/pytorch/fairseq/commit/73bacfb22dc4efc33219c590142ead36c23c338d,"Adding github issues/PR templates (#1500)

Summary:
myleott, ngoyal2707 anything else you'd like included?
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1500

Differential Revision: D19038395

Pulled By: lematt1991

fbshipit-source-id: 65bab1534a3822438691bfc9d223ad051e30c572",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1013,Changhan Wang,changhan@fb.com,2019-12-14 14:10:18-08:00,5d7a81099462e9f19715ce5fa37c03816a750e12,https://github.com/pytorch/fairseq/commit/5d7a81099462e9f19715ce5fa37c03816a750e12,"Fix Levenshtein Transformer max length

Summary: Max length limit in Levenshtein Transformer is wrong when input sentences in the batch have the same lengths (likely to happen when batch size is small). This diff provides a bug fix.

Reviewed By: MultiPath

Differential Revision: D19069640

fbshipit-source-id: da0b273adbf9868a1ebc27654b40df08632d8428",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1014,Myle Ott,myleott@fb.com,2019-12-16 08:33:43-08:00,45893be3bc6b4efe91fc943b78f4fe97050a78aa,https://github.com/pytorch/fairseq/commit/45893be3bc6b4efe91fc943b78f4fe97050a78aa,"Update issue templates

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/948

Differential Revision: D19094356

Pulled By: myleott

fbshipit-source-id: f7ec7c62d00b8fe4a4265e6d1fc7e9e41c1e06cf",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1015,Myle Ott,myleott@fb.com,2019-12-16 13:57:16-08:00,a9c930430636e79853203ab9fd921c33855d8ea1,https://github.com/pytorch/fairseq/commit/a9c930430636e79853203ab9fd921c33855d8ea1,"Handle decoding of unknowns with GPT-2 BPE

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/946

Differential Revision: D19084269

Pulled By: myleott

fbshipit-source-id: 324c7e13034c2b482339e710d0266191108663da",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1016,linus,k.lee@aitrics.com,2019-12-16 15:50:13-08:00,212a9c340e0fe9930b39c501404de59dddaa1e9f,https://github.com/pytorch/fairseq/commit/212a9c340e0fe9930b39c501404de59dddaa1e9f,"Fix examples/speech_recognition while using multi num_workers on multi GPU setup (#1454)

Summary:
https://github.com/pytorch/fairseq/issues/1308
tgt in AsrDataset is list of torch tensors and it cause SIGSEGV error because tgt has too many objects to create shared memory in multiprocessing of dataloaders.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1454

Differential Revision: D18929874

Pulled By: okhonko

fbshipit-source-id: 5582b126890a93177258f5e053f32d5c6d32e9ab",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1017,Myle Ott,myleott@fb.com,2019-12-16 17:12:49-08:00,cd521b4a7c7334c5bac08063bdf73768a7a70c9f,https://github.com/pytorch/fairseq/commit/cd521b4a7c7334c5bac08063bdf73768a7a70c9f,"Fix Windows path splitting

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/950

Differential Revision: D19121625

Pulled By: myleott

fbshipit-source-id: 85c9e5a9d84f1d5de0cf37af71426868112f8110",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1018,Myle Ott,myleott@fb.com,2019-12-16 17:14:14-08:00,ebc1f91c7d181fb6123ef854e26ee9b3697d775b,https://github.com/pytorch/fairseq/commit/ebc1f91c7d181fb6123ef854e26ee9b3697d775b,"Fix fairseq/data/token_block_utils_fast.pyx

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/951

Differential Revision: D19124622

Pulled By: myleott

fbshipit-source-id: a44b2fdc0834126829919bc8ad454ff1fbaa8627",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1019,Myle Ott,myleott@fb.com,2019-12-16 17:18:29-08:00,be3515b28944dc9a459ac1d19b2ddcaebaa30c94,https://github.com/pytorch/fairseq/commit/be3515b28944dc9a459ac1d19b2ddcaebaa30c94,"More fully deprecate --raw-text and --lazy-load (fixes #1488)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947

Differential Revision: D19084273

Pulled By: myleott

fbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1020,Myle Ott,myleott@fb.com,2019-12-16 19:16:15-08:00,78d86dcc6c18df26c7a16c5a0f97a6d9e8a994d1,https://github.com/pytorch/fairseq/commit/78d86dcc6c18df26c7a16c5a0f97a6d9e8a994d1,"Fix bug in LM sampling when using bos token

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/945

Differential Revision: D19084265

Pulled By: myleott

fbshipit-source-id: d7788b4311ce9d1ef94e479f7dc3017dd888426c",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1021,Myle Ott,myleott@fb.com,2019-12-16 19:46:03-08:00,05514f8a825798b6cacddcfc8b7f77c4bf1c1433,https://github.com/pytorch/fairseq/commit/05514f8a825798b6cacddcfc8b7f77c4bf1c1433,"Update README to indicate we only support Python >= 3.6 (fixes #1317)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/952

Differential Revision: D19133348

Pulled By: myleott

fbshipit-source-id: 51f96ddb13386143fe0088f19f7cb0674755811f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1022,Myle Ott,myleott@fb.com,2019-12-17 20:15:27-08:00,66729814fa46dcae9b10860afd8d79d9bd73c99e,https://github.com/pytorch/fairseq/commit/66729814fa46dcae9b10860afd8d79d9bd73c99e,"Add --all-gather-list-size

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/954

Differential Revision: D19143235

Pulled By: myleott

fbshipit-source-id: 271d530c9c173c12cc400d4d1c1ab81ce5650d45",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1023,Myle Ott,myleott@fb.com,2019-12-17 20:43:32-08:00,dfde36bc66ae4a15ab7c8fd845deba2f005b941b,https://github.com/pytorch/fairseq/commit/dfde36bc66ae4a15ab7c8fd845deba2f005b941b,"Create build.yml

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1515

Differential Revision: D19151562

Pulled By: myleott

fbshipit-source-id: 426eca1e449cac914d49877678323a6487c0adbe",6,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'torch.cuda.device_count() < 2, )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1024,Myle Ott,myleott@fb.com,2019-12-18 07:03:01-08:00,6b4700cea41460e43bc087fbb5344ba2cda58caf,https://github.com/pytorch/fairseq/commit/6b4700cea41460e43bc087fbb5344ba2cda58caf,"Update README.md

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1521

Differential Revision: D19159323

Pulled By: myleott

fbshipit-source-id: 7e3fefc29229a90bcffe8bb1c5cff3507712d94c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1025,Myle Ott,myleott@fb.com,2019-12-18 12:09:40-08:00,9d7725226da3fcd9c5d1ac02473289f53cd7dd78,https://github.com/pytorch/fairseq/commit/9d7725226da3fcd9c5d1ac02473289f53cd7dd78,"Add .score function to hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/957

Differential Revision: D19161825

Pulled By: myleott

fbshipit-source-id: 726b9c1645058d993eb514b9e91c442683521bd0",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1026,Myle Ott,myleott@fb.com,2019-12-18 16:53:22-08:00,680f85ad8beebedf2a4c3ce129b6b4b3ba61fff3,https://github.com/pytorch/fairseq/commit/680f85ad8beebedf2a4c3ce129b6b4b3ba61fff3,"Add lightconv and dynamic conv models to torch.hub and add missing BPE codes

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/958

Differential Revision: D19167618

Pulled By: myleott

fbshipit-source-id: cfbdc3dc20d0e9ec1feb7a29ad4479f21cf87209",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1027,Peng-Jen Chen,pipibjc@fb.com,2019-12-19 07:07:46-08:00,4c5934ac61354d9b6d164f7317905e4ac2ae1064,https://github.com/pytorch/fairseq/commit/4c5934ac61354d9b6d164f7317905e4ac2ae1064,"Fix multilingual translation errors and add unit test

Summary:
- Fix github issue [1393](https://github.com/pytorch/fairseq/issues/1393), [1315](https://github.com/pytorch/fairseq/issues/1315).
- Add unit test to cover training, validation and generation for multilingual model to make sure they can run without problem. (didn't test the correctness)

Reviewed By: lematt1991

Differential Revision: D19149575

fbshipit-source-id: 9ec9000d037cc5c3bd8457feb527f2305375a442",6,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1028,Myle Ott,myleott@fb.com,2019-12-19 08:54:54-08:00,58ec8c0f28250419ea10dded02ceee9b74626e24,https://github.com/pytorch/fairseq/commit/58ec8c0f28250419ea10dded02ceee9b74626e24,"Fix MHA with bias=False (fixes #1527)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1528

Differential Revision: D19178445

Pulled By: myleott

fbshipit-source-id: 2e08012205de825ded334222d29797f2c125f15e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1029,Alex Xiao,axiao@fb.com,2019-12-20 15:25:51-08:00,9ad6b5a96783df7570b823ca4c73c0f61d1cfbb5,https://github.com/pytorch/fairseq/commit/9ad6b5a96783df7570b823ca4c73c0f61d1cfbb5,"fix bmuf restart causing lr to be reset

Summary:
From https://fb.workplace.com/groups/332923290658088/permalink/505568510060231/, we have observed that learning rate on resume is reset to original value. Based on investigation in https://fb.workplace.com/groups/332923290658088/permalink/505568510060231/?comment_id=506348519982230, it seems like this happens 500 iterations after restarting, which coincides with BMUF warmup. After further debugging, ti seems like this is confirmed to be due to bmuf and because after warmup we reset the optimizer to the initial state  when created.

My proposed fix is to reset the initial state of the optimizer whenever we load the state dict.

Reviewed By: zhengwy888

Differential Revision: D19183595

fbshipit-source-id: 4cdc13378817a7e9a6b658010b152a508991971f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1030,Jiatao Gu,jgu@fb.com,2019-12-21 02:43:47-08:00,a316bd99b72458b553ade4105b3aafaad7b9535d,https://github.com/pytorch/fairseq/commit/a316bd99b72458b553ade4105b3aafaad7b9535d,"CUDA implementation of Levenshtein distance for NAT training (#960)

Summary:
## What does this PR do?
CUDA implementation for Levenshtein distance for NAT and other potential application.
It will make training Levenshtein Transformer slightly faster and clean the functions.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/960

Test Plan: Imported from GitHub. Tested locally.

Reviewed By: cndn

Differential Revision: D19207096

Pulled By: MultiPath

fbshipit-source-id: 4890bbaa851ffd302648c0d949173158dc3167e2",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1031,Myle Ott,myleott@fb.com,2019-12-21 06:27:02-08:00,08a7160518bd149002d9c3e0f3030d106bee1e57,https://github.com/pytorch/fairseq/commit/08a7160518bd149002d9c3e0f3030d106bee1e57,"Fix build

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1535

Differential Revision: D19209137

Pulled By: myleott

fbshipit-source-id: 797a2273abb85b96821bf5a1a9afe4ad7307f9f6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1032,Guntupalli Venkata Sai Kalyan,kalyan-6941@kalyan-6941.csez.zohocorpin.com,2019-12-21 07:11:26-08:00,1512a0e6213c9a84c5bb1eb918147bd6e96092cd,https://github.com/pytorch/fairseq/commit/1512a0e6213c9a84c5bb1eb918147bd6e96092cd,"added patience for early stopping (#959)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/959

Added patience flag to support early stop when the validation metric does not improve for certain no of patience epochs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/904

Differential Revision: D19165379

Pulled By: myleott

fbshipit-source-id: 126d779ff6b10b48bae42213329baa296bf5ca50",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1033,Jiatao Gu,jgu@fb.com,2019-12-21 11:26:39-08:00,fc50455adcff1732cede5404ea7eb88b20c4d5fe,https://github.com/pytorch/fairseq/commit/fc50455adcff1732cede5404ea7eb88b20c4d5fe,"Fix for the CUDA Levenshtein Distance. (#961)

Summary:
## What does this PR do?
Fixes https://github.com/fairinternal/fairseq-py/issues/960
Some unknown editor bug automatically added one ""}"" in the last line and build does not detect that.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/961

Differential Revision: D19209600

Pulled By: MultiPath

fbshipit-source-id: ade0dc09c35dd2e507f742f7fbaca6ac7ba79be3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1034,Yun Wang,yunwang@fb.com,2019-12-22 09:53:54-08:00,7e4c1a4b6a40d0c6cde638dc2c7f2eeb459bc7fb,https://github.com/pytorch/fairseq/commit/7e4c1a4b6a40d0c6cde638dc2c7f2eeb459bc7fb,"Fairspeq: Move TALNet code out of experimental

Summary:
Move code related to TALNet out of experimental. This includes:
* TALNet task;
* TALNet model;
* TALNet data (Hive source, prefeteching, iterator, collater, transform);
* Meters for ranking metrics such as MAP and MAUC.

Reviewed By: zhengwy888

Differential Revision: D19186762

fbshipit-source-id: a560333a06cd23daa97bb23311f04e2ed452a1ad",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1035,Myle Ott,myleott@fb.com,2019-12-24 09:44:48-08:00,6429a2a91996e5bf085269e67545cc3bea4e7361,https://github.com/pytorch/fairseq/commit/6429a2a91996e5bf085269e67545cc3bea4e7361,"Fix hub decoding for GPT-2 BPE with <mask> tokens

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1545

Differential Revision: D19216003

Pulled By: myleott

fbshipit-source-id: 7f26963283f823eb8c5f6b13023e61c815f7b118",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1036,Myle Ott,myleott@fb.com,2019-12-24 09:44:55-08:00,33d6a0247b064efd22e2f6645aa7e30f9f3ac405,https://github.com/pytorch/fairseq/commit/33d6a0247b064efd22e2f6645aa7e30f9f3ac405,"More robust gradient consistency check

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1544

Differential Revision: D19216001

Pulled By: myleott

fbshipit-source-id: cff85226d3689766a2f737e1d381aadace17a700",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1037,Myle Ott,myleott@fb.com,2019-12-24 09:44:58-08:00,6dfe2f4de9fc4b4b55099f481399125e4ec6ebe0,https://github.com/pytorch/fairseq/commit/6dfe2f4de9fc4b4b55099f481399125e4ec6ebe0,"Support label smoothing in MoE

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1542

Differential Revision: D19216153

Pulled By: myleott

fbshipit-source-id: 575bf7a6695f4bd9bf8039ccda25e538eed2bbdd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1038,Myle Ott,myleott@fb.com,2019-12-24 09:45:21-08:00,4333437ffe15f7a5d187dc3ffa3dfa3fecf9bace,https://github.com/pytorch/fairseq/commit/4333437ffe15f7a5d187dc3ffa3dfa3fecf9bace,"Add --truncate-sequence to LM task

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1543

Differential Revision: D19215999

Pulled By: myleott

fbshipit-source-id: e8c4c7ebdf440818eed85421f672020b5a000600",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1039,Myle Ott,myleott@fb.com,2019-12-24 12:12:21-08:00,9b19ede0a986a5362fa0bd78b753c908d613ebb2,https://github.com/pytorch/fairseq/commit/9b19ede0a986a5362fa0bd78b753c908d613ebb2,"Fix keyword arguments in translation_moe task

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1546

Differential Revision: D19225548

Pulled By: myleott

fbshipit-source-id: 43240cb90ca477ab7a790386ab2d9f4fd14e2625",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1040,sai-prasanna,sai.r.prasanna@gmail.com,2019-12-25 19:18:49-08:00,3b53962cd7a42d08bcc7c07f4f858b55bf9bbdad,https://github.com/pytorch/fairseq/commit/3b53962cd7a42d08bcc7c07f4f858b55bf9bbdad,"Refactor hub interface for batched inference (#1539) (#1539)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1508.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1539

Pulled By: myleott

Differential Revision: D19216104

fbshipit-source-id: 14917c1459b8794eeb74c09a16b9899c366242d2",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1041,Juan Miguel Pino,juancarabina@fb.com,2019-12-26 21:27:07-08:00,cae55599a91d6ff21544c70f9cb92544cce6c342,https://github.com/pytorch/fairseq/commit/cae55599a91d6ff21544c70f9cb92544cce6c342,"Fix broken speech recognition example link

Summary: Title.

Reviewed By: myleott

Differential Revision: D19234363

fbshipit-source-id: 5159eaa5c529fbfd9d5e53a4f128529e3dbdd1b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1042,Pengcheng YIN,pcyin@cs.cmu.edu,2020-01-02 11:35:56-08:00,9090dad8d163a3ced087dfd78add0974387603eb,https://github.com/pytorch/fairseq/commit/9090dad8d163a3ced087dfd78add0974387603eb,"Update README for apex (#1563)

Summary:
Recent releases of apex removed the `fused_adam_cuda` function used in https://github.com/pytorch/fairseq/blob/3f4fc5016334255d6908b20202267ca0b0287335/fairseq/optim/adam.py#L220. Users need to use the `--deprecated_fused_adam` option to isntall `fused_adam_cuda`

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1563

Differential Revision: D19260517

Pulled By: myleott

fbshipit-source-id: 69af015f3ef1fa85b98d138c28876ada194c9437",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1043,Myle Ott,myleott@fb.com,2020-01-03 06:26:24-08:00,1e324a5bbe4b1f68f9dadf3592dab58a54a800a8,https://github.com/pytorch/fairseq/commit/1e324a5bbe4b1f68f9dadf3592dab58a54a800a8,"Add metrics.py

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/973

Differential Revision: D19266024

Pulled By: myleott

fbshipit-source-id: 3e2f1b8d10a5ac5ee23183a34f2078ba521c905d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1044,Myle Ott,myleott@fb.com,2020-01-03 11:07:45-08:00,f75411af2690a54a5155871f3cf7ca1f6fa15391,https://github.com/pytorch/fairseq/commit/f75411af2690a54a5155871f3cf7ca1f6fa15391,"Add FusedLAMB optimizer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/971

Differential Revision: D19265752

Pulled By: myleott

fbshipit-source-id: 062748d3b44ef3627d35d65dccef8659709c2b5e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1045,Myle Ott,myleott@fb.com,2020-01-03 14:09:04-08:00,c1848270723fa4be7cfb0bc92a5d14546a80d879,https://github.com/pytorch/fairseq/commit/c1848270723fa4be7cfb0bc92a5d14546a80d879,"Fix scheduling logic for update_freq

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/967

Differential Revision: D19265772

Pulled By: myleott

fbshipit-source-id: cb9174e095160490e329400526e9b43c05cbd669",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1046,Myle Ott,myleott@fb.com,2020-01-06 07:11:39-08:00,fb2d29d2aa041ca77e349eae0a6c86b0d8ddd8b7,https://github.com/pytorch/fairseq/commit/fb2d29d2aa041ca77e349eae0a6c86b0d8ddd8b7,"Fix multilingual translation

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/972

Differential Revision: D19265750

Pulled By: myleott

fbshipit-source-id: 4c432b0d3616a6194c2c0f61f97012937d22db6f",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('AlmostEqual', '(float(train_log[k]), float(train_res_log[k]), delta=delta)'), ('AlmostEqual', '(float(valid_log[k]), float(valid_res_log[k]), delta=delta)')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(cast(train_log[k]), cast(train_res_log[k]))'), ('Equal', '(cast(valid_log[k]), cast(valid_res_log[k]))')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1047,Aleksandra Piktus,piktus@devfair0227.h2.fair,2020-01-06 08:21:03-08:00,fab2e86e51f48301146a37fc86d01fdbf5e8a5f2,https://github.com/pytorch/fairseq/commit/fab2e86e51f48301146a37fc86d01fdbf5e8a5f2,"Add a diverse beam search variant to sequence_generator.py (#953)

Summary:
This PR implements a new generation strategy that we experimented with in project Pinocchio (https://github.com/fairinternal/Pinocchio), see the paper submission in: https://fburl.com/hduj2me7.

Specifically in this PR:
- added a Diverse Beam Search variant as described in https://arxiv.org/abs/1611.08562
- moved the Search object generation out of `sequence_generation.py`, which allows for limiting the number of kwargs passes around
- made sure the above changes are backward compatible based on grep - P124083926
- added test cases covering these scenarios
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/953

Test Plan:
- `python -m unittest tests.test_binaries -v`- including added test cases, see issues below for some details
- `python -m unittest tests.test_sequence_generator -v` - including added test cases
- tested locally in conjunction with the Pinocchio repo
- grepped for all instantiations of `SequenceGeneration`, made sure they're backward compatible

# Issues
- when I try to run all tests with `python -m unittest tests.test_binaries -v` command, the execution gets stuck on `test_binaries.TestTranslation.test_generation` - the test otherwise passes without problems when ran individually. Is this a known problem?
- discovered T59235948 - assigned to fairseq oncall

Reviewed By: myleott, fabiopetroni

Differential Revision: D19142394

Pulled By: ola13

fbshipit-source-id: d24543424c14a9537e7b6485951d9f841da62b07",7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Raises', '(ValueError):'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('HypoTokens', '(hypos[0][0], [w1, w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 0.6, 1.0], [0, 1, 1], 0.5)'), ('HypoTokens', '(hypos[0][1], [w1, w2, eos])'), ('HypoScore', '(hypos[0][1], [0.9, 0.4, 1.0], [0, 2, 1], 0.5)'), ('HypoTokens', '(hypos[1][0], [w1, w2, eos])'), ('HypoScore', '(hypos[1][0], [0.7, 0.4, 0.9], [0, 1, 1], 0.5)'), ('HypoTokens', '(hypos[1][1], [w1, w1, eos])'), ('HypoScore', '(hypos[1][1], [0.7, 0.35, 0.9], [0, 2, 1], 0.5)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1048,Elijah Rippeth,elijah.rippeth@gmail.com,2020-01-06 09:25:33-08:00,05b6123b443d2e08fc177899ed1cf79efa1872f1,https://github.com/pytorch/fairseq/commit/05b6123b443d2e08fc177899ed1cf79efa1872f1,"use results_path to output to file if provided. (#1583)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1586

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
👍
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1583

Differential Revision: D19289796

Pulled By: myleott

fbshipit-source-id: 0c1f3d32d2d6c9f0102ea4544a031c9b2b0a51da",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1049,Elijah Rippeth,elijah.rippeth@gmail.com,2020-01-06 11:10:32-08:00,fdca5f899c9c8683a72b000e577e208b9c53319c,https://github.com/pytorch/fairseq/commit/fdca5f899c9c8683a72b000e577e208b9c53319c,"conditionally include __declspec to function definition to fix Window… (#1584)

Summary:
…s DLL exports.

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/292

☝️ technically this issue should remain open

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
👍
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1584

Differential Revision: D19289792

Pulled By: myleott

fbshipit-source-id: 96af49e2ed808dde3f682906bc2d6e074b522c39",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1050,Myle Ott,myleott@fb.com,2020-01-07 10:26:54-08:00,3960d96c4b2beefc5467b6cf61b10f6b80af629f,https://github.com/pytorch/fairseq/commit/3960d96c4b2beefc5467b6cf61b10f6b80af629f,"Updates for new Apex and add --fp16-no-flatten-grads

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/924

Differential Revision: D18642949

Pulled By: myleott

fbshipit-source-id: 96841e69696b845c30131d4edc9793ed8865fd1d",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1051,freewym,freewym@gmail.com,2020-01-07 20:19:24-08:00,6affd6f6cd2aea3690a64a85a16e5a1e3a70f110,https://github.com/pytorch/fairseq/commit/6affd6f6cd2aea3690a64a85a16e5a1e3a70f110,"fix when the ensembled models do not have a decoder, e.g., FairseqEnc… (#1577)

Summary:
…oderModel
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1577

Differential Revision: D19289850

Pulled By: myleott

fbshipit-source-id: c4193a01e38b4e246dc60603ba8c89b938612ac2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1052,Yun Wang,yunwang@fb.com,2020-01-08 14:08:18-08:00,2995b61dc560d5d43a0f45e1b94db80e84249fd9,https://github.com/pytorch/fairseq/commit/2995b61dc560d5d43a0f45e1b94db80e84249fd9,"Fairspeq: Allow both AudioSet and FB700K for AED training

Summary:
We have two corpora for AED training: AudioSet and FB700K.
This diff allows using the training set of either corpus for training,
and validating on the validation and evaluation sets of both corpora.

Reviewed By: jay-mahadeokar

Differential Revision: D19276282

fbshipit-source-id: 9716a6f3b3f118046c6f81ed67e1eafb67d4e49d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1053,Liam,liangding.liam@gmail.com,2020-01-09 10:39:33-08:00,303fb3f0aa999e56a54223b097d081c94c11174e,https://github.com/pytorch/fairseq/commit/303fb3f0aa999e56a54223b097d081c94c11174e,"fix parameter error (#1596)

Summary:
change ``cut -f3-`` to ``cut -f2-``
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1596

Differential Revision: D19330020

Pulled By: myleott

fbshipit-source-id: 3b2ce13a6c25d5c838b09d27fd9cf635fc46c210",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1054,Naman Goyal,namangoyal@learnfair1159.h2.fair,2020-01-09 11:35:08-08:00,075a4a5263f5ca1def4ce251d973e8645adfc5da,https://github.com/pytorch/fairseq/commit/075a4a5263f5ca1def4ce251d973e8645adfc5da,"small fixes to multilingual masked lm task (#978)

Summary:
# Before submitting

- [] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/978

Differential Revision: D19330758

fbshipit-source-id: 8b7c31ab9fc1507db37877d7a2c781447da055f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1055,Myle Ott,myleott@fb.com,2020-01-09 13:26:21-08:00,0790c0cfc3038dcd1890262f75c8a01f47d5fc20,https://github.com/pytorch/fairseq/commit/0790c0cfc3038dcd1890262f75c8a01f47d5fc20,"Doc improvements

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1606

Differential Revision: D19330727

Pulled By: myleott

fbshipit-source-id: dc6d100e42566efbc2ebc955689878ed8a820861",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1056,Myle Ott,myleott@fb.com,2020-01-09 13:32:11-08:00,097bb73da2482d312f0f9f69429656c894014bd9,https://github.com/pytorch/fairseq/commit/097bb73da2482d312f0f9f69429656c894014bd9,"Update instructions for evaluating on the RACE dataset

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/976

Differential Revision: D19323551

Pulled By: myleott

fbshipit-source-id: 322a811be046f45da01e404f5f3cb7429f3c81ac",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1057,Myle Ott,myleott@fb.com,2020-01-10 14:19:09-08:00,9da6207e72c650fe4c901515122ab67251f013aa,https://github.com/pytorch/fairseq/commit/9da6207e72c650fe4c901515122ab67251f013aa,"Add Dictionary.pad_to_multiple_ helper

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/970

Differential Revision: D19351834

Pulled By: myleott

fbshipit-source-id: 7149973728ef7595c16de7ac0c8b71747b89f65b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1058,Myle Ott,myleott@fb.com,2020-01-10 15:01:17-08:00,c9a7c06bb971f3a789c94c10eff32f0bdec89741,https://github.com/pytorch/fairseq/commit/c9a7c06bb971f3a789c94c10eff32f0bdec89741,"Remove unused FairseqTask.grad_denom and FairseqCriterion.grad_denom

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/979

Differential Revision: D19347309

Pulled By: myleott

fbshipit-source-id: ee6fc6111994d35e95ca0ef4cd8f5932e621d84b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1059,Myle Ott,myleott@fb.com,2020-01-10 20:09:45-08:00,fe6c2edad0c1f9130847b9a19fbbef169529b500,https://github.com/pytorch/fairseq/commit/fe6c2edad0c1f9130847b9a19fbbef169529b500,"Deprecate --fast-stat-sync and replace with Criterion.logging_outputs_can_be_summed

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/980

Differential Revision: D19351116

Pulled By: myleott

fbshipit-source-id: a67b10637f53a80c37b0ce90eb27ced9709871db",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1060,Alex Xiao,axiao@fb.com,2020-01-10 22:40:00-08:00,0ce722d689d3a5d1dfc3776995c0a3332ec003ef,https://github.com/pytorch/fairseq/commit/0ce722d689d3a5d1dfc3776995c0a3332ec003ef,"add flag to use old adam, needed for backwards compatability

Summary: Old models that use FairseqAdam and eventually use Adam as the internal optimizer can no longer be loaded as checkpoints, because now FairseqAdam seems to always load FusedAdamV1. FusedAdamV1 and Adam are not compatible, meaning all old checkpoints are not loadable with the current code. This fix is to just add a flag to specify if we want to use Adam rather than FusedAdamV1. Later on we'll delete this flag once all models are using FusedAdamV1.

Reviewed By: myleott

Differential Revision: D19358962

fbshipit-source-id: a0af5d50588dc108339a77736dcc8ff5db314dd0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1061,Myle Ott,myleott@fb.com,2020-01-11 13:47:23-08:00,86793391e38bf88c119699bfb1993cb0a7a33968,https://github.com/pytorch/fairseq/commit/86793391e38bf88c119699bfb1993cb0a7a33968,"Deprecate aggregate_logging_outputs API (use reduce_metrics instead) (#1611)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1611

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/974

Differential Revision: D19292402

Pulled By: myleott

fbshipit-source-id: d51327584e048d3e39c133e9ef57a791e0329a66",23,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1062,Alex Xiao,axiao@fb.com,2020-01-11 20:31:04-08:00,7201ebc57ec9bff916ecd2e72b4f1e02ffa74f66,https://github.com/pytorch/fairseq/commit/7201ebc57ec9bff916ecd2e72b4f1e02ffa74f66,"only check grad norms if not using BMUF

Summary: Since D19351116, fairseq is checking if grad norms are consistent across workers. This is causing issues for BMUF since we don't sync every iteration (f161652382). Let's disable checking grad norm consistency for BMUF.

Reviewed By: myleott, jay-mahadeokar

Differential Revision: D19361249

fbshipit-source-id: 1abdfcfe7d64d52ec76341c7d2b1db33c3f12767",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1063,Myle Ott,myleott@fb.com,2020-01-13 09:06:57-08:00,ab6ce425f2894953ba393c80c5614af9d6244d31,https://github.com/pytorch/fairseq/commit/ab6ce425f2894953ba393c80c5614af9d6244d31,"Improve robustness of fast-stat-sync

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/986

Differential Revision: D19372552

Pulled By: myleott

fbshipit-source-id: 58a41ab71a09924b20832810d4e07dea932861cb",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1064,Myle Ott,myleott@fb.com,2020-01-13 09:24:49-08:00,660d69fd2bdc4c3468df7eb26b3bbd293c793f94,https://github.com/pytorch/fairseq/commit/660d69fd2bdc4c3468df7eb26b3bbd293c793f94,"Print base 2 scores in generate.py, interactive.py, eval_lm.py

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/987

Differential Revision: D19373600

Pulled By: myleott

fbshipit-source-id: d0cd0b616a95c7907d1856d786b4ebdb7a084011",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1065,Myle Ott,myleott@fb.com,2020-01-13 14:52:19-08:00,f5398c7e5e5114dcd626389eefcc84a2835262f6,https://github.com/pytorch/fairseq/commit/f5398c7e5e5114dcd626389eefcc84a2835262f6,"fix unit tests

Reviewed By: ngoyal2707

Differential Revision: D19378781

fbshipit-source-id: 51f0fb187d74bca989f6fad584ede4f4a01e9a69",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1066,Myle Ott,myleott@fb.com,2020-01-13 15:07:15-08:00,fa0b710e162f6b1b20f54a67fe8a12cb7858d20e,https://github.com/pytorch/fairseq/commit/fa0b710e162f6b1b20f54a67fe8a12cb7858d20e,"Don't compute alignment for SequenceScorer by default

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/988

Differential Revision: D19373601

Pulled By: myleott

fbshipit-source-id: fcc21365b75626be18d576253b756bf493977f5b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1067,Myle Ott,myleott@fb.com,2020-01-14 07:58:13-08:00,cf8676fd3fb15d3afc60a8b5339b8be1014d951a,https://github.com/pytorch/fairseq/commit/cf8676fd3fb15d3afc60a8b5339b8be1014d951a,"Update train.py to use new metrics API

Summary:
Updating pyspeech's train.py to use the new metrics API introduced in fairseq.

I only migrated one criterion from aggregate_logging_outputs -> reduce_metrics, but there should be fallback logic for the other criterions so we update them later as needed.

Reviewed By: jay-mahadeokar

Differential Revision: D19377721

fbshipit-source-id: df7d7f6f2ab78f48df8923c1242991d0cf28eece",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1068,Jiatao Gu,jgu@fb.com,2020-01-16 01:57:35-08:00,1bb218f0aeac5e68f3f4531510afade67be242ad,https://github.com/pytorch/fairseq/commit/1bb218f0aeac5e68f3f4531510afade67be242ad,"Split from PR#968. add --keep-best-checkpoints (#990)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.
Use scores as the names to save the checkpoints
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990

Differential Revision: D19411250

Pulled By: MultiPath

fbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1069,Myle Ott,myleott@fb.com,2020-01-16 16:12:45-08:00,fb76dac1c4e314db75f9d7a03cb4871c532000cb,https://github.com/pytorch/fairseq/commit/fb76dac1c4e314db75f9d7a03cb4871c532000cb,"Switch to Python logging (+ lint) (#1627)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1627

Python logging offers a number of benefits, such as logging timestamps, better
cross-library compatibility, ability to add multiple output handlers, etc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646

Reviewed By: spencerp

Differential Revision: D15815620

Pulled By: myleott

fbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134",43,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,5,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Logs', '() as logs:'), ('Logs', '() as logs:'), ('Logs', '() as logs:')]","['def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):']",[],"['def tearDown(self):', 'def tearDown(self):', 'def tearDown(self):', 'def tearDown(self):', 'def tearDown(self):']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1070,Elijah Rippeth,elijah.rippeth@gmail.com,2020-01-17 00:13:32-08:00,cec0da29273933687551dc8e508d706d1c1a628b,https://github.com/pytorch/fairseq/commit/cec0da29273933687551dc8e508d706d1c1a628b,"add other platforms to CI. (#1595)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Runs CI for `fairseq` on all major platforms provided by GitHub actions.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1595

Differential Revision: D19438282

Pulled By: myleott

fbshipit-source-id: a64db46d7785e6f583848f27699f6463c4dc3170",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1071,Zeming Lin,ebetica0@gmail.com,2020-01-17 06:28:03-08:00,9ac98bbd6de414f2acee76a9bc43bf157574b7ec,https://github.com/pytorch/fairseq/commit/9ac98bbd6de414f2acee76a9bc43bf157574b7ec,"Add modify_args to parse_args_and_arch (#983)

Summary:
Add modify_args to parse_args_and_arch. We find this useful in using cli_main as a library function. This is necessary since parse_args_and_arch does a double parse of the args, and never gives the intermediate parser for users to modify before parsing args.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/983

Differential Revision: D19392931

Pulled By: myleott

fbshipit-source-id: 1afeffc2143b349e392d38a99a933f72a8e581bd",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1072,Myle Ott,myleott@fb.com,2020-01-17 08:04:10-08:00,909d0ede00ab1382f774d3316529132620d3bd0b,https://github.com/pytorch/fairseq/commit/909d0ede00ab1382f774d3316529132620d3bd0b,"Reverse symlinks in root and fairseq_cli (1/3)

Summary: This is needed to support other build environments (e.g., Windows)

Reviewed By: ngoyal2707

Differential Revision: D19409701

fbshipit-source-id: 034fc3fbecea1ec53745fb111ce1c41186c5ae2b",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1073,Myle Ott,myleott@fb.com,2020-01-17 08:04:10-08:00,b488e1fe567016ad1f4d01d476a405afbb51457c,https://github.com/pytorch/fairseq/commit/b488e1fe567016ad1f4d01d476a405afbb51457c,"Reverse symlinks in root and fairseq_cli (2/3)

Summary: This is needed to support other build environments (e.g., Windows)

Reviewed By: ngoyal2707

Differential Revision: D19409984

fbshipit-source-id: e970510781abf92f1b02d0961bc30e1210b524dd",14,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1074,Myle Ott,myleott@fb.com,2020-01-17 08:04:10-08:00,4b986c51ed89f9895df2f30e57909301b4a4f19b,https://github.com/pytorch/fairseq/commit/4b986c51ed89f9895df2f30e57909301b4a4f19b,"Reverse symlinks in root and fairseq_cli (3/3)

Summary: This is needed to support other build environments (e.g., Windows)

Reviewed By: ngoyal2707

Differential Revision: D19409985

fbshipit-source-id: 23e04125f26fcc8c274cba8dc5b7a88ba528b402",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1075,Zeming Lin,ebetica0@gmail.com,2020-01-17 08:07:41-08:00,09eb02386fc96ea15f44a6b4c04a216461c0fd81,https://github.com/pytorch/fairseq/commit/09eb02386fc96ea15f44a6b4c04a216461c0fd81,"Remove required from --arch (#993)

Summary:
As per https://github.com/fairinternal/fairseq-py/issues/983, we find that you cannot just modify the defaults of arch because it will still ""require"" you to input something on the command line. I'm not sure why this is required where task, criterion, etc (and actually, anything else) are not. Additionally, the [argparse docs](https://docs.python.org/3/library/argparse.html#required) claim that required=True is bad form so should be avoided.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/993

Differential Revision: D19446805

Pulled By: myleott

fbshipit-source-id: 53221bb8bc1cea66197c5cee48a307b88d0d20b7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1076,Zeming Lin,ebetica0@gmail.com,2020-01-17 11:38:34-08:00,122fc1db49534a5ca295fcae1b362bbd6308c32f,https://github.com/pytorch/fairseq/commit/122fc1db49534a5ca295fcae1b362bbd6308c32f,"Add begin_epoch to FairseqTask (#984)

Summary:
Adds a begin_epoch hook to FairseqTask.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/984

Differential Revision: D19429433

Pulled By: myleott

fbshipit-source-id: 367bd4d0d2d2bc995cca9ac151256c77ede36c83",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1077,Jiatao Gu,jgu@fb.com,2020-01-17 12:16:10-08:00,60fbf64f302a825eee77637a0b7de54fde38fb2c,https://github.com/pytorch/fairseq/commit/60fbf64f302a825eee77637a0b7de54fde38fb2c,"Add --eval-bleu for translation

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/989

Reviewed By: MultiPath

Differential Revision: D19411162

Pulled By: myleott

fbshipit-source-id: 74842f0174f58e39a13fb90f3cc1170c63bc89be",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1078,Myle Ott,myleott@fb.com,2020-01-17 13:03:29-08:00,0ef703bc28830b4e92c6c1710572fdd354be16fc,https://github.com/pytorch/fairseq/commit/0ef703bc28830b4e92c6c1710572fdd354be16fc,"Fix accuracy reporting in sentence_* criterions

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/992

Differential Revision: D19451609

Pulled By: myleott

fbshipit-source-id: 3ec88896cf811d74fbb797b4ef2b974ee36bf192",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1079,Myle Ott,myleott@fb.com,2020-01-17 13:04:47-08:00,e198482e71838747de75e756d74eef2d39fb85f5,https://github.com/pytorch/fairseq/commit/e198482e71838747de75e756d74eef2d39fb85f5,"Fix binaries in root dir (#995)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/995

The symlinks approach didn't work with `python train.py`.

Differential Revision: D19451900

fbshipit-source-id: 2988eb48077cf8e0e078b9fca527a675132187db",16,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1080,Myle Ott,myleott@fb.com,2020-01-17 18:49:45-08:00,a77fb606d1a29dbd9aa08b876c594a693cc0ebbb,https://github.com/pytorch/fairseq/commit/a77fb606d1a29dbd9aa08b876c594a693cc0ebbb,"Fix logging during OOM recovery

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/997

Differential Revision: D19456752

Pulled By: myleott

fbshipit-source-id: d73c0a00a67e143548cb7151fed8916eb961b62d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1081,freewym,freewym@gmail.com,2020-01-20 12:14:07-08:00,42a8e7bdbfed1353a56fc9070fd6a0ae45d0672f,https://github.com/pytorch/fairseq/commit/42a8e7bdbfed1353a56fc9070fd6a0ae45d0672f,"fix the problem of passing None to format() when val_loss is None (e.… (#1633)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1633

Differential Revision: D19470727

Pulled By: myleott

fbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1082,Myle Ott,myleott@fb.com,2020-01-20 16:32:55-08:00,9f961964aa943d6853899ffc04916232ea7bfdba,https://github.com/pytorch/fairseq/commit/9f961964aa943d6853899ffc04916232ea7bfdba,"Fix logging of training sets (fixes #1632) (#1634)

Summary:
* fix: mid-epoch validation metrics were previously polluting training metrics
* fix: mid-epoch metrics were not properly saved/restored in checkpoints
* added tests, both for metrics and for mid-epoch reproducibility
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1634

Differential Revision: D19470714

Pulled By: myleott

fbshipit-source-id: 491fa8d830b653cdd6a86095645aabcac758d214",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,11,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMetrics(unittest.TestCase):'],"[('Equal', '(a.get_smoothed_values()[], 1.5)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(a.get_smoothed_values()[], 1)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(layer4.get_smoothed_values()[], 4)'), ('Equal', '(layer3.get_smoothed_values()[], 3)'), ('Equal', '(layer2.get_smoothed_values()[], 2.5)'), ('Equal', '(layer1.get_smoothed_values()[], 1.25)'), ('Equal', '(metrics.get_smoothed_values(name)[], 1.5)'), ('Equal', '(metrics.get_smoothed_values(name)[], 3)'), ('Equal', '(other.get_smoothed_values()[], 2)')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1083,Myle Ott,myleott@fb.com,2020-01-21 06:53:05-08:00,d9ded8464719d6808c94f4b5de80573667ff8204,https://github.com/pytorch/fairseq/commit/d9ded8464719d6808c94f4b5de80573667ff8204,"Fix logging when finetuning roberta

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1000

Differential Revision: D19471951

Pulled By: myleott

fbshipit-source-id: 60f5ae5bddaa37f9299da3e79ca6ffc09912ec0c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1084,Myle Ott,myleott@fb.com,2020-01-21 06:59:41-08:00,6b54873c417460ad1c6a10593977eba6947b0fd6,https://github.com/pytorch/fairseq/commit/6b54873c417460ad1c6a10593977eba6947b0fd6,"Reset CUDA memory cache after first step (reduces chance of OOM)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1001

Differential Revision: D19471949

Pulled By: myleott

fbshipit-source-id: f929aa5b0f3da37abf03ef1b5844bd2637366f5e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1085,Ning Dong,dnn@fb.com,2020-01-21 17:56:37-08:00,2535cab24fbd219909dd8b15ec4f5bcf9afe8965,https://github.com/pytorch/fairseq/commit/2535cab24fbd219909dd8b15ec4f5bcf9afe8965,"Formatting multihead_attention.py

Summary: Just formatting so review could be easier in the diff above.

Reviewed By: myleott

Differential Revision: D18799003

fbshipit-source-id: eb9dd385251edd7c60a285d7f576a7dad888db7b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1086,Ning Dong,dnn@fb.com,2020-01-21 18:24:00-08:00,4e48c4ae5da48a5f70c969c16793e55e12db3c81,https://github.com/pytorch/fairseq/commit/4e48c4ae5da48a5f70c969c16793e55e12db3c81,"Script MultiheadAttention (#1002)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1002

Pull Request resolved: https://github.com/pytorch/translate/pull/681

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1524

Make fairseq MultiheadAttention scriptable. Looking for feedbacks.

1. Add types
2. Move incremental state management logic from util functions to initializers. TorchScript in general doesn't support global dict. As a result modules with multihead attention in it would assign itself fairseq_instance_id in the initializer.
3. There might be opportunities to make assertions and annotations cleaner.

Reviewed By: myleott

Differential Revision: D18772594

fbshipit-source-id: 377aef4bbb7ef51da5b6bac9a87a6f7b03b16fe1",14,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestExportModels(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1087,Ning Dong,dnn@fb.com,2020-01-21 20:28:33-08:00,6a1cf9f390e260bf1830ce27ddf9962f63f2efd0,https://github.com/pytorch/fairseq/commit/6a1cf9f390e260bf1830ce27ddf9962f63f2efd0,"Formatting positional embedding / utils

Summary: Just formatting so review could be easier in the diff above.

Reviewed By: myleott

Differential Revision: D18925110

fbshipit-source-id: 136a72dec0c9238d309a5e5a4942e556cd2590ae",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1088,Ning Dong,dnn@fb.com,2020-01-22 10:53:43-08:00,89a2a0ccdebd0943b2878ff2150f8a5f836cc4aa,https://github.com/pytorch/fairseq/commit/89a2a0ccdebd0943b2878ff2150f8a5f836cc4aa,"Script SinusoidalPositionalEmbedding (#683)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/683

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1612

Make SinusoidalPositionalEmbedding scriptable. Mostly adding types. The only change that affects lots of downstream code is to have max_positions as member variable instead of method.

Reviewed By: myleott

Differential Revision: D18924939

fbshipit-source-id: 2b6486563e9ec5cc34bcf11acdff9054658f4674",9,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1089,Myle Ott,myleott@fb.com,2020-01-22 11:27:43-08:00,f4a9bc2ea624ce7dbcf9d9ec380e064bdc204d48,https://github.com/pytorch/fairseq/commit/f4a9bc2ea624ce7dbcf9d9ec380e064bdc204d48,"Clean up tests

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1004

Differential Revision: D19517900

Pulled By: myleott

fbshipit-source-id: a588efeabd3119dd058067e82d1b21e4d81ae218",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],['def setUp(self):'],[],['def tearDown(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1090,Weiyi Zheng,wyz@fb.com,2020-01-22 15:34:47-08:00,9ebcd6554daab7bac4948de49aeb85bfff81d876,https://github.com/pytorch/fairseq/commit/9ebcd6554daab7bac4948de49aeb85bfff81d876,"fblearner pyspeech manifold migration

Summary:
was planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.

migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.

Reviewed By: zdavid1995

Differential Revision: D19433205

fbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1091,Yun Wang,yunwang@fb.com,2020-01-22 23:40:35-08:00,0ff92b9b4a883270f3c0ebaf5c3cf723f544e3a0,https://github.com/pytorch/fairseq/commit/0ff92b9b4a883270f3c0ebaf5c3cf723f544e3a0,"Remove arg ""expand_steps"" from ""get_targets""

Summary:
The argument `expand_steps` is only used by the binary cross entropy criterion and the wav2vec model. It actually does nothing.

However, because the base `FairseqModel` class's `get_targets` method doesn't have this argument, a model cannot use the binary cross entropy criterion without overriding this method.

This diff removes the unused `expand_steps` argument.

Reviewed By: zhengwy888

Differential Revision: D19526094

fbshipit-source-id: 1243593490d61a2ec3a60357ea627f7bde7a88f3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1092,huffon,huffonism@gmail.com,2020-01-24 09:20:56-08:00,4f71c63771718b8d2865f4b6fdec39232815eb3b,https://github.com/pytorch/fairseq/commit/4f71c63771718b8d2865f4b6fdec39232815eb3b,"Fix padding_idx logical error in Adaptive Input (#1629)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?

I think if we keep pass **padding index of vocabulary** as `padding_idx` to adaptive embedding layers,
there will be no chance to train some words.

e.g. If `cut_off` is (20000,60000) and vocab is larger than 60000,
we can't learn[**20,000+padding_idx**]th word and [**60,000+padding_idx**]th word.
Because those words' ids will be **padding_idx** by subtraction logic and eventually get zero tensors.

So, I changed `self.padding_idx` to `None` after assign vocab's `padding_idx`
**for the first time at head embedding representation**.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1629

Differential Revision: D19557340

Pulled By: myleott

fbshipit-source-id: e0c3b38862374d422a46dc62c248b2ecfbf08fd2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1093,Simone Francia,francia.simone1@gmail.com,2020-01-24 09:58:19-08:00,cce6dcb1cca85955a82879ea5064fe8202e8f412,https://github.com/pytorch/fairseq/commit/cce6dcb1cca85955a82879ea5064fe8202e8f412,"adding support for Italian Umberto ( umberto.commoncrawl and umberto.wikipedia ) from Musixmatch (#1008)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1008

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1639

Differential Revision: D19555377

Pulled By: myleott

fbshipit-source-id: 8ef2b6635a2c609f6ed7dd8ba403eba0787590d8",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1094,Elijah Rippeth,elijah.rippeth@gmail.com,2020-01-24 10:28:44-08:00,f1d856e006a0fcb2b22afaad2eb456a671204557,https://github.com/pytorch/fairseq/commit/f1d856e006a0fcb2b22afaad2eb456a671204557,"fix Windows build (#1007)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1007

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1622

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1631

Differential Revision: D19555401

Pulled By: myleott

fbshipit-source-id: c62dfc109e09a7d732a9fc73ac6feef63a8dd341",12,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1095,Mathias Müller,mathias.mueller@uzh.ch,2020-01-24 10:30:59-08:00,1da061f37f444e492f9cd9c7d058e35dcbf4d2c3,https://github.com/pytorch/fairseq/commit/1da061f37f444e492f9cd9c7d058e35dcbf4d2c3,"Append validation data instead of overwrite (#1642)

Summary:
Very minor fix to avoid overwriting validation data.

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1641.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1642

Differential Revision: D19555371

Pulled By: myleott

fbshipit-source-id: 2c2dd1d3c66605dd42113f2330ba98fe62c53a92",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1096,Joshua Meier,jmeier@fb.com,2020-01-24 13:14:46-08:00,9f4256edf60554afbcaadfa114525978c141f2bd,https://github.com/pytorch/fairseq/commit/9f4256edf60554afbcaadfa114525978c141f2bd,"Standalone LSTM decoder language model (#934)

Summary:
Currently, the LSTM models in Fairseq master can only be used in an encoder/decoder setting, for example, in `class LSTMModel(FairseqEncoderDecoderModel)`. This PR adds a standalone LSTM decoder language model.

Changes:
- adds support for `LSTMDecoder` in cases where an encoder is not present, for instance, where `encoder_output_units=0`.
- fixes bugs in `LSTMDecoder` that only become apparent when using it in a standalone fashion, for example, not handling `src_lengths` as an optional argument.
- adds `class LSTMLanguageModel(FairseqLanguageModel)` for training LSTM language models.
- tests for the `LSTMLanguageModel`. Changes to the `LSTMDecoder` are handled by existing test cases.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/934

Reviewed By: myleott

Differential Revision: D18816310

Pulled By: joshim5

fbshipit-source-id: 4773695a7f5d36aa773da8a45db2e02f76c968a9",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1097,Myle Ott,myleott@fb.com,2020-01-27 10:23:56-08:00,88185fcc3f32bd24f65875bd841166daa66ed301,https://github.com/pytorch/fairseq/commit/88185fcc3f32bd24f65875bd841166daa66ed301,"Cleanup new incremental state API (#1005)

Summary:
* Now that we have `FairseqIncrementalState`, we can move `get_incremental_state` and `set_incremental_state` as methods in that class, instead of having the helper functions in `utils.py`. I think this will eventually help with type checking too.
* The incremental ID logic was overly complicated, we can just use `uuid` to generate a unique ID for every instance.
* Add missing `with_incremental_state` to light/dynamic conv modules.
* Add additional unit test: `test_incremental_state_multihead_attention`

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1005

Test Plan:
* unit tests

Also confirmed this matches master:
```
$ python generate.py ~/data/data-bin/wmt16_en_de_bpe32k --path /checkpoint/myleott/s3/models/wmt16.en-de.joined-dict.transformer/model.pt --beam 4 --lenpen 0.6 --remove-bpe --quiet
(...)
2020-01-22 09:53:38 | INFO | fairseq_cli.generate | Generate test with beam=4: BLEU4 = 29.28, 60.8/35.1/22.8/15.3 (BP=0.997, ratio=0.997, syslen=62859, reflen=63078)
```

Reviewed By: cndn

Differential Revision: D19517908

Pulled By: myleott

fbshipit-source-id: a406490e342d0d30a9231bf823d3350999bda4c0",7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(v1, 1)'), ('Equal', '(v2, 2)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1098,Myle Ott,myleott@fb.com,2020-01-27 15:55:29-08:00,c2671c1720bf86d1e793832bd9417f9e857820a0,https://github.com/pytorch/fairseq/commit/c2671c1720bf86d1e793832bd9417f9e857820a0,"Fix hub model loading

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1012

Differential Revision: D19587452

Pulled By: myleott

fbshipit-source-id: f97e8140224059388093251cb38ba14c4a41af31",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1099,Myle Ott,myleott@fb.com,2020-01-28 08:01:08-08:00,61aad8f9cda61b833dc7ed2b4e45e68236aae943,https://github.com/pytorch/fairseq/commit/61aad8f9cda61b833dc7ed2b4e45e68236aae943,"Force certain optimizers to set --fp16-no-flatten-grads (#1010)

Summary:
When training with `--fp16` we usually flatten the grads since it's faster. But flat grads are not semantically equivalent for certain optimizers (e.g., Adafactor, LAMB), thus the user needed to be aware of this and set `--fp16-no-flatten-grads`. Let's raise a RuntimeError in this case instead.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1010

Differential Revision: D19575773

Pulled By: myleott

fbshipit-source-id: bac99c3026f9870e6127e0fa55f70e8a3e4507dc",13,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestOptimizers(unittest.TestCase):'],"[('Raises', '(RuntimeError):')]",[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],[],['class TestCommonOptions(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1100,Art Matsak,amatsak@gmail.com,2020-01-28 16:21:33-08:00,ad86e4f7abd0cec0043b19ca1b42c3dcc7f7b117,https://github.com/pytorch/fairseq/commit/ad86e4f7abd0cec0043b19ca1b42c3dcc7f7b117,"Fix BART CNN/DM fine-tuning instructions (#1650)

Summary:
The first step in the CNN/DM fine-tuning instructions for BART is misleading (see https://github.com/pytorch/fairseq/issues/1391). This PR fixes the README and adds links to https://github.com/pytorch/fairseq/issues/1391 as well as to a repository with CNN/DM processing code adjusted for BART.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1650

Differential Revision: D19606689

fbshipit-source-id: 4f1771f47d3650035a911ab393ab6df2193c1bf9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1101,Yun Wang,yunwang@fb.com,2020-01-30 10:41:57-08:00,e3d512436aac01271c4e5a280967a3f26db2577b,https://github.com/pytorch/fairseq/commit/e3d512436aac01271c4e5a280967a3f26db2577b,"Fix ranking metrics

Summary: This diff makes TALNet able to compute MAP and MAUC and report them in FBLearner, using the new API in D19292402.

Reviewed By: myleott

Differential Revision: D19633993

fbshipit-source-id: 0eda3d4ccc36f1db2dbcc2daae561071acba7f14",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1102,Ning Dong,dnn@fb.com,2020-01-30 15:18:58-08:00,63ae358e3712612c48f18c1578bc34e372db38a5,https://github.com/pytorch/fairseq/commit/63ae358e3712612c48f18c1578bc34e372db38a5,"Formatting fairseq transformer

Summary: Just formatting so review could be easier in the diff above.

Reviewed By: jhcross

Differential Revision: D19382299

fbshipit-source-id: 12ddaff824842c680018e8d4edf148e0328a29d7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1103,Ning Dong,dnn@fb.com,2020-01-30 15:57:27-08:00,a07cb6f40480928c9e0548b737aadd36ee66ac76,https://github.com/pytorch/fairseq/commit/a07cb6f40480928c9e0548b737aadd36ee66ac76,"Script Fairseq transformer (#1011)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1011

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1620

Make Fairseq transformer scriptable. Discussion points on possible code refactoring:

(1) Original decoder output is a tuple (x, {""attn"": attn, ""inner_states"": inner_states}). TorchScript does not support dictionary with values of different types (attn: Tensor, inner_states: List[Tensor]). Current workaround is to use [attn] for attention field and access via output[""attn""][0] in downstream. This is currently used in fairspeq custom transformer code. Another (maybe) cleaner alternative is to use namedtuple for decoder output but involves tons of downstream changes too.

(2) Currently TorchScript doesn't support **kwargs. Some unused arguments might get passed in due to polymorphism. Now the only workaround I can think of is to add possible unused arguments, (e.g. line 666 in transformer.py)

Reviewed By: myleott

Differential Revision: D19234599

fbshipit-source-id: db3dd364ecf3ae14fb7ac8c0928bd0ebe250f19d",9,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestExportModels(unittest.TestCase):'],[],[],[],[],[],[''],[],[],[],[],['class TestExportModels(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1104,Ning Dong,dnn@fb.com,2020-01-30 16:13:15-08:00,3df10a9529e2d91dc9d4c1068d401b4935938b58,https://github.com/pytorch/fairseq/commit/3df10a9529e2d91dc9d4c1068d401b4935938b58,"Add save and load tests to fairseq export test (#1653)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1653

Earlier we had some issues at pickling. Type information gets lost. Fixed in https://github.com/pytorch/pytorch/pull/32569.

These save_and_load tests are added for protection in the future.

Reviewed By: myleott

Differential Revision: D19435988

fbshipit-source-id: 560ea65ed3493bebcf394327818364b3fcd6fc92",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1105,Gil Keren,gilkeren@fb.com,2020-01-30 17:45:22-08:00,d7250a1c466808b672a429c2b2e6daa5534141f1,https://github.com/pytorch/fairseq/commit/d7250a1c466808b672a429c2b2e6daa5534141f1,"Adding Conv2d, BatchNorm2d, Residual Block to pyspeech.nn

Summary:
Adding some of the main building blocks for constructing convolutional architectures. Note that:
- TDS block is planned to be added soon.
- the layer compute the `lengths` correctly. Currently there is no support for state and streamable architectures. This is planned and will be added later on, once we think about deploying such models.

Differential Revision: D19548528

fbshipit-source-id: 679e51d818f6af21b5b687eb6a996900b81a1465",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1106,Oleksiy Syvokon,oleksiy.syvokon@gmail.com,2020-01-31 08:51:33-08:00,d09f9e959cdcad765c18aa09632bf9cf5e4fb386,https://github.com/pytorch/fairseq/commit/d09f9e959cdcad765c18aa09632bf9cf5e4fb386,"Fix logging format string: consume all arguments (#1658)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes `fairseq-interactive` failure when `--buffer-size` is greater than 1.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1658

Differential Revision: D19660937

Pulled By: myleott

fbshipit-source-id: 6bb5065325e83754d607c9f27b71a3c4ef5cf1e9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1107,Myle Ott,myleott@fb.com,2020-01-31 09:43:55-08:00,810e61ab3ffba169a47d7e72538e082d2467f2b4,https://github.com/pytorch/fairseq/commit/810e61ab3ffba169a47d7e72538e082d2467f2b4,"Set proper device for MHA padding mask (#1013)

Summary:
Fixes GPU training
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1013

Differential Revision: D19663284

Pulled By: myleott

fbshipit-source-id: ed9ec40e5abba2e4b1df5718f25376c4df5de1fb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1108,James Cross,jcross@fb.com,2020-02-03 15:41:05-08:00,8185722b656a3c5eb7eb97ef92f213b14a32d95d,https://github.com/pytorch/fairseq/commit/8185722b656a3c5eb7eb97ef92f213b14a32d95d,"auto-format binarize.py (black + isort) (#1671)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1671

Formatting changes for uniformity and to make subsequent changes more clear.

Reviewed By: cndn

Differential Revision: D19701981

fbshipit-source-id: a40021c2361bef3e30dd58882322dbf9a9cf976a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1109,James Cross,jcross@fb.com,2020-02-03 23:28:03-08:00,c8dee69678c17cf4de240d4bf3c2b84e7cab4897,https://github.com/pytorch/fairseq/commit/c8dee69678c17cf4de240d4bf3c2b84e7cab4897,"already_numberized option for binarizer (#1673)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1673

Provide an option to binarize numberized text files, where the lines consist of space-separated decimal string representations of the underlying dictionary indices.

Faceboook:

Production training currently works by producing files of this type using the VocabProcessor mechanism, and this approach seemed the least disruptive (though another possibility would be to subclass `Binarizer` in an `fb_` file).

Reviewed By: cndn

Differential Revision: D19704131

fbshipit-source-id: c334a1f6fb681c1d4456f39e645021dbd9777d0a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1110,Tomas Soucek,soucek.gns@gmail.com,2020-02-05 09:22:33-08:00,5955fe281a327fccc1c8504220254d600536c2c7,https://github.com/pytorch/fairseq/commit/5955fe281a327fccc1c8504220254d600536c2c7,"label dataset creation in SentencePredictionTask fixed (#1602)

Summary:
**Fixes issue when finetuning with `SentencePredictionTask`**

Labels (instance of `FairseqDataset` in `SentencePredictionTask().datasets[split][""target""]`) would be encoded from `source_dictionary` instead of `_label_dictionary`.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1602

Differential Revision: D19330027

fbshipit-source-id: 048da7a3698eb152fe5867cd5babe50b857a73ab",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1111,Yuqing Tang,yuqtang@fb.com,2020-02-05 13:06:10-08:00,d1c7eae1f5e0ed739dfe1acaa20ee675f613d6f5,https://github.com/pytorch/fairseq/commit/d1c7eae1f5e0ed739dfe1acaa20ee675f613d6f5,"Fix fp16 bug for MHA padding mask

Summary: The lastest fix to the device of MHA padding mask did  not set the dtype property for --fp16. Here is a fix.

Reviewed By: myleott

Differential Revision: D19739599

fbshipit-source-id: ead76cb39dd50ca1770fa5e208d2adad518164b7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1112,Myle Ott,myleott@fb.com,2020-02-07 18:22:07-08:00,42935dcc8c2d390547d14c29cc71f3f3f379bcfb,https://github.com/pytorch/fairseq/commit/42935dcc8c2d390547d14c29cc71f3f3f379bcfb,"Fix attn_mask bug in TransformerEncoderLayer (only affects speech_recognition example) (#1018)

Summary:
Fixes #1339.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1018

Differential Revision: D19796152

Pulled By: myleott

fbshipit-source-id: e2304a549c52804d2b5c4c9b31f8d852827ae943",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1113,Myle Ott,myleott@fb.com,2020-02-10 09:18:10-08:00,f1329312704132e9f57a63008018b9654213dd08,https://github.com/pytorch/fairseq/commit/f1329312704132e9f57a63008018b9654213dd08,"Deprecate dummy_batch and oom_batch from Trainer (#1020)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1020

These aren't used anywhere.

Differential Revision: D19599476

fbshipit-source-id: 65fad4440e3407fad8b11e8c886de749d6abebca",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1114,Myle Ott,myleott@fb.com,2020-02-11 09:19:46-08:00,acaf760b500e54db30bce95b7094b39f7fb417bc,https://github.com/pytorch/fairseq/commit/acaf760b500e54db30bce95b7094b39f7fb417bc,"Only start measuring wps/ups after 10 steps

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1023

Reviewed By: ngoyal2707

Differential Revision: D19834656

Pulled By: myleott

fbshipit-source-id: d4b701f8404630185253e3e944e76b80fb787f2c",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1115,Myle Ott,myleott@fb.com,2020-02-11 10:27:20-08:00,d48a05188899b1a8dcf05711d69b4303d1a6b80f,https://github.com/pytorch/fairseq/commit/d48a05188899b1a8dcf05711d69b4303d1a6b80f,"Support overriding length in CountingIterator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1022

Reviewed By: spencerp

Differential Revision: D19834652

Pulled By: myleott

fbshipit-source-id: 75ad313362448eba4b743bf1967c23473076a062",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1116,Chen Liu,chenliu8@fb.com,2020-02-11 15:04:27-08:00,d25f4bfe0ebff90f3568c3eac7b74898739050b6,https://github.com/pytorch/fairseq/commit/d25f4bfe0ebff90f3568c3eac7b74898739050b6,"Formatting for sequence generator

Summary: This diff is only for formatting

Differential Revision: D19837678

fbshipit-source-id: 5c5076b4a29b31c21437aa90fb613699956ca0aa",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1117,Haoran Li,aimeeli@fb.com,2020-02-11 17:52:08-08:00,17be1dd0f809c610d436cd5dc2017280c0f83c5b,https://github.com/pytorch/fairseq/commit/17be1dd0f809c610d436cd5dc2017280c0f83c5b,"BART in decoupled model

Summary: Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1246

Reviewed By: einolghozati

Differential Revision: D19757100

fbshipit-source-id: 8ba58acf65decf05d7273b1686af1153916bc614",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1118,Yun Wang,yunwang@fb.com,2020-02-11 18:11:55-08:00,c5b32cb2bde59a7bb7987b22864731fe927523d4,https://github.com/pytorch/fairseq/commit/c5b32cb2bde59a7bb7987b22864731fe927523d4,"TALNet: Use both static and dynamic teachers

Summary:
This diff allows using both static and dynamic teachers for TALNet teacher-student training.

A static teacher means its predictions are pre-computed, and stored in a file on Manifold.

A dynamic teacher means loading a model and generating prediction on the fly.

Both types of teachers and the ground truth can be mixed with specified weights.

Also, move `KnowledgeDistillationBinaryCrossEntropyCriterion` and its associated unit test into fb-only folders.

Differential Revision: D19837605

fbshipit-source-id: e25d47331732daba48b1357b7e234e0113611659",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1119,Myle Ott,myleott@fb.com,2020-02-12 05:31:15-08:00,d7b0bf7e55b48466ee7e371a959668dfb8feaaf6,https://github.com/pytorch/fairseq/commit/d7b0bf7e55b48466ee7e371a959668dfb8feaaf6,"Switch fast_stat_sync to use all_reduce_dict

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1024

Differential Revision: D19834661

Pulled By: myleott

fbshipit-source-id: dbac8d2e35e126cd5ebeb6508e55a7e3f44ab69f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1120,Myle Ott,myleott@fb.com,2020-02-12 07:04:52-08:00,4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f,https://github.com/pytorch/fairseq/commit/4cae68037d4ded8fdc6d9c13edbfbbcb21e39f7f,"Update criterions to not call item() in the forward (#1025)

Summary:
Calling `item` results in a device-to-host transfer which slows down training, especially on TPUs.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1025

Differential Revision: D19834666

Pulled By: myleott

fbshipit-source-id: bc8492834fcd9cf23afc1f72449f49ef9cfa377a",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1121,Myle Ott,myleott@fb.com,2020-02-12 14:03:00-08:00,91f05347906e80e6705c141d4c9eb7398969a709,https://github.com/pytorch/fairseq/commit/91f05347906e80e6705c141d4c9eb7398969a709,"Add dummy tasks and model for benchmarking (#1026)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1026

Differential Revision: D19834667

Pulled By: myleott

fbshipit-source-id: 56ab6df5d8145dc37431252de444a2a9728e7898",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1122,Peng-Jen Chen,pipibjc@devfair0209.h2.fair,2020-02-12 18:43:20-08:00,6a5181509aa1fa7d260985157e77211753da544b,https://github.com/pytorch/fairseq/commit/6a5181509aa1fa7d260985157e77211753da544b,"Make multilingual translation logging output summable (#1691)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # 1679

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1691

Differential Revision: D19861866

Pulled By: pipibjc

fbshipit-source-id: ca1607472432d8d4947074f9ab1b6f388ee46006",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1123,Chen Liu,chenliu8@fb.com,2020-02-14 10:49:11-08:00,fba10af9db5edd61f78ccdb9d115c4eafbcc561d,https://github.com/pytorch/fairseq/commit/fba10af9db5edd61f78ccdb9d115c4eafbcc561d,"Scriptable simple sequence generator (#1028)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1028

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1704

Fairseq JIT scriptable Simple Sequence Generator. This scriptable Sequence Generator is fully compatiable with fairseq transformerModel meeting LATTE team demand. In the following Diff, it will support ensemble model for Sequence Generator. The test have verified this implementation is able to load previous sequence generator test, and given out same results.

Reviewed By: myleott

Differential Revision: D19745260

fbshipit-source-id: ad7b39fd5253216aeedc20e5bd40e32192cc2a13",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1124,june-one,june.one@kakaobrain.com,2020-02-15 20:22:48-08:00,7d89d2e42995948aadf91922e51aaae7865ef0de,https://github.com/pytorch/fairseq/commit/7d89d2e42995948aadf91922e51aaae7865ef0de,"fetch pyproject.toml for building cython codes without pre-installation (#1697)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
  - https://github.com/pytorch/fairseq/issues/1060
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
  - I think it is not needed to update the docs
- [ ] Did you write any new necessary tests?
  - Cannot write test code for installing package, but I got test on my forked github repository and local system
    - ex) pip install git+https://github.com/AppleHolic/fairseq@fetch_build_toml

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1060

Hello, I got an error to install fairseq when I install another repository that has an depandancy on fairseq. There is no problem when the cython is already installed, but I wanna solve this problem with [PEP](https://www.python.org/dev/peps/pep-0518/).

This PR gives build-ability cython extension on setup time without pre-installation of cython. And I tested on two cases (install from github, install from local directory).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �!
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1697

Differential Revision: D19926025

Pulled By: myleott

fbshipit-source-id: 8d5252feb87adf42bfdbdaa77475ebbccb898d66",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1125,Weiyi Zheng,wyz@fb.com,2020-02-18 20:05:41-08:00,4923f34790761f41170fd88cd06e4d00ab0c527c,https://github.com/pytorch/fairseq/commit/4923f34790761f41170fd88cd06e4d00ab0c527c,"tts synthesis script

Summary: add a synth.py to pyspeech to run tts synthesis for a particular text.

Differential Revision: D19786089

fbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1126,Myle Ott,myleott@fb.com,2020-02-21 07:05:18-08:00,08dcd08d9c442ec2c35bb041356d2b768ffcb922,https://github.com/pytorch/fairseq/commit/08dcd08d9c442ec2c35bb041356d2b768ffcb922,"Add utils.with_torch_seed (#1044)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1044

Differential Revision: D20030263

Pulled By: myleott

fbshipit-source-id: 374af53646acd6d138227b088e023e168a7ed647",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1127,Myle Ott,myleott@fb.com,2020-02-21 08:29:57-08:00,ac9dab9dfd85d0314f4a12827fbdb16d3d84599e,https://github.com/pytorch/fairseq/commit/ac9dab9dfd85d0314f4a12827fbdb16d3d84599e,"Add huggingface ByteLevelBPETokenizer to fairseq.data.encoders (#1045)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1045

Differential Revision: D20030415

Pulled By: myleott

fbshipit-source-id: fd5949b5c1a30a133a3b641d5989780f0228b60b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1128,Myle Ott,myleott@fb.com,2020-02-21 10:06:38-08:00,38a8d2a4f4f499386555eabd1800ad124d4df181,https://github.com/pytorch/fairseq/commit/38a8d2a4f4f499386555eabd1800ad124d4df181,"Move Highway into character_token_embedder, since it's only used there (#1041)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1041

Differential Revision: D20030277

Pulled By: myleott

fbshipit-source-id: bd5ffd18c434f097ac9c34f658b7620eb543ed7d",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1129,Myle Ott,myleott@fb.com,2020-02-21 11:43:11-08:00,12ab22e06c03878f72f7c30f4b7b24f40707b865,https://github.com/pytorch/fairseq/commit/12ab22e06c03878f72f7c30f4b7b24f40707b865,"Fix deprecation warnings in unit tests (#1043)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1043

Differential Revision: D20030274

Pulled By: myleott

fbshipit-source-id: 34962c1caaf3879af8b527a266852e443b59ffe4",7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1130,Myle Ott,myleott@fb.com,2020-02-21 12:53:49-08:00,f2e86ed896defebe4247fbcb8e6730fb00e89195,https://github.com/pytorch/fairseq/commit/f2e86ed896defebe4247fbcb8e6730fb00e89195,"Fix interaction between tracing and FusedLayerNorm (#1038)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1038

Differential Revision: D20030410

Pulled By: myleott

fbshipit-source-id: c64a2ddfdd379a93359e87fd1a1226c7f19f1158",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1131,Myle Ott,myleott@fb.com,2020-02-21 14:09:35-08:00,e1de989964c83fff6e045afe735ef5c333699824,https://github.com/pytorch/fairseq/commit/e1de989964c83fff6e045afe735ef5c333699824,"Add missing __init__ to fairseq.benchmark (#1042)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1042

Differential Revision: D20030269

Pulled By: myleott

fbshipit-source-id: ebe64eb4e58af447ed53b9f5b5adf2e5f65da359",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1132,Myle Ott,myleott@fb.com,2020-02-21 14:09:48-08:00,8845dcf5ff43ca4d3e733ade62ceca52f1f1d634,https://github.com/pytorch/fairseq/commit/8845dcf5ff43ca4d3e733ade62ceca52f1f1d634,"Move MoE files into examples (#1040)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1040

Differential Revision: D20030279

Pulled By: myleott

fbshipit-source-id: 76b48a62409020039225cf98e8fcf7a494d0b7f8",11,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1133,Chen Liu,chenliu8@fb.com,2020-02-21 14:13:39-08:00,537c2ba7026d2a9814a027e63c58fd33b587879b,https://github.com/pytorch/fairseq/commit/537c2ba7026d2a9814a027e63c58fd33b587879b,"Autoformatting the search.py

Summary:
Autoformat the `search.py` in fairseq

(Note: this ignores all push blocking failures!)

Differential Revision: D20038581

fbshipit-source-id: f2256e6c56948bcee8c029013c23acfd0419252b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1134,Ning Dong,dnn@fb.com,2020-02-21 14:31:50-08:00,3ba384cc6c58a139f0ccfbc4e7f183e7c4dfd839,https://github.com/pytorch/fairseq/commit/3ba384cc6c58a139f0ccfbc4e7f183e7c4dfd839,"split_paths to support manifold path (#1721)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1721

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1031

Manifold path (of format manifold://...) contains "":"" which was used as delimiter in fairseq. Add support for splitting manifold paths with ""|"" as delimiter, as we currently do in pytorch_translate.

Reviewed By: theweiho, myleott

Differential Revision: D19963376

fbshipit-source-id: c1ee5216e8d1c1fcfa82ba62842abad4dc718841",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1135,Myle Ott,myleott@fb.com,2020-02-24 09:19:23-08:00,1eb977007829d34ab7b5a169479b9de45cca9bb8,https://github.com/pytorch/fairseq/commit/1eb977007829d34ab7b5a169479b9de45cca9bb8,"Fix logging when loss is in FP16 (#1047)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1047

Reviewed By: ngoyal2707

Differential Revision: D20068167

Pulled By: myleott

fbshipit-source-id: 46ccca05ed225103a1b2aa6fa80f8fbfe91b040b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1136,Chen Liu,chenliu8@fb.com,2020-02-24 10:03:59-08:00,43a3e7f4874510761cf174cc65f3d42edcc46f30,https://github.com/pytorch/fairseq/commit/43a3e7f4874510761cf174cc65f3d42edcc46f30,"scriptable diverse beam search strategy (#1034)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1034

Scripting the `DiverseBeamSearch` in the search.py. Test with original unit test in the `test_sequence_generator.py.`, which gives out the same output as the previous one.

Reviewed By: myleott

Differential Revision: D19983928

fbshipit-source-id: 317cff53fc7c96d9ff999adb3b5bfc3a4954b0fc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1137,Chen Liu,chenliu8@fb.com,2020-02-24 10:03:59-08:00,43e4cf7c69f628008028e0ab16f5c895afd07514,https://github.com/pytorch/fairseq/commit/43e4cf7c69f628008028e0ab16f5c895afd07514,"scriptable sampling search strategy (#1035)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1035

Scripting the `Sampling` in the search.py. Test with original unit test in the test_sequence_generator.py., which gives out the same output as the previous one.

Reviewed By: myleott

Differential Revision: D19983975

fbshipit-source-id: 168208e782debe2a0b1c0143a566a7d303d77ee0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1138,Chen Liu,chenliu8@fb.com,2020-02-24 10:03:59-08:00,b3ca86a800729222e770937d6454f65ab5947ec5,https://github.com/pytorch/fairseq/commit/b3ca86a800729222e770937d6454f65ab5947ec5,"scriptable diverse siblings search strategy (#1032)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1032

Scripting the `DiverseSiblingsSearch` in the search.py

Reviewed By: myleott

Differential Revision: D19986538

fbshipit-source-id: 2813f85f42104588f059e54e581e991e2ac87ec7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1139,Myle Ott,myleott@fb.com,2020-02-24 13:53:33-08:00,ed4aa2cba6687fdd403d85a6b33efcc096879531,https://github.com/pytorch/fairseq/commit/ed4aa2cba6687fdd403d85a6b33efcc096879531,"Add exception when dummy batch is missing (fixes #1726) (#1735)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1735

Differential Revision: D20034557

Pulled By: myleott

fbshipit-source-id: a2ed5acd5e79b2cfd0b073bb8aabcb39172f7dc5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1140,Myle Ott,myleott@fb.com,2020-02-25 05:52:14-08:00,2728f9b06d9a3808cc7ebc2afa1401eddef35e35,https://github.com/pytorch/fairseq/commit/2728f9b06d9a3808cc7ebc2afa1401eddef35e35,"Add huggingface submodule and GPT2 model (#1019)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1019

Differential Revision: D20044785

Pulled By: myleott

fbshipit-source-id: 022a49f696c0093d577422af5598f6f326022569",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1141,Myle Ott,myleott@fb.com,2020-02-25 06:59:53-08:00,b152183d745d025514a4d4d03a3671cd85acbdcc,https://github.com/pytorch/fairseq/commit/b152183d745d025514a4d4d03a3671cd85acbdcc,"Add instructions to reproduce Understanding Back-translation at Scale (#1021)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1021

Differential Revision: D20077161

Pulled By: myleott

fbshipit-source-id: da7f38dbac9551f29a88be3f421f8e38d9a81133",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1142,Myle Ott,myleott@fb.com,2020-02-25 10:55:09-08:00,f1a9ce847d2f50d7bee0ee9337074a42ca79557b,https://github.com/pytorch/fairseq/commit/f1a9ce847d2f50d7bee0ee9337074a42ca79557b,"Fix dummy batch logic (#1744)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1744

Differential Revision: D20097876

Pulled By: myleott

fbshipit-source-id: 420cbfa53bd8fa3aa4710d16d0e2d1280977dfec",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1143,Myle Ott,myleott@fb.com,2020-02-25 11:23:59-08:00,d88b91b3d077115b9d933b2737150ee9d12c9d72,https://github.com/pytorch/fairseq/commit/d88b91b3d077115b9d933b2737150ee9d12c9d72,"Use global Tensorboard SummaryWriter objects to avoid writing too many files (#1745)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1745

Differential Revision: D20097902

Pulled By: myleott

fbshipit-source-id: cad90f0e074ac1e58a03846bb6fdc25703e6c8f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1144,Myle Ott,myleott@fb.com,2020-02-26 09:10:45-08:00,37bd90845ceb4b086f2ac436575fca60e1018d68,https://github.com/pytorch/fairseq/commit/37bd90845ceb4b086f2ac436575fca60e1018d68,"Update README (#1051)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1051

Differential Revision: D20119560

Pulled By: myleott

fbshipit-source-id: caf089341931990777393b916c846a332b76e9dc",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1145,Myle Ott,myleott@fb.com,2020-02-26 09:15:50-08:00,07eed27d9fa6f1feaf2d09ba25ff26a643e70b3a,https://github.com/pytorch/fairseq/commit/07eed27d9fa6f1feaf2d09ba25ff26a643e70b3a,"Fix GPU decoding (#1050)

Summary:
The earlier sequence generator diffs didn't always preserve devices, so would break on GPU. Unfortunately our unit tests don't run on GPU, so this wasn't detected.

I'm also removing all the `_init_buffers` stuff, since we don't reuse the buffers anymore.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1050

Test Plan: Reran unit tests on GPU

Differential Revision: D20117609

Pulled By: myleott

fbshipit-source-id: 1d47338f2194547c464b474ac215785ecf04841c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1146,Chen Liu,chenliu8@fb.com,2020-02-26 11:06:29-08:00,fdfdbec9e2f2eccfbdd331ae8d165d3ce05bcb73,https://github.com/pytorch/fairseq/commit/fdfdbec9e2f2eccfbdd331ae8d165d3ce05bcb73,"Rewrite the unit test of sequence generator

Summary:
1. Overwrite the base class function `get_normalized_probs` in scriptable TransformerModel
2. Change the unit test setup to match the Transformer decoder output format
3. Initialze the buffer in the simple sequence generator [WIP]
   1. It is the initial step to script the sequence generator from simple scriptable version.
4. Refactor the unit test of simple sequence generator.
5. Change the input format of simple sequence generator and unit test.

Reviewed By: myleott

Differential Revision: D20017859

fbshipit-source-id: a3e93b57c22e49840e460469fa2b1c530346886d",3,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1147,Myle Ott,myleott@fb.com,2020-02-26 20:38:30-08:00,cef5653251913ea3162340720e4c6ef6d912a6f8,https://github.com/pytorch/fairseq/commit/cef5653251913ea3162340720e4c6ef6d912a6f8,"Make FusedLayerNorm wrapper pickleable

Summary: See https://fb.workplace.com/groups/1405155842844877/permalink/3408783919148716/

Differential Revision: D20133239

fbshipit-source-id: 78ee12b7a573af7cfe9acb9fd2557cbbb78b0592",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1148,Myle Ott,myleott@fb.com,2020-02-27 08:19:48-08:00,f8b795f427a39c19a6b7245be240680617156948,https://github.com/pytorch/fairseq/commit/f8b795f427a39c19a6b7245be240680617156948,"Move meters, metrics and progress_bar into fairseq.logging (#1046)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1046

Differential Revision: D20030412

Pulled By: myleott

fbshipit-source-id: bd87391aa9cdb73306ee90a30eeb2bdeff3690f9",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1149,Yinhan Liu,yinhanliu@fb.com,2020-02-27 08:22:40-08:00,5e79322b3a4a9e9a11525377d3dda7ac520b921c,https://github.com/pytorch/fairseq/commit/5e79322b3a4a9e9a11525377d3dda7ac520b921c,"open source mbart (#1033)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1033

Differential Revision: D20122520

Pulled By: yinhanliu

fbshipit-source-id: e2fd93e2fa9b7a8e276acc4316a176ba3ceae4ed",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1150,alexeib,alexei.b@gmail.com,2020-02-27 22:59:21-08:00,08a1edd667fb44f7587d305cea41c85800d004e2,https://github.com/pytorch/fairseq/commit/08a1edd667fb44f7587d305cea41c85800d004e2,"add nan detection (#1052)

Summary:
this will print out first nan/inf in backward and forward pass after the minimum scaling value has been reached. example output:

2020-02-26 17:43:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.000244140625
2020-02-26 17:43:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0001220703125
2020-02-26 17:43:13 | WARNING | fairseq.nan_detector | NaN detected in output of encoder.layers.0.self_attn, shape: torch.Size([45, 88, 512]), forward
2020-02-26 17:43:13 | WARNING | fairseq.nan_detector | NaN detected in output of decoder, shape: torch.Size([88, 39, 6632]), backward
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1052

Reviewed By: edunov

Differential Revision: D20133500

Pulled By: alexeib

fbshipit-source-id: f327e1d635a4596f9303f69bbeafa1f3161ee842",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1151,alexeib,alexei.b@gmail.com,2020-02-29 18:22:11-08:00,3335de5f441ee1b3824e16dcd98db620e40beaba,https://github.com/pytorch/fairseq/commit/3335de5f441ee1b3824e16dcd98db620e40beaba,"add vq-wav2vec (#1029)

Summary:
sanitized vq-wav2vec implementation. i will also add docs to this. i have a fixed-up checkpoint that this code can load and verified that it produces same results as what we used in paper
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1029

Differential Revision: D20129246

Pulled By: alexeib

fbshipit-source-id: f72f455e0c309168e644ab86ec18c768c308da98",21,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1152,Kritika Singh,skritika@fb.com,2020-03-01 13:00:53-08:00,83c3000187bda92c367e2581af10fffaa860fe7b,https://github.com/pytorch/fairseq/commit/83c3000187bda92c367e2581af10fffaa860fe7b,"Allow averaging update based checkpoints given upper bound

Summary: This script currently supports averaging last-n updates, last-n epochs and n epochs before an upper-bound. While the code works for n updates before an upper bound, it was not exposed. This diff changes that.

Reviewed By: myleott

Differential Revision: D20007900

fbshipit-source-id: c865aaa98574b663c2e24ccc64aab519f031de3d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1153,Yinhan Liu,yinhanliu@fb.com,2020-03-02 17:12:13-08:00,c699eb054e2782e2ae4e1d85d1fff5a724dc5c0e,https://github.com/pytorch/fairseq/commit/c699eb054e2782e2ae4e1d85d1fff5a724dc5c0e,"add bart xsum readme (#1055)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (#1664, #1760, #1455).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1055

Differential Revision: D20197177

Pulled By: yinhanliu

fbshipit-source-id: 0df2d468a4015d98b6d1b7816bc832ea815c5e79",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1154,Myle Ott,myleott@fb.com,2020-03-03 14:46:26-08:00,244835d811c2c66b1de2c5e86532bac41b154c1a,https://github.com/pytorch/fairseq/commit/244835d811c2c66b1de2c5e86532bac41b154c1a,"Reset mid-epoch stats every log-interval steps (#1054)

Summary:
A few people have asked for this. Under this new setup, the mid-epoch metrics will average over the log interval, while the end-of-epoch metrics will contain the average over the whole epoch.

I confirmed that end-of-epoch train and valid metrics are unchanged.

![Screen Shot 2020-03-03 at 11 46 23 AM](https://user-images.githubusercontent.com/231798/75798498-a7a52a00-5d44-11ea-89d0-fd99dff67c9d.png)
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1054

Differential Revision: D20161250

Pulled By: myleott

fbshipit-source-id: 663fc17de952485ab7d36982c5c0cdd9d5715f14",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1155,Myle Ott,myleott@fb.com,2020-03-03 16:04:20-08:00,6a4ad3327ac31b54be93ec22887517c22b354698,https://github.com/pytorch/fairseq/commit/6a4ad3327ac31b54be93ec22887517c22b354698,"Update HF GPT2 (#1058)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1058

Differential Revision: D20222225

Pulled By: myleott

fbshipit-source-id: 37fd5b66b1dd5518086c156b61dc8e832b6f20d7",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1156,Myle Ott,myleott@fb.com,2020-03-03 19:55:31-08:00,077c351d7eb330d079f9b6b01ac1e35ba75ffebe,https://github.com/pytorch/fairseq/commit/077c351d7eb330d079f9b6b01ac1e35ba75ffebe,"Fix comments and logger name in validate.py (#1061)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1061

Differential Revision: D20238796

Pulled By: myleott

fbshipit-source-id: cf48bd7f6cdae05e91868a9d2efd91dc8e72bb12",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1157,Henry Dashwood,hcndashwood@gmail.com,2020-03-04 09:10:37-08:00,0ae32b794ed6013d11529203f8d308fcb16fb817,https://github.com/pytorch/fairseq/commit/0ae32b794ed6013d11529203f8d308fcb16fb817,"Update README.md (#1767)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes a link in a README.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1767

Reviewed By: ngoyal2707

Differential Revision: D20248559

Pulled By: myleott

fbshipit-source-id: 0becc73d8bca337286faa40ce100a94cd319fec9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1158,Joseph Spisak,spisakjo@gmail.com,2020-03-04 09:10:38-08:00,2b5c6e556314a717f8a4dafee07ab39c1aa220d5,https://github.com/pytorch/fairseq/commit/2b5c6e556314a717f8a4dafee07ab39c1aa220d5,"Update CODE_OF_CONDUCT.md (#1759)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1759

Differential Revision: D20248557

Pulled By: myleott

fbshipit-source-id: a6054421c4f7d1ce8f520bd1df3c8253e2576396",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1159,Myle Ott,myleott@fb.com,2020-03-04 09:55:32-08:00,dd1298e15fdbfc0c3639906eee9934968d63fc29,https://github.com/pytorch/fairseq/commit/dd1298e15fdbfc0c3639906eee9934968d63fc29,"Ignore duplicate entries in dict.txt (and allow explicit special symbs) (#1063)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1063

Differential Revision: D20248701

Pulled By: myleott

fbshipit-source-id: 5701539e8728816b2e17382788908fd189c5d0e1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1160,Myle Ott,myleott@fb.com,2020-03-04 11:44:59-08:00,cc7e48065d4986680d76c31b5665dfc99709fd18,https://github.com/pytorch/fairseq/commit/cc7e48065d4986680d76c31b5665dfc99709fd18,"Improve Adafactor docs (#1065)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1065

Reviewed By: stephenroller

Differential Revision: D20249786

Pulled By: myleott

fbshipit-source-id: 83a6bc23fa3593d01ca66ab6f0035c622b13b318",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1161,Myle Ott,myleott@fb.com,2020-03-04 11:45:10-08:00,4171b83cfd9486e2afea40724d259af398621d4b,https://github.com/pytorch/fairseq/commit/4171b83cfd9486e2afea40724d259af398621d4b,"Fix #1770 (#1773)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1773

Differential Revision: D20250814

Pulled By: myleott

fbshipit-source-id: 6143053f7260afe2e5c1b6ff17c728b14cfb7c9e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1162,Myle Ott,myleott@fb.com,2020-03-04 16:34:53-08:00,aa79bb9c37b27e3f84e7a4e182175d3b50a79041,https://github.com/pytorch/fairseq/commit/aa79bb9c37b27e3f84e7a4e182175d3b50a79041,"Use 1-based indexing for epochs everywhere (#1053)

Summary:
We are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053

Reviewed By: spencerp

Differential Revision: D20160715

Pulled By: myleott

fbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627",26,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1163,Elijah Rippeth,elijah.rippeth@gmail.com,2020-03-04 16:41:15-08:00,46b773a393c423f653887c382e4d55e69627454d,https://github.com/pytorch/fairseq/commit/46b773a393c423f653887c382e4d55e69627454d,"refactor namespaces in criterion interface (#1729)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1672 in part (part 1: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1729

Differential Revision: D20049353

Pulled By: myleott

fbshipit-source-id: 732077a1cc339c9f7ebe26dae42a7e8d7b5a07b4",15,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1164,Chen Liu,chenliu8@fb.com,2020-03-04 19:47:20-08:00,cb2dc414c692d7de283bec4e4f9c923a66205792,https://github.com/pytorch/fairseq/commit/cb2dc414c692d7de283bec4e4f9c923a66205792,"Enable reorder_incremental_state in sequence generator (#1774)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1774

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1067

Overwrite the reorder_incremental_state function in TransformerModel to temporally unblock the scriptable sequence generator. Since the `self` is not supported in the jit script, it is fundimentally hard to refact the base  `reorder_incremental_state`.

Reviewed By: myleott

Differential Revision: D20150320

fbshipit-source-id: 950ef48536098a9dbf10fe11c59116a4a35856a0",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1165,Zoe Ma,mazhiyi@fb.com,2020-03-05 05:51:50-08:00,adb5b9c71f7ef4fe2f258e0da102d819ab9920ef,https://github.com/pytorch/fairseq/commit/adb5b9c71f7ef4fe2f258e0da102d819ab9920ef,"Fix overflow nandetector (#1064)

Summary:
https://github.com/fairinternal/fairseq-py/issues/1029 added update_num as a required argument to train_step function. Adding this argument to the function call for nan detector.

Before I got:
```
Traceback (most recent call last):
  File ""/private/home/mazhiyi/.conda/envs/fairseq-master/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
    fn(i, *args)
  File ""/private/home/mazhiyi/master/fairseq-py-docrepair/fairseq_cli/train.py"", line 281, in distributed_main
    main(args, init_distributed=True)
  File ""/private/home/mazhiyi/master/fairseq-py-docrepair/fairseq_cli/train.py"", line 100, in main
    train(args, trainer, task, epoch_itr)
  File ""/private/home/mazhiyi/.conda/envs/fairseq-master/lib/python3.6/contextlib.py"", line 52, in inner
    return func(*args, **kwds)
  File ""/private/home/mazhiyi/master/fairseq-py-docrepair/fairseq_cli/train.py"", line 176, in train
    log_output = trainer.train_step(samples)
  File ""/private/home/mazhiyi/.conda/envs/fairseq-master/lib/python3.6/contextlib.py"", line 52, in inner
    return func(*args, **kwds)
  File ""/private/home/mazhiyi/master/fairseq-py-docrepair/fairseq/trainer.py"", line 405, in train_step
    ignore_grad=False
TypeError: train_step() missing 1 required positional argument: 'update_num'
```
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1064

Differential Revision: D20249234

Pulled By: mazhiyi

fbshipit-source-id: 0ee85c719a17cc253cbdc7183dc4243193b08d17",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1166,Myle Ott,myleott@fb.com,2020-03-07 09:09:19-08:00,86916e00a4e946afc844bc79fb2ed6be940b2910,https://github.com/pytorch/fairseq/commit/86916e00a4e946afc844bc79fb2ed6be940b2910,"Fix epoch reporting when restoring checkpoint (#1075)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075

Differential Revision: D20322672

Pulled By: myleott

fbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1167,Myle Ott,myleott@fb.com,2020-03-07 09:10:08-08:00,1f04f81554a22522d2b942a37f80466a899bf6a5,https://github.com/pytorch/fairseq/commit/1f04f81554a22522d2b942a37f80466a899bf6a5,"Fix logging error when tensorboard isn't installed (fixes #1794) (#1076)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1076

Differential Revision: D20322669

Pulled By: myleott

fbshipit-source-id: 84e89ef817171f58c0aade06b7b0181fe483240d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1168,Stefan Schweter,stefan.schweter@bsb-muenchen.de,2020-03-07 10:01:47-08:00,3dd221c90fe6badc3a8f187d88cf6cd1a5f3959e,https://github.com/pytorch/fairseq/commit/3dd221c90fe6badc3a8f187d88cf6cd1a5f3959e,"Readme: Fix link to mBART documentation (#1789)

Summary:
Hi,

this PR updates the link to mBART documentation in main readme.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1789

Differential Revision: D20322673

Pulled By: myleott

fbshipit-source-id: b59c94f49176ba5bbd664791818b5b8ce7402698",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1169,Myle Ott,myleott@fb.com,2020-03-08 06:49:07-07:00,937535dba036dc3759a5334ab5b8110febbe8e6e,https://github.com/pytorch/fairseq/commit/937535dba036dc3759a5334ab5b8110febbe8e6e,"Allow dictionaries to overwrite entries with #fairseq:overwrite comment (#1073)

Summary:
[This commit](https://github.com/pytorch/fairseq/commit/dd1298e15fdbfc0c3639906eee9934968d63fc29) made it so that duplicate entries in a dictionary are ignored. Unfortunately the Camembert model depends on overwriting `<unk>`, `<s>` and `</s>`.

The proposed solution here is to allow the dictionary to have entries like:
```
<unk> 999 #fairseq:overwrite
<s> 999 #fairseq:overwrite
</s> 999 #fairseq:overwrite
, 999
▁de 999
. 999
(...)
```

These will preserve the old overwriting behavior. Thus we can release a new `camembert.v0.tar.gz` with a dictionary like above and it works.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1073

Reviewed By: kahne

Differential Revision: D20284569

Pulled By: myleott

fbshipit-source-id: bf78fbff13c94bf8a6485cbdda62305ddc30c056",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(d.index(), 1)'), ('Equal', '(d.index(), 3)'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)'), ('Equal', '(d.index(), 7)'), ('Equal', '(d.index(), 8)'), ('RaisesRegex', '(RuntimeError, ):'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1170,Katsuya Iida,katsuya@alyc.ai,2020-03-08 12:26:54-07:00,e5facc9984bcfe3de8486db2639cd3c228d66dfa,https://github.com/pytorch/fairseq/commit/e5facc9984bcfe3de8486db2639cd3c228d66dfa,"Error preprocessing multi-lang translation: prepare-iwslt17-multilingual.sh #1777 (#1784)

Summary:
# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [X] Did you make sure to update the docs?
- [X] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1777.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1784

Differential Revision: D20322705

Pulled By: myleott

fbshipit-source-id: 0787225db7f94da0565a2aa7628f2a1ee22f777f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1171,Elijah Rippeth,elijah.rippeth@gmail.com,2020-03-09 06:00:08-07:00,9e1fb47035e87df5d1b7fd86dc152b666dca9217,https://github.com/pytorch/fairseq/commit/9e1fb47035e87df5d1b7fd86dc152b666dca9217,"fix #1802. (#1803)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1802

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1803

Differential Revision: D20330857

Pulled By: myleott

fbshipit-source-id: d98255b1d6b481595c9941831bb61727cad1a8ea",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1172,freewym,freewym@gmail.com,2020-03-09 14:43:57-07:00,a091ae5f3b58564f4aa05b83d26155add486335a,https://github.com/pytorch/fairseq/commit/a091ae5f3b58564f4aa05b83d26155add486335a,"fix epoch check for curriculum learning (#1798)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- Did you write any new necessary tests?

## What does this PR do?
Fixes epoch check for curriculum learning. `epoch_itr.epoch` has not been incremented before calling `next_epoch_itr()`. So the check for curriculum learning should look like the proposed fix.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1798

Differential Revision: D20330784

Pulled By: myleott

fbshipit-source-id: 2d78c70ce84a7ef054a2f700184e71ac1011d165",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1173,Xilun Chen,xilun@fb.com,2020-03-09 16:07:32-07:00,e88749d051c2a406342817e5c16b399b8089ef6e,https://github.com/pytorch/fairseq/commit/e88749d051c2a406342817e5c16b399b8089ef6e,"Fix a crash in Early Stopping when --validate-interval is greater than 1 (#1809)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1809

When --validate-interval is set to greater than 1, the validation loss is set to None for those epochs where no validation was done.

This caused a crash in Early Stopping if the ""--patience"" option was set to a positive value. The reason is that `valid_loss` is compared with `prev_best`, resulting a crash when None is compared to a float:
```
TypeError: '>' not supported between instances of 'NoneType' and 'float'
```

A simple check is added to the should_stop_early() method which skips checking if no validation is done in the current epoch.

Reviewed By: myleott

Differential Revision: D20318486

fbshipit-source-id: 3c2978b87a33241092d7e5fd8cd790cc35227ce4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1174,Myle Ott,myleott@fb.com,2020-03-10 09:01:46-07:00,ef5b71e8b307186b833a97f1894e89f14ccbfeb5,https://github.com/pytorch/fairseq/commit/ef5b71e8b307186b833a97f1894e89f14ccbfeb5,"Make mid-epoch logging more consistent and fix wps logging (#1077)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1077

Differential Revision: D20342179

Pulled By: myleott

fbshipit-source-id: 0e15e3766c3c72b29b57bfd33cfa7be60eba69be",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1175,Marco Gaido,mgaido@fbk.eu,2020-03-10 11:48:30-07:00,431d604f696a15c06fceab56b4ace271bb85e74b,https://github.com/pytorch/fairseq/commit/431d604f696a15c06fceab56b4ace271bb85e74b,"Fix generation with encoder which return an output of shape different from the input (#1792)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1791.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1792

Reviewed By: jmp84

Differential Revision: D20322704

Pulled By: myleott

fbshipit-source-id: 3cfa1bddda06b966e9dc9bc8ff183009d844b23c",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],['hypos[sent][beam][] is not None'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1176,Myle Ott,myleott@fb.com,2020-03-10 12:08:48-07:00,448df3cde688af8d2f8fdcaf5ca5d0b036df5362,https://github.com/pytorch/fairseq/commit/448df3cde688af8d2f8fdcaf5ca5d0b036df5362,"Remove redundant padding_value from LSTMEncoder (fixes #1807) (#1816)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1816

Differential Revision: D20365924

Pulled By: myleott

fbshipit-source-id: e0349004d53f2d5e42eb7f3bf98db093f8c27fbf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1177,Mandeep Baines,msb@fb.com,2020-03-10 14:43:59-07:00,54dacd356d39f13f7d8152de414bb3a2a49e8bb4,https://github.com/pytorch/fairseq/commit/54dacd356d39f13f7d8152de414bb3a2a49e8bb4,"Remove unnecesary copy in optimizers when parameters are fp32 (#1080)

Summary:
In the optimizer we cast weights to fp32, compute the new weights,
and then sync back the weights. If the weights are already fp32, the
cast is a no-op and the copy is unnecessary. While training a very
large model with fp32, I was seeing 1GB DeviceToDevice copies.

Fixed by avoiding the copy back when not needed. Opened
an issue[1] in pyTorch for _copy.

[1] https://github.com/pytorch/pytorch/issues/34525

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1080

Differential Revision: D20369231

Pulled By: myleott

fbshipit-source-id: 2d24e5a667aba80e4db36c50d26c40ebeeeeac73",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1178,Yun Wang,yunwang@fb.com,2020-03-11 00:29:19-07:00,d5618e9ecd05a1edaba5c0991efb3503f121d132,https://github.com/pytorch/fairseq/commit/d5618e9ecd05a1edaba5c0991efb3503f121d132,"PySpeech TALNet: Convert to JIT and quantize

Summary:
Update the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.

The updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.

Also renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.

Reviewed By: jay-mahadeokar

Differential Revision: D20369643

fbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1179,Myle Ott,myleott@fb.com,2020-03-11 05:53:36-07:00,5028ed1b6bedd526dee27ea731284f43e87303f0,https://github.com/pytorch/fairseq/commit/5028ed1b6bedd526dee27ea731284f43e87303f0,"Reduce device-to-host transfers (#1082)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1082

Differential Revision: D20365765

Pulled By: myleott

fbshipit-source-id: 7b6c14303b46b42db1a1e279c84dbe9cb2cf72f2",10,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('True', '(torch.is_tensor(grad_norm))'), ('Equal', '(grad_norm, 0.0)'), ('True', '(torch.is_tensor(grad_norm))'), ('Equal', '(grad_norm, exp_grad_norm)'), ('AlmostEqual', '(grad_norm, torch.tensor(1.0))')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1180,alexeib,alexei.b@gmail.com,2020-03-11 09:27:01-07:00,d935a8fd1999e73551b9e22f9d74894058bed368,https://github.com/pytorch/fairseq/commit/d935a8fd1999e73551b9e22f9d74894058bed368,"fix parameter count for vq-wav2vec (#1085)

Summary:
Fixes #1771
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1085

Differential Revision: D20380105

Pulled By: alexeib

fbshipit-source-id: 72fe7cb9ba5dc48a1ebdd9b1928522aa56276c06",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1181,Naman Goyal,namangoyal@learnfair0891.h2.fair,2020-03-11 19:24:59-07:00,43cf9c977b8470ec493cc32d248fdcd9a984a9f6,https://github.com/pytorch/fairseq/commit/43cf9c977b8470ec493cc32d248fdcd9a984a9f6,"add xsum model to torch hub (#1824)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1824

Reviewed By: myleott

Differential Revision: D20395405

fbshipit-source-id: b3b75ddfc897fbea3bfe9bc445cf2785448d33a3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1182,Mandeep Baines,msb@fb.com,2020-03-12 17:42:25-07:00,b65a85b692544e36f9e83ada91cf4ef529791c69,https://github.com/pytorch/fairseq/commit/b65a85b692544e36f9e83ada91cf4ef529791c69,"multi-gpu fix for fused-adam (#1084)

Summary:
The cuda kernel used by fused-adam was using the default stream
on the default device. The kernel needs use the same device as
the parameter tensor.

Fixed by using a context manager to set correct default device.

With this change my model no longer crashes during training and
I get results nearly the same as old-adam:

old-adam:

2020-03-10 18:18:49 | INFO | train | epoch 001 | loss 20.65 | ppl 1.64498e+06 | wps 0 | ups 2.03 | wpb None | bsz 3 | num_updates 10 | lr 0.01 | gnorm 2.224 | clip 0 | oom 0 | train_wall 6 | wall 6

fused-adam:

2020-03-10 18:20:25 | INFO | train | epoch 001 | loss 20.649 | ppl 1.6448e+06 | wps 0 | ups 1.99 | wpb None | bsz 3 | num_updates 10 | lr 0.01 | gnorm 2.224 | clip 0 | oom 0 | train_wall 6 | wall 7

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1084

Reviewed By: ngoyal2707

Differential Revision: D20419455

Pulled By: msbaines

fbshipit-source-id: bf98126308f41dffb66cea7ff6715d2648a68732",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1183,Naman Goyal,namangoyal@learnfair1277.h2.fair,2020-03-13 20:58:09-07:00,6234e29fe275c885e935e8ba25d93b0d4799fe7b,https://github.com/pytorch/fairseq/commit/6234e29fe275c885e935e8ba25d93b0d4799fe7b,"adding megatron as submodule (#1087)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1087

Reviewed By: myleott

Differential Revision: D20443036

fbshipit-source-id: 834240ad227ce344700625847a69c855c8af7f7f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1184,Myle Ott,myleott@fb.com,2020-03-15 10:34:17-07:00,476606fc1631b7c87873d3e314e61dae8f25a751,https://github.com/pytorch/fairseq/commit/476606fc1631b7c87873d3e314e61dae8f25a751,"Make epochs use 1-based indexing to match D20160715

Summary:
D20160715 made fairseq use 1-based indexing for epochs. This updates pyspeech
to do the same.

Differential Revision: D20393469

fbshipit-source-id: fdeece03793a3a15d9f918033af7530bb071fd4b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1185,Myle Ott,myleott@fb.com,2020-03-16 07:36:40-07:00,4f8b0643c80d6a41039ae29e94fca6b44de8791a,https://github.com/pytorch/fairseq/commit/4f8b0643c80d6a41039ae29e94fca6b44de8791a,"Fix boundary condition in `--patience` (#1845)

Summary:
When `--patience=1`, we should stop as soon as the validation loss decreases.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1845

Differential Revision: D20468392

Pulled By: myleott

fbshipit-source-id: 5ee7741b5086536c48d7b804c7b571749b464f3f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1186,Myle Ott,myleott@fb.com,2020-03-16 07:39:14-07:00,11cc356395b96f54c5497a751a4c3c94dcbdbf87,https://github.com/pytorch/fairseq/commit/11cc356395b96f54c5497a751a4c3c94dcbdbf87,"fairseq requires PyTorch >= 1.4 (#1844)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/1843
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1844

Differential Revision: D20468391

Pulled By: myleott

fbshipit-source-id: 0b2e2ba35c94eeb49d0e6bb05a8fefa4b847f46d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1187,Naman Goyal,namangoyal@learnfair0697.h2.fair,2020-03-16 12:47:52-07:00,bb1750d0442a617b08d7d727c049ab8c8e5cf460,https://github.com/pytorch/fairseq/commit/bb1750d0442a617b08d7d727c049ab8c8e5cf460,"small fixes for mbart finetuning for wmt filtering shared task (#1090)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1090

Differential Revision: D20474341

fbshipit-source-id: dd940f6eb2108b8d6cbd7bb79c1697016d8febcd",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1188,Ning Dong,dnn@fb.com,2020-03-16 19:16:13-07:00,73536de6869bbacc71b1460d561766bbbc697d81,https://github.com/pytorch/fairseq/commit/73536de6869bbacc71b1460d561766bbbc697d81,"Fairseq evaluate integration (#1823)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1823

Create a separate workflow for fairseq native evaluation with a trained checkpoint. This is developed from fairseq/evaluate with simplification, refactoring and manifold migration. If fairseq folks are interested we could unify with their implementation.

preprocess_and_eval() takes in raw test data, lowercase, preprocess and get lc bleu score using fairseq generate.

Since test data is still on gluster and yoda related operators haven't been migrated to Manifold, we are still having sporadic gluster usage across the file.

Integration to base training, right after ensemble_and_sweep will be in next diff.

Fairseq side changes only include ones to support manifold path. cc myleott

Reviewed By: jhcross

Differential Revision: D20194073

fbshipit-source-id: 8dee4cdf9c756daa9f915c1dcad9b580f73031f1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1189,Vlad Lyalin,Guitaricet@gmail.com,2020-03-17 09:09:47-07:00,e13eeab04beaad3c0c3fcbb3eab5b49589a0fc5a,https://github.com/pytorch/fairseq/commit/e13eeab04beaad3c0c3fcbb3eab5b49589a0fc5a,"upd wikitext-103 url (#1847)

Summary:
Very minor update, just changing the url. The old one is 404.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1847

Reviewed By: ngoyal2707

Differential Revision: D20490970

Pulled By: myleott

fbshipit-source-id: 9c2fbe97e3dd450094a95db9f934945e87637ae1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1190,David Příhoda,david.prihoda1@merck.com,2020-03-17 10:05:31-07:00,d865218d52ba8984c66c7af2c4d54e44cfb26615,https://github.com/pytorch/fairseq/commit/d865218d52ba8984c66c7af2c4d54e44cfb26615,"Set perplexity to inf in log stats in case of overflow error (#1834)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1833

Example output after this update:
```
2020-03-12 15:39:26 | WARNING | root | NaN or Inf found in input tensor.
2020-03-12 15:39:26 | INFO | train | epoch 001 | loss 69041.9 | nll_loss 6820 | wps 12712 | ups 6.53 | wpb 1640 | bsz 162 | num_updates 2 | lr 1e-05 | gnorm 2603.12 | loss_scale 16 | train_wall 1 | ppl inf | wall 1
/SFS/user/wp/prihodad/git/fairseq/fairseq/utils.py:351: RuntimeWarning: overflow encountered in power
  return safe_round(np.power(base, loss), round)
2020-03-12 15:39:26 | WARNING | root | NaN or Inf found in input tensor.
2020-03-12 15:39:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 71135.5 | nll_loss 7034 | wps 0 | wpb 1699 | bsz 168 | ppl inf | num_updates 2

```
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1834

Reviewed By: ngoyal2707

Differential Revision: D20490980

Pulled By: myleott

fbshipit-source-id: 320822f9b50185a33fb0346a84b935934b710c98",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1191,Naman Goyal,namangoyal@learnfair0891.h2.fair,2020-03-17 17:45:11-07:00,3822db33002f18f64cb9eb7b154ee5c76de87d5b,https://github.com/pytorch/fairseq/commit/3822db33002f18f64cb9eb7b154ee5c76de87d5b,"adding model parallel multihead attention module (#1088)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1088

Reviewed By: myleott

Differential Revision: D20456534

fbshipit-source-id: e48afe41df210be26e0d5c1628c24cf7f9e81d4b",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1192,Naman Goyal,namangoyal@learnfair0697.h2.fair,2020-03-20 10:06:50-07:00,73e1f6835d8fb490fc0f25292d667860b9a36a9d,https://github.com/pytorch/fairseq/commit/73e1f6835d8fb490fc0f25292d667860b9a36a9d,"added model parallel criterion and modules (#1092)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1092

Reviewed By: myleott

Differential Revision: D20495799

fbshipit-source-id: f96ba2ee663c9cfa943a2dc6a3608e000203b597",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1193,Yu Yan,yuyan2do@gmail.com,2020-03-20 13:18:38-07:00,a84cb7866db1ea9107021ace21b0a9ee6f5670e3,https://github.com/pytorch/fairseq/commit/a84cb7866db1ea9107021ace21b0a9ee6f5670e3,"Beam search perf improve (#1852)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1851 .

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1852

Reviewed By: ngoyal2707

Differential Revision: D20490964

Pulled By: myleott

fbshipit-source-id: 22f6c849408029f5432e531589da29d95e31d392",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1194,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-03-20 16:39:43-07:00,3d536ae36012c7a827f6d57b6b7cf0e59c1d5120,https://github.com/pytorch/fairseq/commit/3d536ae36012c7a827f6d57b6b7cf0e59c1d5120,"adding model parallel model (#1099)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1099

Reviewed By: myleott

Differential Revision: D20565503

fbshipit-source-id: 59049979bc8f58f2808365c75aaf008ab35f887a",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1195,Myle Ott,myleott@fb.com,2020-03-21 11:07:54-07:00,e28d15b33aa4434b04d8dd16427b802d6035ea22,https://github.com/pytorch/fairseq/commit/e28d15b33aa4434b04d8dd16427b802d6035ea22,"Fix some criterion code after recent API change (fixes #1866) (#1874)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1874

Reviewed By: ngoyal2707

Differential Revision: D20565473

Pulled By: myleott

fbshipit-source-id: 25edef9f41f28a41f1c573dcc3da48b674b678e4",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1196,Myle Ott,myleott@fb.com,2020-03-21 11:09:21-07:00,aed8cf4c38db088c2f640fa941a7d2c462d72b16,https://github.com/pytorch/fairseq/commit/aed8cf4c38db088c2f640fa941a7d2c462d72b16,"Move tensor data to CPU in all_gather_list (fixes #1826) (#1875)

Summary:
This should fix https://github.com/pytorch/fairseq/issues/1826.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1875

Reviewed By: ngoyal2707

Differential Revision: D20568241

Pulled By: myleott

fbshipit-source-id: ea8cce60524c2fcee855b37dac046dc30da913c7",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1197,Myle Ott,myleott@fb.com,2020-03-21 11:09:58-07:00,bee6d71646c22eb552ec7d78d439729b38dfa55b,https://github.com/pytorch/fairseq/commit/bee6d71646c22eb552ec7d78d439729b38dfa55b,"Fix MHA and LayerNorm on devices other than cuda:0 (fixes #1860) (#1873)

Summary:
This also includes a fix required for LayerNorm due to a bug in apex: https://github.com/NVIDIA/apex/issues/770
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1873

Reviewed By: ngoyal2707

Differential Revision: D20564542

Pulled By: myleott

fbshipit-source-id: f1603779351671a5a44f606f550281bcdd2aa9b7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1198,David Příhoda,david.prihoda1@merck.com,2020-03-21 16:52:53-07:00,42f65d65776327598a2d3ded2e92e5818c70a125,https://github.com/pytorch/fairseq/commit/42f65d65776327598a2d3ded2e92e5818c70a125,"Support multiple regression targets in sentence prediction (#1831)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1830
Adds tests for RoBERTa (masked_lm, classification, single regression, multiple regression)
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1831

Reviewed By: ngoyal2707

Differential Revision: D20446010

Pulled By: myleott

fbshipit-source-id: 9f37bcedf0910d85446245d71bc234bc74c62da5",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1199,James Cross,jcross@fb.com,2020-03-23 16:06:30-07:00,fd76cb5b41f5d434ddf9d351cb3a26ba123a179c,https://github.com/pytorch/fairseq/commit/fd76cb5b41f5d434ddf9d351cb3a26ba123a179c,"TestEncoder to return type EncoderOut (#1894)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1894

Having a uniform return type for `FairseqEncoder` makes these test models function more similarly to real models.

Reviewed By: myleott, cndn

Differential Revision: D20596971

fbshipit-source-id: a744614c015af9b150f2b0ae8381b1368556f738",1,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1200,Myle Ott,myleott@fb.com,2020-03-24 06:01:43-07:00,c0e3704433f3b6e8ae3d441b28e2f0f8887d9053,https://github.com/pytorch/fairseq/commit/c0e3704433f3b6e8ae3d441b28e2f0f8887d9053,"Fix all_gather_list for arbitrary inputs (#1105)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1105

Reviewed By: jay-mahadeokar

Differential Revision: D20612351

Pulled By: myleott

fbshipit-source-id: 280ea415afb20e879bb5ed810501bc2d3131f9b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1201,Naman Goyal,namangoyal@learnfair0829.h2.fair,2020-03-24 06:24:04-07:00,8b9733641aee820f38c66ef44d1d75d6b6ea19e8,https://github.com/pytorch/fairseq/commit/8b9733641aee820f38c66ef44d1d75d6b6ea19e8,"adding option to ignore lang id token for mbart (#1104)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1104

Reviewed By: myleott

Differential Revision: D20604984

fbshipit-source-id: 7add3a38a53c3c1168c52a786588868571cbf3b6",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1202,Changhan Wang,changhan@fb.com,2020-03-24 07:56:15-07:00,fdac9bbce13895c149f5c67544b03dc0e6a61b42,https://github.com/pytorch/fairseq/commit/fdac9bbce13895c149f5c67544b03dc0e6a61b42,"Byte-Level BPE paper code

Summary:
Implemented byte-level BPE described in [""Neural Machine Translation with Byte-Level Subwords""](https://arxiv.org/abs/1909.03341)
* Added bytes/characters/byte-level BPE tokenizers to fairseq.data.encoder
* Added detokenization option to generate.py
* Added an example under examples/byte_level_bpe
* Implemented Transformer model with Bi-GRU embedding contextualization: `examples/byte_level_bpe/gru_transformer.py`

Reviewed By: myleott

Differential Revision: D20600963

fbshipit-source-id: 3eca4d046056c07f65333123416017a4eac04c8a",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1203,Myle Ott,myleott@fb.com,2020-03-24 11:31:46-07:00,01e5ab5730e9a520c02ec3856901f700d471f929,https://github.com/pytorch/fairseq/commit/01e5ab5730e9a520c02ec3856901f700d471f929,"Handle corner case in utils.clip_grad_norm_ (#1889)

Summary:
This comes up during OOM recovery.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1889

Reviewed By: ngoyal2707

Differential Revision: D20596501

Pulled By: myleott

fbshipit-source-id: 0de365e99448ff0cc7aaba3e2bd9d2570948805e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1204,Myle Ott,myleott@fb.com,2020-03-24 11:33:44-07:00,f353913420b6ef8a31ecc55d2ec0c988178698e0,https://github.com/pytorch/fairseq/commit/f353913420b6ef8a31ecc55d2ec0c988178698e0,"Print tokenized hypothesis in fairseq-interactive (fixes #1881) (#1888)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1888

Reviewed By: ngoyal2707

Differential Revision: D20596497

Pulled By: myleott

fbshipit-source-id: 689a7b8d352aac87e080773676f55a3bd89f555f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1205,Myle Ott,myleott@fb.com,2020-03-24 11:37:48-07:00,e5b978aa629fb4013c01581bc8299913ae95998f,https://github.com/pytorch/fairseq/commit/e5b978aa629fb4013c01581bc8299913ae95998f,"Remove unused bias_kv/zero_attn options from TransformerSentenceEncoder (#1893)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1893

Reviewed By: jingfeidu

Differential Revision: D20599356

Pulled By: myleott

fbshipit-source-id: 2f1013d2fb53bde792d7cb4ae47be8c46a2f2dd5",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1206,Myle Ott,myleott@fb.com,2020-03-24 11:43:21-07:00,3f9303b53bb2d1011971981c4e2390941ab8ad13,https://github.com/pytorch/fairseq/commit/3f9303b53bb2d1011971981c4e2390941ab8ad13,"Fix dummy batch logic in trainer (#1890)

Summary:
We want to wait until `sample_size` is a float before applying dummy batch logic.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1890

Reviewed By: ngoyal2707

Differential Revision: D20596505

Pulled By: myleott

fbshipit-source-id: 11610344f794c9abe3c68e4f8fb4dbf16b58782c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1207,danthe3rd,danthe3rd@users.noreply.github.com,2020-03-24 14:19:14-07:00,02adce37040247cb7ebc5a7448996eddb99c57fe,https://github.com/pytorch/fairseq/commit/02adce37040247cb7ebc5a7448996eddb99c57fe,"Add a timeout for requests (#1869)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
I've seen trainings being frozen in `requests.head` for ever. Discussed with ngoyal2707

About timeouts in the doc of the `requests` module: ""Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely""
https://requests.readthedocs.io/en/master/user/quickstart/#timeouts
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1869

Reviewed By: myleott, ngoyal2707

Differential Revision: D20536626

Pulled By: danthe3rd

fbshipit-source-id: cc325e4ad4c5de85bec96a364103e5e5a74689b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1208,Angela Fan,angela.h.fan@gmail.com,2020-03-24 14:22:08-07:00,538a226a9bd7cd2465edbe416a50d3fbc7fb246f,https://github.com/pytorch/fairseq/commit/538a226a9bd7cd2465edbe416a50d3fbc7fb246f,"updating layerdrop readme (#1902)

Summary:
added links to pretrained models and how to evaluate them, added a couple clarifying points to readme based on questions received
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1902

Reviewed By: louismartin

Differential Revision: D20624919

Pulled By: huihuifan

fbshipit-source-id: a5a8993a70c20c44b0ab3ba752570ab6ada6965e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1209,Myle Ott,myleott@fb.com,2020-03-24 16:06:45-07:00,8377ddca4905150a398067667df3f76d1f5d68b5,https://github.com/pytorch/fairseq/commit/8377ddca4905150a398067667df3f76d1f5d68b5,"Fix FusedLayerNorm with CPU inputs (#1106)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1106

Reviewed By: hudeven

Differential Revision: D20627297

Pulled By: myleott

fbshipit-source-id: 9beff9b13dbc8e47fb53198fc0c4bf7469c9c740",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1210,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-03-25 07:11:02-07:00,84b7686438a055812b856a7c177843b1e8ee1777,https://github.com/pytorch/fairseq/commit/84b7686438a055812b856a7c177843b1e8ee1777,"adding optimizer changes and model parallel option (#1101)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1101

Reviewed By: myleott

Differential Revision: D20578782

fbshipit-source-id: c9618ec78c3a79d3864e076eeb6708eb84894b1d",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1211,freewym,freewym@gmail.com,2020-03-26 07:51:20-07:00,3e2bbb5f4205744d5be00191e180b5edfe196a90,https://github.com/pytorch/fairseq/commit/3e2bbb5f4205744d5be00191e180b5edfe196a90,"remove duplicated code from data.dictionary (#1914)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1914

Reviewed By: ngoyal2707

Differential Revision: D20669790

Pulled By: myleott

fbshipit-source-id: 2bf65407c0ee67788df7cc90eb6c2f3cf411e432",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1212,Myle Ott,myleott@fb.com,2020-03-26 07:53:50-07:00,f2ae57908bacff105bde22dad8fb6f6881251a9d,https://github.com/pytorch/fairseq/commit/f2ae57908bacff105bde22dad8fb6f6881251a9d,"Fix tests (#1110)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1110

Reviewed By: ngoyal2707

Differential Revision: D20649232

Pulled By: myleott

fbshipit-source-id: 55bc18284ac792012aaa794d5102c877ff781f8c",6,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1213,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-03-26 11:41:24-07:00,f0882f2c8ce80981ea2b0af6ec2ecc545f4db119,https://github.com/pytorch/fairseq/commit/f0882f2c8ce80981ea2b0af6ec2ecc545f4db119,"fixed bmuf clip grad norm (#1112)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1112

Reviewed By: myleott

Differential Revision: D20670017

fbshipit-source-id: e16b55b8386b11f90c792e46a1bdfeaff77e8e66",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1214,Haoran Li,aimeeli@fb.com,2020-03-26 12:50:45-07:00,3efae0746efb50e6dd4ada71d52e796b527de0ad,https://github.com/pytorch/fairseq/commit/3efae0746efb50e6dd4ada71d52e796b527de0ad,"changes to make mbart work (#1911)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1911

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1109

Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1289

First remove the assertion since when I load the model for fine-tuning I don't necessarily have all the language directories on my data.
I tried several ideas here for general bart archs, however none of them worked well for mbart on multilingual data, but haven't experimented with bart on en data:
1. initializing ontology embeddings as average pooling of all subword embeddings.
2. initializing pointer attention mha with some layer from bart decoder

Reviewed By: jeanm

Differential Revision: D20530432

fbshipit-source-id: 40b8735888c28c05f6f326980cc6db820699db5c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1215,Louis MARTIN,louisrtm@gmail.com,2020-03-27 08:58:40-07:00,b5dad3b7e02d66dc98d4707bc8aeacf95618ccd7,https://github.com/pytorch/fairseq/commit/b5dad3b7e02d66dc98d4707bc8aeacf95618ccd7,"Refactor validation and save logic in one method (#1096)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Related to issue: https://github.com/fairinternal/fairseq-py/issues/1089

I found it hard to read the save and evaluation logic before (and it was probably hard to maintain as well).
This PR would also fixes some inconsistencies.
The same behaviour is applied for validation during epoch as for validation as the end of epoch. This is useful for patience for instance, the early stopping happens in terms of number of validations (and can therefore happen during one epoch) instead of in number of full epochs, which can be problematic when epochs are very long.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1096

Reviewed By: myleott

Differential Revision: D20557186

Pulled By: louismartin

fbshipit-source-id: 54129c357a5c5cd4b5c26b7dd29f23cc2c6c40d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1216,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-03-27 15:20:45-07:00,f3680fd80426d384f5b3764953e90d7c4b0968f8,https://github.com/pytorch/fairseq/commit/f3680fd80426d384f5b3764953e90d7c4b0968f8,"adding eval lm changes for model parallel (#1113)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1113

Reviewed By: myleott

Differential Revision: D20670665

fbshipit-source-id: 8e2846637195b7200f1f60a8421d2fe5ffab789b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1217,Naman Goyal,namangoyal@learnfair0708.h2.fair,2020-03-27 20:21:01-07:00,d37fdee3da996e1533eb4fe4b68a3683c8ce987b,https://github.com/pytorch/fairseq/commit/d37fdee3da996e1533eb4fe4b68a3683c8ce987b,"adding code to load and save model parallel checkpoint (#1119)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119

Reviewed By: myleott

Differential Revision: D20712488

fbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1218,Weiyi Zheng,wyz@fb.com,2020-03-30 13:37:27-07:00,4d2efae84dc4eaf8c66625ab6af0fdc4aa794da0,https://github.com/pytorch/fairseq/commit/4d2efae84dc4eaf8c66625ab6af0fdc4aa794da0,"support manifold in average_checkpoint.py

Summary: use PathManager to support averaging checkpoints.

Reviewed By: myleott

Differential Revision: D20725346

fbshipit-source-id: 44b91f8652826da72c82087f8fbab7ae7d179423",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1219,Myle Ott,myleott@fb.com,2020-03-31 08:58:22-07:00,5065077dfc1ec4da5246a6103858641bfe3c39eb,https://github.com/pytorch/fairseq/commit/5065077dfc1ec4da5246a6103858641bfe3c39eb,"Use cross entropy from apex for improved memory efficiency (#1122)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1122

Reviewed By: ngoyal2707

Differential Revision: D20745717

Pulled By: myleott

fbshipit-source-id: 877a1185f17952461ef204d8ad7f05b8d37b1fd9",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1220,Changhan Wang,changhan@fb.com,2020-03-31 09:49:49-07:00,13c540e9f9c57bd7b83c052201395ddae73b47ac,https://github.com/pytorch/fairseq/commit/13c540e9f9c57bd7b83c052201395ddae73b47ac,"fix VGGTransformer

Summary: A recent change in `fairseq.modules.transformer_layer.TransformerDecoderLayer.forward()` return type broke L659. Specifically, the return type is now either a tuple of 2 or a tuple of 3, while L659 supposed only the latter. This diff provides a fix to this.

Reviewed By: myleott

Differential Revision: D20747601

fbshipit-source-id: 966040569f07bf41325f517e6c6a41675fc3a52d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1221,Myle Ott,myleott@fb.com,2020-04-01 09:38:02-07:00,0e608fdba6cd27bb2aa917e369a0f49a2c55cb1e,https://github.com/pytorch/fairseq/commit/0e608fdba6cd27bb2aa917e369a0f49a2c55cb1e,"Make utils.move_to_cpu also convert float16 -> float32 (#1123)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1123

Reviewed By: ngoyal2707

Differential Revision: D20745668

Pulled By: myleott

fbshipit-source-id: 00a6f8a9daf57d0bfedd4d7ba58889b9bbc2ff66",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1222,Anchit Gupta,anchit@fb.com,2020-04-01 17:51:32-07:00,f6f092f48941bbe5a0118a03be1fb38a58cea97c,https://github.com/pytorch/fairseq/commit/f6f092f48941bbe5a0118a03be1fb38a58cea97c,"Make TransformerDecoupled model scriptable (#1125)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1125

Pull Request resolved: https://github.com/pytorch/translate/pull/695

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1927

- Switches the model to the scripted sequence generator recently implemented in fairseq. Involved making the input/ouput format of this model to conform to that in Fairseq TransformerEncoder/Decoder
- Modify the `EncoderOut` format for fairseq transformer and added optional fields needed for copy ptr decoder
- Switches to using WordEmbedding directly instead of the non scriptable EmbeddingList for src/trg embedding layer
- Small assorted syntactic changes to make it jit scriptable
- Adds a torchscriptify method for this model. Preliminarily latency seems similar to the unexported model. Also verified that the outputs match
- Currently the Roberta decoupled model is not scriptable because the base TransformerSentenceEncoder it is based on is not scriptable. We can look at adding that later

Reviewed By: einolghozati

Differential Revision: D20687247

fbshipit-source-id: 8232972bba2f1b2df4100f3c1776b6bad08a71db",6,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1223,Alex Xiao,axiao@fb.com,2020-04-02 12:54:17-07:00,f20dc238e486d31c11c9447c597182a31cdb50d0,https://github.com/pytorch/fairseq/commit/f20dc238e486d31c11c9447c597182a31cdb50d0,"fix pyspeech jobs hanging at last epoch

Summary:
Fixes problem from https://fb.workplace.com/groups/332923290658088/permalink/571010136849401/.

In general, we seem to sometimes hang at the last epoch because one of the ranks exits early (if the rank is rank 0 then the job will likely fail). This  happens mostly for aligned training because number of batches vary the most there between different ranks.  I've debugged this problem to be:

1. Before D20160715, this hanging would occur if the job fails at the last epoch, restarts, and there is inconsistency between batches for each rank. This is because when we restart at epoch Y update X, if there exists a rank equal to or less than X batches,  then fairseq will detect that rank as end of epoch and move on to epoch Y + 1. If epoch Y + 1 > max_epochs, then this rank exits. Ranks with batches > X will detect not end of epoch and try to sync, causing hanging. This situation is quite rare as we need to fail on final epoch, if you restart before final epoch epoch syncing will guarantee that all ranks are in sync.

2. After D20160715 and D20393469, this hanging would occur very often as epoch syncing no longer works because `next_epoch_itr ` no longer increments the epoch if you just loaded from a checkpoint. This means if you fail and restart at any point you might face the hanging problem above.

The fix for 1 is to explicitly sync the epochs **before** we check if training is done.
The fix for 2 is to increment epoch explicitly when syncing epochs.

Differential Revision: D20746950

fbshipit-source-id: bfce8e78619949667526a3b877a288f5f5cf2dda",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1224,Chen Liu,chenliu8@fb.com,2020-04-02 21:17:45-07:00,cd2555a429b5f17bc47260ac1aa61068d9a43db8,https://github.com/pytorch/fairseq/commit/cd2555a429b5f17bc47260ac1aa61068d9a43db8,"`build_generator` api changes for the scripted SequenceGenerator (#697)

Summary:
Pull Request resolved: https://github.com/pytorch/translate/pull/697

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1922

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1117

We are planning to deprecate the original SequenceGenerator and use the ScriptSequenceGenerator in the Fairseq. Due to the change of scripted Sequence Generator constructor, I change `build_generator` interface in Fairseq, pyspeech and pytorch translate.

Reviewed By: myleott

Differential Revision: D20683836

fbshipit-source-id: d01d891ebd067fe44291d3a0a784935edaf66acd",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1225,Benjamin Bolte,bbolte@fb.com,2020-04-03 10:23:08-07:00,94f73d836c95fda2100093607148b8281de26cfa,https://github.com/pytorch/fairseq/commit/94f73d836c95fda2100093607148b8281de26cfa,"Update LegacyFairseqCriterion to FairseqCriterion

Summary: Updating loss functions from the speech recognition example to `FairseqCriterion` from `LegacyFairseqCriterion`

Reviewed By: myleott

Differential Revision: D20810977

fbshipit-source-id: 922ca8fc107c7b01645d036f761cb276c12d6dc9",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1226,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-04-03 12:01:09-07:00,78a995db2fc0436d08aa98faa0d63785b718340f,https://github.com/pytorch/fairseq/commit/78a995db2fc0436d08aa98faa0d63785b718340f,"adding readme and releasing megatron big model (#1124)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1124

Reviewed By: myleott

Differential Revision: D20749898

fbshipit-source-id: 42bca96d8d65158ae858ceaa7386afedf1696ebb",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1227,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-04-03 14:38:21-07:00,b5a6cef4fe95bbc7128d0130dc8dd9853dfe8fb4,https://github.com/pytorch/fairseq/commit/b5a6cef4fe95bbc7128d0130dc8dd9853dfe8fb4,"fix readme megatron (#1130)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1130

Reviewed By: myleott

Differential Revision: D20846933

fbshipit-source-id: 7f8a0360d9a4b21252bd00a00b563582ec64321b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1228,Louis MARTIN,louisrtm@gmail.com,2020-04-03 16:36:45-07:00,18831f9f8353e7b7902f4d9a651463f50f40ce3f,https://github.com/pytorch/fairseq/commit/18831f9f8353e7b7902f4d9a651463f50f40ce3f,"Fix validation happening twice at the end of epoch (#1934)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes validation happening twice at the end of epoch after refactor. Spotted by freewym
 here: https://github.com/pytorch/fairseq/commit/b5dad3b7e02d66dc98d4707bc8aeacf95618ccd7#r38103577

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1934

Reviewed By: myleott

Differential Revision: D20724205

Pulled By: louismartin

fbshipit-source-id: 8c26c39b9904508780e8542813797c8e1306ca80",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1229,Myle Ott,myleott@fb.com,2020-04-04 11:26:27-07:00,7c0ab23d14882d77ae5017ee71085925c5c03373,https://github.com/pytorch/fairseq/commit/7c0ab23d14882d77ae5017ee71085925c5c03373,"Fix LM generation when input doesn't end with eos (#1131)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1131

Reviewed By: hadasah

Differential Revision: D20850313

Pulled By: myleott

fbshipit-source-id: bf15daf3545e928d23e72d7b9c78d303a1910e88",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1230,Myle Ott,myleott@fb.com,2020-04-05 11:50:10-07:00,e9014fb4245d8408ff763a4f67b8a2365c8b6f3e,https://github.com/pytorch/fairseq/commit/e9014fb4245d8408ff763a4f67b8a2365c8b6f3e,"Fix typing issue in meters when resuming FP16 training (#1132)

Summary:
When we save checkpoints, we move all CUDA tensors to CPU. This includes meter values (e.g., grad norm). Upon reloading the checkpoint, these meter values remain on the CPU, but subsequent meter values are likely to be on GPU, thus raising an exception (PyTorch doesn't support operations between CPU and CUDA tensors). In the case of FP16 training, you get a slightly different exception due to trying to add float16 tensors on CPU, but it's the same underlying cause.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1132

Reviewed By: ngoyal2707

Differential Revision: D20850925

Pulled By: myleott

fbshipit-source-id: df12b051f2eae3566a1f4cd1b621ed1c8fdf0050",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1231,Chen Liu,chenliu8@fb.com,2020-04-06 17:45:48-07:00,bc936813485a3af8c3b46de470a2b46e0dc46f83,https://github.com/pytorch/fairseq/commit/bc936813485a3af8c3b46de470a2b46e0dc46f83,"Deprecate the SequenceGenerator with the Scripted vision (#1120)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1120

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1940

Deprecate the SequenceGenerator in Fairseq with the Scripted vision.

Pass all integration unit tests

- Copy ScriptSequenceGenerator to SequenceGenerator:
  - Modified the forward_decoder to fix bug when using adaptive_softmax in `get_prob_normalize` (marked with the inline comment)
   - Add support for other EnsembleModels as input arg (marked with the inline comment)
 - Add `FBEnsembleModelWithFork` to support folk/join in ensemblemodel
   - Add `test_fb_ensemble_model` to test folk/join feature
   - Still have bugs in folk/join feature when running in the Fairseq interface (like generation and interactive). Need further investigation P128130029. cc cndn, jhcross
- Modified SequenceGenerator initialization the interface
- Clear up the codes: delete unused functions `get_normalized_probs` and `_decode`

Reviewed By: myleott

Differential Revision: D20685075

fbshipit-source-id: 046b76874465a70d8118a97ad670311c6ce1d1c8",8,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],3,15,2,0,0,0,2,0,0,0,0,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestJitSequenceGeneratorBase(unittest.TestCase):', 'class TestExportSearch(unittest.TestCase):', 'class TestSequenceGeneratorBase(unittest.TestCase):']","[('TensorSizeEqual', '(hypo[], pos_scores)'), ('TensorSizeEqual', '(pos_scores.numel(), hypo[].numel())'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('TensorEqual', '(h1[])'), ('AlmostEqual', '(h1[])'), ('Less', '(abs(h1[]), 1e-6)'), ('AlmostEqual', '(h1[])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]","['def setUp(self):', 'def setUp(self):']",[],[],[],"['', '']",[],[],[],[],['class TestSequenceGeneratorBase(unittest.TestCase):'],"[('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1232,Chen Liu,chenliu8@fb.com,2020-04-06 20:38:18-07:00,d369c8801913a0f88cfcc9b78db3410d24285c37,https://github.com/pytorch/fairseq/commit/d369c8801913a0f88cfcc9b78db3410d24285c37,"Script reoder_incremental_state in fairseq baseline model (#1127)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1127

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1953

Script the `reorder_incremental_states` in the base FairseqModel
Remove the overwrite scriptable `reorder_incremental_states` in the TransformerModel
Change the decoder_len, since len(Tuple) is supported in Script

Reviewed By: myleott

Differential Revision: D20797390

fbshipit-source-id: ab29874973adc5dbd556c591942a0e071c81fc52",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1233,Aapo Kyrola,akyrola@fb.com,2020-04-07 00:57:53-07:00,8a528888e42283f813ba0d46ed5a608549215396,https://github.com/pytorch/fairseq/commit/8a528888e42283f813ba0d46ed5a608549215396,"Revert D20797390: Script reoder_incremental_state in fairseq baseline model

Differential Revision:
D20797390

Original commit changeset: ab29874973ad

fbshipit-source-id: efd2d720c96ee90d1e8dc36178e04f0bf5510278",5,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1234,Aapo Kyrola,akyrola@fb.com,2020-04-07 00:57:53-07:00,966436403e5e927e3e7d5b389dad6ef06aaa7e03,https://github.com/pytorch/fairseq/commit/966436403e5e927e3e7d5b389dad6ef06aaa7e03,"Revert D20685075: Deprecate the SequenceGenerator with the Scripted vision

Differential Revision:
D20685075

Original commit changeset: 046b76874465

fbshipit-source-id: 7ec2a2ca3b90251a560e2323c22b52ec7436fecb",8,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,4,0,0,0,0,0,0,0,0,0,2,15,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]",[],[],[],[],[],[],[],[],[],"['class TestJitSequenceGeneratorBase(unittest.TestCase):', 'class TestExportSearch(unittest.TestCase):']","[('TensorSizeEqual', '(hypo[], pos_scores)'), ('TensorSizeEqual', '(pos_scores.numel(), hypo[].numel())'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('TensorEqual', '(h1[])'), ('AlmostEqual', '(h1[])'), ('Less', '(abs(h1[]), 1e-6)'), ('AlmostEqual', '(h1[])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]","['def setUp(self):', 'def setUp(self):']",[],[],[],"['', '']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1235,Myle Ott,myleott@fb.com,2020-04-07 10:06:33-07:00,5feb5645b290412005d4bb327d10b853592cc525,https://github.com/pytorch/fairseq/commit/5feb5645b290412005d4bb327d10b853592cc525,"Update huggingface submodule (#1136)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1136

Reviewed By: ngoyal2707

Differential Revision: D20878271

Pulled By: myleott

fbshipit-source-id: c0df2baa1f15099114777785741b3b730484040e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1236,Myle Ott,myleott@fb.com,2020-04-07 10:31:03-07:00,630701eaa750efda4f7aeb1a6d693eb5e690cab1,https://github.com/pytorch/fairseq/commit/630701eaa750efda4f7aeb1a6d693eb5e690cab1,"Add instructions for paraphrasing model (#1968)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1968

Reviewed By: ngoyal2707

Differential Revision: D20860682

Pulled By: myleott

fbshipit-source-id: b7dced493410a4b9e217e4735eb9cdd0370ad47e",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1237,Chen Liu,chenliu8@fb.com,2020-04-07 13:24:48-07:00,1b749f4a34d0e4c70cb2f98b8cd32b71b9675fe9,https://github.com/pytorch/fairseq/commit/1b749f4a34d0e4c70cb2f98b8cd32b71b9675fe9,"Deprecate the SequenceGenerator with the Scripted vision (#1120)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1120

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1940

Deprecate the SequenceGenerator in Fairseq with the Scripted vision.

Pass all integration unit tests

- Copy ScriptSequenceGenerator to SequenceGenerator:
  - Modified the forward_decoder to fix bug when using adaptive_softmax in `get_prob_normalize` (marked with the inline comment)
   - Add support for other EnsembleModels as input arg (marked with the inline comment)
 - Add `FBEnsembleModelWithFork` to support folk/join in ensemblemodel
   - Add `test_fb_ensemble_model` to test folk/join feature
   - Still have bugs in folk/join feature when running in the Fairseq interface (like generation and interactive). Need further investigation P128130029. cc cndn, jhcross
- Modified SequenceGenerator initialization the interface
- Clear up the codes: delete unused functions `get_normalized_probs` and `_decode`

Reland reverted diff D20685075

Reviewed By: cndn

Differential Revision: D20895977

fbshipit-source-id: 424ee318e67d5d6ffed3edb92c7fa78485ba34af",8,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],3,15,2,0,0,0,2,0,0,0,0,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestJitSequenceGeneratorBase(unittest.TestCase):', 'class TestExportSearch(unittest.TestCase):', 'class TestSequenceGeneratorBase(unittest.TestCase):']","[('TensorSizeEqual', '(hypo[], pos_scores)'), ('TensorSizeEqual', '(pos_scores.numel(), hypo[].numel())'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.size(), t2.size(), )'), ('Less', '((t1 - t2).abs().max(), 1e-4)'), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('TensorEqual', '(h1[])'), ('AlmostEqual', '(h1[])'), ('Less', '(abs(h1[]), 1e-6)'), ('AlmostEqual', '(h1[])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]","['def setUp(self):', 'def setUp(self):']",[],[],[],"['', '']",[],[],[],[],['class TestSequenceGeneratorBase(unittest.TestCase):'],"[('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1238,Myle Ott,myleott@fb.com,2020-04-07 14:34:53-07:00,adff51b4a67c5000aabbe0e00a7bc4b28e855794,https://github.com/pytorch/fairseq/commit/adff51b4a67c5000aabbe0e00a7bc4b28e855794,"Fix breakage after upstream change in sacrebleu (#1137)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1137

Reviewed By: ngoyal2707

Differential Revision: D20895317

Pulled By: myleott

fbshipit-source-id: 39242263c832c303f27fa7ef1e48005010cb61be",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1239,Shun Kiyono,kiyono@ecei.tohoku.ac.jp,2020-04-07 14:44:23-07:00,be76dbe09f04897ae39930e50a5d43bfb20be5e0,https://github.com/pytorch/fairseq/commit/be76dbe09f04897ae39930e50a5d43bfb20be5e0,"Fix typo in docstring (#1969)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes minor typo in docstring.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1969

Reviewed By: ngoyal2707

Differential Revision: D20895327

Pulled By: myleott

fbshipit-source-id: 75838fb78841cceb135c92cbb3dfa65c4b18d5bf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1240,Chen Liu,chenliu8@fb.com,2020-04-07 14:59:30-07:00,d37529ed234ea9173ed35f6797a51a85378ecfca,https://github.com/pytorch/fairseq/commit/d37529ed234ea9173ed35f6797a51a85378ecfca,"Script reoder_incremental_state in fairseq baseline model (#1127)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1127

Pull Request resolved: https://github.com/pytorch/fairseq/pull/1953

Script the `reorder_incremental_states` in the base FairseqModel
Remove the overwrite scriptable `reorder_incremental_states` in the TransformerModel
Change the decoder_len, since len(Tuple) is supported in Script

Relanded reverted diff D20797390

Reviewed By: myleott

Differential Revision: D20896200

fbshipit-source-id: cc4ae34f89f16007656cce6ec6f7e01b13899278",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1241,Alex Xiao,axiao@fb.com,2020-04-08 17:30:41-07:00,bd140de6d9f86e85402b95d19a4bb6c0941121ba,https://github.com/pytorch/fairseq/commit/bd140de6d9f86e85402b95d19a4bb6c0941121ba,"fix pyspeech integ tests using sequence generator

Summary: integ tests for aml speech trainers failing: f181040536, tracked to D20895977, seems like sequence generator doesn't work for audio input anymore

Reviewed By: jay-mahadeokar, liuchen9494

Differential Revision: D20927274

fbshipit-source-id: 4d9b26cf3333d552fc46761319686fafe5c99fba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1242,Ning Dong,dnn@fb.com,2020-04-08 17:46:22-07:00,08691f8d0b32aa427affeece1a5716a9a0f34a38,https://github.com/pytorch/fairseq/commit/08691f8d0b32aa427affeece1a5716a9a0f34a38,"Support quantization in Fairseq Sequence generator

Summary: The fix in MHA is suggested by driazati, to avoid JIT compilation for if branch in MHA forward when in scripting. Without this quantization wouldn't work. Details in https://fb.workplace.com/groups/2240361332735959/permalink/626166461295703/

Reviewed By: jhcross

Differential Revision: D20881076

fbshipit-source-id: b50347b45cd7dbdef02ac7b71316ba734019f57e",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1243,Ning Dong,dnn@fb.com,2020-04-10 14:37:49-07:00,b142b7d9ec792dbd9ff855e3e076947908dae392,https://github.com/pytorch/fairseq/commit/b142b7d9ec792dbd9ff855e3e076947908dae392,"Script _no_repeat_ngram in fb_simple_sequence_generator (#1963)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1963

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1128

It's a common issue that short inputs (< 5 tokens) get repeated due to default length constraint (max_len_a=1.1, max_len_b=5) https://fb.workplace.com/groups/2286753504877951/permalink/2674177509468880/.

In the future we want to use no_ngram_repeat to handle the issue. The functionality is in sequence generator but it needs to be scripted for production use.

Reviewed By: liuchen9494

Differential Revision: D20801865

fbshipit-source-id: c3085f19921adb85415636d16ce31e3826642335",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1244,James Cross,jcross@fb.com,2020-04-10 18:17:09-07:00,c4697e83cb5aeaaa8e8b887e2cf5d987d24901e0,https://github.com/pytorch/fairseq/commit/c4697e83cb5aeaaa8e8b887e2cf5d987d24901e0,"TorchScript support for AANTransformer

Summary: Moving ``_test_save_and_load()` up top top-level for possible reuse across classes.

Reviewed By: cndn

Differential Revision: D20971566

fbshipit-source-id: b9d9c554d03f26cd43eee9f209e1c1367679af72",1,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestExportModels(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],['class TestExportModels(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1245,freewym,freewym@gmail.com,2020-04-13 12:10:52-07:00,5920ed7c91f714417de997a44c053b5a8419f4ec,https://github.com/pytorch/fairseq/commit/5920ed7c91f714417de997a44c053b5a8419f4ec,"add a missing argument within Dictionary::string (#1920)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1920

Reviewed By: ngoyal2707

Differential Revision: D20993011

Pulled By: myleott

fbshipit-source-id: 0ade6f1f15ebe6062a186007ee259ccb3c1ec8b2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1246,Naman Goyal,namangoyal@learnfair1309.h2.fair,2020-04-13 13:48:23-07:00,6563407fcb84be52a5cf0e2e64f9230b97271e59,https://github.com/pytorch/fairseq/commit/6563407fcb84be52a5cf0e2e64f9230b97271e59,"Add mbart sweep script (#1145)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1145

Reviewed By: myleott

Differential Revision: D20998740

fbshipit-source-id: 626130bf02c1fe9aa8d581a23f161c5ef4642a13",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1247,Naman Goyal,namangoyal@learnfair1309.h2.fair,2020-04-13 22:38:24-07:00,1a3aa5f85a5da5af001651c5d2ed7e07aab0746e,https://github.com/pytorch/fairseq/commit/1a3aa5f85a5da5af001651c5d2ed7e07aab0746e,"added code to continue pretraining on superset of langs (#1146)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1146

Reviewed By: pipibjc

Differential Revision: D21005197

fbshipit-source-id: 9a6a21bf5c05fadbe86bc30d40f59e4b4871bd41",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1248,Myle Ott,myleott@fb.com,2020-04-14 10:25:18-07:00,88daeb748b31ad27de6c34630968e0fc191e4326,https://github.com/pytorch/fairseq/commit/88daeb748b31ad27de6c34630968e0fc191e4326,"Fix #1983 (#2014)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2014

Reviewed By: ngoyal2707

Differential Revision: D21017595

Pulled By: myleott

fbshipit-source-id: 05bf5092183dbfef62f08ee711d242ea58192845",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1249,Ronan Riochet,riochet@hotmail.com,2020-04-14 10:56:46-07:00,97d29d78e51e49de50e5105bcf4f9ebbd9fd7387,https://github.com/pytorch/fairseq/commit/97d29d78e51e49de50e5105bcf4f9ebbd9fd7387,"update checkpoint mkdir behavior (issue #1986) (#2011)

Summary:
Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.

https://github.com/pytorch/fairseq/issues/1986
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2011

Reviewed By: ngoyal2707

Differential Revision: D21017208

Pulled By: myleott

fbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1250,Louis MARTIN,louisrtm@gmail.com,2020-04-15 06:43:28-07:00,f57ac6ef3f7588d81fe838f47f5249e30733c5c0,https://github.com/pytorch/fairseq/commit/f57ac6ef3f7588d81fe838f47f5249e30733c5c0,"Other camembert models (#1147)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Update readme with latest models.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1147

Reviewed By: huihuifan

Differential Revision: D21039627

Pulled By: louismartin

fbshipit-source-id: 46f9e43d9793097b6e57a33e405f72c1e15989ea",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1251,Nayan Singhal,naysing@fb.com,2020-04-16 11:22:59-07:00,89e75fa315815b74e4569e5d7f09b86a709f67cf,https://github.com/pytorch/fairseq/commit/89e75fa315815b74e4569e5d7f09b86a709f67cf,"Fix BMUF using 1 GPU

Summary:
With 1 GPU, BMUF is no longer required. Instead, it just works like a simple model training.

Add unit test too for Single GPU BMUF

Reviewed By: jay-mahadeokar

Differential Revision: D21033060

fbshipit-source-id: 9030187c05d49548222c8d1e2fe9534a6c6c4389",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('AlmostEqual', '(results[0], results[1])'), ('AlmostEqual', '(results[0], results[1])'), ('AlmostEqual', '(results[0], results[1])')]",[],[],[],[],[],[],[],[],[],[],"[('AlmostEqual', '(results[0], results[1])')]",[],[],[],[],[],[],[],[],[],"['len(results) == 2', 'len(results) == 2', 'len(results) == 2', 'len(results) == 1']",[],[],[],[],[],[],[],[],[],[],[],[],['len(results) == 2'],[],[],[],[],[],[],[],[],[],[],[],[]
1252,Xianfeng Rui,xfrui@fb.com,2020-04-16 15:47:34-07:00,57526c63433c0b1c997fc91c0881867532567266,https://github.com/pytorch/fairseq/commit/57526c63433c0b1c997fc91c0881867532567266,"Update Fairseq LSTM to jitable version (#2016)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2016

It is to update Fairseq LSTM to jitable version

Reviewed By: cndn

Differential Revision: D20937370

fbshipit-source-id: 26f677fcb58bbeaa507d303e9a81060ff78f0502",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestJitLSTMModel(unittest.TestCase):'],"[('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('TensorEqual', '(result[0], scripted_result[0])'), ('TensorEqual', '(result[1], scripted_result[1])')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1253,Ning Dong,dnn@fb.com,2020-04-19 15:29:00-07:00,6379573c9e56620b6b4ddeb114b030a0568ce7fe,https://github.com/pytorch/fairseq/commit/6379573c9e56620b6b4ddeb114b030a0568ce7fe,"Fully quantize Fairseq transformer (#1993)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1993

F.linear -> nn.Linear so FBGEMM backend could quantize the linear projection. We observed 3x+ speedup.

Add backward compatibility code.

Reviewed By: jhcross

Differential Revision: D20967830

fbshipit-source-id: 11d2c98dd5c1965691d6df433e8428499c9c4dc0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1254,Myle Ott,myleott@fb.com,2020-04-21 05:16:41-07:00,ec57664caebcdfe6bbb320f1200e2600580cd408,https://github.com/pytorch/fairseq/commit/ec57664caebcdfe6bbb320f1200e2600580cd408,"Revert ""Fully quantize Fairseq transformer (#1993)"" (#2032)

Summary:
This reverts commit 6379573c9e56620b6b4ddeb114b030a0568ce7fe.

It doesn't tie weights and breaks old checkpoints.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2032

Reviewed By: cndn, ngoyal2707

Differential Revision: D21141945

Pulled By: myleott

fbshipit-source-id: b2f2ce8092a1bf8bcd6a7e422a69306e342b8cdd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1255,Marco Gaido,marcogaido91@gmail.com,2020-04-21 05:59:03-07:00,4ec169b988b858cd61dcad64211c7a741e8c738c,https://github.com/pytorch/fairseq/commit/4ec169b988b858cd61dcad64211c7a741e8c738c,"Fix max_position resolution with tuples having len > 2 (#2028)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2027 .

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2028

Reviewed By: ngoyal2707

Differential Revision: D21134466

Pulled By: myleott

fbshipit-source-id: 070d7f971bc8d88ec1ca43d52797e2f0b07fb6af",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(resolved, (2000, 100, 2000))')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1256,Angela Fan,angela.h.fan@gmail.com,2020-04-21 09:26:26-07:00,1c8ab79ca59b466120e3df448673cab840f571ea,https://github.com/pytorch/fairseq/commit/1c8ab79ca59b466120e3df448673cab840f571ea,"quant noise code, readme, start of adding quantization (#1896)

Summary:
FUNCTIONALITY:
This diff provides two core pieces of functionality
- Adds training with quantization noise from ""Training with Quantization Noise for Extreme Model Compression"" - controlled by the ""quant_noise"" and ""quant_noise_block_size"" parameters. Added in embeddings, attention, FFN for BERT and Transformer LM training
- Adds quantization with product quantization based on code from ""And the bit goes down: Revisiting the quantization of neural networks"" (Stock et al, 2019). This is applied to a fairseq trained model to quantize after training.

TODO:
-> Pierre, look at quantization code
-> int4 and int8 quantization will be added soon.

EVALUATED TEST CASES:

0. Training of LM and BERT models starts from scratch with no errors -> yes

1. Retrain LM from scratch with code, no quantization, reproduces Wikitext-103 LM results -> yes, see /checkpoint/angelafan/qn_open_source_noise

2. Reload previously trained LM from scratch, not trained with quant noise, reproduces Wikitext-103 LM results -> yes

3. Train LM from scratch with code, no trained with quant noise, reproduces Wikitext-103 LM results -> yes, see /checkpoint/angelafan/qn_open_source_baseline

4. Train BERT model from scratch with code, no quantization, training curve looks the same as before -> yes

5. Check wps during training and wps during inference, no large change from before -> yes

6. Check structured dropout isn't being applied at eval time -> yes

7. Works in combination with LayerDrop -> yes
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1896

Reviewed By: myleott

Differential Revision: D20609420

Pulled By: huihuifan

fbshipit-source-id: 94468dd811c4caaaef46a9fab2b8d381f9d2b955",40,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestQuantization(unittest.TestCase):'],[],['def setUp(self):'],[],['def tearDown(self):'],[],"['not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1257,Myle Ott,myleott@fb.com,2020-04-21 10:46:21-07:00,d502958b4d3356c19e217bee834167b99a945423,https://github.com/pytorch/fairseq/commit/d502958b4d3356c19e217bee834167b99a945423,"Fix LSTM LM unit tests (#2021)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2021

Reviewed By: cndn

Differential Revision: D21092383

Pulled By: myleott

fbshipit-source-id: c6074fe14cc977b3674d77c1c1bc8fb726108934",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['', '']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1258,Angela Fan,angela.h.fan@gmail.com,2020-04-21 11:11:13-07:00,91f7cf649ba594b51890319365e5612076faefef,https://github.com/pytorch/fairseq/commit/91f7cf649ba594b51890319365e5612076faefef,"Update README.md (#2038)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2038

Reviewed By: pierrestock

Differential Revision: D21158644

Pulled By: huihuifan

fbshipit-source-id: 432d01ff5d52363fc4e79c07391b78ac1823c385",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1259,Vinayak Tantia,tantia@fb.com,2020-04-21 15:52:15-07:00,0dac0ff3b1d18db4b6bb01eb0ea2822118c9dd13,https://github.com/pytorch/fairseq/commit/0dac0ff3b1d18db4b6bb01eb0ea2822118c9dd13,"Integrating SlowMo into fairseq and adding SlowMo to fbcode

Summary:
This diff contains the following changes -
* Adding SlowMo algorithm to fbcode (this contains the latest implementation - complete with reduced memory usage for slow momentum, faster forward, linting among other things)
* Integration of SlowMo algorithm into fairseq (includes changes to the code to integrate as well as arguments for SlowMo)
* Scripts for calling SlowMo
* Addition of log-dir in addition to save-dir to allow different directories to be used for logging and saving

Reviewed By: myleott, mikerabbat

Differential Revision: D19184997

fbshipit-source-id: b42b298ac5297fb83a3335fa7ce262c8f48fb2bc",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1260,Gil Keren,gilkeren@fb.com,2020-04-23 00:31:20-07:00,38f35cccea01482d143c488c75fceb352875d654,https://github.com/pytorch/fairseq/commit/38f35cccea01482d143c488c75fceb352875d654,"Adding a prehook to StopwatchMeter and metrics.log_stop_time

Summary: This allows performing torch.cuda.synchronize before stoppping the stopwatch, which allows better timing of torch code parts.

Reviewed By: myleott, jay-mahadeokar

Differential Revision: D21159159

fbshipit-source-id: 14e011193499b3fe5bc0020bbcd4c499438e80a7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1261,Ning Dong,dnn@fb.com,2020-04-24 13:27:23-07:00,b1af3e33d570e287c0142c239b6b611ce094bae2,https://github.com/pytorch/fairseq/commit/b1af3e33d570e287c0142c239b6b611ce094bae2,"Modify gated unit tests to fix Fairseq OSS (#2059)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2059

test_ensemble_sequence_generator and test_export_ensemble_model are green on fbcode master but Pytorch 1.5 release cut happened before the TorchScript fix, so updating the gate to 1.6
Remove quantization test from fairseq as FBGEMMS is binded at OSS side. Will add the test back in fbtranslate but land this first to fix OSS side failures.

Reviewed By: myleott

Differential Revision: D21231873

fbshipit-source-id: 8a2ad7dbed118ca8e3f4c351c399a82fd9740445",1,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1262,James Cross,jcross@fb.com,2020-04-24 14:56:56-07:00,7b3df95f287bc0d844f64fe45717123d06dacb97,https://github.com/pytorch/fairseq/commit/7b3df95f287bc0d844f64fe45717123d06dacb97,"quantization compatiblity: fairseq transformer (#2052)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2052

Convert the transformer model output projection to use an `nn.Linear` module in order to support quantization during TorchScript inference.

Reviewed By: myleott

Differential Revision: D21222066

fbshipit-source-id: fc78736bf645567d006975f72eeab095210b054e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1263,Dmitriy Genzel,dgenzel@fb.com,2020-04-27 12:48:58-07:00,dd518ef62cab87f52c7a1e3dbecb54024dacb62e,https://github.com/pytorch/fairseq/commit/dd518ef62cab87f52c7a1e3dbecb54024dacb62e,"Fix an evaluation bug in fairseq-generate (#1158)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1158

When using BPE in --sacrebleu mode, the scores were computed before BPE was removed (H- strings, not D- strings). This is now fixed.

In addition, added warnings that not using --sacrebleu for scoring with target-side BPE is a bad idea.

Reviewed By: myleott

Differential Revision: D21260024

fbshipit-source-id: f8cf9e3a42e501043b794c841297940ab9e2b75a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1264,Gil Keren,gilkeren@fb.com,2020-04-29 11:02:49-07:00,411531734df8c7294e82c68e9d42177382f362ef,https://github.com/pytorch/fairseq/commit/411531734df8c7294e82c68e9d42177382f362ef,"adding a buffered iterator

Summary:
Torch's DataLoader keeps a buffer of only 2 ready batches only, which cannot be changed. This causes a data loading bottleneck at times where data preparation time fluctuates.

Adding BufferedIterator, which is a generic wrapper for an iterator, implementing a buffer using queue.

Adding FairseqTask support, and in BatchSamplerIterator as default.

Reviewed By: myleott

Differential Revision: D21261026

fbshipit-source-id: 23d4bc6181fe1f9a7ee7ad7d18491594725c0f53",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1265,Ning Dong,dnn@fb.com,2020-04-29 20:31:44-07:00,4725487bbc3bdee89c45ced0a8664cffd8e1ab01,https://github.com/pytorch/fairseq/commit/4725487bbc3bdee89c45ced0a8664cffd8e1ab01,"Refactor fb_sequence_generator for TS models to work with multithreading (#2060)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2060

Now I separate out the forward pass from SequenceGenerator. 2 changes to make exported TS model work under multithreading settings.
1. Avoid stateful attribute including incremental states, src_lengths in EnsembleModel / SequenceGenerator. This is the main reason I hesitated to merge with the original version. myleott, is there specific reason why we have incremental states as module attributes, instead of initializing it in forward pass?
2. Replace in place operator with out of place equivalent. This caused a bit of code bloat, mostly for index_put operators

e.g.
    eos_mask[:, :beam_size][blacklist] = torch.tensor(0).to(eos_mask)

becomes

    slices = torch.arange(eos_mask.numel()).view(eos_mask.shape)[:, :beam_size][
                blacklist
            ]
    eos_mask = eos_mask.index_put([slices], torch.tensor(0).to(eos_mask))

Reviewed By: myleott

Differential Revision: D20995796

fbshipit-source-id: 2ff99d339e5e5a0f4ee6079f395116ba7289f532",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1266,Ning Dong,dnn@fb.com,2020-04-29 21:09:33-07:00,928dc47e7e72f3e6ed96e50942e7fb8892cdcf32,https://github.com/pytorch/fairseq/commit/928dc47e7e72f3e6ed96e50942e7fb8892cdcf32,"Override reorder_incremental_states in fairseq transformer (#2073)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2073

We identified issues in TorchScript builtin hasattr occasionally has inconsistency so reorder_incremental_state is not always triggered in exported model. We had these overrides once before named_module / id were supported in TorchScript and Chen removed in D20896200. Plan to have these until hasattr issue gets fixed.

With this we observed ~1 bleu score regression is gone, see test plan.

Reviewed By: jhcross

Differential Revision: D21273014

fbshipit-source-id: 6bd1a898d5279fcc7dca4c0ff0ff11401cf9e41c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1267,Halil Akin,halilakin@fb.com,2020-04-30 10:31:56-07:00,c74857141784cebb0c947d42cfd604cc8aa59945,https://github.com/pytorch/fairseq/commit/c74857141784cebb0c947d42cfd604cc8aa59945,"Add a test for fp16 to fairseq

Reviewed By: myleott

Differential Revision: D21315518

fbshipit-source-id: df17efeec6fb2b576371b124d78e9294cef3e74c",1,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1268,Elijah Rippeth,erippeth@mitre.org,2020-04-30 13:15:52-07:00,36cccc8124aa8563ee65be6b38fdece456bb57a3,https://github.com/pytorch/fairseq/commit/36cccc8124aa8563ee65be6b38fdece456bb57a3,"Fixes #2075 (#2076)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2075

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2076

Reviewed By: ngoyal2707

Differential Revision: D21326848

Pulled By: myleott

fbshipit-source-id: a1d92aaf4959e063549f3201abb68f4720b4bf5a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1269,Xutai Ma,xutai_ma@jhu.edu,2020-04-30 13:23:50-07:00,12d5e6ff168d4334407633d495123cdaa4166f1a,https://github.com/pytorch/fairseq/commit/12d5e6ff168d4334407633d495123cdaa4166f1a,"Monotonic multihead attention (#1707)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Add code for published paper from FB

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

*Still WIP*
jmp84
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1707

Reviewed By: jmp84

Differential Revision: D21304498

Pulled By: xutaima

fbshipit-source-id: 073d522e0eeef3e02c83e4617b8e5b697ff6979b",28,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1270,Gil Keren,gilkeren@fb.com,2020-04-30 17:05:10-07:00,6089e8327b41d98f83fe21a0a55cc88dd3db0bd9,https://github.com/pytorch/fairseq/commit/6089e8327b41d98f83fe21a0a55cc88dd3db0bd9,"Adding buffered iterator - fix for failing tests

Summary: D21261026 will cause failures because the trainer assumes a new signature for `task.get_batch_iterator`. Changing back to the original signature, and using `args.data_buffer_size` inside the  `get_batch_iterator` of the base task class.

Reviewed By: zhengwy888

Differential Revision: D21317530

fbshipit-source-id: 8d2de900bf8ad91e9451df08ad08d9cbaee86011",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1271,Muhammad,moyle2010@gmail.com,2020-05-01 04:03:53-07:00,1f2bf68f95f2676a600cabb549180c974f8c3a56,https://github.com/pytorch/fairseq/commit/1f2bf68f95f2676a600cabb549180c974f8c3a56,"used masked tokens in masked_lm models (#2050)

Summary:
## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2042
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2050

Reviewed By: ngoyal2707

Differential Revision: D21202925

Pulled By: lematt1991

fbshipit-source-id: 747bdd20df0912606295f488a967854ade64ffe0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1272,Myle Ott,myleott@fb.com,2020-05-01 04:05:25-07:00,7a6519f84fed06947bbf161c7b66c9099bc4ce53,https://github.com/pytorch/fairseq/commit/7a6519f84fed06947bbf161c7b66c9099bc4ce53,"Bugfixes (#1159)

Summary:
Several bugfixes to get tests passing on OSS master
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1159

Reviewed By: ngoyal2707

Differential Revision: D21331993

Pulled By: myleott

fbshipit-source-id: 327ae19f6797f92b8c6083a49d5f5edb0872223e",9,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1273,Myle Ott,myleott@fb.com,2020-05-04 07:10:23-07:00,b2ee110c853c5effdd8d21f50a8437485bafb285,https://github.com/pytorch/fairseq/commit/b2ee110c853c5effdd8d21f50a8437485bafb285,"Bugfixes (#1164)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1164

Reviewed By: ngoyal2707

Differential Revision: D21373232

Pulled By: myleott

fbshipit-source-id: f31c65c6f2ebd9a603099e0cbe9e32c47585f50d",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1274,Myle Ott,myleott@fb.com,2020-05-04 07:14:30-07:00,89d18af12792442f8ce5df86027aaa3f908240ec,https://github.com/pytorch/fairseq/commit/89d18af12792442f8ce5df86027aaa3f908240ec,"Cleanup transformer (#1160)

Summary:
This also fixes https://github.com/pytorch/fairseq/issues/2079
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1160

Reviewed By: ngoyal2707

Differential Revision: D21338290

Pulled By: myleott

fbshipit-source-id: 266bda0921a42b218127f83ab7aa8cc8282582cd",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1275,Peng-Jen Chen,pipibjc@fb.com,2020-05-05 20:44:37-07:00,a06083f8a154bc01a9c6c96b63767a62543d3802,https://github.com/pytorch/fairseq/commit/a06083f8a154bc01a9c6c96b63767a62543d3802,"Fix score reference when using translation_from_pretrained_bart task

Summary: `translation_from_pretrained_bart` task doesn't have `target_lang` variable. We need to access it through `args` like line 104.

Reviewed By: myleott

Differential Revision: D21369854

fbshipit-source-id: a405eb6525d2484c2a7a91e0db47def8ae203d30",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1276,James Cross,jcross@fb.com,2020-05-07 18:34:46-07:00,be86e7ebefb21f694b190487b789f4e61132fc13,https://github.com/pytorch/fairseq/commit/be86e7ebefb21f694b190487b789f4e61132fc13,"fairseq transformer: enable decoder_output_dim (#2096)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2096

No change to existing behavior.

Allows the use of an extra learned linear projection (bottleneck layer) before the output projection. This structure was already supported in `TransformerDecoder` via args.decoder_output_dim, used in architectures such as `transformer_lm`, but this change surfaces a command-line option for the basic transformer architecture.

Reviewed By: cndn

Differential Revision: D21443249

fbshipit-source-id: cdf5806c97ce03a77befa14bc482c81c7b9c83a1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1277,Marco Gaido,marcogaido91@gmail.com,2020-05-10 06:11:24-07:00,11345a7608a6b73feb53046f6c0eef3dc4d3fc08,https://github.com/pytorch/fairseq/commit/11345a7608a6b73feb53046f6c0eef3dc4d3fc08,"Pass all net_inputs in SequenceGenerator (#2090)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2022.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2090

Reviewed By: cndn

Differential Revision: D21385984

Pulled By: myleott

fbshipit-source-id: 1428e02e625b8625df71a83c05dcf933c3f899df",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('HypoTokens', '(hypos[0][0], [w1, eos])'), ('HypoScore', '(hypos[0][0], [0.9, 1.0])')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],"['in kwargs', 'kwargs[] is not None']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1278,Myle Ott,myleott@fb.com,2020-05-11 10:33:00-07:00,4a80e54c9d6c3ab07a91b33dc8eee6cbf0cdc057,https://github.com/pytorch/fairseq/commit/4a80e54c9d6c3ab07a91b33dc8eee6cbf0cdc057,"Generalize moving of tensors to CPU in checkpoints (#2098)

Summary:
This is needed for TPUs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2098

Reviewed By: ngoyal2707

Differential Revision: D21455095

Pulled By: myleott

fbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1279,Myle Ott,myleott@fb.com,2020-05-11 12:41:33-07:00,6209d7d6b2c41fccb01e00671261be80ba86029a,https://github.com/pytorch/fairseq/commit/6209d7d6b2c41fccb01e00671261be80ba86029a,"Fix eval_lm (fixes #2083) and a few other small things (#2100)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2100

Reviewed By: ngoyal2707

Differential Revision: D21456309

Pulled By: myleott

fbshipit-source-id: 291711589fca9f158e0fdbf01194da3e66fbd0aa",7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1280,Myle Ott,myleott@fb.com,2020-05-14 07:15:19-07:00,eed4865c17bee7bf44feac64271d188ff2deb81b,https://github.com/pytorch/fairseq/commit/eed4865c17bee7bf44feac64271d188ff2deb81b,"Make Dictionary constructor arguments keyword-only (#2099)

Summary:
This is a minor safety thing, since otherwise it may be tempting to call `Dictionary(filename)` instead of `Dictionary.load(filename)`. After this change, all arguments to `Dictionary.__init__` must be given explicitly.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2099

Reviewed By: ngoyal2707

Differential Revision: D21549563

Pulled By: myleott

fbshipit-source-id: 262c31fbe01db0bb6288bd5b826376ae3e8aa61d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1281,Xu Song,xusong.vip@gmail.com,2020-05-14 07:15:49-07:00,4455f67011a0b2449f3f7da10e85a100088c53c5,https://github.com/pytorch/fairseq/commit/4455f67011a0b2449f3f7da10e85a100088c53c5,"Remove unused bias_kv/zero_attn options #1893 (#2113)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2113

Reviewed By: ngoyal2707

Differential Revision: D21550974

Pulled By: myleott

fbshipit-source-id: 63883342a24f7ef67a365f04a90b5d96601bae25",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1282,Myle Ott,myleott@fb.com,2020-05-14 10:21:26-07:00,9a718e29855713a51877237b2dcc25e39c234c82,https://github.com/pytorch/fairseq/commit/9a718e29855713a51877237b2dcc25e39c234c82,"Various fixes (#2127)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2127

Reviewed By: ngoyal2707

Differential Revision: D21550962

Pulled By: myleott

fbshipit-source-id: ddbe3f287f170862378e0702fc378a4fe400793a",7,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],"['', '']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1283,Myle Ott,myleott@fb.com,2020-05-14 13:55:12-07:00,803c0a6d11fcca6dcff8b8d0a4170338b15b26ff,https://github.com/pytorch/fairseq/commit/803c0a6d11fcca6dcff8b8d0a4170338b15b26ff,"Update iterators to support counting, rename CountingIterator.count -> n and add tests (#1166)

Summary:
A few changes here:
- update GroupedIterator and ShardedIterator to support counting. This will be useful on TPUs, since the TPU dataloading threads may advance faster than we can process them.
- add tests for the above
- in CountingIterator, rename `count` -> `n`. This is needed because `count` is overloaded for iterables (e.g., `list` defines a different `count` method, which is actually a search function).
- in CountingIterator, rename `override_len` -> `total` to be more consistent with other iterators (e.g., tqdm). This functionality was unused previously (it's only needed for TPUs), so the rename is easy.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1166

Reviewed By: ngoyal2707

Differential Revision: D21373525

Pulled By: myleott

fbshipit-source-id: 102f3d50ed1a5163a7d1216ca5a179564a05dfe4",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,18,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(itr.n, 0)'), ('Equal', '(next(itr), ref[0])'), ('Equal', '(itr.n, 1)'), ('Equal', '(next(itr), ref[1])'), ('Equal', '(itr.n, 2)'), ('Equal', '(itr.n, 5)'), ('Equal', '(next(itr), ref[5])'), ('Equal', '(itr.n, 9)'), ('Equal', '(next(itr), ref[9])'), ('Equal', '(list(itr), [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]])'), ('Equal', '(list(itr), [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]])'), ('Equal', '(list(itr), [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])'), ('Equal', '(list(itr), x)'), ('Equal', '(list(itr), [0, 2, 4, 6, 8])'), ('Equal', '(list(itr), [1, 3, 5, 7, 9])'), ('Equal', '(list(itr), [0, 3, 6, 9])'), ('Equal', '(list(itr), [1, 4, 7, None])'), ('Equal', '(list(itr), [2, 5, 8, None])')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(next(itr), 0)'), ('Equal', '(next(itr), 1)'), ('Equal', '(next(itr), 5)'), ('Equal', '(next(itr), 9)')]",[],[],[],[],[],[],[],[],[],"['itr is None', 'len(ref) == 10', 'itr is not None']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1284,michal.turski,michal.turski@applica.pl,2020-05-18 10:51:50-07:00,132ee8a535359aef62eff3e264cba3cfe4ea921f,https://github.com/pytorch/fairseq/commit/132ee8a535359aef62eff3e264cba3cfe4ea921f,"Extended api of LSTM encoder. (#2030)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1862.
Nobody responded to my issue. Nevertheless change is very small, therefore i think is doesn't need much discussion.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2030

Reviewed By: joshim5, ngoyal2707

Differential Revision: D21584250

Pulled By: myleott

fbshipit-source-id: 28f0ccaca0df2860806178dbce02bcc12d7115d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1285,Myle Ott,myleott@fb.com,2020-05-18 11:04:12-07:00,775122950d145382146e9120308432a9faf9a9b8,https://github.com/pytorch/fairseq/commit/775122950d145382146e9120308432a9faf9a9b8,"TPU support (#2137)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2137

Reviewed By: ngoyal2707

Differential Revision: D21588555

Pulled By: myleott

fbshipit-source-id: d0c38498356aa8a97cb347bb1b943bb58e59489e",17,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1286,Xing,xhlperso@gmail.com,2020-05-20 11:07:09-07:00,e773446a85344e3f50dde3c7ed689e8f5daae163,https://github.com/pytorch/fairseq/commit/e773446a85344e3f50dde3c7ed689e8f5daae163,"examples/translation: Specifiy the correct bpe when loading WMT19 (#1951)

Summary:
# Description

In [examples/translation](https://github.com/pytorch/fairseq/tree/master/examples/translation), the code will not run if you change the model from `transformer.wmt16` to `transformer.wmt19`, since the BPE they are using are different. I corrected that with a note at the end of the section.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1951

Reviewed By: ngoyal2707

Differential Revision: D21663490

Pulled By: myleott

fbshipit-source-id: 13010dbec0ef5202355e0b3eb6d77b1958e80e97",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1287,Xu Song,xusong.vip@gmail.com,2020-05-20 11:19:52-07:00,e8d2ebb9ae35f7f60cadeed268feed14f6c4521b,https://github.com/pytorch/fairseq/commit/e8d2ebb9ae35f7f60cadeed268feed14f6c4521b,"revise roberta comment (#2144)

Summary:
Only project the masked tokens, not unmasked tokens.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes roberta document

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2144

Reviewed By: ngoyal2707

Differential Revision: D21663245

Pulled By: myleott

fbshipit-source-id: 9a1e76eb8be8d73a0a5be3e05bce6eefdf6c5f6d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1288,masonreznov,14233549+masonreznov@users.noreply.github.com,2020-05-20 12:30:12-07:00,fa6a3cd0b7f022f66f4a4b1d8c7cb8a66de2e25e,https://github.com/pytorch/fairseq/commit/fa6a3cd0b7f022f66f4a4b1d8c7cb8a66de2e25e,"Typo in readme. (#2150)

Summary:
Use of ""set tokenizer"" instead of ""get tokenizer"".

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2150

Reviewed By: ngoyal2707

Differential Revision: D21663241

Pulled By: myleott

fbshipit-source-id: e2b341e60ae32182f8b6052a8554f1fd1595a41d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1289,Jonathan Timcheck,jonathan.timcheck@gmail.com,2020-05-20 13:17:15-07:00,eedf27aaff043a54217a047ad62be58bbc34d8db,https://github.com/pytorch/fairseq/commit/eedf27aaff043a54217a047ad62be58bbc34d8db,"Fix generation for hf_gpt2 (#2139)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?

Fixes generation for hf_gpt2.

Before fix:
```
env CUDA_VISIBLE_DEVICES=""2,3"" fairseq-interactive data-bin/wikitext-103 \
  --task language_modeling \
  --path $PREFIX/checkpoints_gpt2/transformer_wikitext-103/checkpoint_best.pt
```
![image](https://user-images.githubusercontent.com/22627794/82102310-05021380-96c4-11ea-8073-1ae6919559ba.png)

After fix:
![image](https://user-images.githubusercontent.com/22627794/82102316-092e3100-96c4-11ea-825a-c41254ce9efe.png)

This test follows the [language model example](https://github.com/pytorch/fairseq/tree/master/examples/language_model), but with hf_gpt2.

Trained for one epoch:
```
env CUDA_VISIBLE_DEVICES=""2,3"" fairseq-train --task language_modeling \
  data-bin/wikitext-103 \
  --save-dir $PREFIX/checkpoints_gpt2/transformer_wikitext-103 \
  --dropout 0.1 \
  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \
  --lr 0.0005 --reset-optimizer --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \
  --tokens-per-sample 1024 --sample-break-mode none \
  --max-tokens 1024 --update-freq 16 \
  --fp16 \
  --arch hf_gpt2 --max-target-positions 1024 \
  --skip-invalid-size-inputs-valid-test
```

Details of fix:
add the unexpected keyword argument, encoder_out, to forward()
incremental_state is {}, not None => change to handle this case

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2139

Reviewed By: ngoyal2707

Differential Revision: D21663260

Pulled By: myleott

fbshipit-source-id: bafbfc7b37d0b49a459e0b64e90da5c13a991d6d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1290,henryzhou7,zhouyh.7@gmail.com,2020-05-20 16:21:48-07:00,9f25ffb02ab54a443ba181f732d5dcc00db7bea8,https://github.com/pytorch/fairseq/commit/9f25ffb02ab54a443ba181f732d5dcc00db7bea8,"overflow error when doing target.sum().long() (#1175)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).
Offline running code gave an overflow bug without throwing an exception.
The bug caused sample size to be very big (1e14), making the loss become 0 when training vq-wav2vec.
Changed the order for doing .sum().long() fixed the overflowing bug.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1175

Reviewed By: myleott

Differential Revision: D21669343

Pulled By: alexeib

fbshipit-source-id: 59b4d83a8ad61c3fec99a1db4af1ac75a1c08eba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1291,Guntupalli Venkata Sai Kalyan,kalyan-6941@kalyan-6941.csez.zohocorpin.com,2020-05-26 15:44:10-07:00,eb509f0c584ebae01834e773fb83584102a4f4da,https://github.com/pytorch/fairseq/commit/eb509f0c584ebae01834e773fb83584102a4f4da,"move tensor's device from gpu to cpu (#2174)

Summary:
# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Partially Fixes https://github.com/pytorch/fairseq/issues/2173 .

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2174

Reviewed By: ngoyal2707

Differential Revision: D21725035

Pulled By: myleott

fbshipit-source-id: 16e0a66a104a9713c5d98fee97d3d97261b72c94",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1292,Sepehr Sameni,Sepehr.Sameni@gmail.com,2020-05-26 15:49:13-07:00,434eedd4ef4f09d352d44dbbf8eb53b4c0ac0a36,https://github.com/pytorch/fairseq/commit/434eedd4ef4f09d352d44dbbf8eb53b4c0ac0a36,"correct link in quant_noise (#2184)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2184

Reviewed By: ngoyal2707

Differential Revision: D21725053

Pulled By: myleott

fbshipit-source-id: bdb93af6695e96d4b44a58f104adb3c748a5fb40",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1293,Myle Ott,myleott@fb.com,2020-05-26 15:55:53-07:00,145bc9de1278414812b2aef837be9ca0e9c1aebc,https://github.com/pytorch/fairseq/commit/145bc9de1278414812b2aef837be9ca0e9c1aebc,"Several small fixes (incl. set default --data-buffer-size=10) (#2163)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163

Reviewed By: ngoyal2707

Differential Revision: D21665601

Pulled By: myleott

fbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1294,Xu Song,xusong.vip@gmail.com,2020-05-27 06:23:49-07:00,95294bfbb627c7ba140e73ac27c8e98012045916,https://github.com/pytorch/fairseq/commit/95294bfbb627c7ba140e73ac27c8e98012045916,"refactor superclass of MaskedLMModel (#2170)

Summary:
masked_lm is actually encoder-only model

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Class Refactor

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2170

Reviewed By: ngoyal2707

Differential Revision: D21725071

Pulled By: myleott

fbshipit-source-id: 75fd36008f3e3425f8f5180472734394046dfb77",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1295,Marco Gaido,mgaido@fbk.eu,2020-05-27 07:48:21-07:00,5453e4355b274645074d0068f668ac5bcea9905c,https://github.com/pytorch/fairseq/commit/5453e4355b274645074d0068f668ac5bcea9905c,"Avoid NaN in speech_recognition with input having only 1 spec… (#1864)

Summary:
…trogram

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1863.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1864

Reviewed By: yqwangustc

Differential Revision: D21663642

Pulled By: myleott

fbshipit-source-id: f411c5c01c7505375bec6d47554e85fb70877e9c",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class DataUtilsTest(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['not torch.isnan(out).any()', '(out == sample_len1).all()']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1296,Myle Ott,myleott@fb.com,2020-05-27 09:56:08-07:00,be5313acc7856e69a4d91aa804bca254cc2886c2,https://github.com/pytorch/fairseq/commit/be5313acc7856e69a4d91aa804bca254cc2886c2,"Add bart.base to README (fixes #2189) (#2190)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2190

Reviewed By: ngoyal2707

Differential Revision: D21742525

Pulled By: myleott

fbshipit-source-id: fa29e3e36eb136a337fb1277cad3996ae2b22546",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1297,Myle Ott,myleott@fb.com,2020-05-27 10:21:49-07:00,2f7e3f33235b787de2e34123d25f659e34a21558,https://github.com/pytorch/fairseq/commit/2f7e3f33235b787de2e34123d25f659e34a21558,"Support multi-GPU validation in fairseq-validate (#2162)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2162

Reviewed By: ngoyal2707

Differential Revision: D21663181

Pulled By: myleott

fbshipit-source-id: d01e64f97482f76bd601cd8b20232c0ef637bb8a",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1298,Myle Ott,myleott@fb.com,2020-05-28 07:23:22-07:00,8e48f45aa469bbff85613520ffc161c0850e4744,https://github.com/pytorch/fairseq/commit/8e48f45aa469bbff85613520ffc161c0850e4744,"Miscellaneous fixes (#2193)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2193

Reviewed By: ngoyal2707

Differential Revision: D21748548

Pulled By: myleott

fbshipit-source-id: d9f64540b55b4d427b3da6ad04a35f7b988b049a",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1299,Yongqiang Wang,yqw@fb.com,2020-05-29 11:17:42-07:00,29b8a4deb58ca9798b61690a31de1ea57de92122,https://github.com/pytorch/fairseq/commit/29b8a4deb58ca9798b61690a31de1ea57de92122,"improving error logging for ""gradients are inconsistent between workers""

Summary:
""gradients are inconsistent between workers"" are becoming increasingly
more annoying and very difficult to debug. It may be caused by:

- grad_norm becomes NaN in some worker
- all_reduce is inconsistent

We now will explicitly raise NaN or Inf if grad_norm is NaN/Inf, and print out
grad_norm values if they are inconsistent

Reviewed By: myleott

Differential Revision: D21716726

fbshipit-source-id: 70593f001bc16b9e1cda460169a29b2be6aaed2c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1300,Duc Le,duchoangle@fb.com,2020-05-30 15:23:43-07:00,1e40a48037eefc5ceb7672ea0fa11db629a35113,https://github.com/pytorch/fairseq/commit/1e40a48037eefc5ceb7672ea0fa11db629a35113,"Enforce max limit of buffer size

Reviewed By: jay-mahadeokar

Differential Revision: D21804332

fbshipit-source-id: 51997455560a6b67f66d1401ef7095d4a1de4027",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1301,Mike Ruberry,mruberry@fb.com,2020-06-03 09:45:20-07:00,fad3cf0769843e767155f4d0af18a61b9a804f59,https://github.com/pytorch/fairseq/commit/fad3cf0769843e767155f4d0af18a61b9a804f59,"Updates floor division to use floor division operator

Summary:
Performing floor division with torch.div is deprecated and will soon throw a runtime error. This diff updates the floor division to use the floor division operator.

Created from Diffusion's 'Open in Editor' feature.

Reviewed By: myleott

Differential Revision: D21848480

fbshipit-source-id: c9374e9406f4ba388f30315294eee7a2a4fcfecc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1302,Naman Goyal,namangoyal@devfair0110.h2.fair,2020-06-03 12:58:40-07:00,0b462f899925a1da6c91749bce0e0ed347604607,https://github.com/pytorch/fairseq/commit/0b462f899925a1da6c91749bce0e0ed347604607,"adding model parallel roberta code (#1181)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1181

Reviewed By: myleott

Differential Revision: D21862030

fbshipit-source-id: 532ef608652e63f5490d554af486b87364af100e",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1303,Naman Goyal,namangoyal@learnfair0329.h2.fair,2020-06-03 14:04:07-07:00,2cc8f6e5f2b9c3c36c64bc775f6ed61d4b8d97e0,https://github.com/pytorch/fairseq/commit/2cc8f6e5f2b9c3c36c64bc775f6ed61d4b8d97e0,"some fixes for model parallel roberta (#1182)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1182

Reviewed By: myleott

Differential Revision: D21868455

fbshipit-source-id: c12f90701ec36e55a72da393ca85c1198f23af04",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1304,Wei Ho,weiho@fb.com,2020-06-03 18:49:00-07:00,ea092c2aa697eb7c362447e663922d2dfe2f6da1,https://github.com/pytorch/fairseq/commit/ea092c2aa697eb7c362447e663922d2dfe2f6da1,"Split out fairseq GPU tests & add new deeplearning_fairseq_gpu contbuild using remote execution

Reviewed By: myleott

Differential Revision: D21472387

fbshipit-source-id: efde278baf6a05e8a81a9630b44c7e7e7c7fe7fc",5,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],3,1,3,0,3,0,6,0,0,0,1,1,1,1,0,1,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestTranslationGPU(unittest.TestCase):', 'class TestQuantization(unittest.TestCase):', 'class TestOptimizersGPU(unittest.TestCase):']","[('Raises', '(RuntimeError):')]","['def setUp(self):', 'def setUp(self):', 'def setUp(self):']",[],"['def tearDown(self):', 'def tearDown(self):', 'def tearDown(self):']",[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],['import unittest'],['class TestQuantization(unittest.TestCase):'],"[('Raises', '(RuntimeError):')]",['def setUp(self):'],[],['def tearDown(self):'],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1305,Myle Ott,myleott@fb.com,2020-06-05 06:04:31-07:00,5abc774eead6a9b47b372cf5cde22aee49587edf,https://github.com/pytorch/fairseq/commit/5abc774eead6a9b47b372cf5cde22aee49587edf,"Re-enable test_transformer_fp16 GPU test

Reviewed By: theweiho

Differential Revision: D21890628

fbshipit-source-id: 4088884dd2a82a831f1c129e675eb233c469242a",3,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1306,Joshua Meier,jmeier@fb.com,2020-06-05 09:28:22-07:00,023f7af21f0987814289b6f605821213f927bfc6,https://github.com/pytorch/fairseq/commit/023f7af21f0987814289b6f605821213f927bfc6,"enable choice of max-tokens in masked_lm (#1180)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Adds option to set max-positions in masked_lm model. This option is available in the RoBERTa model, so it should be available here too.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1180

Reviewed By: myleott

Differential Revision: D21904170

Pulled By: joshim5

fbshipit-source-id: 37168dbf1a2758620d5e8e05c7e8a9ef8d09c765",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1307,Mike Ruberry,mruberry@fb.com,2020-06-05 09:54:49-07:00,b1f9a8f5665ac4f8c8cd002e59beeb2837e320e4,https://github.com/pytorch/fairseq/commit/b1f9a8f5665ac4f8c8cd002e59beeb2837e320e4,"Updates div sometimes performing floor division to explicitly perform either true division or floor division

Summary:
torch.div will soon throw a runtime error when it would have performed floor division. This diff updates this instance of div to use either the true division or floor division operators as appropriate so the behavior doesn't change and the test won't throw a runtime error.

Created from Diffusion's 'Open in Editor' feature.

Reviewed By: myleott

Differential Revision: D21900423

fbshipit-source-id: 363c3e64d25608a033cd2942dcbb039a73018596",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1308,Joshua Meier,jmeier@fb.com,2020-06-05 12:13:34-07:00,152a3fe14322c3cb8a38a31ddbff59e7536ab689,https://github.com/pytorch/fairseq/commit/152a3fe14322c3cb8a38a31ddbff59e7536ab689,"Support residual connections in LSTM models (#1103)

Summary:
Adds support for residual connections in LSTM models.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1103

Reviewed By: myleott

Differential Revision: D21639942

Pulled By: joshim5

fbshipit-source-id: a02ddfe080a847fd91a9c6a5074cb6dc782f7727",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1309,Joshua Meier,jmeier@fb.com,2020-06-05 12:55:56-07:00,2699f4a28b70cb7a2ec5890f71b6d6f27fd0af92,https://github.com/pytorch/fairseq/commit/2699f4a28b70cb7a2ec5890f71b6d6f27fd0af92,"add random sequence truncation (#1173)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Allows taking random crops in language_modeling task.
Discussed with myleott robert-verkuil tomsercu in meeting yesterday. Ultimately took a different, more general approach to implementing this.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1173

Reviewed By: myleott

Differential Revision: D21904561

Pulled By: joshim5

fbshipit-source-id: 66e8dfb10a0d36b76acd2eb181e00db6fc2433fc",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1310,Xilun Chen,xilun@fb.com,2020-06-08 10:14:23-07:00,e03bfd9bf447ff9b040887968cb6152e5bba3a4a,https://github.com/pytorch/fairseq/commit/e03bfd9bf447ff9b040887968cb6152e5bba3a4a,"Check if the checkpoint is from the latest version before updating the state_dict in TransformerDecoder.upgrade_state_dict_named() (#2222)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2222

When share_input_output_embed is set to True, the existing code always overrides output_projection.weight with embed_tokens.weight

This is unncessary, and caused a very obscure bug in our custom BART model.

Added a check to skip the update to state_dict if f""{name}.output_projection.weight"" is already in the checkpoint.

Reviewed By: myleott

Differential Revision: D21915833

fbshipit-source-id: d298e24394be2ee85c8f686ba459b7e4cbd4298a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1311,Gil Keren,gilkeren@fb.com,2020-06-08 10:50:33-07:00,5c4f0f89035c20dda080a6072ac56ae594815938,https://github.com/pytorch/fairseq/commit/5c4f0f89035c20dda080a6072ac56ae594815938,"Better exception handling for the data buffer thread

Summary: When data-buffer-size != 0 was used, an exception happening in the data preparation (therefore in the buffer thread) was not raised properrly, and the main thread hanged on `queue.get`. This fixes it, by raising the error to the main thread.

Reviewed By: myleott

Differential Revision: D21917739

fbshipit-source-id: 8d3f875b663b37625f44a943fb3904e25216db06",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1312,Mike Ruberry,mruberry@fb.com,2020-06-09 13:01:31-07:00,2e1da09a9c374d0a909e623054a33e9907bc0d82,https://github.com/pytorch/fairseq/commit/2e1da09a9c374d0a909e623054a33e9907bc0d82,"Updates argument to np.arange to avoid performing floor division using torch.div

Summary:
Performing floor division with torch.div is deprecated and will soon throw a runtime error. Perhaps surprisingly, calling np.arange on a torch tensor can use torch.div to perform floor division. Taking the number from the tensor using .item() should prevent this issue and keep this code working.

Created from Diffusion's 'Open in Editor' feature.

Reviewed By: lematt1991

Differential Revision: D21941120

fbshipit-source-id: 4d76451d4b33d487946af1c2f9ed21eca858cb06",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1313,Joshua Meier,jmeier@fb.com,2020-06-10 11:56:43-07:00,242269d439dc9df346c8aaf7947aad4581d1894d,https://github.com/pytorch/fairseq/commit/242269d439dc9df346c8aaf7947aad4581d1894d,"Fix truncation in sentence_ranking (#1185)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes current breaking change on master.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1185

Reviewed By: myleott

Differential Revision: D21924644

Pulled By: joshim5

fbshipit-source-id: 0eabd2393c76060dcf1568eba308878a90af7a87",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1314,Yongqiang Wang,yqw@fb.com,2020-06-12 22:11:13-07:00,86edf989dd6a53827d509ad268e3f333261e2425,https://github.com/pytorch/fairseq/commit/86edf989dd6a53827d509ad268e3f333261e2425,"cast grad_norm to float in case fp16 training

Summary:
we found that grad_norm could become inf because it is accumulated in
meter many times; and fp16 it becomes easy to overflow. Using fp32 for each `grad_norm`
cost minimum memory

Reviewed By: myleott

Differential Revision: D22015643

fbshipit-source-id: 429d24bbb9c9a785edf0bfb06480497022f80418",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1315,Alex Xiao,axiao@fb.com,2020-06-15 15:26:24-07:00,8570277f91d6bed03d71cc9c8326f096cd06b0d2,https://github.com/pytorch/fairseq/commit/8570277f91d6bed03d71cc9c8326f096cd06b0d2,"dataset sampling for minibatch training

Summary:
**Motivation:**

We have 3 datasets: Portal, Video, and Messenger Voice Clips. We want to specify a distribution [p1, p2, p3] such that we sample utterances from Portal with prob p1, etc.

Previously, D21675421 samples from datasets by **batches**.  This is not acceptable for minibatch training, as we need to maintain LSTM states across consecutive batches. As a result, we need utterance level sampling, not batch level.

**Design**

1. Created a new MultiCorpusDataset, similar to MultiCorpusSampledDataset, except it does sampling on utterance level. Specifically, everytime `ordered_indices` is called, a new sample of the multiple datasets is generated based on an input distribution. We ensure that the randomness of this is seeded by the input seed and the epoch, to enable reproducibility on loading from checkpoints.
2. Created MiniBatchMultiCorpusDataset, which adds minibatch specific logic to MultiCorpusDataset, mainly for handling things like the start frame and deleting cache.
3. Refactored different sampling strategies into a single `build_sampled_dataset` for easy re-use.
4. Added flag --reset-iterator, enabling us to reset the batch iterator every epoch, enabling a new `ordered_indices` to be generated every epoch
5. some minor refactoring of existing code

**Usage**

1. In your data.json, include extra splits in addition to ""train"" (i.e. ""portal"", ""video""), with whatever transforms/handle file you want.
2. In your flow, provide ""--extra-splits portal 0.2 video 0.3"" and ""--reset-iterator"" as flags
3. Enjoy WER improvements

Differential Revision: D21887303

fbshipit-source-id: 6b377bed8a68a8e72e2528f8a5a28b675eebaadf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1316,Myle Ott,myleott@fb.com,2020-06-16 11:45:10-07:00,14ee059a36092a4216ec38601ca32be11383eecb,https://github.com/pytorch/fairseq/commit/14ee059a36092a4216ec38601ca32be11383eecb,"Dataloading fixes (#1189)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1189

Reviewed By: ngoyal2707

Differential Revision: D22052683

Pulled By: myleott

fbshipit-source-id: afdfda291907ad4441af51cfc9e44f1bd01ea696",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1317,Rohit Kopparthy,rkopparthy65@fb.com,2020-06-16 12:15:00-07:00,3c16b002b94f22a62cbc257b5d339b6d9f4d5a07,https://github.com/pytorch/fairseq/commit/3c16b002b94f22a62cbc257b5d339b6d9f4d5a07,"Scripting ConvTransformer

Summary: This diff is building off of D21986239 to script the ConvTransformer Model instead of the VggTransformer. The changes made in data_utils.py were copied over from D20443519. A new file called test_convtransformer.py was added to test scripting the model. The scripted model compiles and also produces the same output as before scripting.

Reviewed By: myleott

Differential Revision: D22022654

fbshipit-source-id: 8f5a36a9af391142b468818650be3af218235fc2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1318,Yongqiang Wang,yqw@fb.com,2020-06-17 13:45:19-07:00,c294e2fcfb299290e53023d1e7cf3a53d27195a4,https://github.com/pytorch/fairseq/commit/c294e2fcfb299290e53023d1e7cf3a53d27195a4,"print out all the CUDA environment information (including name, memory size,

Summary:
Recently, we found there are more and more likely that different
generations (V100 vs P100) / memory size (16GB, 32GB) GPUs are mixed up in
training, while the users do not even know about this. Print out this message
can be helpful for debugging

Reviewed By: myleott

Differential Revision: D21782630

fbshipit-source-id: 7e1075e1b928d969594bbee92275a819cf1a0877",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1319,Gil Keren,gilkeren@fb.com,2020-06-17 18:48:29-07:00,82f99df8e4f54cc91ec1dffa7e5cc506c91fb465,https://github.com/pytorch/fairseq/commit/82f99df8e4f54cc91ec1dffa7e5cc506c91fb465,"Gradually releasing the restrictions on data-buffer-size

Summary: the buffer was a suspect in creating some everstore overload, therefore was restricted in D21804332. But since it's part in those problems was inconclusive, and the everstore read limit was increased for the speech group, gradually increasing it back.

Differential Revision: D22076534

fbshipit-source-id: cb01d50d4df5843b86f7d730e1805a88ea3f41d8",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1320,Wei Ho,weiho@fb.com,2020-06-18 18:36:34-07:00,d617c292f8101e8c62d6b0660bc321ef3e43a138,https://github.com/pytorch/fairseq/commit/d617c292f8101e8c62d6b0660bc321ef3e43a138,"Apply black formatter to fairseq_cli/train.py

Differential Revision: D22125634

fbshipit-source-id: a05f483ac4b564f5d7a21f5ae3605615e7fcd263",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1321,Rohit Kopparthy,rkopparthy65@fb.com,2020-06-19 10:50:22-07:00,8d8d773c5724f4a153216184821e512906c86149,https://github.com/pytorch/fairseq/commit/8d8d773c5724f4a153216184821e512906c86149,"Set EncoderOut Attributes to None instead of torch.empty(0)

Summary: The ConvTransformer model throws an error during training because of certain attributes having been changed to torch.empty(0) instead of None to meed torchscript type requirements. Existing assertion checks only check if these attributes are not None, rather than not torch.empty(0). To fix this, types have been modified to Optional types and allowed to stay as None like before.

Reviewed By: zhengwy888

Differential Revision: D22115126

fbshipit-source-id: de3c7b64c5e7142c860a354f778b8b818a7b0bb8",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1322,Mandeep Baines,msb@fb.com,2020-06-19 16:21:53-07:00,6f6461b81ac457b381669ebc8ea2d80ea798e53a,https://github.com/pytorch/fairseq/commit/6f6461b81ac457b381669ebc8ea2d80ea798e53a,"Add tracepoints (#1192)

Summary:
There is no overhead when the profiling is not enabled.
When running using profile.py, I measure an overhead of 3%.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1192

Reviewed By: sidgoyal78

Differential Revision: D22102341

Pulled By: msbaines

fbshipit-source-id: ffddb9cceb853df88db34195be18bae7723d4c98",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1323,Myle Ott,myleott@fb.com,2020-06-22 06:47:35-07:00,3ea511d89936caab6e7bf605366152b96cd95bcd,https://github.com/pytorch/fairseq/commit/3ea511d89936caab6e7bf605366152b96cd95bcd,"Revert Dataloader changes

Summary:
D22052683 may have introduced a memory leak, revert those parts for now

The original motivation is described here: https://github.com/pytorch/fairseq/issues/2168.
Previously I/O was bursty when training with large update frequency.
This meant to even it out, but possibly introduced a memory leak.

More context on the change can be found here: https://github.com/pytorch/fairseq/issues/2168

Reviewed By: yqwangustc

Differential Revision: D22156157

fbshipit-source-id: 390ff39bc3e268d6312971768c34fe44d4bd84b7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1324,Joshua Meier,jmeier@fb.com,2020-06-22 10:01:21-07:00,8eb9123f560d32940f96a01369d61c1684dce085,https://github.com/pytorch/fairseq/commit/8eb9123f560d32940f96a01369d61c1684dce085,"Patch masked_lm memory leak on GPUs (#1195)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes memory leak in masked_lm criterion.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1195

Reviewed By: myleott

Differential Revision: D22155285

Pulled By: joshim5

fbshipit-source-id: 9414e307e1e2d2a9225884dc94aae964a1627682",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1325,Mike Ruberry,mruberry@fb.com,2020-06-22 11:55:51-07:00,320bf8cf963772fe09429b8fb9990371a0e22db3,https://github.com/pytorch/fairseq/commit/320bf8cf963772fe09429b8fb9990371a0e22db3,"Updates full to no longer use deprecated integer fill_value type inference

Summary:
In PyTorch 1.5 using an integer fill_value and not setting the dtype or out kwarg with torch.full was deprecated, and soon will throw a runtime error. In the future, torch.full will infer its dtype from the fill_value, and these would produce integer, not float, tensors. This update maintains the current behavior.

Created from Diffusion's 'Open in Editor' feature.

Reviewed By: myleott

Differential Revision: D22161456

fbshipit-source-id: b5d687e4de83dba6e76cae6e61b5106bf5b320db",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1326,Tony Lekhtman,tony.lek1995@gmail.com,2020-06-22 15:27:34-07:00,a9cb84df689d4e9343085d2434087c1b308a68a7,https://github.com/pytorch/fairseq/commit/a9cb84df689d4e9343085d2434087c1b308a68a7,"Update hub_utils.py (#2253)

Summary:
fix bug for print_alignment

# Before submitting

- [ V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?   not relevant
- [ ] Did you write any new necessary tests?  not relevant

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1880 .

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2253

Reviewed By: huihuifan

Differential Revision: D22162948

Pulled By: myleott

fbshipit-source-id: 3ec5508506184a9effa330fbcd43ffe917b533c6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1327,Yi-Hsiu Liao,yihsiu_liao@Yi-Hsius-MacBook-Pro.local,2020-06-22 18:24:48-07:00,e187f6e116c3926cfa693ee6440d277843d0972a,https://github.com/pytorch/fairseq/commit/e187f6e116c3926cfa693ee6440d277843d0972a,"add maybe_no_sync for multilingual_translation task (#2238)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x ] Did you write any new necessary tests?

## What does this PR do?
This PR reduces unnecessary communication overhead between GPUs since we only need to sync up once for all lang-pairs. We see significant training speedup especially with large number of lang-pairs.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2238

Reviewed By: pipibjc

Differential Revision: D22149086

Pulled By: myleott

fbshipit-source-id: 6fff09e5a51b49bdcf5bc3986c0719b19d31c0a9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1328,gvskalyan,gvskalyan2@gmail.com,2020-06-22 18:39:41-07:00,88c58b6718292be10311afc0aa7f829dc3fc0c27,https://github.com/pytorch/fairseq/commit/88c58b6718292be10311afc0aa7f829dc3fc0c27,"Preprocess dict number (#2228)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2227 .

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2228

Reviewed By: huihuifan

Differential Revision: D22163032

Pulled By: myleott

fbshipit-source-id: a5afbfca2d9a11563026f47cd246654e131d92fb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1329,Ronan Riochet,ronanr@fb.com,2020-06-22 18:40:50-07:00,d5d2cf3cd5f25c8a413328891c353c0379c22442,https://github.com/pytorch/fairseq/commit/d5d2cf3cd5f25c8a413328891c353c0379c22442,"Add timeout kwarg to EpochBatchIterator (#2261)

Summary:
Add an optional ```timeout``` argument to ```EpochBatchIterator```.

I need it to fix this issue: https://github.com/pytorch/pytorch/issues/2474

I could do something more general, allowing one to pass ```**dataloader_kwargs``` to ```torch.utils.data.DataLoader```, if you think it's worth.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2261

Reviewed By: huihuifan

Differential Revision: D22162936

Pulled By: myleott

fbshipit-source-id: 959b408a53356c19c04fc5ae94aad5f164a32dcd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1330,Myle Ott,myleott@fb.com,2020-06-22 18:53:19-07:00,d0ccc3e02e1a9015d05cade8dfc61896948275c7,https://github.com/pytorch/fairseq/commit/d0ccc3e02e1a9015d05cade8dfc61896948275c7,"Add FairseqDecoder.reorder_incremental_state_scripting for TorchScript (#1190)

Summary:
The main changes are in fairseq_incremental_decoder.py. I made the base `reorder_incremental_state` implementation a no-op and instead we expect callers (e.g., SequenceGenerator) to call `reorder_incremental_state_scripting`.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1190

Test Plan:
I ran unit tests both in PyTorch 1.5 and nightly (1.6).

I also tested some of the pretrained translation models, but it'd be good to test with some prod runs.

Reviewed By: jhcross

Differential Revision: D22095614

Pulled By: myleott

fbshipit-source-id: 484b8d47b4feda4efe52233a3d46a207d0816766",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1331,Marco Gaido,mgaido@fbk.eu,2020-06-23 06:46:50-07:00,a12c5c5de896390775ac45addaa4f4f90534d9b7,https://github.com/pytorch/fairseq/commit/a12c5c5de896390775ac45addaa4f4f90534d9b7,"Add max position params to speech recognition (#1783)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1782.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1783

Reviewed By: okhonko

Differential Revision: D21663633

Pulled By: myleott

fbshipit-source-id: 5f3b4b7df83e27d866efb489daeffb3b38a66f38",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1332,Myle Ott,myleott@fb.com,2020-06-24 09:54:46-07:00,da94e58c703866236b29242ae413146be69fe94f,https://github.com/pytorch/fairseq/commit/da94e58c703866236b29242ae413146be69fe94f,"TPU support for Translation (#2245)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2245

Reviewed By: ngoyal2707

Differential Revision: D22070745

Pulled By: myleott

fbshipit-source-id: e43a96a585366b10d997a12522e8cd6496294ad2",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1333,Myle Ott,myleott@fb.com,2020-06-24 10:03:35-07:00,f0a61a2774aff2efbc1adb0b5daee346a8401605,https://github.com/pytorch/fairseq/commit/f0a61a2774aff2efbc1adb0b5daee346a8401605,"Miscellaneous fixes (#1196)

Summary:
Incorporate several fixes, incl. from OSS contributors:
- fix model argument in sequence generator in semisupervised_translation.py
- fix aggregate logging in semisupervised_translation.py
- Fix EOS token in multilingual_denoising
- Handle missing eos_idx in data_utils.collate_tokens
- Better OOM handling for single-GPU training
- fix prepend_bos argument in translation_from_pretrained_bart.py …
- Fix eos_idx in multilingual_denoising
- Small logging fixes
- Fix fb_hub on PyTorch 1.6
- Better variable names
- Add support for model parallel to interactive.py
- Use `//` operator to fix Integer division warning
- Set default `--clip-norm=0.0`
- Cleanup some binaries in root directory

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1196

Reviewed By: ngoyal2707

Differential Revision: D22162202

Pulled By: myleott

fbshipit-source-id: 835b0c0ad9246827f9d915fdb4e89d7b5be2475d",40,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1334,Ning Dong,dnn@fb.com,2020-06-24 22:29:58-07:00,d2b5265a60c4016d9fc3cbd55ad983dd86f1aa6f,https://github.com/pytorch/fairseq/commit/d2b5265a60c4016d9fc3cbd55ad983dd86f1aa6f,"Merge FBSequenceGenerator & SequenceGenerator

Summary: See discussion in D20995796 (https://github.com/pytorch/fairseq/commit/4725487bbc3bdee89c45ced0a8664cffd8e1ab01). Will merge 2 diffs if this looks good to you myleott jhcross

Reviewed By: myleott

Differential Revision: D21214974

fbshipit-source-id: ebb59b0491a8c209bed2420a0cd94e9c41d05f2e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1335,Belinda Li,belindali@fb.com,2020-06-27 16:10:14-07:00,894ae64858b62927d849c0fbc05e8f55d680a4f1,https://github.com/pytorch/fairseq/commit/894ae64858b62927d849c0fbc05e8f55d680a4f1,"Add Linformer to internal fairseq

Summary: Adding linformer

Reviewed By: myleott

Differential Revision: D22253918

fbshipit-source-id: 0bb86dddae1be09450544cb25530400e914c640f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1336,Daniel Adkins,dadkins@fb.com,2020-06-30 12:51:52-07:00,a87cafda718c7706e6f1694f0d39fc589ed2b264,https://github.com/pytorch/fairseq/commit/a87cafda718c7706e6f1694f0d39fc589ed2b264,"update fairseq binarizer to use PathManager

Summary:
Currently, fairseq binarizer does not work with Manifold files, making it incompatible with some internal procedures. This change preserves the old functionality while allowing Manifold files to be passed into binarizer functions.

motivated by theweiho: ""I think we should change Binarizer to use PathManager so that it can handle either Manifold path or POSIX path"" (D22241626)

Reviewed By: akinh

Differential Revision: D22293525

fbshipit-source-id: d1bf4f8b50dda6a9214ee2fbe45e112ca9628f60",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1337,Myle Ott,myleott@fb.com,2020-07-06 08:22:25-07:00,fc29aab2030055585a087727d93a2083edbb1678,https://github.com/pytorch/fairseq/commit/fc29aab2030055585a087727d93a2083edbb1678,"Fix model parallel training after quantization/interactive.py changes (#1202)

Summary:
- fix model parallel training after output_projection changes
- fix training with non-vocab parallel criterions
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1202

Reviewed By: ngoyal2707

Differential Revision: D22266462

Pulled By: myleott

fbshipit-source-id: c7bb9a95c01f5fdaf415a709a93bacb15336271c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1338,Myle Ott,myleott@fb.com,2020-07-07 10:21:30-07:00,97ca0c022c74232690aae1c67f8d535602d071be,https://github.com/pytorch/fairseq/commit/97ca0c022c74232690aae1c67f8d535602d071be,"Fix data hang with buffered iterator (#1206)

Summary:
According to Tom Birch: ""I think there's an issue with torch.utils.data.dataloader._MultiProcessingDataLoaderIter when next(...) is supposed to raise StopIteration it just blocks indefinitely instead."" This PR is a workaround that fixes the issue.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1206

Reviewed By: froody

Differential Revision: D22411150

Pulled By: myleott

fbshipit-source-id: 7cdfa67cf55e9cff81cf7d4904f1d38bfa36a0d0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1339,Siddharth Shah,siddshah@fb.com,2020-07-07 16:14:11-07:00,578164a0ef642307bc1bf4e63dd29a8f70a176ea,https://github.com/pytorch/fairseq/commit/578164a0ef642307bc1bf4e63dd29a8f70a176ea,"0 warmup in tri stage lr scheduler

Summary:
Current code fails due to division by zero. This diff allows for zero warmup in
tri stage scheduler.

Reviewed By: myleott

Differential Revision: D22416482

fbshipit-source-id: dedb41ac141528314dc86cd73b8b67e699bf457b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1340,Gil Keren,gilkeren@fb.com,2020-07-07 16:41:05-07:00,7816946ff92db0a0d0ec89faedd986e3b83d96d5,https://github.com/pytorch/fairseq/commit/7816946ff92db0a0d0ec89faedd986e3b83d96d5,"Fix memory leak with small data-buffer-size

Summary:
As part of zhengwy888's debugging of a memory leak, he suggested that trimming the number of batches in pyspeech's train.py may cause the BufferedIterator to leave some batches in the queue, causing a memory leak.

Therefore, propagating `take` to the buffered iterator, which should prevent the consumer thread from hanging on `queue.put`.

Reviewed By: myleott

Differential Revision: D22405263

fbshipit-source-id: 80f40a355652016af4ba8c386b623cb0552b1928",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1341,Wei Ho,weiho@fb.com,2020-07-08 00:23:48-07:00,9f92b05e2a10f1c559d44dc1c264af31723d1d76,https://github.com/pytorch/fairseq/commit/9f92b05e2a10f1c559d44dc1c264af31723d1d76,"TorchElastic for fairseq FBTranslate

Summary: Use TorchElastic for multi-node, multi-GPU training

Reviewed By: cndn

Differential Revision: D22083634

fbshipit-source-id: 3673308671b0bc985b6012ee5327d604d995409f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1342,Myle Ott,myleott@fb.com,2020-07-08 13:04:55-07:00,d73e543e3853bb813d8f7955a06ce19359810707,https://github.com/pytorch/fairseq/commit/d73e543e3853bb813d8f7955a06ce19359810707,"Update LinformerSentenceEncoder to inherit from TransformerSentenceEncoder

Summary:
It seems we can make this work by setting `compress_layer` in `build_transformer_sentence_encoder_layer` and adding an ""init_fn"" callback.

Doing this refactoring now since the stacked diff (D22048889) broke Linformer training, so safer to inherit from TransformerSentenceEncoder directly.

Reviewed By: ngoyal2707

Differential Revision: D22411012

fbshipit-source-id: d4ecb71eedd6ddf49abbb1e700d0f2af24e39e5a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1343,m_fomicheva,mari.fomicheva@gmail.com,2020-07-08 13:04:55-07:00,28876638114948711fd4bd4e350fdd6809013f1e,https://github.com/pytorch/fairseq/commit/28876638114948711fd4bd4e350fdd6809013f1e,"Implemented applying dropout at inference time (#2308)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2308

Implemented Monte Carlo dropout. Added README to reproduce the results from our paper
that applies this idea for unsupervised quality estimation of NMT (joint work of Facebook AI and the University of Sheffield):

Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, Lucia Specia. Unsupervised Quality Estimation for Neural Machine Translation. Accepted to TACL

Retaining dropout at test time is not possible in the current code base. The statement
```
if not self.retain_dropout:
  model.eval()
```
in `SequenceGenerator` does not have any effect, since model `training` attribute is already set to False by the method `make_generate_fast_`, which is applied before initializing `SequenceGenerator` in `generate.py`. `make_generate_fast_` throws an exception when trying to set `training` to True after its application. Also, if I am not mistaken `self.training=True` can have other effects, so setting it to True only for the purpose of retaining dropout at test time might be confusing. I propose an alternative implementation where `retain_dropout` is an attribute of FairseqModel class.

# Before submitting

- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [Y] Did you make sure to update the docs?
- [Y] Did you write any new necessary tests?

## What does this PR do?
New feature.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2151

Reviewed By: ngoyal2707

Differential Revision: D22048889

Pulled By: myleott

fbshipit-source-id: 0d0d4784a7314fc7a45b76341fd3b8232b3e2cf0",39,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestInferenceDropout(unittest.TestCase):'],[],['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['self.transformer_model.encoder.dropout_module.apply_during_inference', 'self.transformer_model.decoder.dropout_module.apply_during_inference', 'layer.dropout_module.apply_during_inference', 'not self.transformer_model.encoder.dropout_module.apply_during_inference', 'not self.transformer_model.decoder.dropout_module.apply_during_inference', 'not layer.dropout_module.apply_during_inference', 'not layer.dropout_module.apply_during_inference', 'self.transformer_model.encoder.dropout_module.training', 'layer.dropout_module.training', 'not self.transformer_model.decoder.dropout_module.training', 'not layer.dropout_module.training', 'self.transformer_model.encoder.dropout_module.apply_during_inference', 'not self.transformer_model.decoder.dropout_module.apply_during_inference', 'not layer.dropout_module.apply_during_inference']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1344,Mandeep Baines,msb@fb.com,2020-07-08 14:46:02-07:00,16e9661bd968cf66b02d7870c038d7219da3a5b9,https://github.com/pytorch/fairseq/commit/16e9661bd968cf66b02d7870c038d7219da3a5b9,"avoid fp16 unscales and multiply_grads (#1201)

Summary:
The object of this patch is to avoid fp16 unscale calls which can
potentially under/over-flow by 1) scaling grad_narm instead of
unscaling grads before calculating grad_norm and 2) using scale
argument to step (if supported by optimizer). By letting the
optimizer scale we avoid multiply_grads (saving on GPU compute/mem).
We also get better precision since the unscale occurs in the kernel
resulting in an FP32 unscaled grad instead of an FP16 unscaled grad.

A side-effect of this patch is a noticeable WPS win due to a
multi-tensor kernel being used for grad_norm and because we
avoid multiply_grads.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1201

Test Plan:
Verified grad_norm and loss before and after.

Before:

epoch 001 | loss 19.506 | ppl 744403 | wps 13966.7 | ups 0.21 | wpb 65536 | bsz 128 | num_updates 50 | lr 6.34875e-06 | gnorm 8.173 | loss_scale 10 | train_wall 250 | wall 259

After:

epoch 001 | loss 19.506 | ppl 744363 | wps 14003 | ups 0.21 | wpb 65536 | bsz 128 | num_updates 50 | lr 6.34875e-06 | gnorm 8.173 | loss_scale 10 | train_wall 250 | wall 258

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D22251842

Pulled By: msbaines

fbshipit-source-id: e6d82cdd3c95e7770835abe054db4b50e6ad569e",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1345,Aditya Pillai,adityapillai@fb.com,2020-07-09 13:20:51-07:00,5d88d379cab13c8351b39341306d7a30a2a8acb8,https://github.com/pytorch/fairseq/commit/5d88d379cab13c8351b39341306d7a30a2a8acb8,"bug fix: use cls.load_dictionary for multilingual translation

Summary:
Currently, multilingual translation imports Dictionary and calls its load function.

However, this does not permit extending the class with a different load_dictionary function to modify its behavior.

Reviewed By: myleott, chtran

Differential Revision: D22441356

fbshipit-source-id: b0ef159182b15adb479b117581ddcd2f65724980",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1346,Myle Ott,myleott@fb.com,2020-07-14 14:11:59-07:00,ffecb4e3496379edf5ecae1483df5b7e0886c264,https://github.com/pytorch/fairseq/commit/ffecb4e3496379edf5ecae1483df5b7e0886c264,"Small fixes (#1215)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1215

Reviewed By: ngoyal2707, msbaines

Differential Revision: D22514719

Pulled By: myleott

fbshipit-source-id: 5f15ba501fd66af1eb49b5702aff940f06c3d91f",26,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1347,Mandeep Baines,msb@fb.com,2020-07-15 16:07:02-07:00,a541b19d853cf4a5209d3b8f77d5d1261554a1d9,https://github.com/pytorch/fairseq/commit/a541b19d853cf4a5209d3b8f77d5d1261554a1d9,"Add dummy task for translation benchmarking (#1212)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1212

Test Plan:
python train.py \
    -a transformer \
    --clip-norm 0.4 --optimizer adam --lr 0.001 \
    --dropout 0.0 \
    --decoder-layers 7 \
    --encoder-layers 7 \
    --encoder-ffn-embed-dim 2048 \
    --decoder-ffn-embed-dim 2048 \
    --encoder-embed-dim 1024 \
    --decoder-embed-dim 1024 \
    --max-tokens 8192 \
    --criterion cross_entropy --max-update 50 \
    --attention-dropout 0.0 \
    --adam-betas '(0.9, 0.98)' \
    --disable-validation --no-save \
    --task dummy_mt

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D22484873

Pulled By: msbaines

fbshipit-source-id: bc61165ab91290d0b6aa2077c968ab537bce8a6a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1348,Mandeep Baines,msb@fb.com,2020-07-15 17:12:24-07:00,9c21a715d66e7833b0af80895c0550d555f2fd0d,https://github.com/pytorch/fairseq/commit/9c21a715d66e7833b0af80895c0550d555f2fd0d,"Fix regression in memory-efficient-fp16 (#1216)

Summary:
The fused_adam optimizer divides by the scale while our logic
multiplies by the scale. I'm surprised this even worked. The
first few iterations had nearly similar loss with the old
code and even converged.

However, Jun Ru noticed that the loss are very different after
more iterations.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1216

Reviewed By: myleott, shruti-bh

Differential Revision: D22536377

Pulled By: msbaines

fbshipit-source-id: 9328a1764a1895572c18567f99bee3330f25179e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1349,Mandeep Baines,msb@fb.com,2020-07-16 08:08:37-07:00,84896af72c01f2b5f7e3c1c65ed28be697a3b32f,https://github.com/pytorch/fairseq/commit/84896af72c01f2b5f7e3c1c65ed28be697a3b32f,"Fix memory-efficient-fp16 when using update_freq other than 1 (#1219)

Summary:
Tested the following model and verified that gnorms and losses match
the following commit:

commit 3b7cf7558499cf70690913b76f35d0bc755e62ae
Author: m_fomicheva <mari.fomicheva@gmail.com>
Date:   Wed Jul 8 13:04:55 2020 -0700

The loss and gnorm are identical to the number of digits reported in the logs
and the ppl is very close to many signficant digits.

Thanks again to Jun Ru for reporting.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1219

Test Plan:
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4

Before (commit 3b7cf755):

2020-07-15 12:17:28 | INFO | train_inner | epoch 001:     45 / 3151 loss=19.083, ppl=555252, wps=7165.8, ups=0.22, wpb=32768, bsz=64, num_updates=41, lr=5.22398e-06, gnorm=6.895, loss_scale=8, train_wall=5, wall=208
2020-07-15 12:17:33 | INFO | train_inner | epoch 001:     46 / 3151 loss=19.042, ppl=539620, wps=7176.6, ups=0.22, wpb=32768, bsz=64, num_updates=42, lr=5.34895e-06, gnorm=6.662, loss_scale=8, train_wall=5, wall=213
2020-07-15 12:17:37 | INFO | train_inner | epoch 001:     47 / 3151 loss=18.908, ppl=492042, wps=7188.8, ups=0.22, wpb=32768, bsz=64, num_updates=43, lr=5.47393e-06, gnorm=6.231, loss_scale=8, train_wall=5, wall=217
2020-07-15 12:17:42 | INFO | train_inner | epoch 001:     48 / 3151 loss=18.894, ppl=487224, wps=7192, ups=0.22, wpb=32768, bsz=64, num_updates=44, lr=5.5989e-06, gnorm=6.078, loss_scale=8, train_wall=5, wall=222
2020-07-15 12:17:47 | INFO | train_inner | epoch 001:     49 / 3151 loss=18.829, ppl=465781, wps=7182.5, ups=0.22, wpb=32768, bsz=64, num_updates=45, lr=5.72388e-06, gnorm=5.819, loss_scale=8, train_wall=5, wall=226
2020-07-15 12:17:51 | INFO | train_inner | epoch 001:     50 / 3151 loss=18.752, ppl=441564, wps=7185.4, ups=0.22, wpb=32768, bsz=64, num_updates=46, lr=5.84885e-06, gnorm=5.521, loss_scale=8, train_wall=5, wall=231

After:

2020-07-15 15:13:10 | INFO | train_inner | epoch 001:     45 / 3151 loss=19.083, ppl=555249, wps=7220.5, ups=0.22, wpb=32768, bsz=64, num_updates=41, lr=5.22398e-06, gnorm=6.895, loss_scale=8, train_wall=5, wall=207
2020-07-15 15:13:14 | INFO | train_inner | epoch 001:     46 / 3151 loss=19.042, ppl=539617, wps=7216.3, ups=0.22, wpb=32768, bsz=64, num_updates=42, lr=5.34895e-06, gnorm=6.662, loss_scale=8, train_wall=5, wall=212
2020-07-15 15:13:19 | INFO | train_inner | epoch 001:     47 / 3151 loss=18.908, ppl=492041, wps=7220.8, ups=0.22, wpb=32768, bsz=64, num_updates=43, lr=5.47393e-06, gnorm=6.231, loss_scale=8, train_wall=5, wall=216
2020-07-15 15:13:24 | INFO | train_inner | epoch 001:     48 / 3151 loss=18.894, ppl=487228, wps=7229.4, ups=0.22, wpb=32768, bsz=64, num_updates=44, lr=5.5989e-06, gnorm=6.078, loss_scale=8, train_wall=5, wall=221
2020-07-15 15:13:28 | INFO | train_inner | epoch 001:     49 / 3151 loss=18.829, ppl=465783, wps=7231.2, ups=0.22, wpb=32768, bsz=64, num_updates=45, lr=5.72388e-06, gnorm=5.819, loss_scale=8, train_wall=5, wall=225
2020-07-15 15:13:33 | INFO | train_inner | epoch 001:     50 / 3151 loss=18.752, ppl=441559, wps=7224.5, ups=0.22, wpb=32768, bsz=64, num_updates=46, lr=5.84885e-06, gnorm=5.521, loss_scale=8, train_wall=5, wall=230

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D22560914

Pulled By: msbaines

fbshipit-source-id: f2fdc3daa46de0b75f26cb4d5712e92d1a820d60",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1350,Yuqing Tang,yuqtang@fb.com,2020-07-16 09:32:44-07:00,c0b52268539b861d93fde2a5931d909028a085b4,https://github.com/pytorch/fairseq/commit/c0b52268539b861d93fde2a5931d909028a085b4,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: new datasets (#1205)

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Major work is in fairseq/data/multilingual
   - fairseq/data/multilingual/sampled_multi_dataset.py to enable sampling and virtual data sizes
   - fairseq/data/multilingual/sampled_multi_epoch_dataset.py to enable virtual epoch data size to start training without going through the whole data (which reduces the loading from 1.5 hours into <30 seconds)
    - [next diff] fairseq/data/multilingual/multilingual_data_manager.py to support a few sophisticated multilingual data combinations
    - [next diff] fairseq/data/multilingual/sampling_method.py to support basic sampling functions
- [next diff] A new task to glue all things together: fairseq/tasks/translation_multi_simple_epoch.py
- Minor changes to
    - fairseq/data/language_pair_dataset.py to (1) have language IDs in the batch if they are set, (2) allow a preset max_size of batch; (2) corresponding changes to fairseq/data/data_utils.py
    - [next diff] fairseq/data/denoising_dataset.py to (1) allow additional transformation; (2) allow a preset max_size of batch;
    - [next diff] fairseq/data/iterators.py to allow dynamic batch sampler
    - [next diff] fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1205

Test Plan:
buck test mode/dev //deeplearning/projects/fairseq-py:test_cpu -- 'test_translation_multi_simple_epoch \(tests\.test_binaries\.TestTranslation\)'

https://our.intern.facebook.com/intern/testinfra/testrun/3659174727046259

Started new test run: https://our.intern.facebook.com/intern/testinfra/testrun/3659174727046259
      ✓ deeplearning/projects/fairseq-py:test_cpu - test_translation_multi_simple_epoch (tests.test_binaries.TestTranslation) 331.967 1/1 (passed)
Finished test run: https://our.intern.facebook.com/intern/testinfra/testrun/3659174727046259
Summary (total time 352.88s):
  PASS: 1
  FAIL: 0
  SKIP: 0
  FATAL: 0
  TIMEOUT: 0
  OMIT: 0

Reviewed By: myleott

Differential Revision: D22463947

Pulled By: tangyuq

fbshipit-source-id: e430c040231035af73141dc736960bd972bd4b6e",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1351,Yuqing Tang,yuqtang@fb.com,2020-07-16 09:32:44-07:00,033daef0fc4fd8b44ae350a4ce2c7da299bbeb7f,https://github.com/pytorch/fairseq/commit/033daef0fc4fd8b44ae350a4ce2c7da299bbeb7f,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: multiligual dataset manager

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Major work is in fairseq/data/multilingual
    -  fairseq/data/multilingual/multilingual_data_manager.py to support a few sophisticated multilingual data combinations
    -  fairseq/data/multilingual/sampling_method.py to support basic sampling functions

Reviewed By: pipibjc

Differential Revision: D22483471

fbshipit-source-id: 3d9d2643877a29333915975020e419508887b3ae",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1352,Yuqing Tang,yuqtang@fb.com,2020-07-16 09:32:44-07:00,e52d071ee8b24fd371b5235938abeb1d0ae8e1ec,https://github.com/pytorch/fairseq/commit/e52d071ee8b24fd371b5235938abeb1d0ae8e1ec,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: new multiligual task

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- A new task to glue all things together: fairseq/tasks/translation_multi_simple_epoch.py
- Minor changes to
    - fairseq/data/iterators.py to allow dynamic batch sampler
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: pipibjc

Differential Revision: D22483484

fbshipit-source-id: 283b67e538508f330b0968609b7dae64d26bea05",7,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1353,Myle Ott,myleott@fb.com,2020-07-16 09:58:43-07:00,77df83ab6e07468dbc8b31a5b84766f7fedd250d,https://github.com/pytorch/fairseq/commit/77df83ab6e07468dbc8b31a5b84766f7fedd250d,"Consolidate distributed init code into distributed_utils.call_main (#1218)

Summary:
We use `distributed_utils.call_main` in most of the other CLI tools (e.g., generate.py, eval_lm.py), but not train.py.

The only place where they're different is that train.py supports TPUs and the `after_distributed_init_fn` hook. We can add that support to `distributed_utils.call_main` and merge them.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1218

Reviewed By: jhcross

Differential Revision: D22556771

Pulled By: myleott

fbshipit-source-id: 4f7110155f5f5d96905ef0bd17a4aa243ec8c443",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1354,Duc Le,duchoangle@fb.com,2020-07-16 10:55:49-07:00,75d354c92ba6d96d45a8d6fb6f28183817efe203,https://github.com/pytorch/fairseq/commit/75d354c92ba6d96d45a8d6fb6f28183817efe203,"NNLM training in PySpeech

Summary:
Enable support for NNLM training in PySpeech. This implementation slightly modifies Fairseq's `LanguageModelingTask` in a few ways:

1. `source` and `input` used during training are slightly different (see `_maybe_add_bos` under `PySpeechLMDataset`).
2. The underlying model is `PySpeechEncoderModel` instead of `FairseqDecoder`. This lets us interface more easily with PySpeech, and the jitted model can easily be used in C++.

Reviewed By: jay-mahadeokar

Differential Revision: D22077479

fbshipit-source-id: 4918b26ba78de8786870060ada0bc3d3a28d64b0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1355,James Cross,jcross@fb.com,2020-07-16 17:43:42-07:00,3655cf266e32a2272d6deac6069a594977880084,https://github.com/pytorch/fairseq/commit/3655cf266e32a2272d6deac6069a594977880084,"optional limit on total training time (#2333)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2333

This change adds a new option (`--stop-time-hours`) which if specified limits the total training time to that number of hours. In order to stop training within the inner training loop (after the first update exceeding the time limit) the starting time is stored on the trainer.

In addition, in order to persist the training time when when restoring from checkpoints (important because training runs are sometimes killed due to resource constraints), training time already completed is stored as extra state in the checkpoints (though this change is backward compatible with existing checkpoints).

Reviewed By: myleott

Differential Revision: D22573166

fbshipit-source-id: 01c59274a1c196acc8a3a0243814167e1d368b1a",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1356,Stanislau Hlebik,stash@fb.com,2020-07-17 17:07:23-07:00,7ea5e3b341d9ccf0005ba7695f1b9c39a11a145e,https://github.com/pytorch/fairseq/commit/7ea5e3b341d9ccf0005ba7695f1b9c39a11a145e,"remediation of S205607

fbshipit-source-id: 5113fe0c527595e4227ff827253b7414abbdf7ac",8,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1357,Stanislau Hlebik,stash@fb.com,2020-07-17 17:07:23-07:00,698e3b91ffa832c286c48035bdff78238b0de8ae,https://github.com/pytorch/fairseq/commit/698e3b91ffa832c286c48035bdff78238b0de8ae,"remediation of S205607

fbshipit-source-id: 798decc90db4f13770e97cdce3c0df7d5421b2a3",8,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1358,Myle Ott,myleott@fb.com,2020-07-20 08:24:30-07:00,93f5128509278f425afb6bcf0da574c0af0e0c16,https://github.com/pytorch/fairseq/commit/93f5128509278f425afb6bcf0da574c0af0e0c16,"Misc fixes (#2342)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2342

Reviewed By: ngoyal2707

Differential Revision: D22601110

Pulled By: myleott

fbshipit-source-id: 7a704c07d507692f274c31ec74b090134fa9dee3",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1359,Wei Ho,weiho@fb.com,2020-07-20 19:57:29-07:00,c1e734b2dd7024044c8dee551620146e4f872ad4,https://github.com/pytorch/fairseq/commit/c1e734b2dd7024044c8dee551620146e4f872ad4,"Ensure checkpoints are properly saved when hitting stop time limit

Summary:
Context: https://fburl.com/tasks/3nhdm1rv and https://fburl.com/test/4i6icfbd

This should fix test_e2e_base_training_wo_prepare_data breakage due to D22598579: when stop time limit https://fburl.com/diffusion/rrp4jst5 gets triggered, a checkpoint isn't saved as it should in https://fburl.com/diffusion/hbn0c6o5. The error isn't related to Manifold usage in export_ensemble - that's just the first place where we notice that no checkpoint was written.

So for toy tests that finish quickly, it's possible for training to end before any checkpoint has been saved.

It was affecting dev-nosan but not opt probably because opt training finishes quickly enough to not hit the stop time limit?

Reviewed By: akinh

Differential Revision: D22639199

fbshipit-source-id: ec4da15bb14e14c2066af6946d7a34db333178eb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1360,Jun Ru Anderson,andersonic@fb.com,2020-07-23 17:05:40-07:00,84e32315af7b4a479450a1e65b9e277226d75576,https://github.com/pytorch/fairseq/commit/84e32315af7b4a479450a1e65b9e277226d75576,"Move DynamicLossScaler into its own file and separate opti… (#1221)

Summary:
…mizer and scaling logic

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Separates (some of) scaling logic from optimizer logic. This increases readability in its own right and also makes the addition of new scalers more straightforward

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1221

Reviewed By: myleott

Differential Revision: D22696726

Pulled By: andersonic

fbshipit-source-id: c3a19184de0f17e75766894286c20feacdb3e010",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1361,Chau Tran,chau@fb.com,2020-07-24 16:50:00-07:00,f448f36462a77036b814e740a44990f8c3ba8760,https://github.com/pytorch/fairseq/commit/f448f36462a77036b814e740a44990f8c3ba8760,"fix sentencepiece vocab processor behavior with unknown token

Summary: We don't want to use INVALID_ID=-1 for tokens not found in dictionary, use UNKNOWN=3 instead

Reviewed By: cndn

Differential Revision: D22480966

fbshipit-source-id: 79b232e70efe7a0336a149ae494b24590afe5ea0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1362,Yuqing Tang,yuqtang@fb.com,2020-07-24 22:28:34-07:00,4f618a758ccd6b1924508ccbfb32eaacc3ea11c5,https://github.com/pytorch/fairseq/commit/4f618a758ccd6b1924508ccbfb32eaacc3ea11c5,"A more general solution to strip symbols from generation output

Summary:
An attempt to get more general output stripping in generator.

Context:
In the mBART pull request: https://github.com/fairinternal/fairseq-py/commit/7fbd17a3a67e1e4950855d646d6f4dd9db76bc63, eos was introduced in sequence_generator.py for removing the the lang tokens. However it only serves the special case where eos is modified to be the language token ID.

Reviewed By: myleott

Differential Revision: D22668233

fbshipit-source-id: 9bdaaefb28508cb74d8b2f3bd99160646b442959",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1363,Yuqing Tang,yuqtang@fb.com,2020-07-27 18:09:22-07:00,108bb2560b1ec01524ba723bc7c69186875afa0a,https://github.com/pytorch/fairseq/commit/108bb2560b1ec01524ba723bc7c69186875afa0a,"Multilingual v1: [improvement] Reset size cache per epoch in sampled_multi_epoch_dataset.py to save memory

Summary: Reset size cache per epoch in sampled_multi_epoch_dataset.py to save memory

Reviewed By: akinh

Differential Revision: D22754902

fbshipit-source-id: 1001cf37f0f47a90ffd10295b48c3e5a77283bc8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1364,Aditya Pillai,adityapillai@fb.com,2020-07-28 14:53:45-07:00,df5bf427348f8a9ccd8dbfbd70362648b82b6ae5,https://github.com/pytorch/fairseq/commit/df5bf427348f8a9ccd8dbfbd70362648b82b6ae5,"Create safe num_updates access on retrying failed training

Summary:
The num_updates reference is unsafe since it is only defined in the for-loop without any assertion that the loop will execute at least once. We could either try to add this assertion to make it more clear as to what is happening, or we could set num_updates to some default value if the loop is never executed.

I'm open to doing either one.

Reviewed By: chtran

Differential Revision: D22598029

fbshipit-source-id: 4d20b6a415a783fe8c450f3998b7767861da8c06",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1365,Aditya Pillai,adityapillai@fb.com,2020-07-28 14:53:45-07:00,578ee3d456034f12af13e446bba9447844374356,https://github.com/pytorch/fairseq/commit/578ee3d456034f12af13e446bba9447844374356,"Use cls.load_dictionary for multilingual data manager

Summary:
In order to dynamically change the dictionary loaded, we extend a class and override the `load_dictionary` class function.

To preserve this behavior with `translation_multi_simple_epoch`, we can pass in the `load_dictionary` function from the task to the multilingual data manager. Once the dictionary is loaded and instantiated, there are only non-static class calls, which go through the custom dictionary object created with `load_dictionary`.

Reviewed By: chtran

Differential Revision: D22598008

fbshipit-source-id: 23d7a510fb695df81ebfe4f991e5b5e3db13a1bd",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1366,Yuqing Tang,yuqtang@fb.com,2020-07-31 22:40:05-07:00,ef8d62a8cd005f36cf26ac2d10baa12b2f8b6754,https://github.com/pytorch/fairseq/commit/ef8d62a8cd005f36cf26ac2d10baa12b2f8b6754,"Enable fairseq fblearner sweep backend to use manifold for checkpoint savings

Summary: Deffault fairseq fblearner sweep backend uses gluser to save checkoints which is slow for large models. This diff enables fairseq fblearner sweep to use manifold for checkpoint savings.

Reviewed By: akinh

Differential Revision: D22770343

fbshipit-source-id: ec5174e490c35f2c7d11ccf69f3b6e9adcd8ac7b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1367,Yuqing Tang,yuqtang@fb.com,2020-07-31 22:40:05-07:00,8b9eaacf6b2d502cd7886dd7bf702a46ab37f058,https://github.com/pytorch/fairseq/commit/8b9eaacf6b2d502cd7886dd7bf702a46ab37f058,"Fixed multilingual data manager in handling manifold sharding data

Summary: multilingual data manager did not handle manifold sharding data; this is a fix.

Reviewed By: akinh

Differential Revision: D22823795

fbshipit-source-id: 2f3293ff9a5c1db22ef20e29168d35c1518fef1b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1368,Yuqing Tang,yuqtang@fb.com,2020-08-03 15:29:02-07:00,488254c88030d4e1fbfb85dbb8a90c97256bf491,https://github.com/pytorch/fairseq/commit/488254c88030d4e1fbfb85dbb8a90c97256bf491,"Add total data size logging to sampled_multi_dataset

Summary: Add total data size logging to help debugging

Reviewed By: akinh

Differential Revision: D22889901

fbshipit-source-id: 37bd0e5cae29398be44874721f84b9670578695f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1369,Yuqing Tang,yuqtang@fb.com,2020-08-03 16:45:30-07:00,e0cd7f98f3a2f441783fb5f5eaff1c8b69b14404,https://github.com/pytorch/fairseq/commit/e0cd7f98f3a2f441783fb5f5eaff1c8b69b14404,"Enable multilingual task to strip language tokens in generation outputs

Summary: Enable multilingual task to strip language tokens in generation outputs by default

Reviewed By: pipibjc

Differential Revision: D22673703

fbshipit-source-id: df235b54e296265fe9c4fc07ea202ed5fa6713cb",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1370,Yuqing Tang,yuqtang@fb.com,2020-08-03 16:45:30-07:00,8449c5f4e85d7658e533ffae3dac716d04cb2f0e,https://github.com/pytorch/fairseq/commit/8449c5f4e85d7658e533ffae3dac716d04cb2f0e,"Add fairseq evaluate flows for multilingual models

Summary:
Mutlingual needs to evaluate multiple directions together with extra arguments.
(1) evaluate_multi_flow to evaluate bleu without tokenization (assuming models are no raw texts and depending on sacreblue)

Reviewed By: pipibjc

Differential Revision: D22748035

fbshipit-source-id: aa3d21af104965398ec945055c027f2826d5cab4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1371,Xu Song,xusong.vip@gmail.com,2020-08-04 08:13:54-07:00,bc3ea11d4da39d618d8c6abea4b8445d4932c1fd,https://github.com/pytorch/fairseq/commit/bc3ea11d4da39d618d8c6abea4b8445d4932c1fd,"Update language_pair_dataset.py (#2367)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

add sort_order for `prev_output_tokens`

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2367

Reviewed By: pipibjc

Differential Revision: D22727492

Pulled By: myleott

fbshipit-source-id: 932f073b3b938682d2189e6c072a26bee7169a98",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1372,Valentin Malykh,madrugado@users.noreply.github.com,2020-08-04 08:18:53-07:00,b689b6ff3ab7b806217b8aa41821bb8fc85f7cd8,https://github.com/pytorch/fairseq/commit/b689b6ff3ab7b806217b8aa41821bb8fc85f7cd8,"FIX: bos is always at 0th element (#2369)

Summary:
small bug fix in dataset creation

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2369

Reviewed By: pipibjc

Differential Revision: D22727599

Pulled By: myleott

fbshipit-source-id: bb7f18d85b72a19667e2a6844bbe172a3397bafb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1373,Xu Song,xusong.vip@gmail.com,2020-08-04 08:20:51-07:00,bddb25ab712e54f0c1eb2d82947e6a1dedd35f42,https://github.com/pytorch/fairseq/commit/bddb25ab712e54f0c1eb2d82947e6a1dedd35f42,"Update README.md (#2381)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes link

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2381

Reviewed By: pipibjc

Differential Revision: D22900368

Pulled By: myleott

fbshipit-source-id: c442246662ddfddef9b8fbe616bf480b5c41c21a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1374,Oren Amsalem,oren.amsalem1@mail.huji.ac.il,2020-08-04 08:22:04-07:00,33cefe372812f42eb6b1fb5dcc07f3f7f810c5ea,https://github.com/pytorch/fairseq/commit/33cefe372812f42eb6b1fb5dcc07f3f7f810c5ea,"Update README.md - spelling (#2360)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2360

Reviewed By: pipibjc

Differential Revision: D22727472

Pulled By: myleott

fbshipit-source-id: 8f4276edae48bfe6bbf103255bc899c93912312e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1375,Rakesh Chada,rakesh.chada@gmail.com,2020-08-04 08:24:25-07:00,b040dae714ff33172731f876e2120848acf10c64,https://github.com/pytorch/fairseq/commit/b040dae714ff33172731f876e2120848acf10c64,"Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2351

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2365

Reviewed By: pipibjc

Differential Revision: D22727384

Pulled By: myleott

fbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1376,Oren Amsalem,oren.amsalem1@mail.huji.ac.il,2020-08-04 08:25:02-07:00,c0aefe8fdd87aaaa5c044cbe58f19243b8e48d95,https://github.com/pytorch/fairseq/commit/c0aefe8fdd87aaaa5c044cbe58f19243b8e48d95,"pep 8 - use ""x not in y"" rather than ""not x in y"" (#2388)

Summary:
https://stackoverflow.com/questions/8738388/x-not-in-y-or-not-x-in-y
https://www.python.org/dev/peps/pep-0008/#programming-recommendations
E713: https://pycodestyle.pycqa.org/en/latest/intro.html#error-codes

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2388

Reviewed By: pipibjc

Differential Revision: D22900393

Pulled By: myleott

fbshipit-source-id: 0d89645922a9689e84b9c1a92e2616b08b63d6c8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1377,michal.pietruszka,michal.pietruszka@applica.ai,2020-08-04 08:26:48-07:00,e3a5eafe97276fc48ecf6311dc8dcdf98390a774,https://github.com/pytorch/fairseq/commit/e3a5eafe97276fc48ecf6311dc8dcdf98390a774,"Fix `mode` in ReduceLROnPlateau scheduler to follow `args.maximize_be… (#2354)

Summary:
…st_checkpoint_metric`

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes #(2205).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Lots of fun!

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2354

Reviewed By: pipibjc

Differential Revision: D22636665

Pulled By: myleott

fbshipit-source-id: c5159db124099ee980bdba46ec23008abad20751",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1378,आलोक,aalok.sathe@gmail.com,2020-08-04 08:29:45-07:00,2ae88d01acd2a9116667db9a00fb3bdf0962d100,https://github.com/pytorch/fairseq/commit/2ae88d01acd2a9116667db9a00fb3bdf0962d100,"typo: Sanskrit* (#2394)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2394

Reviewed By: pipibjc

Differential Revision: D22900433

Pulled By: myleott

fbshipit-source-id: 82b85003ff8df7ad5033da8ae873234ecdd9ef87",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1379,Fady Essam,fadyesam1996@gmail.com,2020-08-04 08:40:25-07:00,627ccc83700782139a02c429fce87b5c11894b18,https://github.com/pytorch/fairseq/commit/627ccc83700782139a02c429fce87b5c11894b18,"Fix fairseq-generate score printing (issue #2355) (#2356)

Summary:
I think .format() should be added to the return line as the latest sacrebleu.corpus_bleu() now returns an object not a string

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2356

Reviewed By: pipibjc

Differential Revision: D22636662

Pulled By: myleott

fbshipit-source-id: 3ec7f963069622b0f7b792610244dc7f6fd28800",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1380,Sam Shleifer,sshleifer@gmail.com,2020-08-04 10:13:44-07:00,cf87f759b9f09769dc416761738cee72382252df,https://github.com/pytorch/fairseq/commit/cf87f759b9f09769dc416761738cee72382252df,"cleanup mBART doc (#2391)

Summary:
- uses downloaded paths in mbart commands, as defaults.
- corrects path to `sentencepiece.bpe.model`

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2391

Reviewed By: pipibjc

Differential Revision: D22900413

Pulled By: myleott

fbshipit-source-id: f04df08350257742dd263a48dca960114598059c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1381,alexeib,alexei.b@gmail.com,2020-08-04 14:18:03-07:00,621e834103b13318cb48d41fc713b580f0da6b24,https://github.com/pytorch/fairseq/commit/621e834103b13318cb48d41fc713b580f0da6b24,"wav2vec 2.0 (#1220)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1220

Test Plan: Please see examples/wav2vec/README.md for instructions

Reviewed By: edunov

Differential Revision: D22707565

Pulled By: alexeib

fbshipit-source-id: 0c0d4ca7acc933ef7c0062f8dce550b94e414680",37,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1382,alexeib,alexei.b@gmail.com,2020-08-04 18:10:34-07:00,aa62039d463d95767b69bd1b85e5694a7b2a2d40,https://github.com/pytorch/fairseq/commit/aa62039d463d95767b69bd1b85e5694a7b2a2d40,"fix wav2vec docs (#1236)

Summary:
fixes wav2vec 2.0 and wav2vec docs (incl issue #2418)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1236

Reviewed By: ngoyal2707

Differential Revision: D22934517

Pulled By: alexeib

fbshipit-source-id: aaffd05c5e6d22cf4b5d912ddaa170530b65b378",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1383,alexeib,alexei.b@gmail.com,2020-08-05 02:25:33-07:00,54baa2e72d1228ca7d40b78f676ed782285ae6c7,https://github.com/pytorch/fairseq/commit/54baa2e72d1228ca7d40b78f676ed782285ae6c7,"fix validation bug (#1237)

Summary:
clone unmasked features so validation actually works

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1237

Reviewed By: HenryZhou7

Differential Revision: D22942085

Pulled By: alexeib

fbshipit-source-id: aeb6519bf2df8b52fde315503043347e7252ebb0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1384,Mandeep Baines,msb@fb.com,2020-08-05 15:23:41-07:00,9bcf326093b94cf063aca8c05040f15c6f731341,https://github.com/pytorch/fairseq/commit/9bcf326093b94cf063aca8c05040f15c6f731341,"fix LegacyDDP to work with gpipe (#1213)

Summary:
Neither c10d or no_c10d work with pipeline parallelism and gradient checkpointing.
Added minimal set of changes that get no_c10d working with gpipe.

1.  use per-GPU batch buffers (this was not sufficient)
2. simplify when all_reduce happens via an explicit all_reduce call after backward

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1213

Test Plan:
fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --ddp-backend no_c10d

Before:

2020-07-23 20:22:54 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=263610, ups=1.01, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=66
2020-07-23 20:22:55 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259423, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=67
2020-07-23 20:22:56 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=256937, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=68
2020-07-23 20:22:57 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403792, wps=260323, ups=0.99, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=69
2020-07-23 20:22:58 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=258095, ups=0.98, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=70

After:

2020-07-23 20:20:14 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=268872, ups=1.03, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=66
2020-07-23 20:20:15 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=263022, ups=1, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=67
2020-07-23 20:20:16 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=265523, ups=1.01, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=68
2020-07-23 20:20:17 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403792, wps=259082, ups=0.99, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=69
2020-07-23 20:20:18 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=250407, ups=0.95, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=70

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D22488572

Pulled By: msbaines

fbshipit-source-id: a6ab5185fa69c5e0508cdca469e692d1caa8add1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1385,Yuqing Tang,yuqtang@fb.com,2020-08-05 18:16:08-07:00,8aa06aa03b596de58d106d3f55ff43e2b9aa0b80,https://github.com/pytorch/fairseq/commit/8aa06aa03b596de58d106d3f55ff43e2b9aa0b80,"Enable different directions to have different number of shards

Summary:
When different directions have very different sizes of data, they will be sharded into different number of shards while keeping the size of each shard roughly the same. This diff enables multilingual sharding mechanism to deal with it.

* small datasets can have only one shard; and they will only appear in the first shard
* larger dataset will continue to fill in the shards incrementally
* the number of shard of the whole combined dataset is defined by the number of shards of the largest direction dataset
* temperature-based sampling will be based on an approximation of data sizes: [size(d_i * num_shard_i) for i in range(num_data_sets)]

Reviewed By: pipibjc

Differential Revision: D22885256

fbshipit-source-id: 87e4f78191dd9f1ee56bc5aba8dbd94e078a9ada",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1386,Yuqing Tang,yuqtang@fb.com,2020-08-06 10:18:52-07:00,0bb7bc3777b880c282df794fb7edb56d7280449b,https://github.com/pytorch/fairseq/commit/0bb7bc3777b880c282df794fb7edb56d7280449b,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: myleott

Differential Revision: D22483494

fbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Raises', '(Exception) as context:'), ('True', '('), ('True', '(reset_optimizer)'), ('True', '(reset_lr_scheduler)'), ('True', '(reset_meters)'), ('False', '(reset_optimizer)'), ('False', '(reset_lr_scheduler)'), ('False', '(reset_meters)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1387,Myle Ott,myleott@fb.com,2020-08-07 14:17:45-07:00,631023200a8e0c8531ccdb9e5d7f54411149efdf,https://github.com/pytorch/fairseq/commit/631023200a8e0c8531ccdb9e5d7f54411149efdf,"Fix LegacyDistributedDataParallel (#2447)

Summary:
The ""need_reduction"" check was to help any legacy callers who might not be using the fairseq Trainer.

Unfortunately the mechanism doesn't work during validation, since there we call forward multiple times without calling backward.

Let's remove the check -- pretty much everyone should be using fairseq Trainer.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2447

Reviewed By: cndn, msbaines

Differential Revision: D23005086

Pulled By: myleott

fbshipit-source-id: 3a58342024096f2f3abc7bf1cdcce75e56a06aa0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1388,Yuqing Tang,yuqtang@fb.com,2020-08-07 15:19:26-07:00,b479cd946faa304178c480ae8deda04a94ee54a7,https://github.com/pytorch/fairseq/commit/b479cd946faa304178c480ae8deda04a94ee54a7,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: readme and example scripts

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

Readme and example scripts.

Reviewed By: pipibjc

Differential Revision: D22483502

fbshipit-source-id: 983a9949a088d9dbbdb24a5c07fa92044e1c65f1",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1389,Yuqing Tang,yuqtang@fb.com,2020-08-07 17:36:17-07:00,587c179818d55a95d8b0d8bbd40f245a1b0c3692,https://github.com/pytorch/fairseq/commit/587c179818d55a95d8b0d8bbd40f245a1b0c3692,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: added denoising data to training

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/data/denoising_dataset.py to (1) allow additional transformation; (2) allow a preset pad_to_length of batch;

Reviewed By: myleott

Differential Revision: D22483451

fbshipit-source-id: 2ffa9f95186dde2a42e0c356fea3f33c42711c82",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1390,Yuqing Tang,yuqtang@fb.com,2020-08-07 19:54:18-07:00,e882cfaaa38eafdba5126cad7cbdd399be677ab0,https://github.com/pytorch/fairseq/commit/e882cfaaa38eafdba5126cad7cbdd399be677ab0,"Use PlasmaArray for itermediate indices in SampledMultilDataset to avoid copying during pickling

Summary: Use PlasmaArray for itermediate indices in SampledMultilDataset to avoid copying during pickling

Reviewed By: pipibjc

Differential Revision: D22889904

fbshipit-source-id: 795be21ab3d34ca1e993883956ebd6622edab472",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1391,alexeib,alexei.b@gmail.com,2020-08-08 21:24:27-07:00,522c76ba1646cd5ec2cd4be29392f53d40aec50a,https://github.com/pytorch/fairseq/commit/522c76ba1646cd5ec2cd4be29392f53d40aec50a,"no stochastic cropping (#1241)

Summary:
this matches what i had in my development branch for wav2vec 2.0

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1241

Reviewed By: HenryZhou7

Differential Revision: D23020339

Pulled By: alexeib

fbshipit-source-id: 582203d627c6abce2baa4aaf91d4e5601b9a45d1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1392,Yuqing Tang,yuqtang@fb.com,2020-08-10 09:54:45-07:00,c0a187d86e2cfa192f137656072050a0407c71f3,https://github.com/pytorch/fairseq/commit/c0a187d86e2cfa192f137656072050a0407c71f3,"Fixed a bug in sharing source and target data set for reversed directions in multilingual data manager

Summary: We can reuse source and target datasets for reserved direction to save CPU memory. However, the old implementation has two bugs: (1) with the align_dataset present in lang_pair, the current logic will never allow reversed soure and target data sharing; (2) the key of cached datasets does not include direction, so different directions' source and target datasets can mess up with each other. This is a fix.

Reviewed By: pipibjc

Differential Revision: D22889908

fbshipit-source-id: e64ec36819ec32e711fd009228af5d4a569754d4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1393,Yuqing Tang,yuqtang@fb.com,2020-08-10 11:09:03-07:00,8453a08cd18f3972fa62584cfc3a278bcc3af16c,https://github.com/pytorch/fairseq/commit/8453a08cd18f3972fa62584cfc3a278bcc3af16c,"Fixed a bug in fairseq training loop where valid_losses is not evaluated before reference

Summary:
It is possible that valid_losses is not evaluated in train.py. Here is a fix.

# Facebook:
see f209151313 for an example.

Reviewed By: myleott

Differential Revision: D22894276

fbshipit-source-id: a3351d8d77e68ee64ea7c61a1e2c237b5fcfeeae",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1394,Yuqing Tang,yuqtang@fb.com,2020-08-10 19:27:07-07:00,4c55744ec4cb26749cf2cf8dac89942f26ce4bd2,https://github.com/pytorch/fairseq/commit/4c55744ec4cb26749cf2cf8dac89942f26ce4bd2,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: correct parameter names to be consistent with suggestioned revisions made to [1/x]

Summary: In revision to [1/x], it was sugguested to change max_size into pad_to_length. Here let's correct parameter names in other datasets to be consistent with revisions made to [1/x].

Reviewed By: myleott

Differential Revision: D22994888

fbshipit-source-id: cdc51af9eae7f122c644f5727eaf084013855ceb",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1395,Ruslan Mavlyutov,mavlyutov@fb.com,2020-08-12 11:16:46-07:00,9438fb58ecdbfc7a816893e4c2ba15e03062f921,https://github.com/pytorch/fairseq/commit/9438fb58ecdbfc7a816893e4c2ba15e03062f921,"Optimize `filter_by_size` for `LanguagePairDataset`

Summary:
Speedup filtering by using numpy native filter methods.

Summary from the test plan:

Used inputs from f208190331 (as suggested in the task).

Function runtime:

Original run (nothing will be removed): ~10s
Remove ~100K from ~200M: ~20s
Remove all indices from ~200M samples: ~40s

Details are in the test plan

Reviewed By: myleott, akinh

Differential Revision: D23045469

fbshipit-source-id: 8b755c0c203e51d057130a8df994ff8eb76c9b2b",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1396,Kritika Singh,skritika@fb.com,2020-08-12 19:53:53-07:00,7f3a5f6d5f6c571cce4fecd453a197c3dc521539,https://github.com/pytorch/fairseq/commit/7f3a5f6d5f6c571cce4fecd453a197c3dc521539,"Use safe_round in ctc reduce_metrics

Summary:
Validation step was failing with this error:
```
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/trainer.py"", line 644, in valid_step
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/trainer.py"", line 949, in _reduce_and_log_stats
    logging_output = agg.get_smoothed_values()
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/logging/meters.py"", line 268, in get_smoothed_values
    for key in self.keys()
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/logging/meters.py"", line 269, in <listcomp>
    if not key.startswith(""_"")
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/logging/meters.py"", line 260, in get_smoothed_value
    return meter.fn(self)
  File ""/mnt/xarfuse/uid-188222/f6ace8b6-seed-f166b132-0f4f-40ae-8261-c83ec5d2e63c-ns-4026533670/fairseq/criterions/ctc.py"", line 234, in <lambda>
    if meters[""_w_total""].sum > 0
TypeError: type Tensor doesn't define __round__ method

```
This issue was also raised in T71334670. Quoting Myle from https://fb.workplace.com/groups/fairseq/permalink/694410594696062/:
""Hmm, yeah, we now return tensors for multi-GPU since the stats are synced over GPU/NCCL. You can use utils.item(trainer.get_meter(...)) as a workaround.""

Reviewed By: alexeib

Differential Revision: D23035154

fbshipit-source-id: 65537f3b5d9d1112d9e3a1964618ec524bc28378",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1397,alexeib,alexei.b@gmail.com,2020-08-13 15:26:49-07:00,fe3b63643c6c15cac70d0958242dbfb6bdf710e3,https://github.com/pytorch/fairseq/commit/fe3b63643c6c15cac70d0958242dbfb6bdf710e3,"delete windows build which always fails (#1245)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1245

Reviewed By: myleott

Differential Revision: D23110691

Pulled By: alexeib

fbshipit-source-id: bedf432f8710ba5c1a651410d27b39e282b80e11",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1398,alexeib,alexei.b@gmail.com,2020-08-13 16:06:11-07:00,3a7b04fa098f6a94888c6700860afa44a22d9c9d,https://github.com/pytorch/fairseq/commit/3a7b04fa098f6a94888c6700860afa44a22d9c9d,"fix wav2vec seq2seq training (#1244)

Summary:
fixes training wav2vec seq2seq models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1244

Reviewed By: aconneau, jmp84

Differential Revision: D23110102

Pulled By: alexeib

fbshipit-source-id: 7db38d4f59826000eac58fdd7e6cbb8e9cbb5b43",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1399,Xu Song,xusong.vip@gmail.com,2020-08-13 21:19:09-07:00,2497a9dc6ebcd58b410e7d8e265219cf0b0dbfbb,https://github.com/pytorch/fairseq/commit/2497a9dc6ebcd58b410e7d8e265219cf0b0dbfbb,"Small fix for load_indexed_dataset parameter (#2430)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

`dictionary` is not a required parameter for `load_indexed_dataset`.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2430

Reviewed By: ngoyal2707

Differential Revision: D23006666

Pulled By: myleott

fbshipit-source-id: 10f2850ecb294795575573f99efac7f12a118d5d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1400,Alex Xiao,axiao@fb.com,2020-08-14 02:45:33-07:00,96d767231dbcd99abaad54ce07f1d25577d41b9f,https://github.com/pytorch/fairseq/commit/96d767231dbcd99abaad54ce07f1d25577d41b9f,"use utterance level sampling strategy for stella giga data training

Summary:
Before, we would use batch level sampling with fairseq's MultiCorpusSampledDataset. This means we would sample a batch of portal data, a batch of video data, etc.

The main disadvantage of this is that the implementation for this requires the sampling to happen **after** splitting up the data into batches. This means we don't know the underlying data of each batch until right before the forward call for that batch, which means when splitting up the data for batching fairseq overestimates the tokens per item by just taking the max across all datasets from which the sample could come from. This causes batching inefficiency because it overestimates the actual batch size.

In D21887303 (https://github.com/pytorch/fairseq/commit/8570277f91d6bed03d71cc9c8326f096cd06b0d2) we added utterance level sampling, which samples **before** splitting up the data for batching on an utterance level. This is achieved through a new dataset introduced in that diff and refreshing the batch iterator every epoch. This enables accurate batch size calculation.

Reviewed By: jay-mahadeokar

Differential Revision: D22280839

fbshipit-source-id: 8d6c8b82267184558fe3ec9e1b77fe6f24e0376f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1401,Myle Ott,myleott@fb.com,2020-08-14 10:23:45-07:00,983163494663e24b611f1ba8d5d47a3edc00e2e5,https://github.com/pytorch/fairseq/commit/983163494663e24b611f1ba8d5d47a3edc00e2e5,"Misc fixes (#2448)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448

Reviewed By: ngoyal2707

Differential Revision: D23011193

Pulled By: myleott

fbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba",31,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1402,Yuqing Tang,yuqtang@fb.com,2020-08-14 11:01:01-07:00,3217945c43aeb07754d74f87836b64e008504597,https://github.com/pytorch/fairseq/commit/3217945c43aeb07754d74f87836b64e008504597,"Always use valid and test data from shard 0 to avoid the need to copy valid and test data to all shards

Summary: Always use valid and test data from shard 0 to avoid the need to copy valid and test data to all shards

Reviewed By: pipibjc

Differential Revision: D23033446

fbshipit-source-id: dba4566321c7283484c94a6042b942da98d28605",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1403,Ning Dong,dnn@fb.com,2020-08-14 16:48:17-07:00,f1ec983f2e01e54a710a1d2881fec27795d8d6c6,https://github.com/pytorch/fairseq/commit/f1ec983f2e01e54a710a1d2881fec27795d8d6c6,"Set export=True in LayerNorm when under TorchScript execution

Summary: When under TorchScript execution, set export=True as FusedLayerNorm doesn't work with JIT yet (See torch.jit.unused decorator).

Reviewed By: myleott

Differential Revision: D23088062

fbshipit-source-id: ba27ac8f598ddf80cf6ae460192ad8a3f83644ca",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1404,Changhan Wang,changhan@fb.com,2020-08-14 22:20:21-07:00,bd20dbda918cdec93ab6d1fe5bba0ce064a60103,https://github.com/pytorch/fairseq/commit/bd20dbda918cdec93ab6d1fe5bba0ce064a60103,"Fix typos in WER scorer

Summary:
Fix typos in WER scorer
- The typos lead to using prediction length as the denominator in the formula, which is wrong.

Reviewed By: alexeib

Differential Revision: D23139261

fbshipit-source-id: d1bba0044365813603ce358388e880c1b3f9ec6b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1405,Myle Ott,myleott@fb.com,2020-08-17 16:21:48-07:00,110f9f0cc781354eee358b28445d2096cdbd4a14,https://github.com/pytorch/fairseq/commit/110f9f0cc781354eee358b28445d2096cdbd4a14,"Stricter boundary checks on CountingIterator (#2491)

Summary:
We were previously a bit too lenient with boundary conditions to support `CountingIterator.take`. Let's instead handle this more explicitly.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2491

Reviewed By: ngoyal2707

Differential Revision: D23172408

Pulled By: myleott

fbshipit-source-id: 90d24b044812982f7d3eb4cdb39f3db3016a884d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1406,Myle Ott,myleott@fb.com,2020-08-17 16:27:55-07:00,d1d28793cdc05cf26d61a7b42c69ead046611ba8,https://github.com/pytorch/fairseq/commit/d1d28793cdc05cf26d61a7b42c69ead046611ba8,"Stricter boundary checks for iterator sizes (#2490)

Summary:
There have been issues with some dynamic datasets where the iteration count stored in the checkpoint overflows the actual iterator size, but we've been unable to reproduce it in any reliable way. This overflow can apparently cause the epoch to advance when loading checkpoints, which is undesirable.

This PR changes two things. First at the end of an epoch we advance the iterator to the next epoch directly in state_dict, so that we can distinguish this overflow corner case and the more typical end-of-epoch situation. We then raise an exception in the case of iterator overflow, which will hopefully help us (via the community) find a more reliable repro for the underlying issue.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2490

Reviewed By: ngoyal2707

Differential Revision: D23172070

Pulled By: myleott

fbshipit-source-id: 6905cde8e83e56881d2583c74667717e08edf95e",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1407,Yuqing Tang,yuqtang@fb.com,2020-08-18 16:05:51-07:00,5af2fdd32a4b3cce31d248c647d6f4152e72d525,https://github.com/pytorch/fairseq/commit/5af2fdd32a4b3cce31d248c647d6f4152e72d525,"Have translation task always use valid/test in the the first shard to avoid copying valid/test data across all shards

Summary: Have translation task always use valid/test in the the first  shard to avoid copying valid/test data across all shards

Reviewed By: pipibjc

Differential Revision: D23180874

fbshipit-source-id: 20fe431438f2bb22fc955773397a7f4c08a1f014",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1408,James Cross,jcross@fb.com,2020-08-18 17:36:24-07:00,77983ee1a52c4e011e54cc6bfa5352b7811ec96d,https://github.com/pytorch/fairseq/commit/77983ee1a52c4e011e54cc6bfa5352b7811ec96d,"fbtranslate: fairseq logging to manifold

Summary:
Manifold logging for Fairseq training in fbtranslate is currently broken because `distributed_train` dispatches separate processes for each training GPU. If these write to a local temporary file, we must copy to Manifold in the spawned process rather than from the dispatching function since they are run on different machines.

(Previously this was coordinated by using Gluster, a shared filesystem available from all machines.)

Reviewed By: akinh

Differential Revision: D23200236

fbshipit-source-id: 8dbeef9ed3cc04d73321827200c16df48dddff61",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1409,Jun Ru Anderson,andersonic@fb.com,2020-08-19 16:03:14-07:00,68c87f0abf95e84b2c9105911503f604611429d6,https://github.com/pytorch/fairseq/commit/68c87f0abf95e84b2c9105911503f604611429d6,"optimize mixed precision (#1248)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Implements the multiply_factor optimization used in memory efficient fp16 training to mixed precision training. The methods multiply_grads and clip_grad_norm do not touch each gradient, but rather a ""multiply factor"" that is then factored in when unscaling gradients.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1248

Reviewed By: myleott

Differential Revision: D23201396

Pulled By: andersonic

fbshipit-source-id: 6c6f64542893e0ecac72e132464bb334dcb9874d",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,6,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestGradientScaling(unittest.TestCase):'],"[('Equal', '(loss, torch.tensor(1., device=, dtype=torch.float16))'), ('AlmostEqual', '(grad_norm.item(), 2.2361, 4)'), ('Equal', '(model.weight, torch.tensor([[3.0996]], device=, dtype=torch.float16, requires_grad=True))'), ('Equal', '(model.bias, torch.tensor([5.1016], device=, dtype=torch.float16, requires_grad=True))'), ('Equal', '(optimizer.scaler.loss_scale, 2.)'), ('True', '(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device=, requires_grad=True))))')]",['def setUp(self):'],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1410,alexeib,alexei.b@gmail.com,2020-08-19 20:09:02-07:00,54b934417d95baa1b0076089c61bde32728e34cf,https://github.com/pytorch/fairseq/commit/54b934417d95baa1b0076089c61bde32728e34cf,"libri_labels fix + zero padding (#1249)

Summary:
fix libri_labels.py to output files without .txt extension
zero-pad examples instead of initializing with random values and hoping they wont get used

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1249

Reviewed By: ngoyal2707

Differential Revision: D23227559

Pulled By: alexeib

fbshipit-source-id: c30fa4b8242c6b52098b3f9d9c4ccb23902be2e6",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1411,Myle Ott,myleott@fb.com,2020-08-20 06:40:45-07:00,adbd89fd4be9e68100bf9a4ba9eed1e7fb2e4040,https://github.com/pytorch/fairseq/commit/adbd89fd4be9e68100bf9a4ba9eed1e7fb2e4040,"Misc fixes (#2492)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2492

Reviewed By: ngoyal2707

Differential Revision: D23177728

Pulled By: myleott

fbshipit-source-id: 32424f61cab57f759f87e16e8d5144d3eed5ae36",20,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],"['def tearDown(self):', 'def tearDown(self):']",[],[],[],[],[],[],[],[],[],[],['def tearDown(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1412,Matt Post,post@cs.jhu.edu,2020-08-20 11:58:27-07:00,bd1b35d9b7cb21b2e7c17201d831c17560265b67,https://github.com/pytorch/fairseq/commit/bd1b35d9b7cb21b2e7c17201d831c17560265b67,"Added constrained decoding (#1536) (#2402)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

This PR implements constrained decoding ([Hokamp & Liu, 2017](https://www.aclweb.org/anthology/P17-1141/); [Post & Vilar, 2018](https://www.aclweb.org/anthology/N18-1119/)) with vectorization for batching ([Hu et al., 2019](https://www.aclweb.org/anthology/N19-1090/)). In addition, it add *ordered constraints*, where the constraints are generated on the target side in order, with zero or more unconstrained tokens in between. This variant allows for optimizations that increase speed and BLEU scores (when testing with random scraps from the references).

### Usage and quick start

It works with `fairseq-interactive` via a new command-line option: `fairseq-interactive --constraints [ordered,unordered]`, defaulting to `ordered` if nothing is provided. When active, it will split lines from STDIN on `\t`, with separate constraints each separated by a tab. For example (after downloading the [Fairseq WMT19 German--English model](https://github.com/pytorch/fairseq/blob/master/examples/wmt19/README.md)):

```bash
echo -e ""Die maschinelle Übersetzung ist schwer zu kontrollieren.\thard\tinfluence"" \
  | [normalize.py](https://gist.github.com/mjpost/4c54446b7030d7c64b57461d27090650) \
  | [tok.py](https://gist.github.com/mjpost/ed7456f6a987c533102fc121678ed302) \
  | PYTHONPATH=$HOME/code/fairseq-constraints fairseq-interactive $modeldir \
  --bpe fastbpe \
  --bpe-codes $modeldir/bpecodes \
  --constraints \
  --constraints-both
  -s de -t en \
  --path $modeldir/model1.pt \
  --max-tokens 1000 \
  --beam 5 \
```

Adding the `--constraints-both` option causes it to batch-decode the input sentence both with and without the constraints. When run with the Fairseq WMT19 German--English model, the following results are produced (here run on a CPU, don't be alarmed by the times!)

```text
S-0     Die masch@@ in@@ elle Über@@ setzung ist schwer zu kontrollieren .
W-0     1.844   seconds
C-0     hard
C-0     influence
H-0     -1.5333266258239746     Mach@@ ine trans@@ lation is hard to influence .
D-0     -1.5333266258239746     Machine translation is hard to influence .
P-0     -0.5434 -0.1423 -0.1930 -0.1415 -0.2346 -1.8031 -0.1701 -11.7727 -0.1815 -0.1511
S-0     Die masch@@ in@@ elle Über@@ setzung ist schwer zu kontrollieren .
W-0     1.844   seconds
H-0     -0.3731671869754791     Mach@@ ine trans@@ lation is difficult to control .
D-0     -0.3731671869754791     Machine translation is difficult to control .
P-0     -0.5434 -0.1423 -0.1930 -0.1415 -0.2346 -1.1430 -0.1665 -0.8482 -0.1678 -0.1514
2020-07-31 12:17:55 | INFO | fairseq_cli.interactive | Total time: 12.803 seconds; translation time: 3.688
```

Note the new tags present in the output:

* `C-#` records active constraints (after applying preprocessing) for a sentence
* `W-#` reports the sentence-level translation time (a useful unrelated feature I hope you'll accept)

Some unit tests are written (`fairseq/test_constraints.py`) but not yet integrated. Advice here on where to place this is welcome. I also have not run this through lint; if someone can tell me the command to run, I'd appreciate it.

### Implementation notes

This is largely self-contained, implemented in a new `LexicallyConstrainedBeamSearch` class in `search.py`. It does require a few minimal hooks from `_generate()` in `sequence_generator.py`, to ensure that constraints are updated at each timestep. (Edit: most changes in that file are documentation clarifications, corrections, and updates). Unconstrained sentences that are intermingled with constrained ones will not incur any time penalty, so long as they do not occur in the same batch.

Addresses https://github.com/pytorch/fairseq/issues/1536.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2402

Reviewed By: alexeib

Differential Revision: D23188945

Pulled By: myleott

fbshipit-source-id: 9f5ed855f7a1dcf535b091c0ccf98b07fb9cbdd6",25,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],3,0,3,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestHelperRoutines(unittest.TestCase):', 'class TestUnorderedConstraintState(unittest.TestCase):', 'class TestOrderedConstraintState(unittest.TestCase):']",[],"['def setUp(self):', 'def setUp(self):', 'def setUp(self):']",[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['torch.equal(packed, expected_tensor)', 'ConstraintNode.print_graph(c) == expected, f', 'c.token_counts() == gold_counts, f', 'all_tokens == state.next_tokens(), f', 'result == expected, f', 'result == expected, f']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1413,Myle Ott,myleott@fb.com,2020-08-20 15:42:59-07:00,703fd48bb1468ed2df22a681511a03f1937ed60f,https://github.com/pytorch/fairseq/commit/703fd48bb1468ed2df22a681511a03f1937ed60f,"Fix README and #2496 (#2505)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2505

Reviewed By: shruti-bh

Differential Revision: D23247882

Pulled By: myleott

fbshipit-source-id: 1cfc9e0128e1aa55a1aca31d8dd30f231558e70f",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1414,Weiyi Zheng,wyz@fb.com,2020-08-20 17:03:48-07:00,83d701ac10a328e54422a9244337d8e569bcf96d,https://github.com/pytorch/fairseq/commit/83d701ac10a328e54422a9244337d8e569bcf96d,"log latest value on loss_scale

Summary: the log_scalar defaults to logging average, which isn't very useful.

Reviewed By: myleott

Differential Revision: D23226068

fbshipit-source-id: 78a67366344608d452141523fb900f99706a8a17",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1415,Alex Xiao,axiao@fb.com,2020-08-20 20:07:45-07:00,49940c8d25d61a251e290d96fe3bbbc9f210408f,https://github.com/pytorch/fairseq/commit/49940c8d25d61a251e290d96fe3bbbc9f210408f,"fix mismatch length of counting iterator when truncated

Summary:
PySpeech integration training tests have recently been stuck at end of epoch.

Digging into it, it looks like this is because the end of epoch check relies on this (https://fburl.com/diffusion/xt09z6n9):

```
def end_of_epoch(self) -> bool:
     """"""Returns whether the most recent epoch iterator has been exhausted""""""
     return not self._cur_epoch_itr.has_next()
```

which is implemented like this in CountingIterator:

    def has_next(self):
        """"""Whether the iterator has been exhausted.""""""
        return self.n < len(self)

It seems like D23172408 (https://github.com/pytorch/fairseq/commit/110f9f0cc781354eee358b28445d2096cdbd4a14) modified CountingIterator such that `len(self) > len(iter(self))` when `take()` is used. This mismatch causes `has_next` to return `True` for some  PySpeech processes even when all elements in `iter(self))` have been consumed, causing training to get stuck.

My proposed fix is to remove the `self.early_stop`  variable and just directly modify `self.total` and `self.iterable`, ensuring `len(self) == len(iter(self))`

Reviewed By: myleott

Differential Revision: D23250734

fbshipit-source-id: efb5a38216783bded67f501135b2f68b9246b9dd",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(len(itr), len(list(iter(itr))))'), ('Equal', '(len(itr), 5)'), ('Equal', '(next(itr), ref[0])'), ('Equal', '(next(itr), ref[1])'), ('Equal', '(next(itr), ref[4])'), ('False', '(itr.has_next())')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1416,alexeib,alexei.b@gmail.com,2020-08-21 20:35:54-07:00,39f0911ac6ea9d4fe9f4714d6343a2b1a1343609,https://github.com/pytorch/fairseq/commit/39f0911ac6ea9d4fe9f4714d6343a2b1a1343609,"fix validation interval updates with loss scaling and remove filtering by size from audio pretraining (#1252)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1252

Reviewed By: xuqiantong

Differential Revision: D23277400

Pulled By: alexeib

fbshipit-source-id: 169fa4d2cb91e9a089fe9d58e8085213e4b800e1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1417,Alex Xiao,axiao@fb.com,2020-08-21 20:56:28-07:00,17fd14d3fb8dc01bac2be44cc72c64e66a41feb2,https://github.com/pytorch/fairseq/commit/17fd14d3fb8dc01bac2be44cc72c64e66a41feb2,"fix convtransformer test

Summary: in D23188945 (https://github.com/pytorch/fairseq/commit/bd1b35d9b7cb21b2e7c17201d831c17560265b67) looks like the sequence generator was modified to only support 2d src tokens, but for audio seems like we need 3d

Reviewed By: zhengwy888

Differential Revision: D23274482

fbshipit-source-id: 2e66c897c9d2c929158c2c9f858eac607af302bf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1418,Yuqing Tang,yuqtang@fb.com,2020-08-24 10:27:29-07:00,585c330ea35711e2d6ac7c56bdea44c82669b243,https://github.com/pytorch/fairseq/commit/585c330ea35711e2d6ac7c56bdea44c82669b243,"Enable no target data inference for multilingual tasks

Summary: Multilingual data manager always check target dataset which is not necessary for inference time. Here is a fix.

Reviewed By: pipibjc

Differential Revision: D23282118

fbshipit-source-id: cbfd6f17919694fbb69a3ae85cda8e9c96df6764",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1419,alexeib,alexei.b@gmail.com,2020-08-24 12:16:56-07:00,2239cdadfa5ecf09e08887c77c05f5d7a8d532bd,https://github.com/pytorch/fairseq/commit/2239cdadfa5ecf09e08887c77c05f5d7a8d532bd,"fix generate bugs introduced by constraint decoding (#1254)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1254

Reviewed By: ngoyal2707

Differential Revision: D23287443

Pulled By: alexeib

fbshipit-source-id: 6fd358019a72abd0863308571d7cb63b47603964",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1420,Elijah Rippeth,elijah.rippeth@gmail.com,2020-08-24 17:34:31-07:00,226f0e45391ac1dcacb72ff68b523bf7b2ebceda,https://github.com/pytorch/fairseq/commit/226f0e45391ac1dcacb72ff68b523bf7b2ebceda,"fix #2483 by explicitly specifying a cross-platform dtype. (#2521)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2483

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2521

Reviewed By: michaelauli, lematt1991

Differential Revision: D23303214

Pulled By: alexeib

fbshipit-source-id: 4b60afc7b902c07bc36fad2eeb116a8ed7f5ffe2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1421,Jun Ru Anderson,andersonic@fb.com,2020-08-26 09:26:38-07:00,fc27170a9e70c6485331d8c84d56142a98de8a84,https://github.com/pytorch/fairseq/commit/fc27170a9e70c6485331d8c84d56142a98de8a84,"change supports_step_with_scale check (#1255)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Changes the supports_step_with_scale check in fp16_optimizer.py to use fp32_optimizer.supports_step_with_scale / wrapped_optimizer.supports_step_with-scale rather than self.supports_step_with_scale.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1255

Reviewed By: myleott, stanvp

Differential Revision: D23298512

Pulled By: andersonic

fbshipit-source-id: a346375a3d8b0c2fec33322e2adfaee268c25423",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1422,Joshua Meier,jmeier@fb.com,2020-08-27 17:23:13-07:00,59ac9c0c12c68afdf1f03fdfc4437b70182406db,https://github.com/pytorch/fairseq/commit/59ac9c0c12c68afdf1f03fdfc4437b70182406db,"Add a FastaDataset (#1187)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Adds some fantastic work done by Zeming Lin ebetica. FASTA is the predominant format used by biologists for DNA, RNA and proteins. It looks like something like this:
```
>name of your protein1
MSHFAHSDFAHSDFHWEHJW
FHDSJFASJDAHASFASDFIAA
>name of your protein2
MAHASDFMASFJADSFMSMSM
MASDFJASDJ
```

There's no need for BPE or other fancy preprocessing, so we can read the FASTA file directly in fairseq with no speed hit compared to binarized data. Building the index is important, but we can just cache that, similar to the other cached indexed datasets.

We hope this reduces the barrier for biologists to use fairseq, making this great framework even more accessible to the computational biology community!

This dataset is used internally in proteinseq, [see here for an example](https://github.com/fairinternal/proteinseq/pull/178/files).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1187

Reviewed By: myleott

Differential Revision: D22020223

Pulled By: ebetica

fbshipit-source-id: 372ebc199c0c9200645c79fa7722aded931e9038",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1423,Yuqing Tang,yuqtang@fb.com,2020-08-28 10:09:43-07:00,0cde6b4e508ef82f7c7a4df01ab6231ce1d257ee,https://github.com/pytorch/fairseq/commit/0cde6b4e508ef82f7c7a4df01ab6231ce1d257ee,"Added shared dictionary check for translation_multi_simple_epoch task.

Summary: translation_multi_simple_epoch task only supports shared dictionary across all languages, so add the check in the task setup.

Reviewed By: pipibjc

Differential Revision: D23288388

fbshipit-source-id: 4236a096bcb75429b486ef8a9244e3ef0d5095f0",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1424,alexeib,alexei.b@gmail.com,2020-08-28 16:34:51-07:00,4bfd70d400c1b4c474ce617b1359a0250329d6f0,https://github.com/pytorch/fairseq/commit/4bfd70d400c1b4c474ce617b1359a0250329d6f0,"fix transformer lm defaults (#1258)

Summary:
these changes allow loading older checkpoints created before these flags were added

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1258

Reviewed By: edunov

Differential Revision: D23388753

Pulled By: alexeib

fbshipit-source-id: 12b48ebf1a36bd4b24034d8ed68398f6e1526052",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1425,Alex Xiao,axiao@fb.com,2020-08-28 17:49:48-07:00,0989eca746ada5a2439010ffb60b17efdc378270,https://github.com/pytorch/fairseq/commit/0989eca746ada5a2439010ffb60b17efdc378270,"add more detailed logging for fp16 diverging

Summary: We often get a generic ""minimum loss scale reached"" when fp16 training diverges. Would be useful to have a breakdown on where exactly the gradient norm becomes too big.

Reviewed By: myleott

Differential Revision: D23297774

fbshipit-source-id: 69da1cca1be22f15af633f8efe4e7b491cf4f6f9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1426,Myle Ott,myleott@fb.com,2020-08-31 11:28:14-07:00,fe1b1bbe17a5fa6c2b3505735deb3c61fe5b68cc,https://github.com/pytorch/fairseq/commit/fe1b1bbe17a5fa6c2b3505735deb3c61fe5b68cc,"Misc fixes (#2524)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2524

Reviewed By: ngoyal2707

Differential Revision: D23318746

Pulled By: myleott

fbshipit-source-id: 6db6a87aac178847bd0da26db09b1a63632a724f",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1427,Mu Tian,mtian@fb.com,2020-08-31 23:02:05-07:00,251c86940600b932f9925d630e1a77b55cc24d02,https://github.com/pytorch/fairseq/commit/251c86940600b932f9925d630e1a77b55cc24d02,"hydra fairseq - add yaml files

Summary: hydra fairseq - add yaml files

Reviewed By: alexeib

Differential Revision: D22403786

fbshipit-source-id: 81fb5902c1fbcf7b03d111037327ab0f8bfb57f2",21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1428,Mandeep Singh Baines,mandeep.baines@gmail.com,2020-09-01 18:16:15-07:00,5d7ed6ab4f92d20ad10f8f792b8703e260a938ac,https://github.com/pytorch/fairseq/commit/5d7ed6ab4f92d20ad10f8f792b8703e260a938ac,"Initial support for ZeRO optimizer state sharding (#1259)

Summary:
FairseqOSS will work with any optimizer and dtype.

TODO(future PR):
* support reduce instead of all_reduce
* support gradient sharding
* support parameter sharding

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259

Test Plan:
Verified that checkpoint save and restore work.

Verified that grad_norm, loss, and ppl are identical with and without
sharding enable.

Before:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50
...
2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds

After:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os
...
2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D23432082

Pulled By: msbaines

fbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1429,alexeib,alexei.b@gmail.com,2020-09-03 20:36:09-07:00,d8e2cf28e22584a3e7606783e7abf38135a05eff,https://github.com/pytorch/fairseq/commit/d8e2cf28e22584a3e7606783e7abf38135a05eff,"wav2vec 2.0 fixes (#1266)

Summary:
Fixes #2563 (input quantizer)
Fixes #2538 (documentation for fine-tuning)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1266

Reviewed By: aconneau, vineelpratap

Differential Revision: D23521820

Pulled By: alexeib

fbshipit-source-id: 61f2c9baf126554dcf5b7a315e4f4f54577c24bf",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1430,Juan Miguel Pino,juancarabina@fb.com,2020-09-03 21:17:41-07:00,ac730aec61523031016afc8922c04bfc7a7ac42b,https://github.com/pytorch/fairseq/commit/ac730aec61523031016afc8922c04bfc7a7ac42b,"Remove `BeamContainer` from sequence generator (#2567)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2567

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1263

The PyTorch Mobile liter interpreter does not support object creation (CREATE_OBJECT).
I tried using `sorted` and `lambda` to replace the sorting code but TorchScript does not support lambda functions. This change enables to save a scripted model to the lite interpreter. I can make as much testing as possible and push this as a core change, that would benefit on-device MT and on-device sequence modeling in general. Or I'm happy to make this an experimental change with an `fb_` file.

Reviewed By: myleott, jhcross

Differential Revision: D23440771

fbshipit-source-id: c6e5381159857f613b143935b806c5f89464a33b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1431,Wei Ho,weiho@fb.com,2020-09-04 04:45:03-07:00,7f4c7481a54a9d1da0a3c65084cb79c70196ddd2,https://github.com/pytorch/fairseq/commit/7f4c7481a54a9d1da0a3c65084cb79c70196ddd2,"Apply Black auto-formatter to multilingual_data_manager.py

Reviewed By: jmp84

Differential Revision: D23496278

fbshipit-source-id: 60b77a17a227e3a6e547600fabc0cd49c4f8543d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1432,Alex Xiao,axiao@fb.com,2020-09-04 14:25:14-07:00,e171c8d86a939cf4ebc483cd649bee1935379771,https://github.com/pytorch/fairseq/commit/e171c8d86a939cf4ebc483cd649bee1935379771,"Account for checkpoint updates when calling take on CountingIterator

Summary:
Recently some of our runs are getting:

""RuntimeError: Mismatch between actual and expected iterable length. Please report this to the fairseq developers.""

f214567466

We never ran into this before because this is a new check by fairseq to be more strict with iterators.

Fix is to:

1. Account for the offset (i.e. load from checkpoint mid epoch) when propagating `take`. This fixes the issue of `next` returning too many things, which is what causes the error.

2. Update the underlying iterator when calling `take` on `BufferedIterator` and the length of the `BufferedIterator`. Although this doesn't cause the error, it is necessary to maintain consistency.

Reviewed By: myleott

Differential Revision: D23443012

fbshipit-source-id: 73c26db8392e5508a61acfda7ca40a24df89fabb",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(len(itr), len(list(iter(itr))))'), ('Equal', '(len(itr), 5)'), ('Equal', '(len(buffered_itr), 5)'), ('Equal', '(len(list(iter(buffered_itr))), 5)'), ('Equal', '(next(itr), ref[0])'), ('Equal', '(next(itr), ref[1])'), ('Equal', '(next(itr), ref[4])'), ('False', '(itr.has_next())'), ('Raises', '(StopIteration, next, buffered_itr)'), ('Equal', '(len(itr), 5)'), ('Equal', '(len(buffered_itr), 1)'), ('Equal', '(next(itr), ref[0])'), ('False', '(itr.has_next())'), ('Raises', '(StopIteration, next, buffered_itr)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1433,Mu Tian,mtian@fb.com,2020-09-04 17:05:34-07:00,e7f76c44817e9766acca3aacf0d4e8807a6a2d03,https://github.com/pytorch/fairseq/commit/e7f76c44817e9766acca3aacf0d4e8807a6a2d03,"hydra-fairseq - add dataclass

Summary: hydra fairseq - add main common dataclasses as structured config

Reviewed By: alexeib

Differential Revision: D23375458

fbshipit-source-id: 4cb2802e523990d4e2b1a87e3cf1bc4dc852bc5b",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1434,Alex Xiao,axiao@fb.com,2020-09-04 22:11:06-07:00,0ffb94151f597ecb677551289e7046a21fb5ebaf,https://github.com/pytorch/fairseq/commit/0ffb94151f597ecb677551289e7046a21fb5ebaf,"fix deleting source_iter

Summary: D23443012 (https://github.com/pytorch/fairseq/commit/e171c8d86a939cf4ebc483cd649bee1935379771) removed this iterator, so we need to remove this line otherwise jobs will fail. I guess we never ran into this during testing since we never actually finished an entire epoch and consumed everything since we call `take`

Reviewed By: tangyuq

Differential Revision: D23554132

fbshipit-source-id: 232e950a0a436419f6c5139e35caa81e3594fe38",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1435,Myle Ott,myleott@fb.com,2020-09-09 06:18:51-07:00,87350759e399d73d13596c7c26539ba18a4145ea,https://github.com/pytorch/fairseq/commit/87350759e399d73d13596c7c26539ba18a4145ea,"Add FairseqDataset.can_reuse_epoch_itr_across_epochs (#2525)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2525

Reviewed By: ngoyal2707

Differential Revision: D23318762

Pulled By: myleott

fbshipit-source-id: c9c7236a2c9dc127716f5078d92d60df1fe5f716",13,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1436,Myle Ott,myleott@fb.com,2020-09-09 06:18:51-07:00,1cc8e95cece54152b6960e7880a65da98d8ac58a,https://github.com/pytorch/fairseq/commit/1cc8e95cece54152b6960e7880a65da98d8ac58a,"Don't cache epoch iterators when using sharded datasets (#1268)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1268

We previously had a memory leak when using sharded datasets. In particular,
each sharded dataset is a new FairseqDataset instance, and the cache is keyed
by the `dataset` instance. Since we never clear the cache, this would
eventually cause the system to run out of CPU RAM.

This diff disables caching when using sharded datasets.

Note that we also change the signature to `get_batch_iterator`, which needs to
propagate to many places. We previously avoided this update when adding
`data_buffer_size`, so I'm also adding that everywhere.

Reviewed By: ngoyal2707

Differential Revision: D23319135

fbshipit-source-id: 6bcd6aee141ad9cc234448c49106a8dbf8ea1800",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1437,lematt1991,lematt1991@gmail.com,2020-09-09 13:13:47-07:00,df45f42efdaab751e698726bc1922f7643aa9276,https://github.com/pytorch/fairseq/commit/df45f42efdaab751e698726bc1922f7643aa9276,"Fix `ChoiceEnum` Lint Error (#2596)

Summary:
For some reason, this fixes flake8 errors, no idea why...

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2596

Reviewed By: myleott

Differential Revision: D23602637

Pulled By: lematt1991

fbshipit-source-id: b6070a8693eda79f0598fdce92b94c4de569c4fa",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1438,Mu Tian,mtian@fb.com,2020-09-09 17:00:56-07:00,42c5dcbd18c85dcdc9424886f3880c184d589f0d,https://github.com/pytorch/fairseq/commit/42c5dcbd18c85dcdc9424886f3880c184d589f0d,"hydra fairseq 3 - inherit from legacy for fairseq classes

Summary: hydra fairseq 3 - inherit from legacy for fairseq classes

Reviewed By: alexeib

Differential Revision: D23375457

fbshipit-source-id: ef9d19f2d02f2326eea44a70f1f6e1668b420840",40,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1439,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-09-10 21:20:51-07:00,d47067937abacfe87f2963adca8daeada3c631fe,https://github.com/pytorch/fairseq/commit/d47067937abacfe87f2963adca8daeada3c631fe,"Pipeline parallel transformer in fairseq (#1257)

Summary:
## What does this PR do?
* Add pipeline parallelism (inter-layer model parallelism) in fairseq for Transformer models
* This involves creating a modular version of the Transformer model that can be expressed as an `nn.Sequential`
* This uses `fairscale`'s `Pipe` for pipeline parallel execution
* Some other relevant code changes include -
  * updating fp16_optimizer buffers to be per-device so that we can scale to a model size that does not fit on a single GPU
  * ability to convert the state dict of a regular Transformer checkpoint into a pipeline parallel Transformer

## Testing

### Regular Transformer
```
2020-08-25 20:42:40 | INFO | train_inner | epoch 001:    105 / 884 loss=10.123, nll_loss=9.657, ppl=807.41, wps=30505.2, ups=6.77, wpb=4503.2, bsz=173.9, num_updates=100, lr=0.00012, gnorm=2.382, loss_scale=4, train_wall=14, wall=0
2020-08-25 20:42:53 | INFO | train_inner | epoch 001:    205 / 884 loss=9.8, nll_loss=9.305, ppl=632.39, wps=34882.4, ups=7.66, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.29, loss_scale=4, train_wall=13, wall=0
2020-08-25 20:43:06 | INFO | train_inner | epoch 001:    305 / 884 loss=9.91, nll_loss=9.434, ppl=691.51, wps=33456.9, ups=7.62, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.117, loss_scale=4, train_wall=13, wall=0
2020-08-25 20:43:19 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.75, wps=34137.3, ups=7.66, wpb=4457.2, bsz=191.2, num_updates=400, lr=6e-05, gnorm=1.347, loss_scale=4, train_wall=13, wall=0
```

### 1xMP (single-gpu) Pipeline Parallel Transformer
```
2020-08-25 20:44:16 | INFO | train_inner | epoch 001:    105 / 884 loss=10.123, nll_loss=9.657, ppl=807.41, wps=31227.6, ups=6.93, wpb=4503.2, bsz=173.9, num_updates=100, lr=0.00012, gnorm=2.382, loss_scale=4, train_wall=13, wall=0
2020-08-25 20:44:29 | INFO | train_inner | epoch 001:    205 / 884 loss=9.8, nll_loss=9.305, ppl=632.39, wps=35378.9, ups=7.77, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.29, loss_scale=4, train_wall=13, wall=0
2020-08-25 20:44:42 | INFO | train_inner | epoch 001:    305 / 884 loss=9.91, nll_loss=9.434, ppl=691.51, wps=34017.8, ups=7.75, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.117, loss_scale=4, train_wall=13, wall=0
2020-08-25 20:44:55 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.75, wps=34661.1,
```

### 2xMP Pipeline Parallel Transformer
```
2020-08-26 12:10:13 | INFO | train_inner | epoch 001:    105 / 884 loss=10.185, nll_loss=9.728, ppl=848.2, wps=29247.7, ups=6.5, wpb=4488.8, bsz=172.4, num_updates=100, lr=0.00012, gnorm=2.539, loss_scale=4, train_wall=15, wall=17
2020-08-26 12:10:28 | INFO | train_inner | epoch 001:    205 / 884 loss=9.798, nll_loss=9.303, ppl=631.54, wps=30663.7, ups=6.73, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.263, loss_scale=4, train_wall=15, wall=32
2020-08-26 12:10:42 | INFO | train_inner | epoch 001:    305 / 884 loss=9.908, nll_loss=9.432, ppl=690.89, wps=30600.4, ups=6.97, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.091, loss_scale=4, train_wall=14, wall=46
2020-08-26 12:10:57 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.57, wps=29345.2, ups=6.58, wpb=4457.2, bsz=191.2, num_updates=400, lr=6e-05, gnorm=1.344, loss_scale=4, train_wall=15, wall=62
```
### 4xMP Pipeline Parallel Transformer
```
2020-08-26 13:27:25 | INFO | train_inner | epoch 001:    105 / 884 loss=10.185, nll_loss=9.728, ppl=848.1, wps=11158.8, ups=2.48, wpb=4488.8, bsz=172.4, num_updates=100, lr=0.00012, gnorm=2.538, loss_scale=4, train_wall=41, wall=44
2020-08-26 13:28:03 | INFO | train_inner | epoch 001:    205 / 884 loss=9.798, nll_loss=9.303, ppl=631.51, wps=12078, ups=2.65, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.263, loss_scale=4, train_wall=38, wall=82
2020-08-26 13:28:40 | INFO | train_inner | epoch 001:    305 / 884 loss=9.908, nll_loss=9.432, ppl=690.88, wps=11738.5, ups=2.67, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.091, loss_scale=4, train_wall=37, wall=119
2020-08-26 13:29:18 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.58, wps=11810.4, ups=2.65, wpb=4457.2, bsz=191.2, num_updates=400, lr=6e-05, gnorm=1.344, loss_scale=4, train_wall=38, wall=157
```

### 8xMP Pipeline Parallel Transformer
```
2020-08-26 13:37:57 | INFO | train_inner | epoch 001:    105 / 884 loss=10.185, nll_loss=9.728, ppl=848.13, wps=5077.9, ups=1.13, wpb=4488.8, bsz=172.4, num_updates=100, lr=0.00012, gnorm=2.539, loss_scale=4, train_wall=90, wall=96
2020-08-26 13:39:20 | INFO | train_inner | epoch 001:    205 / 884 loss=9.798, nll_loss=9.303, ppl=631.53, wps=5472.2, ups=1.2, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.263, loss_scale=4, train_wall=83, wall=179
2020-08-26 13:40:43 | INFO | train_inner | epoch 001:    305 / 884 loss=9.908, nll_loss=9.432, ppl=690.88, wps=5313.2, ups=1.21, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.091, loss_scale=4, train_wall=83, wall=262
2020-08-26 13:42:05 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.57, wps=5409.3, ups=1.21, wpb=4457.2, bsz=191.2, num_updates=400, lr=6e-05, gnorm=1.344, loss_scale=4, train_wall=82, wall=344
```

### 8xMP Pipeline Parallel Transformer + Checkpointing

```
2020-08-26 13:56:58 | INFO | train_inner | epoch 001:    105 / 884 loss=10.185, nll_loss=9.728, ppl=848.1, wps=3908, ups=0.87, wpb=4488.8, bsz=172.4, num_updates=100, lr=0.00012, gnorm=2.539, loss_scale=4, train_wall=115, wall=120
2020-08-26 13:58:48 | INFO | train_inner | epoch 001:    205 / 884 loss=9.798, nll_loss=9.303, ppl=631.52, wps=4152.5, ups=0.91, wpb=4553, bsz=194.6, num_updates=200, lr=8.48528e-05, gnorm=1.263, loss_scale=4, train_wall=110, wall=230
2020-08-26 14:00:37 | INFO | train_inner | epoch 001:    305 / 884 loss=9.908, nll_loss=9.432, ppl=690.88, wps=4026.4, ups=0.92, wpb=4391.7, bsz=148.7, num_updates=300, lr=6.9282e-05, gnorm=1.091, loss_scale=4, train_wall=109, wall=339
2020-08-26 14:02:26 | INFO | train_inner | epoch 001:    405 / 884 loss=9.81, nll_loss=9.319, ppl=638.56, wps=4101.6, ups=0.92, wpb=4457.2, bsz=191.2, num_updates=400, lr=6e-05, gnorm=1.343, loss_scale=4, train_wall=109, wall=448
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1257

Reviewed By: myleott

Differential Revision: D23593137

Pulled By: shruti-bh

fbshipit-source-id: 840227be6d8bec438e8360b30fe1fdbc4d97dd9c",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1440,Alexei Baevski,abaevski@fb.com,2020-09-10 22:58:57-07:00,fd080b308e1e3361d6c498b235496080fa6599e5,https://github.com/pytorch/fairseq/commit/fd080b308e1e3361d6c498b235496080fa6599e5,"hydra fairseq 4 - migrated several common utils from options to utils

Summary: hydra fairseq 4 - migrated several common utils from options to utils

Reviewed By: myleott

Differential Revision: D23413473

fbshipit-source-id: 01e377de0fdca77a321924ab3768b7fafe3da32e",15,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1441,lematt1991,lematt1991@gmail.com,2020-09-11 13:27:19-07:00,23d8502bdde88a3e58e0910e2ee49834f8478b39,https://github.com/pytorch/fairseq/commit/23d8502bdde88a3e58e0910e2ee49834f8478b39,"Only move to CUDA device if available in clip_grad_norm_ (#2607)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fix tests from d47067937abacfe87f2963adca8daeada3c631fe

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2607

Reviewed By: shruti-bh

Differential Revision: D23650531

Pulled By: lematt1991

fbshipit-source-id: ac2e8b07968535e7454836db9cc0b659a0cdf887",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1442,Yuqing Tang,yuqtang@fb.com,2020-09-17 20:42:59-07:00,380227cc477f731f2315d62aa3d867720231538c,https://github.com/pytorch/fairseq/commit/380227cc477f731f2315d62aa3d867720231538c,"Optimize sampled_multi_dataset.py and sampled_multi_epoch_dataset.py

Summary:
# Facebook:

The research implementation is not stable for multilingual data at scale of 1.5 - 5 billions due to
 * large inter-progress memory consumption in PlasmaArray
 * caching of a few indices and sizes arrays
For example, the following runs failed after training for a while: f217270435 and f216525055

This diff simplifies the implementation and optimizes it to have more stable training at scale:
* removed PlasmaArray
* introduced the lang_pair_dataset.latest_filter_by_size implementation to sampled_multi_dataset
* optimized the sizes() implementation

Reviewed By: akinh

Differential Revision: D23634750

fbshipit-source-id: c3ded98ef2c84e3d4512f1693c0cbf493b91c1df",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1443,Yuqing Tang,yuqtang@fb.com,2020-09-18 14:40:39-07:00,4bd52fb55b28ac99b987a755c729ac1a79e3b5d3,https://github.com/pytorch/fairseq/commit/4bd52fb55b28ac99b987a755c729ac1a79e3b5d3,"Bring back cached sizes() of sampled_multi_epoch_dataset

Summary:
# Facebook:
D23634750 (https://github.com/pytorch/fairseq/commit/380227cc477f731f2315d62aa3d867720231538c) removed the cache of SampledMultiEpochDataset. This results in repeating calls of sizes() function for num_tokens making large dataset impossible to do batch_by_size.

This diff brings it back.

Reviewed By: pipibjc

Differential Revision: D23781668

fbshipit-source-id: b0eabe639500ff87d47abe53a9256644d7ee4d1e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1444,Yuqing Tang,yuqtang@fb.com,2020-09-18 14:56:48-07:00,732ddcc5ae0244ca95fdb2edbd8c4843441abb50,https://github.com/pytorch/fairseq/commit/732ddcc5ae0244ca95fdb2edbd8c4843441abb50,"Better estimation of shard data sizes for sampling ratios computation in multilingual training

Summary: Previously we use current_data_size * num_shards as the total data size of a sharding dataset. It can be a bad estimation for small shards. Here as the training goes one, multilingual_data_manager will cache the shard size so that it can have exact total data sizes after a pass over the data.

Differential Revision: D23683717

fbshipit-source-id: fcd7fd170327091bfd0bed599f1d930c71f62570",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1445,Wei Ho,weiho@fb.com,2020-09-18 18:01:35-07:00,30fe5f5a6b715cf583a9482fc7fc20eb1b7f7bd6,https://github.com/pytorch/fairseq/commit/30fe5f5a6b715cf583a9482fc7fc20eb1b7f7bd6,"Add to-many decoding support to FBTranslate TorchScript model

Summary: Add support for our TorchScript model to decode to multiple output languages by specifying decoder language tokens

Reviewed By: cndn

Differential Revision: D23188553

fbshipit-source-id: 2a985dc3aa0b24e7297f23e449b3479c155e6611",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1446,Yuqing Tang,yuqtang@fb.com,2020-09-18 21:40:35-07:00,1bd84232571da80c6ca7fea2b260091258d8c718,https://github.com/pytorch/fairseq/commit/1bd84232571da80c6ca7fea2b260091258d8c718,"Lazy manifold download for translation and multilingual translation

Summary:
# Facebook:

Downloading all shards to local is formidable for 5 billion pairs of bitext. This diff enables manifold lazy download to avoid downloading all shards before training starts

* Cache the downloading of individual files as needed
* For small files, directly use PathManager.open

Changes in the following files will serve most translation tasks:
* deeplearning/projects/fairseq-py/fairseq/data/data_utils.py
* deeplearning/projects/fairseq-py/fairseq/data/indexed_dataset.py

Reviewed By: theweiho

Differential Revision: D23670815

fbshipit-source-id: 4f90c184d8832edd14f3c30cdc8e1bfad59946a9",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1447,Ning Dong,dnn@fb.com,2020-09-19 17:56:52-07:00,9ae39465daff48650ed7956adf2980a697016d50,https://github.com/pytorch/fairseq/commit/9ae39465daff48650ed7956adf2980a697016d50,"Replace string literals for special symbols in Torchscript decoder

Summary: repeat title. Due to TorchScript constraints it's not trivial to sync fairseq dictionary symbols in VocabConstants.

Reviewed By: theweiho

Differential Revision: D23774348

fbshipit-source-id: ec8db5ca41ee448ed317e76eabd4971f833015cc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1448,Alex Xiao,axiao@fb.com,2020-09-21 13:45:35-07:00,d5f7b50e1cf7d99abbf1ddfd1a985969c13ff4c3,https://github.com/pytorch/fairseq/commit/d5f7b50e1cf7d99abbf1ddfd1a985969c13ff4c3,"fix memory leak in pyspeech

Summary:
Recently pyspeech users have reported memory leaks, where the RAM used by the job would spike up at the start of every epoch. Kritika and I debugged this and we ran bisect:

hg bisect -c ""./script.sh"" (script.sh: P143079305, training json: P143079313)

This identified my diff in D23443012 (https://github.com/pytorch/fairseq/commit/e171c8d86a939cf4ebc483cd649bee1935379771) as the culprit. I then found out it was specifically the line where we wrap the torch dataloader with itertools.islice that causes issues.

Differential Revision: D23817986

fbshipit-source-id: 22eee7d384944febddc197bb734e9db79d73e6ea",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1449,Matt Post,post@cs.jhu.edu,2020-09-22 08:19:06-07:00,7c96648dea371b50c37582c860b9c63ac0242514,https://github.com/pytorch/fairseq/commit/7c96648dea371b50c37582c860b9c63ac0242514,"Fix inaccuracy in constrained decoding README (#2641)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

* The current README **incorrectly states** that Sockeye 2 no longer supports constrained decoding.
* Other minor updates

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2641

Reviewed By: ngoyal2707

Differential Revision: D23833539

Pulled By: myleott

fbshipit-source-id: 0ee09830fb3566a1291d3cb41184a1ae400a7836",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1450,Xu Song,xusong.vip@gmail.com,2020-09-22 08:25:34-07:00,cad08709450714c8a815836c98fcfcee6aff09b0,https://github.com/pytorch/fairseq/commit/cad08709450714c8a815836c98fcfcee6aff09b0,"Update transformer_layer.py (#2611)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

In some cases, we want to use the decoder-only model from a pretrained transformer (`encoder_attn` is not None).

This commit is minor but important, which not only makes `TransformerDecoderLayer` more robust but also make it compitible with decoder-only model from pretrained transformer.

If you want to use decoder from pretrained transformer, you can just set `encoder_out` as None.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2611

Reviewed By: ngoyal2707

Differential Revision: D23833554

Pulled By: myleott

fbshipit-source-id: c8b910ec051d8cb0bda4e41b5d9ff249ff88d32b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1451,Gagandeep Singh,gagandeep.singh1@nuance.com,2020-09-22 09:14:32-07:00,22007c4419da1108af2f5ac54560c73f047e7b36,https://github.com/pytorch/fairseq/commit/22007c4419da1108af2f5ac54560c73f047e7b36,"Pad sequence lengths to a multiple (#2642)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Provides an option to pad the sequence lengths to a multiple. We found this provided significant speed-up when using tensor-cores (multiple of 8). Currently the code only allows for the number of sequences in a batch to be a multiple of 8 which we found to be not as important as padding the batch length to a multiple of 8.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2642

Reviewed By: ngoyal2707

Differential Revision: D23833542

Pulled By: myleott

fbshipit-source-id: 7af53c3ee3d6388eafdd2f4d29f6b696cbc4fa3b",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1452,Xu Song,xusong.vip@gmail.com,2020-09-22 09:31:39-07:00,9e4088bc3d2630d5a4285138662fed4426190e73,https://github.com/pytorch/fairseq/commit/9e4088bc3d2630d5a4285138662fed4426190e73,"Update sentence_prediction.py (#2594)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

It is more preferable to use `cls` rather than the actual class name to construct an instance.

Actual class name may cause issues if you subclass it and use the class method.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2594

Reviewed By: myleott

Differential Revision: D23607595

Pulled By: lematt1991

fbshipit-source-id: 2ca5bb455876817094e8d6153db466e2c8f65192",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1453,Myle Ott,myleott@fb.com,2020-09-22 11:44:59-07:00,66f66a4fc7139fdaa724d0854520ef5f68eb42ea,https://github.com/pytorch/fairseq/commit/66f66a4fc7139fdaa724d0854520ef5f68eb42ea,"Gate psutil import to make tests pass (#1282)

Summary:
Gate psutil import to make tests pass

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1282

Reviewed By: tangyuq

Differential Revision: D23822037

Pulled By: myleott

fbshipit-source-id: c652c7931147ecd377d78322840e343c55cb85a2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1454,Yuqing Tang,yuqtang@fb.com,2020-09-22 11:46:23-07:00,3f484789de756ed03a19df9c2ca0bcae205aae97,https://github.com/pytorch/fairseq/commit/3f484789de756ed03a19df9c2ca0bcae205aae97,"Add sizes to TransformEosLangPairDataset

Summary: LangPairDataset was updated with sizes to improve efficiency, so also added it to TransformEosLangPairDataset.

Reviewed By: pipibjc

Differential Revision: D23817426

fbshipit-source-id: d62a4ac3535a06618ff70c717ab55f43fc6d48c6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1455,Kaushik Rangadurai,krangadu@fb.com,2020-09-22 14:04:38-07:00,3a979a40e66a3a4006da55809b2f4dd6bd517d20,https://github.com/pytorch/fairseq/commit/3a979a40e66a3a4006da55809b2f4dd6bd517d20,"Remove in-place operations in TransformerSentenceEncoder for captum insights

Summary:
When using captum insights, we can't take gradients on leaf operations that have in-place operations.

This Notebook - N355930 demonstrates this with a simple example

Reviewed By: myleott

Differential Revision: D23827055

fbshipit-source-id: d49b8ec9a016ccc7195438be9b1782e08479d90d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1456,Alexei Baevski,abaevski@fb.com,2020-09-22 21:15:34-07:00,66101b0564d99daa598ead2cc75615e96b362411,https://github.com/pytorch/fairseq/commit/66101b0564d99daa598ead2cc75615e96b362411,"hydra fairseq 5 - add module dataclasses

Summary: hydra fairseq 5 - add module dataclasses

Reviewed By: myleott

Differential Revision: D23416496

fbshipit-source-id: f8cf811d73ed81391eb0e48ecf5b6a4104341b08",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1457,alexeib,alexei.b@gmail.com,2020-09-24 23:26:54-07:00,53f135765c018c5241ed41ab7c1bc19be257fa90,https://github.com/pytorch/fairseq/commit/53f135765c018c5241ed41ab7c1bc19be257fa90,"fix tensor layout issues going from 1.4 to 1.5/6 (#1294)

Summary:
this fixes decoding with wav2letter decoder during training. it is an issue with pytorch 1.5+ where cpu() no longer implies contiguous() and so data_ptr points to non-contiguous layouts.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1294

Reviewed By: pipibjc

Differential Revision: D23925604

Pulled By: alexeib

fbshipit-source-id: 68d5ce7cf45c008bace3dc4a995cec6b46567b3d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1458,Seppo Enarvi,Seppo.Enarvi@nuance.com,2020-09-25 08:28:10-07:00,3b7d85c91f0afaa8b78a3bcb9b216a2ff38c1b01,https://github.com/pytorch/fairseq/commit/3b7d85c91f0afaa8b78a3bcb9b216a2ff38c1b01,"Transformer with integrated pointer-generator network (#2529)

Summary:
This pull request implements a variant of the Transformer model that uses an attention distribution for pointing to input words. The attention distribution over the input words is interpolated with the normal output distribution over the vocabulary words, as in [See et al. (2017)](https://arxiv.org/abs/1704.04368). This allows the model to generate words that appear in the input, even if they don't appear in the vocabulary, helping especially with small vocabularies.

The mechanism for copying out-of-vocabulary words from the input has been implemented differently to See et al. In their [implementation](https://github.com/abisee/pointer-generator) they convey the word identities through the model in order to be able to produce out-of-vocabulary words. We wanted to minimize changes to the Fairseq code base and took a different approach, which I'll describe below. The entire implementation is contained in one file (plus there's one new test).

Copying out-of-vocabulary words is possible by pre-processing the input and post-processing the output. The user may add special words to the end of the vocabulary that can be used in place of `<unk>` tokens to identify different input positions (e.g. `<unk-0>`, `<unk-1>`, `<unk-2>`, ...). The number of these special words is given to the model with the `--source-position-markers` argument—the model simply maps all of these to the same word embedding as `<unk>`. With a simple post-processing the user may retrieve word at position N in the original text and use it in place of `<unk-N>`.

I didn't find a good place to document this usage of this model, so let me know if you think I should improve documentation somewhere.

This feature has not yet been discussed via a GitHub issue, but I'll open a new issue for discussion.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2529

Reviewed By: ngoyal2707

Differential Revision: D23398430

Pulled By: myleott

fbshipit-source-id: f2f26c8ce8802ae6cf95515637660348ff3fc457",8,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1459,Joshua Meier,jmeier@fb.com,2020-09-25 13:55:55-07:00,8dde7de8a22d6e59c4101fe0de618f888c33ba81,https://github.com/pytorch/fairseq/commit/8dde7de8a22d6e59c4101fe0de618f888c33ba81,"Fix dummy batch issues (#1293)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes issues when `sample.keys` changes across subsets. Without this diff, dummy batches are reused across epochs, which causes a hang if subsets have different keys.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1293

Reviewed By: myleott

Differential Revision: D23924558

Pulled By: joshim5

fbshipit-source-id: d57cb1f545f649ec4dac62a43b088e637797c90f",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1460,Alexei Baevski,abaevski@fb.com,2020-09-26 15:50:04-07:00,94a1b924f3adec25c8c508ac112410d02b400d1e,https://github.com/pytorch/fairseq/commit/94a1b924f3adec25c8c508ac112410d02b400d1e,"hydra fairseq 6 - add_args from dataclass except migrated model

Summary: hydra fairseq 6 - add_args from dataclass except migrated model

Reviewed By: myleott

Differential Revision: D23416669

fbshipit-source-id: 223f773384dab95d5a90095379c9b88e4a12754d",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1461,Myle Ott,myleott@fb.com,2020-09-28 15:31:18-07:00,a524832d1d6883de90bd0c6bc5fd039d6f87a000,https://github.com/pytorch/fairseq/commit/a524832d1d6883de90bd0c6bc5fd039d6f87a000,"Publish Linformer to public fairseq

Summary: Initial open source release for Linformer

Reviewed By: madian9

Differential Revision: D22771263

fbshipit-source-id: bf08c64c5ecb899db9da00b79d09f6308347c915",10,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1462,Myle Ott,myleott@fb.com,2020-09-29 07:26:18-07:00,caea771afafbcb3471f9007ca1cd46a4d3d8c869,https://github.com/pytorch/fairseq/commit/caea771afafbcb3471f9007ca1cd46a4d3d8c869,"Fix tests (#2670)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2670

Reviewed By: ngoyal2707

Differential Revision: D23982491

Pulled By: myleott

fbshipit-source-id: 629b791d6c05dd67b63dcc2da0313c6799f777f8",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1463,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-09-29 09:07:22-07:00,df1b3c6a8bb391e21c2c930b9050dec8523a37cc,https://github.com/pytorch/fairseq/commit/df1b3c6a8bb391e21c2c930b9050dec8523a37cc,"Allow splitting of embeddings for more even distribution of optimizer states across DDP nodes when using DeepSpeed Zero (#1295)

Summary:
* Embedding parameters take up a lot of optimizer state memory when the vocab size is huge (e.g. >100K) and the embedding dimension is huge (e.g. 4-6K)
* This causes the DDP worker carrying the optimizer states of the embedding parameter to be more overloaded than other DDP workers.
* To avoid this, this PR adds the ability to divide embedding parameters into chunks (maintaining functional parity with having a single embedding parameter) so that different DDP workers can be assigned different embedding parameter chunks for OSS and one DDP worker doesn't have to hold optimizer states for a huge embedding parameter.

Testing details in this PR: https://github.com/fairinternal/fairseq-py/pull/1226

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1295

Reviewed By: myleott

Differential Revision: D23980434

Pulled By: shruti-bh

fbshipit-source-id: a8731b4016aff3f944b0706327694457348d0979",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1464,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-09-29 14:28:30-07:00,73ad5d4abe9442454ace141e1743df610a9aecae,https://github.com/pytorch/fairseq/commit/73ad5d4abe9442454ace141e1743df610a9aecae,"Pass long string arguments as files (#1296)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1296

Reviewed By: huihuifan

Differential Revision: D23991462

Pulled By: shruti-bh

fbshipit-source-id: 00dd0de22414b20c587e45b9e9108e9946808c77",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1465,Alexei Baevski,abaevski@fb.com,2020-09-29 15:48:46-07:00,82c116d20073c9c54ace4555fecfbcd782fb46a0,https://github.com/pytorch/fairseq/commit/82c116d20073c9c54ace4555fecfbcd782fb46a0,"split parallel transformer lm base arch

Summary: move base_arch to parallel transformer lm so it doesnt not depend on regular transformer lm

Reviewed By: myleott

Differential Revision: D23417282

fbshipit-source-id: 32e2d7294b4ec0d52598d3da829e80922cb4b576",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1466,Monideep De,monideep.de@gmail.com,2020-09-29 17:45:05-07:00,e4a5427ef4ffad63fa265acbade098bb963a814b,https://github.com/pytorch/fairseq/commit/e4a5427ef4ffad63fa265acbade098bb963a814b,"Updated link to wav2letter repository (#2663)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # 2662.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2663

Reviewed By: alexeib

Differential Revision: D24001964

Pulled By: myleott

fbshipit-source-id: 6a264c9cf53d77bb0062c41ec5ff03c5552f3e55",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1467,Joshua Meier,jmeier@fb.com,2020-09-30 22:31:01-07:00,356973065100c43798a416ce0235a4582e5cb48d,https://github.com/pytorch/fairseq/commit/356973065100c43798a416ce0235a4582e5cb48d,"Fix bug in subsample dataset (shuffle was never assigned) (#1298)

Summary:
Fix bug in subsample dataset (shuffle was never assigned)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1298

Reviewed By: myleott

Differential Revision: D23940719

Pulled By: joshim5

fbshipit-source-id: 8127d42ecf9e359312df104f8ee90bda25589023",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1468,Joshua Meier,jmeier@fb.com,2020-10-01 06:26:36-07:00,805d00aa7c50c6fc17bbcc30b10255777bf03607,https://github.com/pytorch/fairseq/commit/805d00aa7c50c6fc17bbcc30b10255777bf03607,"support multiple data parallel groups for ZeRO optimizer state sharding (#1326)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/fairinternal/fairseq-py/issues/1299. Previously, optimizer state sharding was causing incorrect results when used with model parallel. This resolves the issue. In order to use this with Fairseq, you'll also need to use the version of [Fairscale from the PR I opened there](https://github.com/facebookresearch/fairscale/pull/121).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1326

Reviewed By: myleott

Differential Revision: D24042124

Pulled By: joshim5

fbshipit-source-id: cdc8be2a00096cf7ae4e4918915431b5d493aad3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1469,Joshua Meier,jmeier@fb.com,2020-10-01 06:28:48-07:00,7d2a3e10a9436c2e3a006a94cc3229e2920ff71c,https://github.com/pytorch/fairseq/commit/7d2a3e10a9436c2e3a006a94cc3229e2920ff71c,"Prevent failing when there is no valid set (#1328)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
If the valid set is small or non-existent, Fairseq will fail because `valid_losses` will not have been defined anywhere. This fixes the issue.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1328

Reviewed By: myleott

Differential Revision: D24042313

Pulled By: joshim5

fbshipit-source-id: ef45ade8e2d5f853364a05e03f566968e9aaa089",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1470,Seppo Enarvi,Seppo.Enarvi@nuance.com,2020-10-01 12:35:54-07:00,c049749c7a7c08cca9e4663c85bd3961f4b260f8,https://github.com/pytorch/fairseq/commit/c049749c7a7c08cca9e4663c85bd3961f4b260f8,"Fix full-context alignment with transformer_align model (#2675)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2673.

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2673 (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2675

Reviewed By: ngoyal2707

Differential Revision: D24001793

Pulled By: myleott

fbshipit-source-id: 6b4e9270e5f5a31ba1b65ae2ae717019108af913",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1471,Myle Ott,myleott@fb.com,2020-10-01 15:54:15-07:00,65d87b0605b2a930397a99ea11083d7b55f03277,https://github.com/pytorch/fairseq/commit/65d87b0605b2a930397a99ea11083d7b55f03277,"Fixes for TPUs (#1324)

Summary:
- support training on a single TPU core
- fix clip grad norm logic
- log memory usage
- fix --memory-efficient-bf16
- print XLA compilation warnings on every device

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1324

Reviewed By: ngoyal2707

Differential Revision: D24024226

Pulled By: myleott

fbshipit-source-id: 5ae178e663e69923776196da3c0e3217efdecb61",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1472,Yuqing Tang,yuqtang@fb.com,2020-10-01 16:05:29-07:00,0557ed8b0df90fe671bcb745f384ef7fd0386ab3,https://github.com/pytorch/fairseq/commit/0557ed8b0df90fe671bcb745f384ef7fd0386ab3,"Enable FileContentsAction to handle manifold file

Summary: The latest FileContentsAction can not handle manifold files

Reviewed By: shruti-bh

Differential Revision: D24059296

fbshipit-source-id: 05cfa227b55d297498c03c33347a24e1914064b4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1473,Myle Ott,myleott@fb.com,2020-10-02 10:48:15-07:00,f902a363abc578906f29239f995cacce5e93a807,https://github.com/pytorch/fairseq/commit/f902a363abc578906f29239f995cacce5e93a807,"Small fixes (#1325)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1325

Reviewed By: ngoyal2707

Differential Revision: D24024198

Pulled By: myleott

fbshipit-source-id: c3b776970d625eff21a26bf7c86cd28ef9e9d2ef",11,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1474,Dmitriy Genzel,dgenzel@fb.com,2020-10-02 12:35:58-07:00,7c392f7d0ea54485c303fd8349ea34b446acaada,https://github.com/pytorch/fairseq/commit/7c392f7d0ea54485c303fd8349ea34b446acaada,"Provide proper diagnostic for an empty batch

Summary:
Dummy batch logic as introduced in D23924558 (https://github.com/pytorch/fairseq/commit/8dde7de8a22d6e59c4101fe0de618f888c33ba81) does not provide a proper diagnostic message if the first batch given is empty. This change makes sure that the assertion in lines 799-804 is actually triggered in this case.

Otherwise this change is a noop.

Reviewed By: joshim5

Differential Revision: D24081932

fbshipit-source-id: 45fc49fb8a5f9a49f858683f97b5b27455f9a8d5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1475,Myle Ott,myleott@fb.com,2020-10-02 19:00:29-07:00,7c292af66f61b1125854218519bf81d494e5b11e,https://github.com/pytorch/fairseq/commit/7c292af66f61b1125854218519bf81d494e5b11e,"Fix hub (#2687)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2687

Reviewed By: alexeib

Differential Revision: D24095130

Pulled By: myleott

fbshipit-source-id: 7d371bccb550ec68b2b9b39dfa4c0718356508d6",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('True', '(all(')]",[],[],[],[],[],[],[],[],[],[],"[('True', '(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device=, requires_grad=True))))')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1476,Alexei Baevski,abaevski@fb.com,2020-10-02 21:23:15-07:00,5e82514d687289a73a6dec33b555217acd97cb0d,https://github.com/pytorch/fairseq/commit/5e82514d687289a73a6dec33b555217acd97cb0d,"update registries (#1330)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1330

construct args from dataclasses

Reviewed By: myleott

Differential Revision: D23973591

fbshipit-source-id: 6d7a4b80c2a815bcabd6f955513e5cd8f5cf5ab4",37,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1477,alexeib,alexei.b@gmail.com,2020-10-05 19:07:38-07:00,e3c4282551e819853952284681e9ed60398c5c4a,https://github.com/pytorch/fairseq/commit/e3c4282551e819853952284681e9ed60398c5c4a,"remove max_sentences from args, use batch_size instead (#1333)

Summary:
now that we are moving to using dataclasses to define fairseq configuration, having aliases for options is no longer practical. this pr removes ""max-sentences"" argument while keeping its alias ""batch-size"", which is more appropriate

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1333

Reviewed By: shruti-bh

Differential Revision: D24121305

Pulled By: alexeib

fbshipit-source-id: 34343cea54c8f2c8b059c38ef9f29b66e76df9fb",34,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1478,Joshua Meier,jmeier@fb.com,2020-10-06 13:13:42-07:00,e056de1fb641425974b0941704e63b9b42fe8a1f,https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f,"Rework dummy batch logic (#1331)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
This PR reworks the dummy batch logic in fairseq. It does away with `""DUMMY""` strings and instead has `trainer.get_train_iterator` and `trainer.get_valid_iterator` set up the dummy batch by taking the first sample in the dataset.

See conversation here where the issue was reported and discussed: https://fb.workplace.com/groups/fairseq/permalink/1241915899501646/

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1331

Reviewed By: dgenzel2

Differential Revision: D24093421

Pulled By: joshim5

fbshipit-source-id: 71e747d90496f5158d07f0c2db87b6a6a974ef4f",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1479,Alexei Baevski,abaevski@fb.com,2020-10-06 16:59:20-07:00,bcc81f6d5291c3996c8b2472282458dead46343f,https://github.com/pytorch/fairseq/commit/bcc81f6d5291c3996c8b2472282458dead46343f,"replace max-sentences with batch-size for dependencies

Summary: this fixes some regressions introduced by D24121305 (https://github.com/pytorch/fairseq/commit/e3c4282551e819853952284681e9ed60398c5c4a). fairseq configuration is changing from command line to dataclasses (via hydra eventually) which no longer supports option aliases. one such alias is --max-sentences / --batch-size, and D24121305 (https://github.com/pytorch/fairseq/commit/e3c4282551e819853952284681e9ed60398c5c4a) removed --max-sentences as --batch-size is more appropriate (fairseq is not just an nlp framework dealing with sentences). unfortunately it seems some existing flows broke and this diff attempts to fix this

Differential Revision: D24142488

fbshipit-source-id: 075180ea10a9d706a3f8d64b978d66dfd83c3d2b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1480,Dmitriy Genzel,dgenzel@fb.com,2020-10-06 22:02:10-07:00,63aca3ff6cf1bcf87edb72ab35f48ddc106bb9a6,https://github.com/pytorch/fairseq/commit/63aca3ff6cf1bcf87edb72ab35f48ddc106bb9a6,"Fix a bad merge from D24093421

Summary:
Fix a bad merge from D24093421 (https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f)

Also linted

Reviewed By: joshim5

Differential Revision: D24154657

fbshipit-source-id: 98afbd9f7ea4200756f07f3271b66fe6477dafff",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1481,Dmitriy Genzel,dgenzel@fb.com,2020-10-07 09:17:06-07:00,b880744c1e6b7c526ab8a2c22161b105778e807e,https://github.com/pytorch/fairseq/commit/b880744c1e6b7c526ab8a2c22161b105778e807e,"Fix a bad merge from D24154657 - second try

Summary: This removes a line that reset the dummy batch in begin_epoch. In actuality the batch is reset in get_{train,valid}_iterator, and this line was resetting it to None unnecessarily.

Reviewed By: joshim5, jhcross

Differential Revision: D24157057

fbshipit-source-id: 59ac68327094ceff70f66d7b471fa810997fe84e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1482,alexeib,alexei.b@gmail.com,2020-10-07 16:38:49-07:00,5379461e613263911050a860b79accdf4d75fd37,https://github.com/pytorch/fairseq/commit/5379461e613263911050a860b79accdf4d75fd37,"lm rescoring attempt (#1242)

Summary:
CUDA_VISIBLE_DEVICES=1 PYTHONPATH=/private/home/abaevski/fairseq-py-master python fairseq_cli/generate.py /checkpoint/henryzhou7/dataset/libri/960h/raw3/decoder --task audio_pretraining --seed 1 --nbest 1 --gen-subset dev_other --max-tokens 600000 --path ~/models/wav2vec2/vox_960h_seq2seq_10kwp.pt --labels 10k --remove-bpe 'wordpiece' --quiet --beam 50 --temperature 1 --scoring wer --lm-path /checkpoint/henryzhou7/wp_lm/transformer_raw3_adam_cosine2node/lr_1e-4_updatefreq_8/checkpoint_best.pt --lm-weight 1

results:

no lm: 4.30577896347444
lm (1.5): 24.691650853889943
lm (1): 10.884539582804846
lm (0.5): 4.894205665744457
lm (0.25): 4.012853671917862
lm (0.1): 4.087637055489084
lm (0.05): 4.194788887144875

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1242

Reviewed By: kahne

Differential Revision: D23277386

Pulled By: alexeib

fbshipit-source-id: 062f483bd45ddd2dd5ff24a8a35cc1c4f34ce6ab",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1483,Alex Xiao,axiao@fb.com,2020-10-08 17:36:06-07:00,fcb75729c01c112b3a58539777260d352eb4cd5d,https://github.com/pytorch/fairseq/commit/fcb75729c01c112b3a58539777260d352eb4cd5d,"don't store DDP model in fairseq nan detector

Summary:
I ran into an error with nan detector recently: f221506814
```
torch.nn.modules.module.ModuleAttributeError: 'AddJoinerSparse' object has no attribute '_parameters'
```
full log: P144224349

It seems to fail when we do a deepcopy of the joiner in our implementation of transducer transformer: https://fburl.com/diffusion/78jkfk2z

Very interestingly, it seems to be copying a DistributedDataParallel object! This seemed really weird, since a user module shouldn't really contain a reference to a DistributedDataParallel object.

After investigation this seems to be because of the backward hooks that `NanDetector` adds to the module. The backward hooks reference NanDetector, which references `model`, which is the `DistributedDataParallel` object.

The fix is then to not store a reference to the `DistributedDataParallel` in `NanDetector`

Reviewed By: zhengwy888

Differential Revision: D24058995

fbshipit-source-id: 48209339243d8b23b078274b780e850335839e89",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1484,Wei Ho,weiho@fb.com,2020-10-09 13:33:33-07:00,bf06ca7cab3b0ef9f572453a76aa35a42feab1f6,https://github.com/pytorch/fairseq/commit/bf06ca7cab3b0ef9f572453a76aa35a42feab1f6,"Improve dictionary & checkpoint reading w/ local caching

Reviewed By: myleott

Differential Revision: D24148700

fbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1485,Yuqing Tang,yuqtang@fb.com,2020-10-09 17:08:49-07:00,aa39ab1b4568479bf9a1360cfcdd4f4fce5f1838,https://github.com/pytorch/fairseq/commit/aa39ab1b4568479bf9a1360cfcdd4f4fce5f1838,"Adapt prod fairseq eval to latest vocab changes

Summary:
# Facebook:
With changes in D23653256, we don't have thrift vocab anymore. This diff changes the fairseq eval accordingly.

Reviewed By: chtran

Differential Revision: D23831452

fbshipit-source-id: 5e4f39140c9f25d99324fb7eded42c7fca439d3f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1486,alexeib,alexei.b@gmail.com,2020-10-11 19:30:50-07:00,60442af216d551e4afc9d4fab1c056c1051725cc,https://github.com/pytorch/fairseq/commit/60442af216d551e4afc9d4fab1c056c1051725cc,"add support for ""const"" argparse converts for now (#1338)

Summary:
Fixes issue #2705
Re: [pytorch/fairseq] The registries update forces the ""--remove-bpe"" option to require an argument (#2705)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1338

Reviewed By: myleott

Differential Revision: D24242258

Pulled By: alexeib

fbshipit-source-id: 0eafcae8de3476c4237b1a32bad203dd9e940cc3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1487,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-10-12 10:54:19-07:00,3ab136e41ef3eb91f55d997377d3a2c18e1e1438,https://github.com/pytorch/fairseq/commit/3ab136e41ef3eb91f55d997377d3a2c18e1e1438,"Support generation with huge pipeline parallel Transformer models (#1297)

Summary:
## What is this PR about?
* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)
* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint
* Support generation with PipelineParallelTransformer models

## Testing

```
python fairseq_cli/generate.py \
    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \
    --batch-size 1 \
    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \
    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \
    --task translation_multi_simple_epoch \
    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \
    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \
    --dataset-impl mmap \
    --distributed-world-size 1 --distributed-no-spawn \
    --pipeline-model-parallel \
    --pipeline-chunks 1 \
    --pipeline-encoder-balance '[26]' \
    --pipeline-encoder-devices '[0]' \
    --pipeline-decoder-balance '[26]' \
    --pipeline-decoder-devices '[0]' \
    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt
2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca
use misalignment in pretraining and finetuning.
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb
t', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',
 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '
pt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples
2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt
balance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
S-2521  __en__ No.
T-2521  No.
H-2521  -2.4406352043151855     y no.
D-2521  -2.4406352043151855     y no.
P-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565
S-2261  __en__ Why?
T-2261  ¿Por qué?
H-2261  -1.7077901363372803     ¿Y por qué?
D-2261  -1.7077901363372803     ¿Y por qué?
P-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297

Reviewed By: myleott, msbaines

Differential Revision: D23991647

Pulled By: shruti-bh

fbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1488,Alex Xiao,axiao@fb.com,2020-10-12 11:23:44-07:00,a9baca376616bed56e5df5115d7adf8059c0d296,https://github.com/pytorch/fairseq/commit/a9baca376616bed56e5df5115d7adf8059c0d296,"add supports_fetch_outside_dataloader property to avoid creating real first batches for datasets that only expect to be used inside dataloader workers

Summary:
D24093421 (https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f) added `first_batch` to iterators in fairseq. This means that FairseqDataset objects now might need to fetch data outside the dataloader workers. This causes issues with certain datasets, in particular datasets that fetch data via everstore/memcache, since these clients open a ton of file descriptors based on how many items are fetched. Opening too many file descriptors causes forking to fail in python multiprocessing.

To fix this, lets have a property `supports_fetch_outside_dataloader` in the FairseqDataset that allows us to decide if it is safe to fetch the first batch. If it is not, we will revert back to the original behavior before D24093421 (https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f) which is to just use ""DUMMY"", and set this as a real batch late.r

Reviewed By: yqwangustc

Differential Revision: D24234470

fbshipit-source-id: 7ad66a6de622ce26f59f00d00b19700fbd992921",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1489,Elijah Rippeth,elijah.rippeth@gmail.com,2020-10-12 14:01:46-07:00,e0d5d8e669528be579d7aa4749fbcfe5cacdce90,https://github.com/pytorch/fairseq/commit/e0d5d8e669528be579d7aa4749fbcfe5cacdce90,"refactor build_generator to reduce code duplication. (#2716)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2693

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2716

Reviewed By: kahne

Differential Revision: D24243384

Pulled By: myleott

fbshipit-source-id: cdf4fb3b97d87dd8dbb0ea7cdb5f286277892d81",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1490,Sharvil Nanavati,sharvil.nanavati@gmail.com,2020-10-12 15:19:50-07:00,fc1c38aa1c70e1d1ef45a6af335e3c6571ba436d,https://github.com/pytorch/fairseq/commit/fc1c38aa1c70e1d1ef45a6af335e3c6571ba436d,"Fix broken links to Wav2Vec2 Large checkpoints (#2657)

Summary:
Typo fixes.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2657

Reviewed By: alexeib

Differential Revision: D24237518

Pulled By: myleott

fbshipit-source-id: d101a8b8cc9c8d725eb63265c85be92f6b2c5a6c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1491,Helen Craig,craigca5@yahoo.com,2020-10-13 08:06:03-07:00,f910ea9d4cf9c9964ec307dde3144622c4b61e62,https://github.com/pytorch/fairseq/commit/f910ea9d4cf9c9964ec307dde3144622c4b61e62,"Update sequence_scorer.py (#2715)

Summary:
Changing so attention is returned for joint alignment example.

related to this issue:
https://github.com/pytorch/fairseq/issues/2695
And this one:
https://github.com/pytorch/fairseq/issues/2634

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2715

Reviewed By: pipibjc

Differential Revision: D24237512

Pulled By: myleott

fbshipit-source-id: 2b2be8002ab20b89fd6a8ef6e9d2b74063c5c7c8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1492,Myle Ott,myleott@fb.com,2020-10-13 08:07:20-07:00,c005362349075fb5952ece139481232cc49e2286,https://github.com/pytorch/fairseq/commit/c005362349075fb5952ece139481232cc49e2286,"Misc fixes (#1341)

Summary:
- Fix all_reduce_dict for non-homogenous sized tensors (fixes #2707)
- Add extra docs for tutorial-specific code (fixes #2554)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1341

Reviewed By: pipibjc

Differential Revision: D24264129

Pulled By: myleott

fbshipit-source-id: 58619bb34afe51ea956abe8e6b41505f35417c09",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1493,Elijah Rippeth,elijah.rippeth@gmail.com,2020-10-13 08:07:57-07:00,d6cdc2f47b74e3126df748c0da02be43d7356a07,https://github.com/pytorch/fairseq/commit/d6cdc2f47b74e3126df748c0da02be43d7356a07,"bump fairseq version in preparation for a release. (#2717)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1948

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

This may be something to discuss - it seems like there's a lot of confusion about what features are supported when in fairseq. Hopefully versioning will allow for more discrete cuts.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2717

Reviewed By: pipibjc

Differential Revision: D24244678

Pulled By: myleott

fbshipit-source-id: 4d4c7bd13387c43fb11c64d7e62985d212b5a02a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1494,Wei Ho,weiho@fb.com,2020-10-13 12:32:58-07:00,85f097141d83d6aac378838b6c0c8f2a0f77154f,https://github.com/pytorch/fairseq/commit/85f097141d83d6aac378838b6c0c8f2a0f77154f,"Remove FileContentsAction for --langs

Summary:
# Facebook:

Revert changes made in D24059296 (https://github.com/pytorch/fairseq/commit/0557ed8b0df90fe671bcb745f384ef7fd0386ab3) since it breaks normal --langs usage. To specify languages in a file, use --lang-dict instead

Also update integration test params so it can catch this

Reviewed By: tangyuq

Differential Revision: D24224622

fbshipit-source-id: 292eeb86e02528128ced09f8165045be9c847c19",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1495,Nicola De Cao,nicola.decao@uva.nl,2020-10-14 08:28:12-07:00,086fe1c5d1317caad090b2ff60f965d2dfa130f7,https://github.com/pytorch/fairseq/commit/086fe1c5d1317caad090b2ff60f965d2dfa130f7,"adding search.PrefixConstrainedBeamSearch (#2646)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
This adds a new decoding strategy `search.PrefixConstrainedBeamSearch` that limits the vocabulary of the next token generation given a prefix (that is the previously generated tokens during beam search). An end user has just to give the optional argument `prefix_allowed_tokens_fn` to `.generate` or `.sample` to activate `PrefixConstrainedBeamSearch`. `prefix_allowed_tokens_fn(batch_id, tokens)` is a callback function that given the `batch_id` and `tokens` returns the list of allowed token for the next generation step.

## Did you have fun?
YES! �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2646

Reviewed By: fabiopetroni

Differential Revision: D24006805

Pulled By: myleott

fbshipit-source-id: 40b1a866c6ea9f936272db27e2a020b18dbf8164",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1496,Vasiliy Alekseev,alvasian@yandex.ru,2020-10-14 09:30:29-07:00,5e831033069b52b09905e0bf8ba104d016e04efd,https://github.com/pytorch/fairseq/commit/5e831033069b52b09905e0bf8ba104d016e04efd,"Fix apply_sparse_mask signature in MultiheadAttention (#2587)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2574 (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2587

Reviewed By: myleott

Differential Revision: D23607694

Pulled By: lematt1991

fbshipit-source-id: b8fd27cf9a4fc4287f333a4422ad43fa93128615",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1497,Louis Martin,louisrtm@gmail.com,2020-10-14 09:46:03-07:00,4948d890a4701170c5a84b62cfd310e08af39273,https://github.com/pytorch/fairseq/commit/4948d890a4701170c5a84b62cfd310e08af39273,"Fix link in CamemBERT readme (#2722)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fix link in CamemBERT readme

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2722

Reviewed By: louismartin

Differential Revision: D24307327

Pulled By: myleott

fbshipit-source-id: c3c29a19de06a8062fa7f7212ad6df0d549ad25f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1498,Myle Ott,myleott@fb.com,2020-10-14 10:30:28-07:00,c4d322ad9d3e6907e54976289d95c3d9b571a5c3,https://github.com/pytorch/fairseq/commit/c4d322ad9d3e6907e54976289d95c3d9b571a5c3,"Fix library usage of --user-dir (primarily affects tests) (#1346)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1346

Reviewed By: xianxl

Differential Revision: D24306363

Pulled By: myleott

fbshipit-source-id: 90c4b59031f04b925ad12a13a96d9225ab0a09b4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1499,Chau Tran,chau@devfair0209.h2.fair,2020-10-14 10:31:15-07:00,a2d0be4989c0be5c2f08358e85d3c568029fd6dd,https://github.com/pytorch/fairseq/commit/a2d0be4989c0be5c2f08358e85d3c568029fd6dd,"Add CRISS README and code to fairseq (#1344)

Summary:
# Before submitting

- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [Y] Did you make sure to update the docs?
- [N/A] Did you write any new necessary tests?

## What does this PR do?
Add code to reproduce results from Cross-lingual Retrieval for Iterative Self-supervised Training.
## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1344

Test Plan:
Imported from GitHub, without a `Test Plan:` line.

See https://github.com/fairinternal/fairseq-py/tree/criss_pr/examples/criss

Reviewed By: myleott

Differential Revision: D24268469

Pulled By: chtran

fbshipit-source-id: d4dd36b22bde3c364ce6e935bd39baf8f96e0735",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1500,Changhan Wang,changhan@fb.com,2020-10-14 12:27:45-07:00,1d1c145387e37bb10800190f8d31b144b4e7e182,https://github.com/pytorch/fairseq/commit/1d1c145387e37bb10800190f8d31b144b4e7e182,"speech-to-text OSS

Summary:
Imported from https://github.com/fairinternal/fairseq-py/pull/1284. Updated according to PR comments.

Main changes:
* New task: `fairseq.tasks.speech_to_text`
  * Multilingual support: multiple train sub-splits, temperature-based sampling, language ID tokens
* New dataset: `fairseq.data.audio.speech_to_text_dataset`
* Added accuracy metrics and BOS prefix removal to label smoothed cross entropy
* New models: Transformer (`fairseq.models.speech_to_text.s2t_transformer`) and BLSTM (`fairseq.models.speech_to_text.berard`)
* Extended scorers:
  * Added a base scorer class: `fairseq.scorers.BaseScorer` (the parent class for all scorers except the BLEU scorer in CPP)
  * Added an evaluation tokenizer: `fairseq.scorers.eval_tokenizer` which leverages sacreBLEU's built-in tokenizers and allows character-level tokenization as well as punctuation removal (for WER scoring).
  * Added chrF scorer: `fairseq.scorers.chrf`
* Online Mel-filter bank speech feature extraction (via CPP-based pyKaldi or Python-based TorchAudio): `fairseq.data.audio.audio_utils`
* Online speech feature transforms: `fairseq.data.audio.feature_transforms.*`
* Fixed the subsampled sequence lengths in VGGTransformer (`examples.speech_recognition.models.vggtransformer`)
* Examples under `examples/speech_to_text`:
  * LibriSpeech (ASR): better results than VGGTransformer with smaller Transformer-based models
  * MuST-C (ST): comparable to [SOTA results](https://arxiv.org/pdf/2004.10234.pdf) but with less tricks

Reviewed By: jmp84

Differential Revision: D24065273

fbshipit-source-id: 5f842ca9c826f92d4af660705611885fe440a9ab",22,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1501,Changhan Wang,changhan@fb.com,2020-10-14 12:27:45-07:00,6d3712ed91cc2341f3b70090c34e5f9bf5f839ab,https://github.com/pytorch/fairseq/commit/6d3712ed91cc2341f3b70090c34e5f9bf5f839ab,"Update scorer with evaluation-time tokenizer and chrF

Summary:
* Added evaluation-time tokenizer (using sacreBLEU's built-in tokenizers): `fairseq.scoring.tokenizer`
* Added chrF scorer: `fairseq.scoring.chrf`
* Updated sacreBLEU scorer with evaluation-time tokenizer
* Updated WER scorer with evaluation-time tokenizer: There are cases where we train ASR models without pre-tokenization or punctuation removal. The tokenization/normalization is done at evaluation time before scoring.

Reviewed By: myleott

Differential Revision: D24219634

fbshipit-source-id: ecde21cb19206b96efff7606e101d476d5687888",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1502,Sergey Edunov,edunov@fb.com,2020-10-14 12:55:40-07:00,3c118ad1bad66137c95e0a0d970e8b57b6848065,https://github.com/pytorch/fairseq/commit/3c118ad1bad66137c95e0a0d970e8b57b6848065,"Adding tok.sh and installation scripts for the dependecies (#1339)

Summary:
Adding tok.sh needed to evaluate performance of multilingual models. Aside tok.sh added installation script ""install_dependecies.sh"" that will install all the needed dependencies except Arabic. Arabic requires downloading separate installation packages and signing licensing agreements, so it can't be automated.

# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [X] Did you make sure to update the docs?
- [X] Did you write any new necessary tests?

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1339

Reviewed By: shruti-bh

Differential Revision: D24311526

Pulled By: edunov

fbshipit-source-id: fe9d46b0c7d7dc090e03f504e048b0c6eb616df2",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1503,Sergey Edunov,edunov@fb.com,2020-10-14 14:18:39-07:00,3544f5f24eb52f3a7c5f2dba78462ca08d52c1f0,https://github.com/pytorch/fairseq/commit/3544f5f24eb52f3a7c5f2dba78462ca08d52c1f0,"Releasing single pre-finetuning models (#1347)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1347

Reviewed By: michaelauli, shruti-bh

Differential Revision: D24315287

Pulled By: edunov

fbshipit-source-id: d94955866b5424ab9c6a78982140e2bd7d1b279b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1504,Xian Li,xianl@devfair0251.h2.fair,2020-10-15 09:23:54-07:00,573c2f4b60a50dc7c4ff17084b753c05452381f9,https://github.com/pytorch/fairseq/commit/573c2f4b60a50dc7c4ff17084b753c05452381f9,"Opensource code for Deep Transformer with Latent Depth (#2703)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Opensource code for Deep Transformer with Latent Depth (https://arxiv.org/pdf/2009.13102.pdf).

New features and design choices made:

- New feature: allow non-residual block to be weighted by sample z (generated per batch) instead of `x = residual + x`.
- Design choice: move  `x = residual + x` in transformer_layer.py into a function where the subclass (with latent depth) could overwrite it to `x = residual + z*x`.

- New feature: allow TransformerEncoder or TransformerDecoder to have additional logits parameters which will generate the samples z.
- Design choice: added subclass LatentTransformerEncoder and LatentTransformerDecoder, which has additional attributes for the logits parameters, and instantiate the corresponding LatentTransformerEncoderLayer and LatentTransformerDecoderLayer.

- New feature: allow multilingual_translation task to train with latent depth (results in the paper).
- Design choice:
  - added additional arguments in the multilingual_translation task.
  - added option for multilingual_transformer to use LatentTransformerEncoder and LatentTransformerDecoder besides standard TransformerEncoder.
  - added option in multilingual_translation task's `train_step` to generate the samples z and compute the KL (and sparsity) loss per batch.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2703

Reviewed By: myleott

Differential Revision: D24155059

Pulled By: xianxl

fbshipit-source-id: f3e41639429f9664ec5565839709aa857a643668",15,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1505,Xian Li,xianl@devfair0251.h2.fair,2020-10-15 14:05:28-07:00,05a5232d04a6e5eccf0e1392b17b4908e5035d44,https://github.com/pytorch/fairseq/commit/05a5232d04a6e5eccf0e1392b17b4908e5035d44,"fix README.md (#2735)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2735

Reviewed By: myleott

Differential Revision: D24343492

Pulled By: xianxl

fbshipit-source-id: c61c717756307036f9d89de5a8ded66784f1acf7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1506,Yuqing Tang,yuqtang@fb.com,2020-10-15 16:15:25-07:00,698820b2bb3e128c3d43c834525d1210769974ad,https://github.com/pytorch/fairseq/commit/698820b2bb3e128c3d43c834525d1210769974ad,"Enable shard epoch id recovery from checkpoints

Summary: shard_epoch is not recovered when epoch id is loaded from checkpoints. This diff fixed it.

Reviewed By: chtran

Differential Revision: D24323687

fbshipit-source-id: a3ee84e8eef7ea75b62c6b0c3870d0cc80ad8f78",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1507,Armen Aghajanyan,armen.ag@live.com,2020-10-16 14:30:35-07:00,f2fa07106c4cb8faa70615a63bb31a141c1e3828,https://github.com/pytorch/fairseq/commit/f2fa07106c4cb8faa70615a63bb31a141c1e3828,"RXF OS Implementation (#2455)

Summary:
## What does this PR do?
Implements R3F and R4F coming from Facebook Research: https://arxiv.org/abs/2008.03156

This code was used to generate all the results from the paper excluding probing results.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2455

Reviewed By: myleott

Differential Revision: D23444863

Pulled By: AkshatSh

fbshipit-source-id: b724a6d6cc9cebfdb4bd219828afbb5679f2259b",11,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1508,Myle Ott,myleott@fb.com,2020-10-16 17:35:01-07:00,2d900bf30814d64035dc78012f9cc7b4fc063ea1,https://github.com/pytorch/fairseq/commit/2d900bf30814d64035dc78012f9cc7b4fc063ea1,"Fix tests (#1352)

Summary:
We need to keep `--num-workers=0` during tests

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1352

Reviewed By: alexeib

Differential Revision: D24375411

Pulled By: myleott

fbshipit-source-id: 9975ed5405f3b19b4dd0877ca15ee3081b185942",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1509,alexeib,alexei.b@gmail.com,2020-10-17 13:37:15-07:00,bc0474d96e2201d3fac390d6896b4cfca5c0f561,https://github.com/pytorch/fairseq/commit/bc0474d96e2201d3fac390d6896b4cfca5c0f561,"fix infer.py (#1354)

Summary:
this fixes infer.py that was broken by #2716 / D24243384 (https://github.com/pytorch/fairseq/commit/e0d5d8e669528be579d7aa4749fbcfe5cacdce90)

we should prob add some tests for infer.py.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1354

Reviewed By: myleott

Differential Revision: D24381516

Pulled By: alexeib

fbshipit-source-id: b49e6bed7d239a55b8536d13c75fd5287330b1b1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1510,phantomcoder1996,reemagody1996@gmail.com,2020-10-18 10:45:54-07:00,7a3f20d0fad62d696aed17d801b840d5c25cc4f5,https://github.com/pytorch/fairseq/commit/7a3f20d0fad62d696aed17d801b840d5c25cc4f5,"fix a bug that caused the label generation script for Librispeech not to work on windows (#2745)

Summary:
## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2744

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2745

Reviewed By: alexeib

Differential Revision: D24381143

Pulled By: myleott

fbshipit-source-id: 61690f30e988a9d477e6d5c927c49b10652925c7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1511,Myle Ott,myleott@fb.com,2020-10-18 13:04:09-07:00,5695cdfb2caf85b3670d6bda7c0bc31666138263,https://github.com/pytorch/fairseq/commit/5695cdfb2caf85b3670d6bda7c0bc31666138263,"Disable isort on several files

Summary: isort introduces some import/circular dependency issues. Ideally we'll fix those in the future, but for now just disable isort on many of the `__init__.py` files so that we can apply black+isort across the repo.

Reviewed By: alexeib

Differential Revision: D24377771

fbshipit-source-id: 9a16343a13f162582722b4147959caea29682bbe",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1512,Myle Ott,myleott@fb.com,2020-10-18 18:13:29-07:00,a48f235636557b8d3bc4922a6fa90f3a0fa57955,https://github.com/pytorch/fairseq/commit/a48f235636557b8d3bc4922a6fa90f3a0fa57955,"Apply black+isort (#1357)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357

Reviewed By: alexeib

Differential Revision: D24377772

fbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c",396,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,50,0,0,0,0,4,0,0,0,0,1,50,0,0,0,0,4,0,0,0,0,21,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMaskedLanguageModel(unittest.TestCase):'],"[('AlmostEqual', '('), ('AlmostEqual', '('), ('AlmostEqual', '('), ('Equal', '(d.index(), 1)'), ('Equal', '(d.index(), 3)'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)'), ('Equal', '(d.index(), 7)'), ('Equal', '(d.index(), 8)'), ('RaisesRegex', '(RuntimeError, ):'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)'), ('Equal', '(loss, torch.tensor(1.0, device=, dtype=torch.float16))'), ('Equal', '('), ('Equal', '('), ('Equal', '(optimizer.scaler.loss_scale, 2.0)'), ('True', '('), ('Less', '(abs(nll_loss - nll_logging_output[]), 1e-6)'), ('Less', '(abs(nll_loss - smooth_logging_output[]), 1e-6)'), ('Equal', '(a.get_smoothed_values()[], 1.5)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(a.get_smoothed_values()[], 1)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(layer4.get_smoothed_values()[], 4)'), ('Equal', '(layer3.get_smoothed_values()[], 3)'), ('Equal', '(layer2.get_smoothed_values()[], 2.5)'), ('Equal', '(layer1.get_smoothed_values()[], 1.25)'), ('Equal', '(metrics.get_smoothed_values(name)[], 1.5)'), ('Equal', '(metrics.get_smoothed_values(name)[], 3)'), ('Equal', '(other.get_smoothed_values()[], 2)'), ('AlmostEqual', '('), ('AlmostEqual', '('), ('True', '('), ('True', '('), ('True', '('), ('True', '('), ('True', '('), ('True', '('), ('True', '('), ('True', '('), ('HypoTokens', '(hypos_id[0], data[id][])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(next(itr)[][0].item(), 50)'), ('Equal', '(next(itr)[][0].item(), 0)'), ('Equal', '(next(itr)[][0].item(), 0)')]",[],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],['class TestMaskedLanguageModel(unittest.TestCase):'],"[('AlmostEqual', '(output_tbc.data.transpose(0, 1).transpose(1, 2), output1d.data)'), ('AlmostEqual', '(conv_tbc.weight.grad.data.transpose(0, 2), conv1d.weight.grad.data)'), ('AlmostEqual', '(input_tbc.grad.data.transpose(0, 1).transpose(1, 2), input1d.grad.data)'), ('Equal', '(d.index(), 1)'), ('Equal', '(d.index(), 3)'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)'), ('Equal', '(d.index(), 7)'), ('Equal', '(d.index(), 8)'), ('RaisesRegex', '(RuntimeError, ):'), ('Equal', '(d.index(), 4)'), ('Equal', '(d.index(), 5)'), ('Equal', '(d.index(), 6)'), ('Equal', '(loss, torch.tensor(1., device=, dtype=torch.float16))'), ('Equal', '(model.weight, torch.tensor([[3.0996]], device=, dtype=torch.float16, requires_grad=True))'), ('Equal', '(model.bias, torch.tensor([5.1016], device=, dtype=torch.float16, requires_grad=True))'), ('Equal', '(optimizer.scaler.loss_scale, 2.)'), ('True', '(all('), ('Less', '(abs(nll_loss - nll_logging_output[]), 1e-6)'), ('Less', '(abs(nll_loss - smooth_logging_output[]), 1e-6)'), ('Equal', '(a.get_smoothed_values()[], 1.5)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(a.get_smoothed_values()[], 1)'), ('Equal', '(b.get_smoothed_values()[], 2)'), ('Equal', '(layer4.get_smoothed_values()[], 4)'), ('Equal', '(layer3.get_smoothed_values()[], 3)'), ('Equal', '(layer2.get_smoothed_values()[], 2.5)'), ('Equal', '(layer1.get_smoothed_values()[], 1.25)'), ('Equal', '(metrics.get_smoothed_values(name)[], 1.5)'), ('Equal', '(metrics.get_smoothed_values(name)[], 3)'), ('Equal', '(other.get_smoothed_values()[], 2)'), ('AlmostEqual', '(float(train_log[k]), float(train_res_log[k]), delta=delta)'), ('AlmostEqual', '(float(valid_log[k]), float(valid_res_log[k]), delta=delta)'), ('True', '(self.hypoTokens(hypos[0][0], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[0][0], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[0][1], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[0][1], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[1][0], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[1][0], [1.0, 0.4, 1.0]) or'), ('True', '(self.hypoTokens(hypos[1][1], [w1, w1, eos]) or'), ('True', '(self.hypoScore(hypos[1][1], [1.0, 0.4, 1.0]) or'), ('HypoTokens', '(hypos_id[0], data[id][])'), ('TensorEqual', '(hypo[], torch.LongTensor(tokens))'), ('AlmostEqual', '(hypo[], pos_scores)'), ('Equal', '(pos_scores.numel(), hypo[].numel())'), ('Less', '(abs(score - hypo[]), 1e-6)'), ('Equal', '(next(itr)[][0].item(), 50)'), ('Equal', '(next(itr)[][0].item(), 0)'), ('Equal', '(next(itr)[][0].item(), 0)')]",[],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],"['len(d) == 2', 'd[0][][0] == 1', 'd[1][][0] == 2', 'len(d) == 3', 'd[0][][0] == 1', 'd[1][][0] == 2', 'd[2][][0] == 2', 'len(d) == 3', 'd[0][][0] == 1', 'd[1][][0] == 1', 'd[2][][0] == 2', '(', '(', '(', '(', '(', 'hypos[sent][beam][] is not None', 'hasattr(args, )', '(', 'in kwargs', 'kwargs[] is not None']",[],[],[],[],[],[],[],[],[],[],[],[],"['ConstraintNode.print_graph(c) == expected, f', 'c.token_counts() == gold_counts, f', 'all_tokens == state.next_tokens(), f', 'result == expected, f', 'result == expected, f', 'hypos[sent][beam][] is not None', 'hasattr(args, )', 'self.args.probs.dim() == 3, \\', 'in kwargs', 'kwargs[] is not None']",[],[],[],[],[],[],[],[],[],[],[],[]
1513,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-10-19 06:10:36-07:00,65e11a37d5f5660bc5a02d4779c16afb0101ec54,https://github.com/pytorch/fairseq/commit/65e11a37d5f5660bc5a02d4779c16afb0101ec54,"Readme with instructions to generate and evaluate with a 12B model (#1351)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1351

Reviewed By: edunov

Differential Revision: D24386349

Pulled By: huihuifan

fbshipit-source-id: ade362d7cb64e24e6b2689ba87c53636073d2246",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1514,Angela Fan,angela.h.fan@gmail.com,2020-10-19 09:09:24-07:00,e3168f74a84523415e46d848e4f4ec9a2713ad6f,https://github.com/pytorch/fairseq/commit/e3168f74a84523415e46d848e4f4ec9a2713ad6f,"minor fix for linter (#1360)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1360

Reviewed By: myleott

Differential Revision: D24393217

Pulled By: huihuifan

fbshipit-source-id: a110ef6958b1e15cd8c4e23b610db5cfc994f06d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1515,Myle Ott,myleott@fb.com,2020-10-19 09:22:28-07:00,9b8b46407094ea1671e9ed89b6db3f57e8665536,https://github.com/pytorch/fairseq/commit/9b8b46407094ea1671e9ed89b6db3f57e8665536,"Package config and examples with fairseq (#1356)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1356

Reviewed By: alexeib

Differential Revision: D24385688

Pulled By: myleott

fbshipit-source-id: 72c4a702d93d2854a6409d42913d7413207cb61e",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],['sys.platform.lower() == )'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1516,Myle Ott,myleott@fb.com,2020-10-19 14:13:23-07:00,de5c2cb35aa57b7d95d75b574d937141707db0ea,https://github.com/pytorch/fairseq/commit/de5c2cb35aa57b7d95d75b574d937141707db0ea,"Fix model parallel LM (#1358)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1358

Reviewed By: alexeib

Differential Revision: D24393064

Pulled By: myleott

fbshipit-source-id: ee88fd1e7b203d7df6b7a65d3b1b1469e8fe9b6e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1517,Alexei Baevski,abaevski@fb.com,2020-10-19 20:15:47-07:00,c76cb6dfb93b531369a0a7593227b31c3b99c0a3,https://github.com/pytorch/fairseq/commit/c76cb6dfb93b531369a0a7593227b31c3b99c0a3,"composite criterion should still use legacy criterion as it will break with subsequent diff

Summary: see title

Reviewed By: myleott

Differential Revision: D24393903

fbshipit-source-id: 4b972b8150c7228fb32977675c6c60b13d5194d0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1518,alexeib,alexei.b@gmail.com,2020-10-20 00:31:00-07:00,3b27ed7996b0315f471c795cf9b7dfcc18467cbe,https://github.com/pytorch/fairseq/commit/3b27ed7996b0315f471c795cf9b7dfcc18467cbe,"Enable Hydra configs in fairseq (#1343) (#1510)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1510

this is the main pr that switches on hydra functionality in fairseq

we migrate ""args"" object into omegaconf ""DictConfig"" at all legacy entry points

in addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother

i am going through code that references migrated fairseq components and changing it to inherit from ""Legacy*"" components instead. hopefully tests will catch most of this

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343

Reviewed By: myleott

Differential Revision: D23973928

Pulled By: alexeib

fbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9",85,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1519,Alexei Baevski,abaevski@fb.com,2020-10-20 13:43:15-07:00,f6677b675524d22a4df9f2304f63ee382594c9e3,https://github.com/pytorch/fairseq/commit/f6677b675524d22a4df9f2304f63ee382594c9e3,"fix #2761, #2760

Summary:
Fixes issue #2761 and #2760
args from registries were not added to argparse

Reviewed By: myleott

Differential Revision: D24422792

fbshipit-source-id: c8a8e835965da5c4f527bd589bd621371441e7fe",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1520,Xu Song,xusong.vip@gmail.com,2020-10-20 15:41:33-07:00,8248a12a6433f45b1757fac206f453f24b88403a,https://github.com/pytorch/fairseq/commit/8248a12a6433f45b1757fac206f453f24b88403a,"Upgrade args: max_sentences to batch_size (#2754)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Upgrade args: `max_sentences` to `batch_size`

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2754

Reviewed By: alexeib

Differential Revision: D24418980

Pulled By: myleott

fbshipit-source-id: 5269c2fc8c434513cc5114f7e9d2eccd0c553fbd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1521,freewym,freewym@gmail.com,2020-10-20 15:43:26-07:00,d6f2c907be8f7351195184981e4f3a9e003a4258,https://github.com/pytorch/fairseq/commit/d6f2c907be8f7351195184981e4f3a9e003a4258,"remove unnecessary logging configs (#2733)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
It's sufficient to set logging.basicConfig in the most outside calling code like train.py or generate.py. Actually the setting of logging.basicConfig () (like [here](https://github.com/pytorch/fairseq/blob/master/fairseq_cli/generate.py#L54)) will been overwritten if logging.basicConfig is set in the inner part of the whole code.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2733

Reviewed By: alexeib

Differential Revision: D24418987

Pulled By: myleott

fbshipit-source-id: 862d200023357de8947799f380e513f4c411b143",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1522,Myle Ott,myleott@fb.com,2020-10-20 15:43:36-07:00,9b0611e6786a048b6c4a70e36051027671d951a7,https://github.com/pytorch/fairseq/commit/9b0611e6786a048b6c4a70e36051027671d951a7,"Fix torch.hub (fixes #2756) (#2762)

Summary:
Typically `torch.hub.load(...)` doesn't call `pip install`, so our Cython components never get built. We have a hack in our hubconf that builds these components by running the equivalent of `python setup.py build_ext --inplace` using the setuptools sandbox: https://github.com/pytorch/fairseq/blob/f6677b675524d22a4df9f2304f63ee382594c9e3/hubconf.py#L52-L55.

Unfortunately, this sandbox gets mad if you modify the filesystem, which is what this recent change does: https://github.com/pytorch/fairseq/blob/f6677b675524d22a4df9f2304f63ee382594c9e3/setup.py#L203-L205. Combined this breaks torch.hub.

The solution is that when we're doing `build_ext`, don't setup the symlinks. This is fine, since `build_ext` doesn't actually build a package, so we don't care about including config or examples.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2762

Reviewed By: alexeib

Differential Revision: D24430228

Pulled By: myleott

fbshipit-source-id: e05d075a003ddfde196cb8a86b32882d73808015",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1523,Myle Ott,myleott@fb.com,2020-10-21 07:45:57-07:00,eece1d7082caf84a957c6b9685a43ee5d2beefe2,https://github.com/pytorch/fairseq/commit/eece1d7082caf84a957c6b9685a43ee5d2beefe2,"More detailed error message for data iterator size mismatch (#2768)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2768

Reviewed By: vimalmanohar

Differential Revision: D24446804

Pulled By: myleott

fbshipit-source-id: 19220f2fd3e3db49f7528f6fb17188834b09646f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1524,Changhan Wang,changhan@fb.com,2020-10-21 08:09:17-07:00,ee450dde198e404ae897acd8854665ed8719801e,https://github.com/pytorch/fairseq/commit/ee450dde198e404ae897acd8854665ed8719801e,"S2T multilingual example + bug fix

Summary:
* S2T multilingual example on MuST-C
* A bug fix for `speech_to_text_dataset` (for multilingual setting)

Reviewed By: jmp84

Differential Revision: D24339394

fbshipit-source-id: ef0c0be08137884897b532e45ebc56551d20be48",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1525,Myle Ott,myleott@fb.com,2020-10-21 15:30:18-07:00,0f44e89c383f45ea455bddc3c44ec950b3df91f1,https://github.com/pytorch/fairseq/commit/0f44e89c383f45ea455bddc3c44ec950b3df91f1,"Fix Latent Depth args (#1365)

Summary:
Args should be registered in the Model rather than modules

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1365

Reviewed By: pipibjc

Differential Revision: D24453007

Pulled By: myleott

fbshipit-source-id: d22b0d86a3c940456b394b005acab4bb6a3f5bed",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1526,Myle Ott,myleott@fb.com,2020-10-22 06:24:54-07:00,43c69a7666b59d1d0c8a30c7acefec9822fedcaf,https://github.com/pytorch/fairseq/commit/43c69a7666b59d1d0c8a30c7acefec9822fedcaf,"Fix deprecated usage of nonzero() (#1364)

Summary:
PyTorch requires the `as_tuple` argument now, otherwise it prints warnings. Let's just fix this everywhere

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1364

Reviewed By: edunov

Differential Revision: D24452587

Pulled By: myleott

fbshipit-source-id: 7e6d424792ffec74a6197b2a266600cb13f24770",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1527,Pavel Soriano,pavel.soriano@data.gouv.fr,2020-10-22 06:25:06-07:00,751bcbfcb939b777e61af251d1fce5d4a4dc1f12,https://github.com/pytorch/fairseq/commit/751bcbfcb939b777e61af251d1fce5d4a4dc1f12,"Changed EnvironmentError to RuntimeError in get_from_cache (#2767)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
No need I believe
- [x] Did you write any new necessary tests?
No
## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2724

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Yes! It is not a big PR at all but it allowed me to familiarize with the caching/downloading logic used in fairseq (which is very similar to that used in pytorch/transformers)

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2767

Reviewed By: edunov

Differential Revision: D24456055

Pulled By: myleott

fbshipit-source-id: bc634a9b97f957ecc5a8da57b112ff892e492107",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1528,alexeib,alexei.b@gmail.com,2020-10-22 07:07:27-07:00,18cadab1d0fc6a98988a17e92683f8b83b03a177,https://github.com/pytorch/fairseq/commit/18cadab1d0fc6a98988a17e92683f8b83b03a177,"support new cfg based models; make sure --normalize is consistent in … (#1370)

Summary:
support new cfg based models; make sure --normalize is consistent in infer with the model

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1370

Reviewed By: myleott

Differential Revision: D24467698

Pulled By: alexeib

fbshipit-source-id: 056b3608e3c1fe8acdb3e45e0306de5d874cb4d1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1529,Chau Tran,chau@fb.com,2020-10-22 11:29:08-07:00,31c23baafcac94733f31613c7431997d787a204d,https://github.com/pytorch/fairseq/commit/31c23baafcac94733f31613c7431997d787a204d,"Fix fairseq/criss README

Summary: Add requirements, fix wrong command

Reviewed By: tangyuq

Differential Revision: D24452748

fbshipit-source-id: 4837610ea7e5b5df8caecc685226080cafddb3e0",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1530,Myle Ott,myleott@fb.com,2020-10-22 12:03:15-07:00,11aaffdd18ab610e91ea0f4d394271602853ef04,https://github.com/pytorch/fairseq/commit/11aaffdd18ab610e91ea0f4d394271602853ef04,"rm FairseqModel::upgrade_args, it's not needed anymore (#1363)

Summary:
Tests seems to pass without it, so let's remove it

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1363

Reviewed By: alexeib

Differential Revision: D24452369

Pulled By: myleott

fbshipit-source-id: 186933ff3ee16be61c77a9581658db8e853c1baa",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1531,alexeib,alexei.b@gmail.com,2020-10-22 12:18:07-07:00,f0fcb55d5b2617371cd1cf5c2d3712ea4bd79122,https://github.com/pytorch/fairseq/commit/f0fcb55d5b2617371cd1cf5c2d3712ea4bd79122,"fix #2764 (#1368)

Summary:
fix interactive.py + add args from tasks before registries (where we catch errors)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1368

Reviewed By: myleott

Differential Revision: D24462871

Pulled By: alexeib

fbshipit-source-id: 307b829c935aa5061bdd79d8cc339eaf87fd8845",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1532,Myle Ott,myleott@fb.com,2020-10-22 12:44:02-07:00,b8a938e96e08e5b39deb585d6cc6690de062dd4d,https://github.com/pytorch/fairseq/commit/b8a938e96e08e5b39deb585d6cc6690de062dd4d,"BART hub fixes + improvements (#1342)

Summary:
- Make BART hub interface extend from GeneratorHubInterface (fixes #1748)
- Add mask filling interface for BART

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1342

Reviewed By: ngoyal2707

Differential Revision: D24264195

Pulled By: myleott

fbshipit-source-id: 0885f90a54fabe1672b1bfe137dfbccbc5d25d0e",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1533,Myle Ott,myleott@fb.com,2020-10-22 12:45:19-07:00,e0737c3c2985d2a71f0a30bb29f6d8741b4f87f3,https://github.com/pytorch/fairseq/commit/e0737c3c2985d2a71f0a30bb29f6d8741b4f87f3,"Dynamically generate versions based on commit hash (#2774)

Summary:
This will produce version strings like `1.0.0a0+3065963`, similar to PyTorch version strings.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2774

Reviewed By: alexeib

Differential Revision: D24453517

Pulled By: myleott

fbshipit-source-id: 03a0c324ed6124bbc513ba7edc954abd71d63a0f",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1534,alexeib,alexei.b@gmail.com,2020-10-22 16:30:07-07:00,cd2bba4419629ffc17eb83c669e88b0bd3af6eb9,https://github.com/pytorch/fairseq/commit/cd2bba4419629ffc17eb83c669e88b0bd3af6eb9,"rename remove_bpe to post_process; add aliasing (#1369)

Summary:
some binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369

Reviewed By: myleott

Differential Revision: D24465040

Pulled By: alexeib

fbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7",13,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1535,alexeib,alexei.b@gmail.com,2020-10-23 00:05:52-07:00,2409d5a36e074fe237a734fa2053867fe62b5e01,https://github.com/pytorch/fairseq/commit/2409d5a36e074fe237a734fa2053867fe62b5e01,"refactor dataclass related files, add proper types for static checkin… (#1371)

Summary:
- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)
- add top level FairseqConfig
- change typehints to reflect the correct config type if it is known

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371

Reviewed By: myleott

Differential Revision: D24469026

Pulled By: alexeib

fbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2",19,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1536,Shashank Jain,shajain@fb.com,2020-10-23 14:43:59-07:00,4b0cf6649bc65093fe2d091ecdd4150bc00ec64f,https://github.com/pytorch/fairseq/commit/4b0cf6649bc65093fe2d091ecdd4150bc00ec64f,"Revert ""Fix deprecated usage of nonzero()""

Summary: Reverting the diff because it has already been fixed in https://github.com/pytorch/pytorch/pull/45413

Reviewed By: myleott

Differential Revision: D24511658

fbshipit-source-id: a5561dae50d69a03443ca8a60bebe2cd064e3ee0",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1537,alexeib,alexei.b@gmail.com,2020-10-24 10:20:07-07:00,c147060598f69385a1c2c05bc97dd43b56d73575,https://github.com/pytorch/fairseq/commit/c147060598f69385a1c2c05bc97dd43b56d73575,"add new w2v models (#1373)

Summary:
update readme to add new wav2vec models (incl w/ self training)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1373

Reviewed By: michaelauli

Differential Revision: D24524182

Pulled By: alexeib

fbshipit-source-id: c918971f8009b11855908e71bfcc247cf6776a8f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1538,alexeib,alexei.b@gmail.com,2020-10-24 21:18:44-07:00,6ee0364685fca0ac5cc2721b193f396166a32646,https://github.com/pytorch/fairseq/commit/6ee0364685fca0ac5cc2721b193f396166a32646,"fix building components when no configuration is provided (#1374)

Summary:
see title, in particular fixes evaluating generate.py with --scoring wer

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1374

Reviewed By: kahne

Differential Revision: D24527059

Pulled By: alexeib

fbshipit-source-id: b01994441fda12eafd4e465d147047c6e84a8335",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1539,alexeib,alexei.b@gmail.com,2020-10-25 12:53:07-07:00,3c414780837dd3506ea82a868ea92628d1fdd576,https://github.com/pytorch/fairseq/commit/3c414780837dd3506ea82a868ea92628d1fdd576,"fix loading emissions (#1375)

Summary:
broken in last change to infer.py

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1375

Reviewed By: xuqiantong

Differential Revision: D24531499

Pulled By: alexeib

fbshipit-source-id: fab60abf67a05c48e1ff750fac3ab6d4c0fa2770",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1540,Vladimir Smirnov,smivvla@gmail.com,2020-10-26 08:17:12-07:00,81677d751de120f69eef0c3eb36e849c977f7814,https://github.com/pytorch/fairseq/commit/81677d751de120f69eef0c3eb36e849c977f7814,"Update README.md (#2796)

Summary:
Fixed link.

# Before submitting

- [-] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [+] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [+] Did you make sure to update the docs?
- [-] Did you write any new necessary tests?

## What does this PR do?
Fixes link.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2796

Reviewed By: nlaptev

Differential Revision: D24538759

Pulled By: myleott

fbshipit-source-id: af947f432c34ca2aec35c9fe59dd1214e363450b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1541,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-10-27 02:13:47-07:00,beeac0ad68594b07594f565bfd5cb6f4f46cd816,https://github.com/pytorch/fairseq/commit/beeac0ad68594b07594f565bfd5cb6f4f46cd816,"Get 12B M2M-100 model generation to work correctly on exactly 2 32gb gpus (#1366)

Summary:
# What does this PR do?
Addresses https://github.com/pytorch/fairseq/issues/2772 where external users can't generate using the model because the README is currently not accurate.
This PR fixes the issues in the README

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1366

Reviewed By: edunov

Differential Revision: D24455634

Pulled By: shruti-bh

fbshipit-source-id: 480a11f8b95d1278162d585700e58d467a35d35a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1542,Myle Ott,myleott@fb.com,2020-10-27 07:45:13-07:00,01be083e46d2e4614dc274b0edf29d0ddd516186,https://github.com/pytorch/fairseq/commit/01be083e46d2e4614dc274b0edf29d0ddd516186,"Centralize hydra init (and support packaged location of configs) (#2784)

Summary:
Configs can either be in `/fairseq/configs` (once the package is installed) or `/configs` (if using an editable installation). This centralizes the hydra init and supports these two possible config locations.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2784

Reviewed By: alexeib

Differential Revision: D24513586

Pulled By: myleott

fbshipit-source-id: 8e10a88177ebcf809d5d37d448d2b384142febef",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1543,Myle Ott,myleott@fb.com,2020-10-27 11:24:58-07:00,1bc83c703ad70d7f62c1e54b197e29b95d07b1f0,https://github.com/pytorch/fairseq/commit/1bc83c703ad70d7f62c1e54b197e29b95d07b1f0,"Misc fixes (#2786)

Summary:
- Rename type -> key in fairseq/tasks/sentence_prediction.py (fixes https://github.com/pytorch/fairseq/issues/2746)
- Update preprocessing docs (fixes https://github.com/pytorch/fairseq/issues/2565)
- Turn off logging in test_fp16_optimizer.TestGradientScaling
- Documentation updates
- Remove some unused code
- Fix noisychannel example (fixes https://github.com/pytorch/fairseq/issues/2213)

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2786

Reviewed By: shruti-bh

Differential Revision: D24515146

Pulled By: myleott

fbshipit-source-id: 86b0f5516c57610fdca801c60e58158ef052fc3a",13,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],['def tearDown(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1544,Elijah Rippeth,elijah.rippeth@gmail.com,2020-10-28 14:52:54-07:00,3c726544d240f610cd35ea264d893d6a6ada074a,https://github.com/pytorch/fairseq/commit/3c726544d240f610cd35ea264d893d6a6ada074a,"fix issue where is_initialized is not available in single-worker paradigm (#2801)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/1205

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2801

Reviewed By: alexeib

Differential Revision: D24579193

Pulled By: myleott

fbshipit-source-id: bcb14bb588d4538398bff4114e0a387fd29818c5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1545,alexeib,alexei.b@gmail.com,2020-10-28 14:58:37-07:00,f6d9313092cf3bc5fa289123b6062b22e463a7da,https://github.com/pytorch/fairseq/commit/f6d9313092cf3bc5fa289123b6062b22e463a7da,"fix eval lm (#1380)

Summary:
fixes eval lm that wasnt parsing arguments correctly

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1380

Reviewed By: myleott

Differential Revision: D24600415

Pulled By: alexeib

fbshipit-source-id: eb56575bef4d20a3cd5cee3dcd279046f085d938",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1546,alexeib,alexei.b@gmail.com,2020-10-28 17:16:56-07:00,65b02d529a45f687da8bbc6ec37611b8a9c96297,https://github.com/pytorch/fairseq/commit/65b02d529a45f687da8bbc6ec37611b8a9c96297,"fix wav2vec infer and finetuning (#1384)

Summary:
Fixes #2807, #2810, #2519

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1384

Reviewed By: myleott

Differential Revision: D24605451

Pulled By: alexeib

fbshipit-source-id: 46ec8f273ac2fab86bd444461e2706c35608b250",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1547,Myle Ott,myleott@fb.com,2020-10-28 18:17:17-07:00,e4e01780f8a087f4a215199ddb83caca2dea16e7,https://github.com/pytorch/fairseq/commit/e4e01780f8a087f4a215199ddb83caca2dea16e7,"Fix dummy LM task (#1381)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1381

Reviewed By: alexeib

Differential Revision: D24603479

Pulled By: myleott

fbshipit-source-id: 5aae8da9c0f20d6526c98b0b37bf9b32a8c78393",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1548,freewym,freewym@gmail.com,2020-10-28 18:19:37-07:00,9c66ff54c4acd8fa3280a9a5ab6d5fe58d1a2cf3,https://github.com/pytorch/fairseq/commit/9c66ff54c4acd8fa3280a9a5ab6d5fe58d1a2cf3,"build_generator() in generator.py should accept cfg.generation instea… (#2813)

Summary:
…d of cfg.task

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2813

Reviewed By: alexeib

Differential Revision: D24604698

Pulled By: myleott

fbshipit-source-id: e41996147203ec47274ded803bab910460a19eb3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1549,alexeib,alexei.b@gmail.com,2020-10-28 18:27:53-07:00,b7d8b9dce2dd5ca6a76e1c6f540945da20922478,https://github.com/pytorch/fairseq/commit/b7d8b9dce2dd5ca6a76e1c6f540945da20922478,"fix architecture params (#1382)

Summary:
fixes architectures not getting applied to migrated models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1382

Reviewed By: myleott

Differential Revision: D24603110

Pulled By: alexeib

fbshipit-source-id: 18f44d3736853282466feed5e8896db95338b097",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1550,Myle Ott,myleott@fb.com,2020-10-28 18:34:13-07:00,4cdc81f6f1b16cbb6e1016e3d06a6e4962edcec0,https://github.com/pytorch/fairseq/commit/4cdc81f6f1b16cbb6e1016e3d06a6e4962edcec0,"Support activation checkpointing in Transformer (#1378)

Summary:
Without activation checkpointing (peak GPU memory usage: 7138MiB)
```
$ python train.py --task dummy_mt --arch transformer --dropout 0.1 --max-tokens 4096 --optimizer adam --lr 0.00001 --log-format simple --log-interval 25 --fp16
(...)
2020-10-28 08:03:03 | INFO | train_inner | epoch 001:     25 / 92 loss=12.67, ppl=6517.2, wps=281380, ups=8.61, wpb=32640, bsz=1088, num_updates=25, lr=1e-05, gnorm=8.541, clip=0, loss_scale=128, train_wall=5, wall=10
2020-10-28 08:03:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-10-28 08:03:06 | INFO | train_inner | epoch 001:     51 / 92 loss=8.938, ppl=490.52, wps=302975, ups=9.28, wpb=32640, bsz=1088, num_updates=50, lr=1e-05, gnorm=6.395, clip=0, loss_scale=64, train_wall=3, wall=12
2020-10-28 08:03:08 | INFO | train_inner | epoch 001:     76 / 92 loss=3.855, ppl=14.47, wps=316039, ups=9.68, wpb=32640, bsz=1088, num_updates=75, lr=1e-05, gnorm=9.078, clip=0, loss_scale=64, train_wall=3, wall=15
2020-10-28 08:03:10 | INFO | fairseq_cli.train | begin validation on ""valid"" subset
2020-10-28 08:03:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.048 | ppl 1.03 | wps 1.09646e+06 | wpb 32640 | bsz 1088 | num_updates 91
```

With activation checkpointing (peak GPU memory usage: 6466MiB)
```
$ python train.py --checkpoint-activations --task dummy_mt --arch transformer --dropout 0.1 --max-tokens 4096 --optimizer adam --lr 0.00001 --log-format simple --log-interval 25 --fp16
(...)
2020-10-28 08:01:50 | INFO | train_inner | epoch 001:     25 / 92 loss=12.67, ppl=6517.22, wps=291110, ups=8.91, wpb=32640, bsz=1088, num_updates=25, lr=1e-05, gnorm=8.541, clip=0, loss_scale=128, train_wall=4, wall=9
2020-10-28 08:01:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-10-28 08:01:52 | INFO | train_inner | epoch 001:     51 / 92 loss=8.938, ppl=490.54, wps=295438, ups=9.05, wpb=32640, bsz=1088, num_updates=50, lr=1e-05, gnorm=6.394, clip=0, loss_scale=64, train_wall=3, wall=12
2020-10-28 08:01:55 | INFO | train_inner | epoch 001:     76 / 92 loss=3.855, ppl=14.47, wps=308351, ups=9.45, wpb=32640, bsz=1088, num_updates=75, lr=1e-05, gnorm=9.082, clip=0, loss_scale=64, train_wall=3, wall=14
2020-10-28 08:01:57 | INFO | fairseq_cli.train | begin validation on ""valid"" subset
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1378

Reviewed By: min-xu-ai

Differential Revision: D24593170

Pulled By: myleott

fbshipit-source-id: 701254e603a2277d22f8b3bcc3ebbade54bb7479",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1551,Anuroop Sriram,anuroops@fb.com,2020-10-29 11:44:46-07:00,6debe29150204a3a98e61057cebf55e160ccb8b7,https://github.com/pytorch/fairseq/commit/6debe29150204a3a98e61057cebf55e160ccb8b7,"Compute WER for Wav2Vec 2.0 Seq2Seq models (#1376)

Summary:
# Before submitting

- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?

## What does this PR do?
Adds support to compute WER for wav2vec2.0 seq2seq models.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1376

Reviewed By: alexeib

Differential Revision: D24611516

Pulled By: anuroopsriram

fbshipit-source-id: dd7daab73ebccc21367dd51f41a11e89c404977b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1552,Myle Ott,myleott@fb.com,2020-10-29 17:07:12-07:00,a4356b1da2b19ebd2e1be5c12ff882026ea4d7d2,https://github.com/pytorch/fairseq/commit/a4356b1da2b19ebd2e1be5c12ff882026ea4d7d2,"Simplify --user-dir and require user-dir module name to be globally unique (#2815)

Summary:
This PR reverts recent changes that attempted to make `--user-dir` work with non-unique module names. But that new approach introduced other issues (e.g., poor compatibility with multiprocessing and Windows), so let's revert to the previous simpler implementation.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2815

Reviewed By: alexeib

Differential Revision: D24611571

Pulled By: myleott

fbshipit-source-id: cecfe28395585ca0401f844f10bd0d49d014c4d8",33,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1553,Yuqing Tang,yuqtang@fb.com,2020-10-30 18:23:14-07:00,de859692ff39cff1ecfd65e8e6860c621fb0e58a,https://github.com/pytorch/fairseq/commit/de859692ff39cff1ecfd65e8e6860c621fb0e58a,"Enable translation_multi_simple_epoch to have different source and target dictionaries

Summary: In past, we always use shared dictionary for multilingual experiments. This diff renables different dictionaries for source and target languages by changing the assertion criteria and reverts back to use specific languages to return source_dict and target_dict.

Reviewed By: chtran

Differential Revision: D24637682

fbshipit-source-id: a982e4f1e48395cc5bf10dc03b98fbe970062f8d",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1554,Shashank Jain,shajain@fb.com,2020-11-02 17:16:03-08:00,de977736f91d23c53e6a60c45822973a615daa15,https://github.com/pytorch/fairseq/commit/de977736f91d23c53e6a60c45822973a615daa15,"Support running batch of sentences together on GPU with BART fill_mask (#2833)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2833

 Add support for filling masks using BART on a batch of sentences. This will be helpful when running on GPU

Reviewed By: myleott

Differential Revision: D24687773

fbshipit-source-id: 1b8005c18a09be526f40e9e2b99207afa38e0f1a",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1555,Joshua Meier,jmeier@fb.com,2020-11-03 14:05:30-08:00,b120fbbe8fdb6fc8412149916fe09c54757bdaf6,https://github.com/pytorch/fairseq/commit/b120fbbe8fdb6fc8412149916fe09c54757bdaf6,"Fix correctness issue with megatron save/load checkpoints (#1386)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2681.

Proof that it's working now:
```
python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/mp-fix4    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10

2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     11 / 78 loss=0.939, ppl=1.92, wps=116.7, ups=0.11, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=2.276, train_wall=0, wall=15
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     12 / 78 loss=0.938, ppl=1.92, wps=15769.2, ups=15.38, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=2.612, train_wall=0, wall=15
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=18658.8, ups=18.2, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=2.798, train_wall=0, wall=15
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=18324.5, ups=17.88, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=2.326, train_wall=0, wall=15
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=17616.5, ups=17.19, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=2.112, train_wall=0, wall=15
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=18624.5, ups=18.17, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=2.123, train_wall=0, wall=16
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     17 / 78 loss=0.887, ppl=1.85, wps=17972.5, ups=17.53, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=2.061, train_wall=0, wall=16
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     18 / 78 loss=0.862, ppl=1.82, wps=14672.4, ups=14.32, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=2.282, train_wall=0, wall=16
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     19 / 78 loss=0.876, ppl=1.83, wps=14398.6, ups=14.05, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=2.261, train_wall=0, wall=16
2020-10-29 18:42:08 | INFO | train_inner | epoch 001:     20 / 78 loss=0.818, ppl=1.76, wps=18652.2, ups=18.2, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=1.969, train_wall=0, wall=16

...relaunch...

2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     11 / 78 loss=0.939, ppl=1.92, wps=98.2, ups=0.1, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=2.276, train_wall=1, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     12 / 78 loss=0.938, ppl=1.92, wps=17137.8, ups=16.72, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=2.612, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=17239.6, ups=16.82, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=2.798, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=18132, ups=17.69, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=2.326, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=17795.1, ups=17.36, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=2.112, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=18021.3, ups=17.58, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=2.123, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     17 / 78 loss=0.887, ppl=1.85, wps=16452.9, ups=16.05, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=2.061, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     18 / 78 loss=0.862, ppl=1.82, wps=17563.3, ups=17.14, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=2.282, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     19 / 78 loss=0.876, ppl=1.83, wps=16770.3, ups=16.36, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=2.261, train_wall=0, wall=0
2020-10-29 18:47:20 | INFO | train_inner | epoch 001:     20 / 78 loss=0.818, ppl=1.76, wps=16808.2, ups=16.4, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=1.969, train_wall=0, wall=0
```

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1386

Reviewed By: myleott

Differential Revision: D24640946

Pulled By: joshim5

fbshipit-source-id: cb141d92496b289a04d53f080ecd4d5ac6941672",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1556,Myle Ott,myleott@fb.com,2020-11-03 20:44:09-08:00,dd52ed0f3896639b3c04aa67c44775f689faf1a5,https://github.com/pytorch/fairseq/commit/dd52ed0f3896639b3c04aa67c44775f689faf1a5,"Small fixes (#1392)

Summary:
- Set default value of clip-norm back to 0.0 (disabled)
- Add comment explaining that we divide loss by log(2) to covert the base
- Fix `--zero-optimizer=os` (fixes #2811)
- Update requirements to PyTorch >= 1.5
- Fix bug in fixed LR schedule

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1392

Reviewed By: alexeib

Differential Revision: D24714231

Pulled By: myleott

fbshipit-source-id: 63dc8cfc74683bbccbf05b44228014eb12ddbfc7",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1557,Myle Ott,myleott@fb.com,2020-11-03 20:46:45-08:00,1a709b2a401ac8bd6d805c8a6a5f4d7f03b923ff,https://github.com/pytorch/fairseq/commit/1a709b2a401ac8bd6d805c8a6a5f4d7f03b923ff,"Reproduce #1781. Add Weights and Biases support

Summary:

Fixes https://github.com/pytorch/fairseq/issues/1790.

Reviewed By: alexeib

Differential Revision: D24579153

fbshipit-source-id: 74a30effa164db9d6376554376e36b1f47618899

Co-authored-by: Nikolay Korolev <korolevns98@gmail.com>
Co-authored-by: Vlad Lyalin <Guitaricet@gmail.com>",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1558,Alex Xiao,axiao@fb.com,2020-11-04 12:56:13-08:00,ea4ccd94de131d6b39163836418696369dd1d034,https://github.com/pytorch/fairseq/commit/ea4ccd94de131d6b39163836418696369dd1d034,"Load and broadcast fairseq checkpoints instead of having each rank load them individually

Summary:
This diff is based on feedback in D24379649

Before when loading checkpoints:

Each rank loads the checkpoint from Manifold.

Now:

Rank 0 loads checkpoint from Manifold. This checkpoint is broadcasted to all other ranks. This saves IO.

Furthermore, when doing zero-sharding, we only broadcast the relevant parts of the optimizer state to each node. This makes checkpoint loading more memory-efficient and should enable loading models beyond 2-3B parameters.

Reviewed By: myleott

Differential Revision: D24660791

fbshipit-source-id: e30b2ea5990083375e4549f0427a112346ba170d",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1559,alexeib,alexei.b@gmail.com,2020-11-04 18:19:10-08:00,b58f4f017ed275aff327046943857b4259f64a47,https://github.com/pytorch/fairseq/commit/b58f4f017ed275aff327046943857b4259f64a47,"end to end hydra configs (#1393)

Summary:
this adds a hydra_train binary that uses hydra configs/command line overrides instead of argparse

use case 1: built in configs + overrides from command line

```
python fairseq_cli/hydra_train.py distributed_training.distributed_world_size=1 dataset.batch_size=2 task.data=/private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/ model=transformer_lm/transformer_lm_gpt task=language_modeling optimization.max_update=5000
```

use case 2: use an external config that is used instead of bundled configs (but dataclass defaults still work)

```
python fairseq_cli/hydra_train.py --config-path ~/fairseq-py-dev/lm --config-name wiki103
```

the config file contains this:

```
# package _group_

model:
  _name: transformer_lm
distributed_training:
  distributed_world_size: 1
dataset:
  batch_size: 2
task:
  _name: language_modeling
  data: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/
  add_bos_token: false
  max_target_positions: 1024
optimization:
  max_update: 50000
  lr: [ 0.25 ]
criterion: cross_entropy
optimizer: adam
lr_scheduler:
  _name: cosine
```

use case 3: use an external config directory that provides additional configs for e.g. models

python fairseq_cli/hydra_train.py distributed_training.distributed_world_size=1 dataset.batch_size=2 task.data=/private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/ model=transformer_lm/2_layers task=language_modeling optimization.max_update=5000 --config-dir ~/fairseq-py-dev/lm/hydra

where ~/fairseq-py-dev/lm/hydra has the following structure:

- model
-- transformer_lm
 --- 2_layers.yaml

and inside 2_layers.yaml is a copy of transformer_lm_gpt.yaml but with decoder_layers set to 2

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1393

Reviewed By: myleott

Differential Revision: D24722252

Pulled By: alexeib

fbshipit-source-id: 758ea431fa099cd7c0e4daf41eff680df1d3b841",33,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1560,Myle Ott,myleott@fb.com,2020-11-05 09:43:02-08:00,f57b14893837716bdaab4cb9a1430b19d4a6ccf7,https://github.com/pytorch/fairseq/commit/f57b14893837716bdaab4cb9a1430b19d4a6ccf7,"Require process group for all helpers in distributed_utils (#1395)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1395

Data parallel command: `python train.py --task dummy_lm   --arch transformer_lm --tokens-per-sample 512   --max-sentences 8 --decoder-attention-heads 8 --dropout 0.0 --activation-dropout 0.0   --optimizer adam --lr 0.0001   --log-format simple --log-interval 1 --no-save --clip-norm 0.0`

Data parallel before:
```
2020-11-04 07:14:16 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 07:14:16 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 07:14:16 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 07:14:16 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 07:14:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-11-04 07:14:16 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 07:14:16 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 07:14:21 | INFO | train_inner | epoch 001:      1 / 1563 loss=16.297, ppl=80495, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=2.501, train_wall=2, wall=5
2020-11-04 07:14:21 | INFO | train_inner | epoch 001:      2 / 1563 loss=15.399, ppl=43203.8, wps=101398, ups=3.09, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=2.101, train_wall=0, wall=6
2020-11-04 07:14:21 | INFO | train_inner | epoch 001:      3 / 1563 loss=14.742, ppl=27411.2, wps=217567, ups=6.63, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=1.888, train_wall=0, wall=6
2020-11-04 07:14:21 | INFO | train_inner | epoch 001:      4 / 1563 loss=14.206, ppl=18899.3, wps=219413, ups=6.69, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=1.91, train_wall=0, wall=6
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:      5 / 1563 loss=13.697, ppl=13282.1, wps=219446, ups=6.69, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=1.98, train_wall=0, wall=6
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:      6 / 1563 loss=13.179, ppl=9274.18, wps=220131, ups=6.71, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.08, train_wall=0, wall=6
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:      7 / 1563 loss=12.634, ppl=6358.37, wps=220236, ups=6.72, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.195, train_wall=0, wall=6
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:      8 / 1563 loss=12.056, ppl=4256.86, wps=220392, ups=6.72, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.259, train_wall=0, wall=6
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:      9 / 1563 loss=11.453, ppl=2804.05, wps=225842, ups=6.89, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.287, train_wall=0, wall=7
2020-11-04 07:14:22 | INFO | train_inner | epoch 001:     10 / 1563 loss=10.842, ppl=1835, wps=238808, ups=7.28, wpb=32768, bsz=64, num_updates=10, lr=0.0001, gnorm=2.311, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 07:14:47 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 07:14:47 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 07:14:47 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 07:14:47 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 07:14:47 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-11-04 07:14:47 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 07:14:47 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 07:14:52 | INFO | train_inner | epoch 001:      1 / 1563 loss=16.297, ppl=80495, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=2.501, train_wall=2, wall=5
2020-11-04 07:14:52 | INFO | train_inner | epoch 001:      2 / 1563 loss=15.399, ppl=43203.8, wps=96089.4, ups=2.93, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=2.101, train_wall=0, wall=5
2020-11-04 07:14:52 | INFO | train_inner | epoch 001:      3 / 1563 loss=14.742, ppl=27411.2, wps=239285, ups=7.3, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=1.888, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      4 / 1563 loss=14.206, ppl=18899.3, wps=233039, ups=7.11, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=1.91, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      5 / 1563 loss=13.697, ppl=13282.1, wps=237484, ups=7.24, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=1.98, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      6 / 1563 loss=13.179, ppl=9274.18, wps=231683, ups=7.07, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.08, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      7 / 1563 loss=12.634, ppl=6358.37, wps=233804, ups=7.13, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.195, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      8 / 1563 loss=12.056, ppl=4256.86, wps=234025, ups=7.14, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.259, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:      9 / 1563 loss=11.453, ppl=2804.05, wps=238426, ups=7.27, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.287, train_wall=0, wall=6
2020-11-04 07:14:53 | INFO | train_inner | epoch 001:     10 / 1563 loss=10.842, ppl=1835, wps=240069, ups=7.32, wpb=32768, bsz=64, num_updates=10, lr=0.0001, gnorm=2.311, train_wall=0, wall=6
```

Model parallel command: `python train.py --task dummy_lm --arch transformer_lm_megatron --decoder-layers 2 --batch-size 2 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 07:12:22 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 07:12:22 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 2
2020-11-04 07:12:22 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 07:12:22 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 07:12:23 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 07:12:23 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      1 / 12500 loss=60.017, ppl=1.16627e+18, wps=0, ups=0, wpb=4096, bsz=8, num_updates=1, lr=0.0001, gnorm=8.531, loss_scale=128, train_wall=2, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      2 / 12500 loss=46.473, ppl=9.77028e+13, wps=48996.6, ups=11.95, wpb=4096, bsz=8, num_updates=2, lr=0.0001, gnorm=15.019, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      3 / 12500 loss=30.525, ppl=1.54543e+09, wps=58424.2, ups=14.25, wpb=4096, bsz=8, num_updates=3, lr=0.0001, gnorm=13.936, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      4 / 12500 loss=18.561, ppl=386799, wps=58399.5, ups=14.24, wpb=4096, bsz=8, num_updates=4, lr=0.0001, gnorm=7.251, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      5 / 12500 loss=15.145, ppl=36230, wps=58275.6, ups=14.21, wpb=4096, bsz=8, num_updates=5, lr=0.0001, gnorm=2.392, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      6 / 12500 loss=14.683, ppl=26304.2, wps=58704.8, ups=14.32, wpb=4096, bsz=8, num_updates=6, lr=0.0001, gnorm=2.487, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:28 | INFO | train_inner | epoch 001:      7 / 12500 loss=14.169, ppl=18418.9, wps=58449.2, ups=14.26, wpb=4096, bsz=8, num_updates=7, lr=0.0001, gnorm=2.45, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:29 | INFO | train_inner | epoch 001:      8 / 12500 loss=13.574, ppl=12197.4, wps=59106.5, ups=14.42, wpb=4096, bsz=8, num_updates=8, lr=0.0001, gnorm=2.393, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:29 | INFO | train_inner | epoch 001:      9 / 12500 loss=12.974, ppl=8047.87, wps=58619.6, ups=14.3, wpb=4096, bsz=8, num_updates=9, lr=0.0001, gnorm=2.317, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:12:29 | INFO | train_inner | epoch 001:     10 / 12500 loss=12.341, ppl=5187.55, wps=58166.5, ups=14.19, wpb=4096, bsz=8, num_updates=10, lr=0.0001, gnorm=2.213, loss_scale=128, train_wall=0, wall=6
```

Model parallel after:
```
2020-11-04 07:11:07 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 07:11:07 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 2
2020-11-04 07:11:07 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 07:11:07 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 07:11:08 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 07:11:08 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 07:11:13 | INFO | train_inner | epoch 001:      1 / 12500 loss=60.017, ppl=1.16627e+18, wps=0, ups=0, wpb=4096, bsz=8, num_updates=1, lr=0.0001, gnorm=8.531, loss_scale=128, train_wall=2, wall=6
2020-11-04 07:11:13 | INFO | train_inner | epoch 001:      2 / 12500 loss=46.473, ppl=9.77028e+13, wps=47018.1, ups=11.47, wpb=4096, bsz=8, num_updates=2, lr=0.0001, gnorm=15.019, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:13 | INFO | train_inner | epoch 001:      3 / 12500 loss=30.525, ppl=1.54543e+09, wps=59292.6, ups=14.46, wpb=4096, bsz=8, num_updates=3, lr=0.0001, gnorm=13.936, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:13 | INFO | train_inner | epoch 001:      4 / 12500 loss=18.561, ppl=386799, wps=57708.9, ups=14.08, wpb=4096, bsz=8, num_updates=4, lr=0.0001, gnorm=7.251, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:      5 / 12500 loss=15.145, ppl=36230, wps=57427.4, ups=14.01, wpb=4096, bsz=8, num_updates=5, lr=0.0001, gnorm=2.392, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:      6 / 12500 loss=14.683, ppl=26304.2, wps=58730.2, ups=14.33, wpb=4096, bsz=8, num_updates=6, lr=0.0001, gnorm=2.487, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:      7 / 12500 loss=14.169, ppl=18418.9, wps=59523.2, ups=14.52, wpb=4096, bsz=8, num_updates=7, lr=0.0001, gnorm=2.45, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:      8 / 12500 loss=13.574, ppl=12197.4, wps=58945.2, ups=14.38, wpb=4096, bsz=8, num_updates=8, lr=0.0001, gnorm=2.393, loss_scale=128, train_wall=0, wall=6
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:      9 / 12500 loss=12.974, ppl=8047.87, wps=59659.2, ups=14.55, wpb=4096, bsz=8, num_updates=9, lr=0.0001, gnorm=2.317, loss_scale=128, train_wall=0, wall=7
2020-11-04 07:11:14 | INFO | train_inner | epoch 001:     10 / 12500 loss=12.341, ppl=5187.55, wps=59681.4, ups=14.56, wpb=4096, bsz=8, num_updates=10, lr=0.0001, gnorm=2.213, loss_scale=128, train_wall=0, wall=7
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24728687

Pulled By: myleott

fbshipit-source-id: 2d387d022ee889494f429b98df1942167896e306",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1561,Myle Ott,myleott@fb.com,2020-11-05 15:17:32-08:00,b4d57c6d49682094efe22fbe2c03fa2c4973869f,https://github.com/pytorch/fairseq/commit/b4d57c6d49682094efe22fbe2c03fa2c4973869f,"Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397

Data parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`

Data parallel before:
```
2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel after:
```
2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`

Data parallel before:
```
2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Model parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9
```

Model parallel after:
```
2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24729295

Pulled By: myleott

fbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1562,Alex Xiao,axiao@fb.com,2020-11-05 16:12:24-08:00,83c39c41388f2e7ba37647d2e8a0cbc97f6f8032,https://github.com/pytorch/fairseq/commit/83c39c41388f2e7ba37647d2e8a0cbc97f6f8032,"fix fd issues for ce training with extra splits

Summary:
Some folks are still reporiting errors like in P145050739,

```
ValueError: too many fds
```

This diff follows up with D24234470 (https://github.com/pytorch/fairseq/commit/a9baca376616bed56e5df5115d7adf8059c0d296), where we add support for `supports_fetch_outside_dataloader` to the rest of the datsets we use, including the sampling datasets enabled by --extra-splits. For why we need to add `supports_fetch_outside_dataloader` see D24234470 (https://github.com/pytorch/fairseq/commit/a9baca376616bed56e5df5115d7adf8059c0d296)

Differential Revision: D24767506

fbshipit-source-id: 4e0252f70a9aa36155843677734f186fe03508c4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1563,alexeib,alexei.b@gmail.com,2020-11-06 10:23:11-08:00,77c704dc866c1e85259153ec98917f5acc7c9d90,https://github.com/pytorch/fairseq/commit/77c704dc866c1e85259153ec98917f5acc7c9d90,"Misc fixes2 (#1402)

Summary:
Fixes #2855
Fixes #2847
Fixes #2841
Fixes #2783

make omegaconf strict warning not show up

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1402

Reviewed By: myleott

Differential Revision: D24777582

Pulled By: alexeib

fbshipit-source-id: 389e110c9de90c4a0744d01982f8071a7a867f09",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1564,Myle Ott,myleott@fb.com,2020-11-06 15:37:42-08:00,b7a2e00958f647497d2980fd7980ae9fcd8a513e,https://github.com/pytorch/fairseq/commit/b7a2e00958f647497d2980fd7980ae9fcd8a513e,"Avoid some device-to-host transfers (#1400)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1400

Reviewed By: msbaines

Differential Revision: D24765749

Pulled By: myleott

fbshipit-source-id: c242f59c88b0d8cb691948f0495af40ba415faff",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1565,alexeib,alexei.b@gmail.com,2020-11-06 22:51:29-08:00,09a5d864fc5a79a0ec6fdf09fa0825f197060683,https://github.com/pytorch/fairseq/commit/09a5d864fc5a79a0ec6fdf09fa0825f197060683,"move configs into fairseq dir (#1403)

Summary:
this way they get shipped together with fairseq package

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1403

Reviewed By: myleott

Differential Revision: D24803076

Pulled By: alexeib

fbshipit-source-id: a9aa6e47a8ef26fae4d54691f1616a721b8f6112",13,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1566,Myle Ott,myleott@fb.com,2020-11-07 15:00:00-08:00,50422496ac7c3ef2e7dd3818a5126ab9ab37ae29,https://github.com/pytorch/fairseq/commit/50422496ac7c3ef2e7dd3818a5126ab9ab37ae29,"Automatically register components in ConfigStore via register_* functions (#1406)

Summary:
We can automatically register everything in ConfigStore on-the-fly. This avoids needing to worry about importing everything in the right order. It also better supports `--user-dir`, since those typically get imported later.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1406

Reviewed By: alexeib

Differential Revision: D24814072

Pulled By: myleott

fbshipit-source-id: 21cfc1a6c497fe98bf4429bfed138030d9999b6a",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1567,alexeib,alexei.b@gmail.com,2020-11-07 16:50:15-08:00,bd2e804b9c2ff1fae202c00e227f1afece12420b,https://github.com/pytorch/fairseq/commit/bd2e804b9c2ff1fae202c00e227f1afece12420b,"add and link hydra docs (#1405)

Summary:
updates hydra integration doc and links to it

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1405

Reviewed By: myleott

Differential Revision: D24808779

Pulled By: alexeib

fbshipit-source-id: a50160e196e469e30e39d6ee47440a569c0154bd",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1568,alexeib,alexei.b@gmail.com,2020-11-07 23:13:14-08:00,108f7204f6ccddb676e6d52006da219ce96a02dc,https://github.com/pytorch/fairseq/commit/108f7204f6ccddb676e6d52006da219ce96a02dc,"add local_rank alias (#1408)

Summary:
Fixes #2859

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1408

Reviewed By: myleott

Differential Revision: D24817281

Pulled By: alexeib

fbshipit-source-id: 4c1a3c7d6b3b940e1293d316253b57e101f3f862",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1569,UriSha,urishaham@mail.tau.ac.il,2020-11-09 10:55:29-08:00,18d3b5c8b0d71e0b828b5a0f5c54ee6769583669,https://github.com/pytorch/fairseq/commit/18d3b5c8b0d71e0b828b5a0f5c54ee6769583669,"Update wikitext url (#2871)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Update WikiText-103 url

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2871

Reviewed By: myleott

Differential Revision: D24835953

Pulled By: alexeib

fbshipit-source-id: 890e911d528c04de0dc056e55866afb46a2bd87f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1570,Myle Ott,myleott@fb.com,2020-11-09 12:25:01-08:00,d10fabd6971f51f59e3039accc248eae8945d6ff,https://github.com/pytorch/fairseq/commit/d10fabd6971f51f59e3039accc248eae8945d6ff,"Make it easier to use non-FairseqDatasets (#1411)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1411

Test Plan: Imported from OSS

Reviewed By: huihuifan

Differential Revision: D24833475

Pulled By: myleott

fbshipit-source-id: 5be599bd2b7d820a208321da53d594d5ae67bf2b",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1571,Myle Ott,myleott@fb.com,2020-11-09 12:25:01-08:00,5cfc50627788fb517e29ccc14ea8f3f12b8068a6,https://github.com/pytorch/fairseq/commit/5cfc50627788fb517e29ccc14ea8f3f12b8068a6,"Fix eval_lm.py to use current cfg for task, also fix --model-overrides (#1412)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1412

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D24833478

Pulled By: myleott

fbshipit-source-id: 4d0720a875541c016a00b28a4f0a9ad77e77e7a8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1572,Myle Ott,myleott@fb.com,2020-11-09 12:25:01-08:00,c19dfe26160c6cee768b31eb6bb149781d9c6eac,https://github.com/pytorch/fairseq/commit/c19dfe26160c6cee768b31eb6bb149781d9c6eac,"Make activation checkpointing interface less restrictive (#1413)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1413

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24833476

Pulled By: myleott

fbshipit-source-id: 380ea7e05c7b188086b2b10c15120ea6636e0a3e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1573,Myle Ott,myleott@fb.com,2020-11-09 12:25:01-08:00,74a59ada8c882b6a43eac190cb0608b3258ce165,https://github.com/pytorch/fairseq/commit/74a59ada8c882b6a43eac190cb0608b3258ce165,"Upgrade DummyLMTask to Hydra (#1415)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1415

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D24833480

Pulled By: myleott

fbshipit-source-id: 007623168467d18166b20ef99f54388eb9d8008a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1574,alexeib,alexei.b@gmail.com,2020-11-09 15:44:32-08:00,b418e46c8b4fedeaf80ef41b4235af33d496ddd4,https://github.com/pytorch/fairseq/commit/b418e46c8b4fedeaf80ef41b4235af33d496ddd4,"migrate audio_pretraining task to hydra (#1407)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1407

Reviewed By: myleott

Differential Revision: D24821909

Pulled By: alexeib

fbshipit-source-id: a58afdd17afab00062bef43cadae380998b23f29",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1575,alexeib,alexei.b@gmail.com,2020-11-09 15:44:32-08:00,4ea1c1eee077cbf85b1110e6f25d691e53270a7b,https://github.com/pytorch/fairseq/commit/4ea1c1eee077cbf85b1110e6f25d691e53270a7b,"migrate wav2vec2 model (#1409)

Summary:
see title
also includes some minor bug fixes

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409

Reviewed By: myleott

Differential Revision: D24822219

Pulled By: alexeib

fbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1576,alexeib,alexei.b@gmail.com,2020-11-09 19:21:13-08:00,6815772651fd639ed16360074aa23e238b29c6ce,https://github.com/pytorch/fairseq/commit/6815772651fd639ed16360074aa23e238b29c6ce,"fix wav2vec inference (#1418)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1418

Reviewed By: michaelauli

Differential Revision: D24847525

Pulled By: alexeib

fbshipit-source-id: e9f5d562ad2ac2904a65852cb9a05af775bebab0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1577,alexeib,alexei.b@gmail.com,2020-11-11 10:06:10-08:00,a66cc28b14ca8ff95e3aadfd3ca77bfb5b00c136,https://github.com/pytorch/fairseq/commit/a66cc28b14ca8ff95e3aadfd3ca77bfb5b00c136,"fix more bugs incl generating from w2v models (#1419)

Summary:
fixes several bugs:
- populating dataclasses from arg objects
- generating from w2v seq2seq models -> fix post processing, and make sure that generate uses the ""task"" args saved in the model that contain important info about dataset (e.g. whether to normalize it or not)
- use task's config object if it exists (so any new fields are picked up)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1419

Reviewed By: myleott

Differential Revision: D24853592

Pulled By: alexeib

fbshipit-source-id: 463762fa4c0de30e5bcbfca51df84714e4d1f464",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1578,alexeib,alexei.b@gmail.com,2020-11-11 10:14:04-08:00,11ea91a33a5788b3b9e7a02cab4bcb158cac8778,https://github.com/pytorch/fairseq/commit/11ea91a33a5788b3b9e7a02cab4bcb158cac8778,"load dataset with saved task config (optionally) (#1423)

Summary:
this adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423

Reviewed By: myleott

Differential Revision: D24875706

Pulled By: alexeib

fbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1579,alexeib,alexei.b@gmail.com,2020-11-11 17:32:30-08:00,e607911dde205e2188d3e62dcde592a6d84b4c46,https://github.com/pytorch/fairseq/commit/e607911dde205e2188d3e62dcde592a6d84b4c46,"fix passing task config in validate.py (#1426)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1426

Reviewed By: aconneau

Differential Revision: D24895299

Pulled By: alexeib

fbshipit-source-id: 7af96952b857fa4616cdafd0268d8ab6cb94c61d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1580,Angela Fan,angela.h.fan@gmail.com,2020-11-13 09:49:40-08:00,b55053373fb8678361a45a1d2c1b462befd9ab1a,https://github.com/pytorch/fairseq/commit/b55053373fb8678361a45a1d2c1b462befd9ab1a,"update m2m readme (#2890)

Summary:
adding smaller models

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2890

Reviewed By: ngoyal2707

Differential Revision: D24935146

Pulled By: huihuifan

fbshipit-source-id: 2ba8e4083b9805d336154e3cc0d6d7bed71cca04",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1581,Yuqing Tang,yuqtang@fb.com,2020-11-13 12:22:46-08:00,dc6a84f1433beaf3f6332ea181231055249be684,https://github.com/pytorch/fairseq/commit/dc6a84f1433beaf3f6332ea181231055249be684,"Make BART models compatiable with JIT

Summary: Bart models are not compatible with JIT. This diff makes minor changes to enable its compabilitity

Reviewed By: myleott

Differential Revision: D24824963

fbshipit-source-id: 41cbcc46c14b0439f5763478b8efe98e5516dc95",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1582,Weiyi Zheng,wyz@fb.com,2020-11-13 16:11:44-08:00,3c5647cebf454c07b52a0fb899c920789381ebda,https://github.com/pytorch/fairseq/commit/3c5647cebf454c07b52a0fb899c920789381ebda,"add grad_norm infinity check

Summary:
add grad_norm check for fp32 cases and single node training as well. Triggers nan detector when the grad_norm check fails, should help debug nan/inf cases.

also fixing a bug (i think) in the original check_grad_norm() where [float('inf'),  float('inf')] can pass the check.

Reviewed By: myleott

Differential Revision: D24849271

fbshipit-source-id: 2382342cd549717f3ff178b9aa29933f486327c8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1583,Pengzhi Gao,pengzhi.gao@petuum.com,2020-11-14 08:45:03-08:00,b987d30c69d773ccbf7d432e1cd0878aa9e50196,https://github.com/pytorch/fairseq/commit/b987d30c69d773ccbf7d432e1cd0878aa9e50196,"Delete duplicate code in RoBERTa model (#2891)

Summary:
Simple fix. Lines 498 and 499 are the same.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2891

Reviewed By: alexeib

Differential Revision: D24953450

Pulled By: myleott

fbshipit-source-id: 7745d066ed1e431edc39e99dd72ec8937235f752",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1584,Stas Bekman,stas@stason.org,2020-11-14 08:46:07-08:00,0d03fbedcf79b63901b8718b4c61c525464cb198,https://github.com/pytorch/fairseq/commit/0d03fbedcf79b63901b8718b4c61c525464cb198,"deprecation warning fixes (#2881)

Summary:
## What does this PR do?

Fixes:

- 2x `DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working`

- 1x `/fairseq/optim/adam.py:98: DeprecationWarning: invalid escape sequence \:`

This is with py38.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2881

Reviewed By: alexeib

Differential Revision: D24959633

Pulled By: myleott

fbshipit-source-id: ac563e194d5f07e3817de55729b0448366a6dc23",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1585,Myle Ott,myleott@fb.com,2020-11-15 19:46:48-08:00,0a848245f3e00ee39a68fddf54f738de11dd8cc8,https://github.com/pytorch/fairseq/commit/0a848245f3e00ee39a68fddf54f738de11dd8cc8,"Add Truncated BPTT example + TransformerXL (#1410)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1410

Test Plan:
- reproduced Transformer-XL results (see README)
- added integration test

Reviewed By: jingfeidu

Differential Revision: D24928966

Pulled By: myleott

fbshipit-source-id: 86376c17ab24d37e72e7c097b6dcec71b1a087a7",9,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not has_hf_transformers, )']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1586,Myle Ott,myleott@fb.com,2020-11-16 09:10:56-08:00,dc1eaf3dde83494037a8727de5897a43c46e0b46,https://github.com/pytorch/fairseq/commit/dc1eaf3dde83494037a8727de5897a43c46e0b46,"Remove unused hf/transformers submodule (#1435)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1435

Reviewed By: huihuifan

Differential Revision: D24973816

Pulled By: myleott

fbshipit-source-id: 1565dfc3f7e8db65ded4af92d1afd7aff8d19294",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1587,alexeib,alexei.b@gmail.com,2020-11-16 11:08:13-08:00,52d774cda9e8926ab42210da05715789fc567d8e,https://github.com/pytorch/fairseq/commit/52d774cda9e8926ab42210da05715789fc567d8e,"fix gumbel temp arg (#1438)

Summary:
Fix #2897

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1438

Reviewed By: myleott

Differential Revision: D24992106

Pulled By: alexeib

fbshipit-source-id: 0cb15c2e865c3e8f7950e8f5e6c54c5000637af2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1588,Juan Miguel Pino,juancarabina@fb.com,2020-11-16 12:41:26-08:00,add65adcc53a927f99a717d90a9672765237d937,https://github.com/pytorch/fairseq/commit/add65adcc53a927f99a717d90a9672765237d937,"Replace encoder output type (#1281)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1281

The PyTorch Mobile lite interpreter does not support NamedTuple creation in forward. One workaround is to replace NamedTuple with a custom class that inherits nn.Module. This class could be initialized in `__init__` and updated in forward. However, lite interpreter does not support list construction with custom classes. So the final solution is to replace the NamedTuple with a dictionary. We cannot have mixed value types in that dictionary, otherwise, this breaks TorchScript export. So the type is List[Tensor] and an empty list corresponds to having a value of None.

Reviewed By: myleott

Differential Revision: D23752010

fbshipit-source-id: 0b152a534a165ce4f84bd4f580d7f29145cfd264",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1589,Yuqing Tang,yuqtang@fb.com,2020-11-16 14:03:25-08:00,d7dd683b3bebcd3b3249db44d7faa7b670e44b8f,https://github.com/pytorch/fairseq/commit/d7dd683b3bebcd3b3249db44d7faa7b670e44b8f,"Add option to skip virtual epoch

Summary:
The current translation_multi_simple_epoch will add extrac layer of virtual epoch abstracts to load part of data and start training earlier. However, for smaller dataset this is not necessary.

This diff makes it skip virtual epoch layer if --virtual-epoch-size is not specified.

Reviewed By: pipibjc

Differential Revision: D24962835

fbshipit-source-id: 7de4293a6996ed075a1ed0c1ff2de94c8ae3df14",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1590,alexeib,alexei.b@gmail.com,2020-11-17 12:52:02-08:00,0e13e2fddedbe569f33167c3aa090cc1aa28a499,https://github.com/pytorch/fairseq/commit/0e13e2fddedbe569f33167c3aa090cc1aa28a499,"Wav2vec hydra (#1439)

Summary:
convert wav2vec 1.0 model to hydra

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1439

Reviewed By: myleott

Differential Revision: D25010596

Pulled By: alexeib

fbshipit-source-id: eb3ae81e7dad4789b217fca9bb4c6413835d75ab",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1591,Shruti Bhosale,bhosale.shruti18@gmail.com,2020-11-17 14:14:12-08:00,43e379e59068fa472c847400b6c653c88b7ffd95,https://github.com/pytorch/fairseq/commit/43e379e59068fa472c847400b6c653c88b7ffd95,"Fast Noisy Channel Online Decoding for Neural Machine Translation (#1436)

Summary:
This PR adds logic to generate translations using noisy channel decoding (i.e. with a channel model `P(source|target)` and language model `P(target)`, in addition to a direct model `P(target|source)`

It also includes additional logic to make noisy channel decoding very fast, without much loss in accuracy.

Most of the logic resides within `examples/fast_noisy_channel` -
- `noisy_channel_translation.py`: Fairseq Task for noisy channel translation
- `noisy_channel_sequence_generator.py`: Sequence Generator for noisy channel decoding - this contains the main logic for scoring the direct, channel and LM models at each step of beam search
- `noisy_channel_beam_search.py`: A variant of beam search that chooses the top-K candidates based on the combined scores from the direct, channel and LM models

TODO: add an integration test to ensure changes in the core fairseq files don't break the logic in fast_noisy_channel

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1436

Reviewed By: myleott, edunov

Differential Revision: D24986498

Pulled By: shruti-bh

fbshipit-source-id: 2ae3b7d68fe4a1cfb61493c363134ab7a16c8647",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1592,alexeib,alexei.b@gmail.com,2020-11-17 17:07:23-08:00,265791b727b664d4d7da3abd918a3f6fb70d7337,https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337,"fix loading ensembles (#1442)

Summary:
fixes loading ensembles. previous change used the state of the first model for all models in the ensemble

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442

Reviewed By: chtran

Differential Revision: D25035706

Pulled By: alexeib

fbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1593,Myle Ott,myleott@fb.com,2020-11-18 10:31:19-08:00,e931009a91c430a66583e80a91d1de9cea656bd2,https://github.com/pytorch/fairseq/commit/e931009a91c430a66583e80a91d1de9cea656bd2,"Fix boundary condition in token_block_utils_fast.pyx (#1445)

Summary:
In cases where the item size in the underlying dataset is 0, it's possible that `remaining` is initialized to 0. We can update the assert to reflect this.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1445

Reviewed By: alexeib

Differential Revision: D25054723

Pulled By: myleott

fbshipit-source-id: 1bb73cce34e973f407436c442b698ce706d97359",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1594,Myle Ott,myleott@fb.com,2020-11-18 14:30:02-08:00,41a61bd4e2835c7bed25cc9f52fe65714379322e,https://github.com/pytorch/fairseq/commit/41a61bd4e2835c7bed25cc9f52fe65714379322e,"Add GitHub Action to build Python wheels (+ minor cleanup in build scripts) (#1447)

Summary:
Here's an example run in a forked repo: https://github.com/fairseq/fairseq/runs/1419699104

We can upload the wheels to PyPI to make `pip install fairseq` easier for folks.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1447

Reviewed By: lematt1991

Differential Revision: D25060753

Pulled By: myleott

fbshipit-source-id: 9fdc28cc7c8a172daac668dd09684ec43e2ff11a",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1595,alexeib,alexei.b@gmail.com,2020-11-18 22:26:34-08:00,6d2cf0ddf64040543c346b3866eb636d14522dde,https://github.com/pytorch/fairseq/commit/6d2cf0ddf64040543c346b3866eb636d14522dde,"convert wav2vec2 asr to hydra (#1444)

Summary: this completes wav2vec migration to hydra

Test Plan:
- training wav2vec2 models
- training wav2vec2 ctc models
- training wav2vec2 seq2seq models
- infer.py eval of ctc models
- generate.py eval of seq2seq models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1444

Reviewed By: myleott

Differential Revision: D25040041

Pulled By: alexeib

fbshipit-source-id: 2aac3b9c659667f7e696628a4b016ee863da68cf",23,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1596,Suyoun Kim,suyounkim@fb.com,2020-11-19 14:56:54-08:00,6e280bff2aecf5e7fe92d46aa2507c6cae036fe3,https://github.com/pytorch/fairseq/commit/6e280bff2aecf5e7fe92d46aa2507c6cae036fe3,"set lr for each epoch by using input dictionary parameter

Summary: [Manual LR Scheduler] set lr for each epoch by using input dictionary parameter

Differential Revision: D25047764

fbshipit-source-id: 4b8ccfa1b1f5db99d73fdb478caa2c6ea8d80a50",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1597,alexeib,alexei.b@gmail.com,2020-11-19 15:02:18-08:00,7171cdec5b15b983764dacbc618c572546a8a692,https://github.com/pytorch/fairseq/commit/7171cdec5b15b983764dacbc618c572546a8a692,"fix interpolated fields not being added to argparse (#1450)

Summary:
we were skipping fields that are interpolated from being added to argparse, but now we add them if they have their own help. this affected --total-num-updates for polynomial decay

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1450

Reviewed By: huihuifan

Differential Revision: D25098158

Pulled By: alexeib

fbshipit-source-id: 105bc67cb5ddfc86475f3b50c5d1b5cc00330d85",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1598,Myle Ott,myleott@fb.com,2020-11-20 05:59:25-08:00,40fbb3744304de0eaa164fc84dd736d9a202a427,https://github.com/pytorch/fairseq/commit/40fbb3744304de0eaa164fc84dd736d9a202a427,"Migrate remaining LR schedulers (#1448)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1448

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25092150

Pulled By: myleott

fbshipit-source-id: fd066a0eba388bb0c344082a8fa1132974d53d40",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1599,Myle Ott,myleott@fb.com,2020-11-20 05:59:25-08:00,3b77a6160097408e01883c69e6f8fed017266311,https://github.com/pytorch/fairseq/commit/3b77a6160097408e01883c69e6f8fed017266311,"Add fairseq-hydra-train and update docs (#1449)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1449

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25094525

Pulled By: myleott

fbshipit-source-id: 430387d11196d3292933bb168cf09ea16ebc0d3b",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1600,Myle Ott,myleott@fb.com,2020-11-20 12:40:49-08:00,94f59bb67bf48d2913dc223fc20b8e94c7ed1bab,https://github.com/pytorch/fairseq/commit/94f59bb67bf48d2913dc223fc20b8e94c7ed1bab,"Remove unused train_masked_language_model helper (#1452)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1452

Test Plan: Imported from OSS

Reviewed By: lematt1991

Differential Revision: D25108462

Pulled By: myleott

fbshipit-source-id: 3c17a9937a4c3edb69f64130dfd866c5f42a4aaf",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1601,Myle Ott,myleott@fb.com,2020-11-20 12:40:49-08:00,fa113ff1dee60bf493b6e61820303ab9d72cabcb,https://github.com/pytorch/fairseq/commit/fa113ff1dee60bf493b6e61820303ab9d72cabcb,"Add test for activation checkpointing (#1453)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1453

Test Plan: Imported from OSS

Reviewed By: sshleifer

Differential Revision: D25108463

Pulled By: myleott

fbshipit-source-id: 3cebce9be7fe503401eabba3f483c26847e7a3c0",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1602,Myle Ott,myleott@fb.com,2020-11-20 12:40:49-08:00,d464af2feb4f5f7149e10241cf1d064071f404e1,https://github.com/pytorch/fairseq/commit/d464af2feb4f5f7149e10241cf1d064071f404e1,"Fix NAT code (#1454)

Summary:
D23752010 (https://github.com/pytorch/fairseq/commit/add65adcc53a927f99a717d90a9672765237d937) broke some GPU-only tests for NAT.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1454

Test Plan: Imported from OSS

Reviewed By: jmp84

Differential Revision: D25108461

Pulled By: myleott

fbshipit-source-id: f32b890221578c421944d6f9a49f06ef1dc075c6",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1603,Wei Ho,weiho@fb.com,2020-11-20 14:45:04-08:00,e419db74e2ae557c60b6cbf304ae9f8cc812d9dd,https://github.com/pytorch/fairseq/commit/e419db74e2ae557c60b6cbf304ae9f8cc812d9dd,"Add extra logging before/after checkpointing

Summary:
Makes it easier for ppl to notice if things break in the middle of writing checkpoint (ex: OOMing)

(Also helps provide timing stats for how long it took to write checkpoints)

Reviewed By: donhusa

Differential Revision: D25120107

fbshipit-source-id: 35a7e9b7fe22a1ffa25fb8b461e7b7bef09fa063",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1604,Myle Ott,myleott@fb.com,2020-11-20 19:16:50-08:00,521fccf93c821cabb3686b768f9d9152486b5bd6,https://github.com/pytorch/fairseq/commit/521fccf93c821cabb3686b768f9d9152486b5bd6,"Fix torch.distributed.launch (fixes #2924) (#1456)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1456

Reviewed By: alexeib

Differential Revision: D25133448

Pulled By: myleott

fbshipit-source-id: 8a7573b69c471b237fffdfc7874f9f6b51143f5a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1605,Kritika Singh,skritika@fb.com,2020-11-21 12:02:02-08:00,2b854d8517c3398e576ff4bf8cc5f59c90af74c4,https://github.com/pytorch/fairseq/commit/2b854d8517c3398e576ff4bf8cc5f59c90af74c4,"wav2vec in PySpeech

Summary:
Define `audio_pretraining_fb` task similar to [`audio_pretraining`](https://www.internalfb.com/intern/codesearch/path/fbsource/fbcode/deeplearning/projects/fairseq-py/fairseq/tasks/audio_pretraining.py) in fairseq. I was earlier trying to stay as close to the fairseq task as possible but  sub-classing `FbSpeechRecognitionTask` from pyspeech made decoding much easier.

The main differences between the fairseq and pyspeech end-to-end processes are:
a. Inputs to the training
`fairseq`: train.tsv, train.labels.txt, valid.tsv, valid.labels.txt, dict.labels.txt
`pyspeech`: data.json. An example is:
```{
  ""fairseq_dict"": ""spm_char_32_fairseq.dict"",
  ""train"": {
    ""handles_file"": ""train_dataaug_full.tsv"",
    ""transforms"": [
      [""RawAudioDatasetTransform"", {}],
      [""SentencePieceEncodeTransform"", {""model_path"": ""spm_char_32.model"", ""append_eos"": false}]
    ]
  },
  ""valid"": {
    ""handles_file"": ""valid_pages_10s_full.tsv"",
    ""transforms"": [
      [""RawAudioDatasetTransform"", {}],
      [""SentencePieceEncodeTransform"", {""model_path"": ""spm_char_32.model"", ""append_eos"": false}]
    ]
  }
}
```
The handles files follow the format `{handle}\t{length}\t{optional_ref}`

b. Encoding the reference into target units: fairseq process was to use a dictionary file to specify the target units, an offline script [libri_labels.py](https://www.internalfb.com/intern/codesearch/path/fbsource/fbcode/deeplearning/projects/fairseq-py/examples/wav2vec/libri_labels.py) to split the reference and LabelEncoder to get the target unit sequence. This is replaced by [`SentencepieceEncodeTransform`](https://www.internalfb.com/intern/codesearch/path/fbsource/fbcode/deeplearning/projects/pyspeech/pyspeech/data/transforms/sentence_piece_encode_transform.py) such that when you pass a sentencepiece model and dictionary (from sentencepiece library) to the training, the transformations happen on the fly.
c. FairseqDataset class: I use `FbEverstoreDataset` in place of `EverstoreAudioDataset` (from fairseq) to be able to use the existing pre-processing transforms like `SentencepieceEncodeTransform` and easier integration with CTC decoder. This required copying over the audio processing and collater code pieces from RawAudioDataset to new PySpeechTransform and collator classes.

Reviewed By: alexeib

Differential Revision: D24265820

fbshipit-source-id: 68e2fef38a0cc1cf316410d83ed405d62a810578",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1606,Tim Dettmers,TimDettmers@users.noreply.github.com,2020-11-21 18:39:00-08:00,b1b02a828fec708aea1718e5336dd941a24f4276,https://github.com/pytorch/fairseq/commit/b1b02a828fec708aea1718e5336dd941a24f4276,"Fixed min-lr / max-lr swap on cosine schedule. (#2916)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
The cosine learning rate scheduler was implemented incorrectly. It annealed to the learning rate (`--lr`) instead of the minimum learning rate (`--min-lr`). This implementation is consistent with the PyTorch [CosineAnnealingLR](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L461).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2916

Reviewed By: alexeib

Differential Revision: D25146468

Pulled By: myleott

fbshipit-source-id: 8704b6954dd40692eb930b882fecfa799ea98b00",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1607,Myle Ott,myleott@fb.com,2020-11-21 18:43:09-08:00,158bd0321c4b915e4bddf738f5cb9d72d192f969,https://github.com/pytorch/fairseq/commit/158bd0321c4b915e4bddf738f5cb9d72d192f969,"Add .github/stale.yml (#2932)

Summary:
Mostly copied from https://github.com/facebook/react/blob/master/.github/stale.yml

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2932

Reviewed By: alexeib

Differential Revision: D25146465

Pulled By: myleott

fbshipit-source-id: c11d695dcbd2f18609c04af2e520317977797e0f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1608,Xu Song,xusong.vip@gmail.com,2020-11-21 19:29:07-08:00,b889b52ae9b91a0114112d00735df56c1aa36fad,https://github.com/pytorch/fairseq/commit/b889b52ae9b91a0114112d00735df56c1aa36fad,"Update hub_utils.py (#2910)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?

 has no attribute 'arg'

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2910

Reviewed By: alexeib

Differential Revision: D25146481

Pulled By: myleott

fbshipit-source-id: 11912bb2bcacd1d2f91da47bb0d868da90b38f17",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1609,alexeib,alexei.b@gmail.com,2020-11-21 22:50:04-08:00,8e328aec86a2afee83c77095d0b5bb2c449ed5c4,https://github.com/pytorch/fairseq/commit/8e328aec86a2afee83c77095d0b5bb2c449ed5c4,"minor fixes (#1457)

Summary:
- some minor fixes
a) secondary loss logging in wav2vec criterion
b) ability to have nested Dict[...] inside config objects
c) remove debug param

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1457

Reviewed By: myleott

Differential Revision: D25145151

Pulled By: alexeib

fbshipit-source-id: 21e6422b91151b00b929447f0c73deced56450cb",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1610,tuanh208,nguyentuanh208@gmail.com,2020-11-21 23:00:33-08:00,7cdef0a9bce575738ffb7b3c5fcad07181f149ac,https://github.com/pytorch/fairseq/commit/7cdef0a9bce575738ffb7b3c5fcad07181f149ac,"Adding --mask-multiple-length and --mask-stdev options to masked_lm task (#2846)

Summary:
NOTE: this implements span masking for RoBERTa as described in vq-wav2vec paper

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Adding --mask-multiple-length and --mask-stdev options to masked_lm task, allowing to mask sequences of multiple lengths when training a masked language model.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Yes �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2846

Reviewed By: myleott

Differential Revision: D25007978

Pulled By: alexeib

fbshipit-source-id: a8b3bcb260c8308641362c8c59706f08142e6be9",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1611,alexeib,alexei.b@gmail.com,2020-11-22 09:33:02-08:00,1519c80aef44849713581afdd83475e832f0dc1c,https://github.com/pytorch/fairseq/commit/1519c80aef44849713581afdd83475e832f0dc1c,"workaround hydra + submit it not supporting custom enums (#1458)

Summary:
this allows using submitit launcher with hydra to launch fairseq jobs

something like this now works:

```
python fairseq_cli/hydra_train.py --multirun hydra/launcher=submitit_slurm hydra.launcher.cpus_per_task=80 hydra.launcher.gpus_per_node=8 hydra.launcher.tasks_per_node=1 hydra.launcher.nodes=2 hydra.launcher.partition=dev hydra.launcher.mem_gb=400 distributed_training.distributed_world_size=16 distributed_training.distributed_port=33333 +optimization.update_freq='[2]' --config-path /private/home/abaevski/fairseq-py/examples/wav2vec/config/pretraining --config-name wav2vec2_base_librispeech
```

(note that one has to specify distributed_port for this to work)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1458

Reviewed By: myleott

Differential Revision: D25150369

Pulled By: alexeib

fbshipit-source-id: 63b74a437fb92afff8b0faa579d07f4539a2f1d8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1612,alexeib,alexei.b@gmail.com,2020-11-22 13:51:53-08:00,74fc8ccce1cfb7367cb57e6605e125030f2b0d31,https://github.com/pytorch/fairseq/commit/74fc8ccce1cfb7367cb57e6605e125030f2b0d31,"default max_tokens_valid and batch_size_valid correctly (#1459)

Summary:
max_tokens_valid and batch_size_valid were not getting defaulted properly, leading to ooms

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1459

Reviewed By: myleott

Differential Revision: D25151434

Pulled By: alexeib

fbshipit-source-id: 0dc0f099973e6abc8ba9b20516da26b4fb2e0e33",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1613,Alexei Baevski,abaevski@fb.com,2020-11-23 14:41:25-08:00,168480c9f11e2db0c1b0a40eb0a901133e05cb4a,https://github.com/pytorch/fairseq/commit/168480c9f11e2db0c1b0a40eb0a901133e05cb4a,"add model criterion

Summary: - add model criterion that allows one to define any kind of loss(es) within your model and then just have this criterion do the logging

Reviewed By: myleott

Differential Revision: D25145814

fbshipit-source-id: bb0f01935b96d5c77f8adad40e931689ce6e3391",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1614,alexeib,alexei.b@gmail.com,2020-11-23 19:07:19-08:00,f13f2990935fce56f62a40b1243cac0ee4668433,https://github.com/pytorch/fairseq/commit/f13f2990935fce56f62a40b1243cac0ee4668433,"fix issubclass() call on python 3.7+ (#1462)

Summary:
Fixes #2897

Also updates readmes to use --config-dir instead of --config-path for hydra runs, and adds __init__.py to config dir

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1462

Reviewed By: myleott

Differential Revision: D25163789

Pulled By: alexeib

fbshipit-source-id: f45f432174771c5c458480f984aedf12130b8522",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1615,Chau Tran,chau@fb.com,2020-11-25 23:04:24-08:00,dea66cc294a18dd4d9e59aa0af8d51f951e83884,https://github.com/pytorch/fairseq/commit/dea66cc294a18dd4d9e59aa0af8d51f951e83884,"Fix mbart checkpoint

Summary:
activation_fn was hardcoded to 'gelu' when mbart was trained, and
activation_fn was not saved to the checkpoint. The
default on master is 'relu', so the old version would use 'relu'. I manually
updated the activation_fn in the checkpoint to 'gelu' and uploaded to
https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz

Reviewed By: myleott

Differential Revision: D25163364

fbshipit-source-id: 365ebbd39ebb341c92b1c9ad71c8fbb2edffb7e6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1616,Yuan Shangguan (June),yuansg@fb.com,2020-11-30 11:38:53-08:00,4ba5b4be98399c3c20d6e3c9b7750461afb83730,https://github.com/pytorch/fairseq/commit/4ba5b4be98399c3c20d6e3c9b7750461afb83730,"Minor changes to manual LR scheduler

Summary:
1. Update to make the learning rate at the beginning of training to be reasonable. Otherwise, job train_loss explodes like this f235003551 or f234959697 or f234961844.
2. Trivial update to avoid printing out massive update2lr dictionary that takes over the entire logging page, and makes logging difficult to see.
Example:
{F347075703}

Differential Revision: D25146200

fbshipit-source-id: 071a591cf823e8c74a0380ec6850dc6b34d82ffc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1617,Myle Ott,myleott@fb.com,2020-11-30 14:19:25-08:00,9cf0bd96d645df23dfbacf6ee28e3ddb441e8717,https://github.com/pytorch/fairseq/commit/9cf0bd96d645df23dfbacf6ee28e3ddb441e8717,"Add/fix tests (#1468)

Summary:
- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)
- add test for LayerDrop (and fix it)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468

Reviewed By: alexeib

Differential Revision: D25223272

Pulled By: myleott

fbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499",3,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,7,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestCheckpointUtils(unittest.TestCase):'],"[('Equal', '(len(ensemble), 2)'), ('Equal', '(ensemble[0].args.seed, 123)'), ('Equal', '(ensemble[1].args.seed, 456)'), ('Equal', '(task.args.seed, 123)'), ('Equal', '(len(ensemble), 1)'), ('Equal', '(len(ensemble[0].encoder.layers), 2)'), ('Equal', '(len(ensemble[0].decoder.layers), 1)')]",['def setUp(self):'],[],['def tearDown(self):'],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1618,Yuan Shangguan (June),yuansg@fb.com,2020-11-30 14:45:20-08:00,f732b403ec15244c41a24b9e28d6c5a411a511df,https://github.com/pytorch/fairseq/commit/f732b403ec15244c41a24b9e28d6c5a411a511df,"Max_update is not backward compatible. Fix in this diff.

Summary:
max_update assertion in tri_stage_lr is introduced in D25040041 (https://github.com/pytorch/fairseq/commit/6d2cf0ddf64040543c346b3866eb636d14522dde).
It requires max-update to be defined, and breaks the backward compatibility of existing recipes. Since max-update is ONLY used when phase-ratio is defined. We recommend this change to keep it from breaking existing model recipes.

Reviewed By: myleott

Differential Revision: D25204247

fbshipit-source-id: 01f6f2f0935dfaff9f23501158af608e5d507145",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1619,Alex Xiao,axiao@fb.com,2020-12-01 11:16:33-08:00,65d88f150c54f9549de0b565411684b52f4e2b50,https://github.com/pytorch/fairseq/commit/65d88f150c54f9549de0b565411684b52f4e2b50,"more accurate nan detection

Summary:
I've been debugging some nan issues related to fp16. This diff adds some improvements to make it more accurate:

1. Zero grad before doing nan detection. This enables the gradients/grad norms calculated to be accurate
2. Account for update frequency when doing nan detection. Without this, infs/nans can go undetected.

Reviewed By: myleott

Differential Revision: D25225729

fbshipit-source-id: 4ffd1dcdf4a643459b814e24e74776b144a068a8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1620,alexeib,alexei.b@gmail.com,2020-12-01 17:44:23-08:00,0db28cdd0e50cad9c36e5e47ffceff40beaf6f60,https://github.com/pytorch/fairseq/commit/0db28cdd0e50cad9c36e5e47ffceff40beaf6f60,"fix generation config being properly passed (#1465)

Summary:
fixes #2961

Also fixes model criterion logging with world size > 1

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1465

Reviewed By: myleott

Differential Revision: D25198394

Pulled By: alexeib

fbshipit-source-id: fa52011a4d56eb41fe4bd59f9bd565632b87fba5",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1621,Sathish Indurthi,sathish.indurthi@gmail.com,2020-12-03 07:45:54-08:00,ffa158ff0cf2aa6c104ae844bfde361f125478f6,https://github.com/pytorch/fairseq/commit/ffa158ff0cf2aa6c104ae844bfde361f125478f6,"fix for MMA criterion initialization (#2911)

Summary:
# Before submitting

- [X ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ X] Did you make sure to update the docs?   N/A
- [ X] Did you write any new necessary tests?  N/A

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2122.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2911

Reviewed By: alexeib

Differential Revision: D25146472

Pulled By: myleott

fbshipit-source-id: 9cf02a9be679c2e1725dd3ae83aafef31900e640",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1622,Myle Ott,myleott@fb.com,2020-12-03 13:22:14-08:00,d7e571c557e0f7833fa244ecb5cd0458ba28670c,https://github.com/pytorch/fairseq/commit/d7e571c557e0f7833fa244ecb5cd0458ba28670c,"Small fixes for TPU (and support them in sweep) (#1433)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1433

Test Plan: Imported from OSS

Reviewed By: huihuifan

Differential Revision: D24959540

Pulled By: myleott

fbshipit-source-id: a3b247be4ae7f4e09e571f972451e1e4ce76d5c5",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1623,wei zhao,zhaoweisjtuer@163.com,2020-12-03 14:19:16-08:00,fa802c1034c4e6a38c80e7ab545b445aabd2d314,https://github.com/pytorch/fairseq/commit/fa802c1034c4e6a38c80e7ab545b445aabd2d314,"Fix another link to mbart checkpoint (#2976)

Summary:
Forget to update another model url after the fix https://github.com/pytorch/fairseq/commit/dea66cc294a18dd4d9e59aa0af8d51f951e83884. chtran

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2976

Reviewed By: tangyuq

Differential Revision: D25257003

Pulled By: chtran

fbshipit-source-id: 2fdb30547ed1fc82ff5cfa038a3b6d8fb9dc60ba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1624,Arthur Guo,arthurguo@fb.com,2020-12-03 15:32:33-08:00,793ec2b19d63b70717a84293b45e583f6c0b9dd5,https://github.com/pytorch/fairseq/commit/793ec2b19d63b70717a84293b45e583f6c0b9dd5,"Enable JIT on LAS Model

Differential Revision: D25290353

fbshipit-source-id: 18ce98d32e49e9cebe1aed14302613a00e8c3c99",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1625,Myle Ott,myleott@fb.com,2020-12-03 18:17:09-08:00,bc4ebcafb4f1535c528aea589d14db56a13bd763,https://github.com/pytorch/fairseq/commit/bc4ebcafb4f1535c528aea589d14db56a13bd763,"Fix tests (#1482)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1482

Reviewed By: michaelauli

Differential Revision: D25318618

Pulled By: myleott

fbshipit-source-id: bed171ffe5ca10e8359be96a15d0fe9bb1a630ea",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1626,Robert Verkuil,rverkuil@fb.com,2020-12-04 04:43:09-08:00,a700e14ea3cbf3e99ca729caa30b5b1c0305c4c4,https://github.com/pytorch/fairseq/commit/a700e14ea3cbf3e99ca729caa30b5b1c0305c4c4,"Increase plasma reconnect attempts (#1480)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Avoids run failures due to insufficient time allowed for plasma client -> server connection.

I've been seeing many failures in my recent runs, which use data sharding, due to plasma connection errors:
[train.stderr.33224323.txt](https://github.com/fairinternal/fairseq-py/files/5637976/train.stderr.33224323.txt)
[train.stderr.33245371.txt](https://github.com/fairinternal/fairseq-py/files/5637980/train.stderr.33245371.txt)
[train.stderr.33261567.txt](https://github.com/fairinternal/fairseq-py/files/5637981/train.stderr.33261567.txt)
[train.stderr.33261589.txt](https://github.com/fairinternal/fairseq-py/files/5637982/train.stderr.33261589.txt)

Currently plasma can attempt client -> server connection 20 times, for a total retry time of ≈8-10s.  When sharding, epoch_itr's intentionally are *not* cached.  Therefore, dataset-related plasma arrays must be remade.  This makes plasma connect errors a persistent concern over training.  Worse with many gpus+workers.

For a single training run that I inspected, the number of retries needed to connect increases, and eventually exceeds 20.
![image](https://user-images.githubusercontent.com/4042063/101068717-10853100-3567-11eb-89ca-7d18a7ef0405.png)

Best solution would be to increase the retry interval.  Looking at plasma [source](https://github.com/apache/arrow/blob/016f76c8c02e769d58b3e785a87674d98ce83367/python/pyarrow/_plasma.pyx#L848-L868), this doesn't look possible.  However, we can increase num_retries.  Hopefully this isn't too annoying for the cluster file system?

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1480

Reviewed By: myleott

Differential Revision: D25313114

Pulled By: joshim5

fbshipit-source-id: ad50c3b29e0698bf197e24f6392bda73b407a548",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1627,Myle Ott,myleott@fb.com,2020-12-04 05:43:50-08:00,8be488ff6b7fc49346c94323085e71e72b6583ea,https://github.com/pytorch/fairseq/commit/8be488ff6b7fc49346c94323085e71e72b6583ea,"Add --load-checkpoint-on-all-dp-ranks (#1478)

Summary:
A recent commit [1] made it so that checkpoints are loaded on rank 0 and then broadcast to other workers. This is valuable to reduce I/O and especially helpful when training with optimizer state sharding, so we don't need to load redundant optimizer state on every worker.

This diff adds a new option (`--load-checkpoint-on-all-dp-ranks`) that optionally reverts to the old behavior.

[1] https://github.com/pytorch/fairseq/commit/ea4ccd94de131d6b39163836418696369dd1d034

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1478

Test Plan: Imported from OSS

Reviewed By: theweiho

Differential Revision: D25291595

Pulled By: myleott

fbshipit-source-id: 57b521c5e6a48f08140f9527162072ea1d4066db",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1628,Myle Ott,myleott@fb.com,2020-12-04 10:57:35-08:00,bb039fa2063dca1b388d6be2f64052b07fb556a2,https://github.com/pytorch/fairseq/commit/bb039fa2063dca1b388d6be2f64052b07fb556a2,"Improve performance of distributed_utils.broadcast_object (#1479)

Summary:
This diff dramatically speeds up and reduces memory usage of distributed_utils.broadcast_object. In particular, rather than pickling the whole state dict (and broadcasting it), we only pickle the non-tensors and broadcast the tensors directly. This improves speed (since pickling is expensive) and also saves RAM, since pickle duplicates the data and by separating out the tensors, we're only left with duplicate copies of non-tensor data in the state dict, which is quite small.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1479

Test Plan: Imported from OSS

Differential Revision: D25291594

Pulled By: myleott

fbshipit-source-id: 521102fae75a3bc71dcd5ac2bf238f7eb534a3d1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1629,Myle Ott,myleott@fb.com,2020-12-04 10:57:35-08:00,6f47704d4deb99061e7562710e1dbd0253b04ea4,https://github.com/pytorch/fairseq/commit/6f47704d4deb99061e7562710e1dbd0253b04ea4,"Add distributed tests (#1481)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1481

Test Plan: Imported from OSS

Reviewed By: theweiho

Differential Revision: D25313776

Pulled By: myleott

fbshipit-source-id: 755bf4b77b2a7a3aee56e2344246ff2087a3af77",3,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,1,0,0,0,0,3,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDistributedUtils(unittest.TestCase):'],[],['def setUp(self):'],[],[],[],[],"['()', '()', '()']",[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['objects_are_equal(ref_obj, obj)']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1630,Alex Conneau,alexis.conneau@gmail.com,2020-12-04 13:45:22-08:00,ba79f7b781929e04c827c6dda9048e4e6e86ba6a,https://github.com/pytorch/fairseq/commit/ba79f7b781929e04c827c6dda9048e4e6e86ba6a,"Adding XLSR-53 model to wav2vec README (#1483)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1483

Reviewed By: alexeib

Differential Revision: D25326050

Pulled By: aconneau

fbshipit-source-id: 7428244d328ea0bbbbaaf23f715cd6d44d329b94",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1631,Rui Hou,rayhou@fb.com,2020-12-04 13:55:37-08:00,579002a0ea7d3161660a7beeb8c56b1f0fc26b63,https://github.com/pytorch/fairseq/commit/579002a0ea7d3161660a7beeb8c56b1f0fc26b63,"Add a dummy first_batch to EpochBatchIterating

Summary: Add a dummy first_batch to EpochBatchIterating

Reviewed By: myleott

Differential Revision: D25337800

fbshipit-source-id: c387c4c39533c161cb160a84ad4f99e71c66f73e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1632,dingjiajia,jiajia.ding@hotmail.com,2020-12-04 17:09:08-08:00,d5218f88275fd57825819c6dab523e30a41b6866,https://github.com/pytorch/fairseq/commit/d5218f88275fd57825819c6dab523e30a41b6866,"update sequence generator device change (#2989)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes Sequence Generator

Reproduce the bug:
1. Background:
**fairseq.utils.move_to_cuda** function support to change device.
2. When you change the file **fairseq_cli/generate.py**
suppose `gpu_id = 1`
- line 134 change from
`model.cuda()`
to
`model.cuda(gpu_id)`
- line 189 change from
`sample = utils.move_to_cuda(sample) if use_cuda else sample`
to
`sample = utils.move_to_cuda(sample, gpu_id) if use_cuda else sample`
2. you will get an error
```
fairseq/sequence_generator.py, line 382, in generate
    cand_bbsz_idx = cand_beams.add(bbsz_offsets)
RuntimeError: binary_op(): expected both inputs to be on same device, but input a is on cuda:1 and input b is on cuda:0

```

The reason of this bug is that `bbsz_offsets` is not assigned to proper cuda device. Therefore, we need to change the line 281 and line 281 of the file **fairseq/sequence_generator.py**

The test file for this fix is the file **tests/test_sequence_generator.py**

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
very happy {emoji:1f642}

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2989

Reviewed By: alexeib

Differential Revision: D25344265

Pulled By: myleott

fbshipit-source-id: 8cb8b389e59a9aa67aec84dbdadcfa2c08c9648f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1633,Alexei Baevski,abaevski@fb.com,2020-12-04 17:34:08-08:00,ba4f54267af5c3f67f1b76a6e804b6ab593d1d39,https://github.com/pytorch/fairseq/commit/ba4f54267af5c3f67f1b76a6e804b6ab593d1d39,"composite optimizer

Summary:
this adds a composite optimizer and pass through learning rate scheduler that allows fairseq models to have separate optimizers (that can optionally have separate lr schedulers) for different parameters. to use this, you add a ""param_group"" field to the parameters you wish to be optimized separately (the rest of the params get automatically placed into a ""default"" group), then specify a composite optimizer with nested optimizers (and, optionally, lr schedulers) for each group name (see example below).

for fp16 training this requires setting fp16_no_flatten_grads to true

one possible area for future improvement is to automatically create param groups based on module names, but this is to be discussed

for example, i can modify wav2vec2 model and add
```python
for p in self.pos_conv.parameters():
    p.param_group = ""pos_conv""
```
in the TransformerEncoder class, just after pos_conv is created

then i create the following config:

```yaml
# package _group_

hydra:
  run:
    dir: .
  job_logging:
    disable_existing_loggers: false

common:
  fp16: true
  log_format: json
  log_interval: 10
  fp16_no_flatten_grads: true

checkpoint:
  save_interval_updates: 20
  keep_interval_updates: 1
  no_epoch_checkpoints: true
  no_save: false

Reviewed By: myleott

Differential Revision: D25152032

fbshipit-source-id: c73ff95146ecc2a04660c67bcad02b637c5c5098",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1634,Myle Ott,myleott@fb.com,2020-12-05 07:36:28-08:00,4df4d0af8d706952013f8edf7da811937b8384c8,https://github.com/pytorch/fairseq/commit/4df4d0af8d706952013f8edf7da811937b8384c8,"Add missing `--optimizer` option to tutorial docs (fixes #2830) (#1485)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1485

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342182

Pulled By: myleott

fbshipit-source-id: 7eb2a4b2b7377da31d4f538053cc196437532db0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1635,Myle Ott,myleott@fb.com,2020-12-05 07:36:28-08:00,72a25a4e52402b6f53aa98cfb739c075c0d6f7ee,https://github.com/pytorch/fairseq/commit/72a25a4e52402b6f53aa98cfb739c075c0d6f7ee,"Rename optimization.min_lr -> optimization.stop_min_lr (#1486)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342181

Pulled By: myleott

fbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5",20,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1636,Myle Ott,myleott@fb.com,2020-12-05 07:36:28-08:00,4817a9142f49793ec2eedbd71fe5bd872e58e7b5,https://github.com/pytorch/fairseq/commit/4817a9142f49793ec2eedbd71fe5bd872e58e7b5,"Cleanup CosineLRScheduler and change defaults (#1487)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1487

Here's the code for CosineLRScheduler that I used as a reference: https://github.com/pytorch/fairseq/blob/577e4fa78a295fd7cd3ee7e9fd4b936ca800ebea/fairseq/optim/lr_scheduler/cosine_lr_schedul

In the reference:
- `warmup_init_lr` defaults to `args.lr[0]`
- `warmup_end_lr` defaults to `args.max_lr`
- `min_lr` defaults to `args.lr[0]`  (note that there's also a `args.min_lr` option defined in the global fairseq config, but this is unused by the cosine scheduler)
- `max_lr` is a required option

This diff removes `max_lr` and replaces it with `lr[0]` to be more
consistent with other LR schedulers. We then add an explicit `min_lr`
option to the Config.

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342180

Pulled By: myleott

fbshipit-source-id: 61281666e68839da8efc4714c2ce8c49dc4c8e6e",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1637,alexeib,alexei.b@gmail.com,2020-12-08 15:47:53-08:00,feb5f07ff4220b7908ff12c6692685784c7c9a71,https://github.com/pytorch/fairseq/commit/feb5f07ff4220b7908ff12c6692685784c7c9a71,"fix wav2vec scripts (#1494)

Summary:
fixes #2942
+ docs + migration of old models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494

Reviewed By: myleott

Differential Revision: D25404601

Pulled By: alexeib

fbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1638,Peng-Jen Chen,pipibjc@fb.com,2020-12-10 11:57:14-08:00,606b3b8c8d7e15dad66b177cde66a04621349e6c,https://github.com/pytorch/fairseq/commit/606b3b8c8d7e15dad66b177cde66a04621349e6c,"Release WMT20 MT/LM models

Summary: Add README to expose wmt20 model paths to download and torch.hub examples.

Reviewed By: ngoyal2707

Differential Revision: D25456298

fbshipit-source-id: 8c78bffb3f539963cbf61e508a56e421929925f0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1639,Peng-Jen Chen,pipibjc@fb.com,2020-12-11 06:22:57-08:00,43e4db8cd77dab63836a9d6dbb239d27ee2641d7,https://github.com/pytorch/fairseq/commit/43e4db8cd77dab63836a9d6dbb239d27ee2641d7,"Add default dataclass to space_tokenizer and nltk_tokenizer

Summary:
The default dataclass of `space_tokenizer` is `None`, this somehow breaks when we load a model with space tokenizer from `torch.hub` (full message: P154020599):

```
...
hydra.errors.MissingConfigException: Could not load tokenizer/space.
Available options:
        moses
```

Adding default dataclass to `FairseqDataclass` to solve the problem.

Note: another fix might be setting the default dataclass to `FairseqDataclass` here:
https://www.internalfb.com/intern/diffusion/FBS/browsefile/master/fbcode/deeplearning/projects/fairseq-py/fairseq/registry.py?commit=1ae2e67d969ca090b41a6fa33ca9aaf360a26d3f&lines=63

Reviewed By: myleott

Differential Revision: D25466594

fbshipit-source-id: aa4ec23731081e266ce641ea3db179169233842c",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1640,Peng-Jen Chen,pipibjc@fb.com,2020-12-11 08:37:38-08:00,5430df004ff4a5928e0295f3c8ac0b29132bd6a8,https://github.com/pytorch/fairseq/commit/5430df004ff4a5928e0295f3c8ac0b29132bd6a8,"Add WMT20 page to fairseq-py/example

Summary: Add WMT20 page to fairseq-py/example to release WMT20 models

Reviewed By: myleott, ngoyal2707

Differential Revision: D25495578

fbshipit-source-id: 088a457e379b5227cf45a5db1901073499a2e4c1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1641,Myle Ott,myleott@fb.com,2020-12-11 09:55:24-08:00,3a2c0a2558aaf363c8ecc6967c9ae63f23a58502,https://github.com/pytorch/fairseq/commit/3a2c0a2558aaf363c8ecc6967c9ae63f23a58502,"Back out ""Improve performance of distributed_utils.broadcast_object""

Summary: Original commit changeset: 521102fae75a

Reviewed By: ngoyal2707

Differential Revision: D25494613

fbshipit-source-id: 64aead87ee84f4294fd37f2de12689b909e11ff1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1642,louismartin,louismartincs@gmail.com,2020-12-11 10:18:23-08:00,e8b195ac069600203da3e7d60ba29d0975dd0afd,https://github.com/pytorch/fairseq/commit/e8b195ac069600203da3e7d60ba29d0975dd0afd,"Use deepcopy for copying cfg #3011 (#3022)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3011.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3022

Reviewed By: myleott

Differential Revision: D25495116

Pulled By: louismartin

fbshipit-source-id: bcd3bc04b92f083882dfbc9110b14bb2ac7c8ce0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1643,Robert Verkuil,rverkuil@fb.com,2020-12-11 11:44:25-08:00,13dbdf279887012ecf1bf955a114b952c8d00927,https://github.com/pytorch/fairseq/commit/13dbdf279887012ecf1bf955a114b952c8d00927,"Add S3 PathHandler to Fairseq (#1441)

Summary:
## Description
This PathHandler supports saving and loading from Amazon S3.  For internal FB use.  Intended for the use case of infrequent loading / saving (e.g. normal checkpoint reading and writing during training or when running an analysis script.)
The PathHandler assumes the full contents of files to be saved / loaded can be stored temporarily in memory.  Further optimizations could support streaming for handling large files.

## TODO:
~~- [ ]  Switch to pid-based S3 client cache, to avoid multithreading problems.~~
- [x] Complete the test cases in test/file_io.py
- [x] Unit tests complete
- [x] Integration tested with the following command, to check for ability to save, resume from saved checkpoint, and re-download a file if local copy is stale.

```bash
pyscrun fairseq-py/fairseq_cli/train.py \
    /checkpoint/bioseq_nonsecure/rverkuil/aws/data \
    --restore-file checkpoint_last.pt \
    --dataset-impl fasta \
    --task masked_lm --criterion masked_lm \
    --arch roberta --dropout 0.0 --attention-dropout 0.0 \
    --encoder-layers 2 \
    --optimizer adam --lr 0.0001 \
    --max-sentences 24 \
    --log-format simple --log-interval 1 \
    --fp16 \
    --max-epoch 20 \
    --max-update 20 \
    --save-dir s3://fairusersglobal/<PATH>/
```

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1441

Reviewed By: myleott

Differential Revision: D25370388

Pulled By: robert-verkuil

fbshipit-source-id: ffefbd7345f8d8ae72513f1cead94469a17c4459",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1644,Peng-Jen Chen,pipibjc@fb.com,2020-12-11 12:17:28-08:00,fc7a787c01cca883330fc16d22b4693fb8277df8,https://github.com/pytorch/fairseq/commit/fc7a787c01cca883330fc16d22b4693fb8277df8,"Update WMT20 README.md (#3027)

Summary:
Fix the format

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3027

Reviewed By: ngoyal2707

Differential Revision: D25500155

Pulled By: pipibjc

fbshipit-source-id: bf6298b0a4a1942e6e0e8aa632e0af7fd5c516a0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1645,Sam Shleifer,sshleifer@gmail.com,2020-12-11 15:20:10-08:00,d4788a9f7082c9b4a1bbe81c6e2c898e7e16ceb9,https://github.com/pytorch/fairseq/commit/d4788a9f7082c9b4a1bbe81c6e2c898e7e16ceb9,"wandb QOL: infer run name from environment or save_dir (#1500)

Summary:
As documented, in the , the name of a run can be controlled using either `wandb.init(name)` or the environment_variable `WANDB_NAME`.
we set
```python
wandb_run_name=os.environ.get(""WANDB_NAME"", cfg.checkpoint.save_dir)
```
to preserve the environment variable functionality documented [here](https://docs.wandb.com/library/environment-variables), and chose a sane default if this is not specified.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1500

Test Plan:
```bash
WANDB_NAME=dummy2 CUDA_VISIBLE_DEVICES=0 python train.py --task dummy_lm --arch \
transformer_lm_gpt2_small --tokens-per-sample 512 --max-sentences 2 \
 --lr 0.0001 --log-format simple --log-interval 1 --optimizer adam \
 --fp16 --no-save --disable-validation --max-update 10 \
 --restore-file x.pt --wandb-project dummy-lm
```

Reviewed By: myleott

Differential Revision: D25497735

Pulled By: sshleifer

fbshipit-source-id: fcd4e2a3263444e5759fae98641963fd3b9f6914",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1646,Davide Caroselli,davide@modernmt.eu,2020-12-11 18:54:40-08:00,39e722ceabff11db00d9dd66998039236b40ae50,https://github.com/pytorch/fairseq/commit/39e722ceabff11db00d9dd66998039236b40ae50,"Fix #3017: restore support for user modules in zip/jar files (#3018)

Summary:
## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3017

My original implementation of the function utils.import_user_module supported external user modules wrapped in zip/jar files (python natively support modules in zip/jar files).

The new piece of code which checks for file existence breaks this functionality.
This trivial fix can solve the problem!

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3018

Reviewed By: alexeib

Differential Revision: D25485611

Pulled By: myleott

fbshipit-source-id: 21667cd87e9e7a99095f8ad21d7b3bfdb547a993",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1647,Hiroyuki Deguchi,deguchi@ai.cs.ehime-u.ac.jp,2020-12-11 18:59:50-08:00,032a404d389307a0e8f7dd2a0d501c78afa78f39,https://github.com/pytorch/fairseq/commit/032a404d389307a0e8f7dd2a0d501c78afa78f39,"Add ""soft"" argument of ""--print-alignment"" (#2985)

Summary:
If the argument is set to ""soft"", print probability for each source
token, like this:

A-0        0.365083,0.328207,0.306710 0.442428,0.340282,0.217290
0.378712,0.367315,0.253973 0.321335,0.425601,0.253064

Each source token is separated from each other by a comma (,) and each
target token is separated from each other by a space ( ).

This option is based on the Marian NMT's option.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2985

Reviewed By: alexeib

Differential Revision: D25344394

Pulled By: myleott

fbshipit-source-id: 659eb8f7af1ccdafacaaa91ce5ddf5d71cb3e775",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1648,Raphael Scheible,raphael.scheible@uniklinik-freiburg.de,2020-12-11 19:09:41-08:00,f3d5045a71ae463bd3f05254d7c4216801a04bc2,https://github.com/pytorch/fairseq/commit/f3d5045a71ae463bd3f05254d7c4216801a04bc2,"add German RoBERTa model (GottBERT) (#2992)

Summary:
# Before submitting

- There is no related issue for this pull request.
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- We did not see any necessity for tests.

## What does this PR do?
Add German RoBERTa model (GottBERT)

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2992

Reviewed By: alexeib

Differential Revision: D25494927

Pulled By: myleott

fbshipit-source-id: b6790124d7c3c8dc387c141706cd8a527cc950ab",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1649,Myle Ott,myleott@fb.com,2020-12-12 07:18:45-08:00,881e9f8920bc3d9c3b526f6e52405f7059c926b4,https://github.com/pytorch/fairseq/commit/881e9f8920bc3d9c3b526f6e52405f7059c926b4,"Fix bug in FP16 training (#1503)

Summary:
Fix a critical bug in FP16 training (introduced on Dec 4: https://github.com/pytorch/fairseq/commit/ba4f54267af5c3f67f1b76a6e804b6ab593d1d39)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1503

Reviewed By: donhusa, daniellepintz

Differential Revision: D25514055

Pulled By: myleott

fbshipit-source-id: 38ebb1f41f365702ce7706846085c7c7cc24a98e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1650,Colin Clement,colin.clement@gmail.com,2020-12-12 08:01:56-08:00,ac11107ed41cb06a758af850373c239309d1c961,https://github.com/pytorch/fairseq/commit/ac11107ed41cb06a758af850373c239309d1c961,"Azure ML Logging to view training/validation progress in AzureML workspace (#2999)

Summary:
# Before submitting

- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [yes, the CLI flag has a help string] Did you make sure to update the docs?
- [no, but the code was successfully tested in training] Did you write any new necessary tests?

## What does this PR do?
Adds a CLI flag `--azureml-logging` to `fairseq-train` which allows fairseq to log to the default Azure ML context to improve training on Azure Machine Learning services. If `azureml-core` is not installed, it fails with a logging message like the `wandb` logging.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Always!

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2999

Reviewed By: alexeib

Differential Revision: D25494986

Pulled By: myleott

fbshipit-source-id: fadd7569aeb72b5b6f9db0508cbec3a138c332d3",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1651,Myle Ott,myleott@fb.com,2020-12-14 10:45:36-08:00,f49bb2c4d165d7c134e0dadc70c6fcda4ccd5e26,https://github.com/pytorch/fairseq/commit/f49bb2c4d165d7c134e0dadc70c6fcda4ccd5e26,"Improve performance of distributed_utils.broadcast_object

Summary:
Recommit D25291594 (https://github.com/pytorch/fairseq/commit/bb039fa2063dca1b388d6be2f64052b07fb556a2)

Original commit changeset: 64aead87ee84

Reviewed By: arendu

Differential Revision: D25518027

fbshipit-source-id: 5da303f835eb4b598728c7c7ef7c07538ea3b9b4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1652,Dexter Ju,da.ju.fr@gmail.com,2020-12-14 11:34:21-08:00,d740093bac8a6115b68ab5de9a6b63099a07497b,https://github.com/pytorch/fairseq/commit/d740093bac8a6115b68ab5de9a6b63099a07497b,"Porting adaptive span to fairseq (#1428)

Summary:
## What does this PR do?
1. We add an enwiki8 character level LM task sweep for transformer XL, which lands at 1.05 matches the performance (1.06): https://github.com/kimiyoung/transformer-xl/tree/master/pytorch
Eval with
```
 PYTHONPATH=. python fairseq_cli/eval_lm.py /private/home/daju/data/enwik8/eos-data-bin/ --path /checkpoint/daju/2020-11-19/enwiki8.transformer_xl.fp16.transformer_xl.adam.cl0.25.cosine.lr0.00025.s2.ngpu4/checkpoint_best.pt --user-dir examples/truncated_bptt/ --task truncated_bptt_lm --batch-size 8 --tokens-per-sample 80  --model-overrides '{""mem_len"":2100,""clamp_len"":820,""same_length"":True}'
```
2. Impalements adaptive span in fairseq code. It reproduces the enwiki8 result at 1.03 comparing to 1.02 (for the 12 L model) reported in https://github.com/facebookresearch/adaptive-span, which is a consistent improvement over the transformer XL baseline listed above with a smaller model.
You can evaluate the example run with:
```
PYTHONPATH=. python fairseq_cli/eval_lm.py /private/home/daju/data/enwik8/eos-data-bin/ --path  /checkpoint/daju/2020-11-20/enwiki8.adaptivespan.headwise.adaptive_span.adagrad_with_grad_clip.ag_cl0.03.fixed.wu32000.lr0.07.s2.loss5e-07.ngpu4/checkpoint_best.pt --user-dir examples/truncated_bptt/ --task truncated_bptt_lm --batch-size 8 --tokens-per-sample 512 --gen-subset test
```

Paper: https://arxiv.org/abs/1905.07799

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1428

Reviewed By: myleott

Differential Revision: D25495754

Pulled By: dexterju

fbshipit-source-id: 15a875a5f82d506a4964dea934a374132ce39f8b",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1653,Myle Ott,myleott@fb.com,2020-12-15 17:19:53-08:00,5a3e51d21187415a66632f65e92c71de61367a93,https://github.com/pytorch/fairseq/commit/5a3e51d21187415a66632f65e92c71de61367a93,"Infer TPU flag automatically and deprecate prepare_for_tpu (#1514)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1514

Reviewed By: ngoyal2707

Differential Revision: D25568540

Pulled By: myleott

fbshipit-source-id: 93cf2dfd55323bc396b893eb6d658f1c6283a88b",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1654,Myle Ott,myleott@fb.com,2020-12-15 17:46:37-08:00,8d7ee5bf813f1fc0a0685e0cb1ef58fcc63e7855,https://github.com/pytorch/fairseq/commit/8d7ee5bf813f1fc0a0685e0cb1ef58fcc63e7855,"Fix hydra with Python 3.8 (#1511)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1511

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570468

Pulled By: myleott

fbshipit-source-id: 98efc6983479e163e6cf0a7ef33decaa1bc429f1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1655,Myle Ott,myleott@fb.com,2020-12-15 17:46:37-08:00,409032596bd80240f7fbc833b5d37485dee85b0e,https://github.com/pytorch/fairseq/commit/409032596bd80240f7fbc833b5d37485dee85b0e,"Fix loading of very old checkpoints (#1512)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512

See https://github.com/pytorch/fairseq/issues/3032 for context

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570470

Pulled By: myleott

fbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1656,Sam Shleifer,sshleifer@gmail.com,2020-12-16 19:06:45-08:00,c8a0659be5cdc15caa102d5bbf72b872567c4859,https://github.com/pytorch/fairseq/commit/c8a0659be5cdc15caa102d5bbf72b872567c4859,"Stronger --checkpoint-activations test (#1505)

Summary:
- captures and inspects train and valid logs using unittest's `assert_logs_equal`
- asserts that `--checkpoint-activations` does not change `train_loss` or `valid_loss`.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1505

Reviewed By: myleott

Differential Revision: D25544991

Pulled By: sshleifer

fbshipit-source-id: 2762095ab4e7c819a803b3556f5774db8c6b6f39",1,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestActivationCheckpointing(unittest.TestCase):'],"[('Logs', '() as logs:')]",[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],['sys.platform.lower() == )'],[],[],[],[],"['len(baseline_logs) == len(ckpt_logs)', 'baseline_train_stats[]', 'baseline_valid_stats[]']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1657,Myle Ott,myleott@fb.com,2020-12-18 07:39:39-08:00,edc321e767ca4e980463d7af7f3d5eb751f60962,https://github.com/pytorch/fairseq/commit/edc321e767ca4e980463d7af7f3d5eb751f60962,"Support atomic saves for checkpoints (#1520)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520

Test Plan: Imported from OSS

Reviewed By: stephenroller

Differential Revision: D25632782

Pulled By: myleott

fbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestIOPath(unittest.TestCase):'],[],[],[],[],[],[],[],[],['from unittest import mock'],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1658,Xu Song,xusong.vip@gmail.com,2020-12-18 07:40:57-08:00,a041e1ae9cd5d69af993f5da6561223ad407f5da,https://github.com/pytorch/fairseq/commit/a041e1ae9cd5d69af993f5da6561223ad407f5da,"Fix parameter (#3045)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

`src_lengths` is not a required parameter in `TransformerEncoder`.
It is a dummy variable.

Maybe more changes should be done to fix this issue in Class such as `Transformer`, `FairseqEncoderDecoderModel`, `BARTModel` etc.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3045

Reviewed By: ngoyal2707

Differential Revision: D25632992

Pulled By: myleott

fbshipit-source-id: 762d595144b611e1a6c236248d7001049afed0ab",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1659,Yiding Tian,yitian@microsoft.com,2020-12-18 07:41:03-08:00,3a597d11731c7b7949072856aa51dbf4581963b0,https://github.com/pytorch/fairseq/commit/3a597d11731c7b7949072856aa51dbf4581963b0,"Removing an unwanted bracket character in logging message. (#3028)

Summary:
# Before submitting

- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [no need] Did you make sure to update the docs?
- [no need] Did you write any new necessary tests?

## What does this PR do?
Just fixing a small typo of logging one additional bracket before starting training.

## PR review
Anyone in the community is free to review the PR once the tests have passed.

> If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

It's really a small change, no need to discuss.

## Did you have fun?
Small change although, do have fun reading the code.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3028

Reviewed By: ngoyal2707

Differential Revision: D25632978

Pulled By: myleott

fbshipit-source-id: 62c85a9727af523d4082678a12a71b78f3ea84c0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1660,Edresson Casanova,edresson1@gmail.com,2020-12-18 07:41:49-08:00,828960f5dace4787ad81aeadca60043c907adc67,https://github.com/pytorch/fairseq/commit/828960f5dace4787ad81aeadca60043c907adc67,"fix inconsistency in wav2vec documentation (#3039)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3039

Reviewed By: ngoyal2707

Differential Revision: D25632983

Pulled By: myleott

fbshipit-source-id: 32a60da9d41e600d2b047171d548f954196c0560",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1661,Tim Dettmers,tim.dettmers@gmail.com,2020-12-18 07:42:44-08:00,9693504a8a75bafd7bddd6caa47cc5aed6821a2b,https://github.com/pytorch/fairseq/commit/9693504a8a75bafd7bddd6caa47cc5aed6821a2b,"Bugfix: Early exist in creating dictionaries due to unexpected f.tell() behavior (#2956)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
This PR fixes an issue with `f.tell()` when creating dictionaries. Before this bugfix, the dictionary generation had silently nondeterministic behavior which worsens with multiple workers. Please the comment in the commit for more details.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2956

Reviewed By: ngoyal2707

Differential Revision: D25633020

Pulled By: myleott

fbshipit-source-id: 08a4ae4a8d6e03f72484baafe012212a99003ada",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1662,Myle Ott,myleott@fb.com,2020-12-18 11:44:05-08:00,36c63c826d2292c9df56065b5816c02eefc87713,https://github.com/pytorch/fairseq/commit/36c63c826d2292c9df56065b5816c02eefc87713,"Refactor eval_lm to support library usage (#1513)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570467

Pulled By: myleott

fbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f",8,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1663,Wei Ho,weiho@fb.com,2020-12-22 18:00:04-08:00,4af87e3c150191f12e6e023c65ec97e38859b7f9,https://github.com/pytorch/fairseq/commit/4af87e3c150191f12e6e023c65ec97e38859b7f9,"Fix validate_and_save logic so validation is done for training terminated by stop_time_hours

Summary:
Also improves stopping criteria logging

The final validation step wasn't being done when training was terminated by stop_time_hours. This was more of an issue for toy test cases (ex: wanting training to terminate in just a few min in order to test subsequent pipeline logic) since that could result in no validation loss ever being produced - which can break the rest of our pipeline

Reviewed By: chtran

Differential Revision: D25630252

fbshipit-source-id: 32cadbf977b0c9775830c1e8eb7f26bac12fe9ae",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1664,Wei Ho,weiho@fb.com,2020-12-23 11:15:53-08:00,16a5fca05d3bcb5c1116ca987fa41c86599dfdf3,https://github.com/pytorch/fairseq/commit/16a5fca05d3bcb5c1116ca987fa41c86599dfdf3,"fairseq checkpoint improvements

Reviewed By: myleott

Differential Revision: D25677238

fbshipit-source-id: b43075034c953491211f19a5464148de4758df83",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1665,Myle Ott,myleott@fb.com,2020-12-23 16:41:59-08:00,996ae207075db47f4061519c7dc39a86ab6d9535,https://github.com/pytorch/fairseq/commit/996ae207075db47f4061519c7dc39a86ab6d9535,"Add --heartbeat-timeout (#1527)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1527

Test Plan:
Train roberta large with FP32 so step time is moderate:

No timeout:
```
CUDA_VISIBLE_DEVICES=0,1 python train.py --task dummy_masked_lm --arch roberta_large --criterion masked_lm --max-sentences 8 --optimizer adam --lr 0.0001 --log-format simple --log-interval 1 --update-freq 16
2020-12-23 13:45:07 | INFO | train_inner | epoch 001:      1 / 391 loss=15.957, ppl=63597.8, wps=0, ups=0, wpb=131072, bsz=256, num_updates=1, lr=0.0001, gnorm=12.446, train_wall=19, wall=22
2020-12-23 13:45:07 | INFO | root | Reducer buckets have been rebuilt in this iteration.
2020-12-23 13:45:20 | INFO | train_inner | epoch 001:      2 / 391 loss=14.75, ppl=27553.9, wps=10635.6, ups=0.08, wpb=131072, bsz=256, num_updates=2, lr=0.0001, gnorm=8.173, train_wall=12, wall=35
2020-12-23 13:45:32 | INFO | train_inner | epoch 001:      3 / 391 loss=13.894, ppl=15223, wps=10653, ups=0.08, wpb=131072, bsz=256, num_updates=3, lr=0.0001, gnorm=5.141, train_wall=12, wall=47
```

Timeout of 1 second (fails):
```
CUDA_VISIBLE_DEVICES=0,1 python train.py --task dummy_masked_lm --arch roberta_large --criterion masked_lm --max-sentences 8 --optimizer adam --lr 0.0001 --log-format simple --log-interval 1 --update-freq 16 --heartbeat-timeout 1
2020-12-23 13:50:11 | ERROR | fairseq.models.distributed_fairseq_model | Killing job for not making progress in 1 seconds. Set --heartbeat-timeout=-1 to disable this timeout.
2020-12-23 13:50:11 | ERROR | fairseq.models.distributed_fairseq_model | Killing job for not making progress in 1 seconds. Set --heartbeat-timeout=-1 to disable this timeout.
```

Timeout of 3 seconds (doesn't fail):
```
CUDA_VISIBLE_DEVICES=0,1 python train.py --task dummy_masked_lm --arch roberta_large --criterion masked_lm --max-sentences 8 --optimizer adam --lr 0.0001 --log-format simple --log-interval 1 --update-freq 16 --heartbeat-timeout 3
2020-12-23 13:55:25 | INFO | train_inner | epoch 001:      1 / 391 loss=15.957, ppl=63597.8, wps=0, ups=0, wpb=131072, bsz=256, num_updates=1, lr=0.0001, gnorm=12.446, train_wall=19, wall=21
2020-12-23 13:55:25 | INFO | root | Reducer buckets have been rebuilt in this iteration.
2020-12-23 13:55:37 | INFO | train_inner | epoch 001:      2 / 391 loss=14.75, ppl=27553.9, wps=10682, ups=0.08, wpb=131072, bsz=256, num_updates=2, lr=0.0001, gnorm=8.173, train_wall=12, wall=33
2020-12-23 13:55:50 | INFO | train_inner | epoch 001:      3 / 391 loss=13.894, ppl=15223, wps=10654.5, ups=0.08, wpb=131072, bsz=256, num_updates=3, lr=0.0001, gnorm=5.141, train_wall=12, wall=46
```

Reviewed By: joshim5, ngoyal2707

Differential Revision: D25696904

Pulled By: myleott

fbshipit-source-id: b2dbb7ddd8ce3ea83491f479314a0b3caa09b4b7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1666,Myle Ott,myleott@fb.com,2020-12-23 18:34:59-08:00,b8ea8a9b72c82192da07e3377adf4ebbde16716d,https://github.com/pytorch/fairseq/commit/b8ea8a9b72c82192da07e3377adf4ebbde16716d,"Fix --context-window and add test (#1526)

Summary:
This was broken in the recent refactoring: https://github.com/pytorch/fairseq/commit/36c63c826d2292c9df56065b5816c02eefc87713

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1526

Reviewed By: sshleifer

Differential Revision: D25697706

Pulled By: myleott

fbshipit-source-id: 4d9a735c0071a0d71a4ae46e1c3fc3aba572117b",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestLMContextWindow(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['len(dictionary) == 14  # 4 extra special symbols', 'dictionary.pad() == 1', 'batch[][0].tolist() == [4, 5, 6, 7, 1, 1]', 'batch[][0].tolist() == [4, 5, 6, 7, 1, 1]', 'batch[][0].tolist() == [6, 7, 8, 9, 10, 11]', 'batch[][0].tolist() == [1, 1, 8, 9, 10, 11]', 'batch[][0].tolist() == [10, 11, 12, 13]', 'batch[][0].tolist() == [1, 1, 12, 13]']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1667,Sam Shleifer,sshleifer@gmail.com,2020-12-28 15:46:51-08:00,982ec329769cd189ba3735eecc9687d072bcdb72,https://github.com/pytorch/fairseq/commit/982ec329769cd189ba3735eecc9687d072bcdb72,"logger: format big numbers with commas for readability (#1525)

Summary:
Before:
```
2020-12-23 11:46:16 | INFO | fairseq_cli.eval_lm | num. model params: 353781760
2020-12-23 11:46:21 | INFO | fairseq.data.data_utils | loaded 89663978 examples from: /private/home/sshleifer/data-bin/new_hybrid_data/train

```
After:
```
2020-12-23 11:46:16 | INFO | fairseq_cli.eval_lm | num. model params: 353,781,760
2020-12-23 11:46:21 | INFO | fairseq.data.data_utils | loaded 89,663,978 examples from: /private/home/sshleifer/data-bin/new_hybrid_data/train
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1525

Test Plan:
Run `fairseq-eval-lm` or `fairseq-train` and look at logs.
For example,
```
export dd2=/private/home/sshleifer/data-bin/new_hybrid_data
export m=/private/home/myleott/models/public_models/LM/roberta_lm.me_fp16.bm_none.tps1024.transformer_lm_gpt2_small.share.adam.b2_0.98.eps1e-08.cl0.0.lr0.003.wu3000.dr0.1.atdr0.1.wd0.01.ms2.uf4.mu100000.s1.ngpu64/model.pt
fairseq-eval-lm  $dd2 \
    --path $m \
    --sample-break-mode complete --gen-subset train \
        --tokens-per-sample 3072 --max-tokens 3072 --context-window 2560 --softmax-batch 1024 --fp16
```

Reviewed By: myleott

Differential Revision: D25693004

Pulled By: sshleifer

fbshipit-source-id: bfeb93fc6607cca2cb7a6e820f51e174d02d1f62",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1668,Ruslan Mavlyutov,mavlyutov@fb.com,2020-12-28 21:03:59-08:00,4e3895be1ccb59e36de85441cd049294cbad2d15,https://github.com/pytorch/fairseq/commit/4e3895be1ccb59e36de85441cd049294cbad2d15,"batch_by_size refactoring: 100x speedup and optimization of memory footprint

Summary: Refactoring batch_by_size. You may be required to rebuild Cython components with: `python setup.py build_ext --inplace`.

Reviewed By: myleott

Differential Revision: D25705733

fbshipit-source-id: a263505276e3d820a9e44b93354ee5ace70d7fc5",6,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestBatchBySize(unittest.TestCase):'],"[('Equal', '(len(validation), len(results), error_msg)'), ('True', '(np.array_equal(first, second), error_msg)')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1669,Samuel Marks,807580+SamuelMarks@users.noreply.github.com,2020-12-29 15:53:03-08:00,e2e80c6f2dca01dd8c04b3e5b0b356abf4b429cf,https://github.com/pytorch/fairseq/commit/e2e80c6f2dca01dd8c04b3e5b0b356abf4b429cf,"Rename ""Arguments:"" to ""Args:"" (#3060)

Summary:
I've written custom parsers and emitters for everything from docstrings to classes and functions. However, I recently came across an issue when I was parsing/generating from the TensorFlow—and now PyTorch—codebases: inconsistent use of `Args:` and `Arguments:` in its docstrings. It is easy enough to extend my parsers to support both variants, however it looks like `Arguments:` is wrong anyway, as per:

  - https://google.github.io/styleguide/pyguide.html#doc-function-args @ [`ddccc0f`](https://github.com/google/styleguide/blob/ddccc0f/pyguide.md)

  - https://chromium.googlesource.com/chromiumos/docs/+/master/styleguide/python.md#describing-arguments-in-docstrings @ [`9fc0fc0`](https://chromium.googlesource.com/chromiumos/docs/+/9fc0fc0/styleguide/python.md)

  - https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html @ [`c0ae8e3`](https://github.com/sphinx-contrib/napoleon/blob/c0ae8e3/docs/source/example_google.rst)

Therefore, only `Args:` is valid. This PR replaces them throughout the codebase.

PS: For related PRs, see pytorch/pytorch/pull/49736

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3060

Reviewed By: ngoyal2707

Differential Revision: D25692815

Pulled By: myleott

fbshipit-source-id: 461543ad3e2acd1fc475da799495841be73250bd",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1670,Xu Song,xusong.vip@gmail.com,2020-12-29 15:53:06-08:00,48a607527a8c2435c63795cca9a05348ef6e1f9d,https://github.com/pytorch/fairseq/commit/48a607527a8c2435c63795cca9a05348ef6e1f9d,"Reorganize self.emb_layer_norm (#3057)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Reorganize `self.emb_layer_norm` in order to keep right order while `print(model)`

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3057

Reviewed By: ngoyal2707

Differential Revision: D25692819

Pulled By: myleott

fbshipit-source-id: d371955152e7fcb9e356351311f194a2418ca4b5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1671,Sam Shleifer,sshleifer@gmail.com,2020-12-30 12:57:02-08:00,bff7f85206f6f64b9455035893d44d66b98e33b0,https://github.com/pytorch/fairseq/commit/bff7f85206f6f64b9455035893d44d66b98e33b0,"fastseq ngram blocking (#1509)

Summary:
Command:
```bash
fairseq-generate \
    ~myleott/data/data-bin/wmt16_en_de_bpe32k/ \
    --path /checkpoint/myleott/s3/models/wmt16.en-de.joined-dict.transformer/model.pt \
    --beam 4 --remove-bpe --lenpen 0.6 --batch-size 256 --no-repeat-ngram-size 3 \
    --gen-subset test --fp16
```

master/devfair: 297.8s (10.08 sentences/s, 286.47 tokens/s)
branch/devfair: 31.9s (94.27 sentences/s, 2678.66 tokens/s)

master/v100: 227.4s (13.21 sentences/s, 375.24 tokens/s)
branch/v100: 13.1s (228.68 sentences/s, 6497.99 tokens/s)
(all BLEU4=29.17)

### ToDo:
- tests

### Future Work
- test other fastseq proposed improvements.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1509

Reviewed By: myleott

Differential Revision: D25587857

Pulled By: sshleifer

fbshipit-source-id: d42af5c50e3f94c90e878f92da5ce5ef3fc8b988",6,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],0,4,0,1,0,0,3,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('TensorEqual', '(cuda_ext_result, desired_result)'), ('TensorEqual', '(baseline_result, desired_result)'), ('TensorEqual', '(base_result, jit_result)'), ('TensorEqual', '(cuda_ext_result, baseline_result)')]",[],"[('Class', '(cls)')]",[],[],"['torch.__version__ < , JIT_MSG)', 'torch.cuda.is_available(), )', 'torch.__version__ < , JIT_MSG)']",[],[],[],[],[],[],[],[],[],[],"['', '', '']",[],[],[],[],"['blocker.use_extension, ']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1672,Wei Ho,weiho@fb.com,2020-12-30 20:15:58-08:00,01fcec5fc3dcc695c59cf3fdf7f178c174edcf0d,https://github.com/pytorch/fairseq/commit/01fcec5fc3dcc695c59cf3fdf7f178c174edcf0d,"Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host

Reviewed By: myleott

Differential Revision: D25719057

fbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1673,Myle Ott,myleott@fb.com,2021-01-02 10:21:42-08:00,336942734c85791a90baa373c212d27e7c722662,https://github.com/pytorch/fairseq/commit/336942734c85791a90baa373c212d27e7c722662,"Better support for WandB (#1530)

Summary:
Logs full config

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1530

Reviewed By: sshleifer

Differential Revision: D25723894

Pulled By: myleott

fbshipit-source-id: bb4b168c774bef498e336bbb3ba92eda4b08df3b",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1674,Myle Ott,myleott@fb.com,2021-01-05 11:27:00-08:00,7e5e45b483ec5896830231ebbd3ed26472bcff47,https://github.com/pytorch/fairseq/commit/7e5e45b483ec5896830231ebbd3ed26472bcff47,"Add omegaconf dependency to hubconf.py (fixes #3093) (#3102)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3102

Reviewed By: alexeib

Differential Revision: D25784689

Pulled By: myleott

fbshipit-source-id: 66b0755e7f8abca2a522266bd58881b789bae581",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1675,Myle Ott,myleott@fb.com,2021-01-05 12:13:07-08:00,540fb42c523e98f066989c3e3b61a18caaca24f5,https://github.com/pytorch/fairseq/commit/540fb42c523e98f066989c3e3b61a18caaca24f5,"Move dep checks before fairseq imports in hubconf.py (fixes #3093) (#3104)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3104

Reviewed By: alexeib

Differential Revision: D25786013

Pulled By: myleott

fbshipit-source-id: 894b104f275573ce824d7f2318d043516f0e0c5c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1676,Alejandro Gaston Alvarez Franceschi,alejandro.alvarez@projectx.ai,2021-01-05 14:33:29-08:00,70b3f529659099b1bfca4099b11fc6d5577f7b0e,https://github.com/pytorch/fairseq/commit/70b3f529659099b1bfca4099b11fc6d5577f7b0e,"Migrate wav2letter to flashlight (#2876)

Summary:
With this PR we start using flashlight bindings instead of wav2letter.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2876

Reviewed By: myleott

Differential Revision: D25785525

Pulled By: alexeib

fbshipit-source-id: 245b3cebffedfd7db26c002ae3d26a1fe66c7156",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1677,alexeib,alexei.b@gmail.com,2021-01-05 17:41:46-08:00,53b474f8ac071da5aad94d255aa698278a492923,https://github.com/pytorch/fairseq/commit/53b474f8ac071da5aad94d255aa698278a492923,"minor changes/fixes (#1536)

Summary:
- some minor guardrails
- ability to suppress crashes (useful for sweepers)
- hydra_train main method returns the best validation value (useful for sweepers)
- add more typing checks to work with python 3.9

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1536

Reviewed By: myleott

Differential Revision: D25768210

Pulled By: alexeib

fbshipit-source-id: 3df421efb261eb61331a9af1da11b8ef34bfd8f9",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1678,alexeib,alexei.b@gmail.com,2021-01-05 17:41:46-08:00,4daa41bedcc8e3ac7fd35dd1ec27088ecc71ebec,https://github.com/pytorch/fairseq/commit/4daa41bedcc8e3ac7fd35dd1ec27088ecc71ebec,"migrate label smoothed cross entropy (#1537)

Summary:
migrate label smoothed cross entropy to hydra

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1537

Reviewed By: myleott

Differential Revision: D25768702

Pulled By: alexeib

fbshipit-source-id: a90fa40802ae67ad81a8f5b0735d5316d41fea2d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1679,Arthur Guo,arthurguo@fb.com,2021-01-05 19:08:27-08:00,cb7398299b4a8c51eb195fc0c054068470e8a1c9,https://github.com/pytorch/fairseq/commit/cb7398299b4a8c51eb195fc0c054068470e8a1c9,"Add typing and JIT support for TransfomerDecoder and dependencies

Reviewed By: zhengwy888

Differential Revision: D25451106

fbshipit-source-id: f9aa7f1ca6120f00c4938d4d72219471035211ca",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1680,Changhan Wang,changhan@fb.com,2021-01-05 20:54:21-08:00,89a4d2bc70fd680c4768803d20707ef65df89b0f,https://github.com/pytorch/fairseq/commit/89a4d2bc70fd680c4768803d20707ef65df89b0f,"S2T bug fix for issue #3095

Summary:
S2T bug fix for issue [#3095](https://github.com/pytorch/fairseq/issues/3095):
- Revert the dropped S2T de-tokenization in `fairseq-generate`
- Update S2T Transformer encoder output to dict type (following the updates on the text Transformer)

Reviewed By: jmp84

Differential Revision: D25788341

fbshipit-source-id: f226fb9d5e001bbc7dd245293819e0a36d5a88e7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1681,Changhan Wang,changhan@fb.com,2021-01-05 22:08:51-08:00,d1d487395e8206c39640f07f3be5b2bce33edee6,https://github.com/pytorch/fairseq/commit/d1d487395e8206c39640f07f3be5b2bce33edee6,"bug fix for scorer config

Summary:
bug fix for scorer config
- additional scorer arguments (e.g. WER punctuation removal) from cli are not passed into `build_scorer` properly

Reviewed By: jmp84

Differential Revision: D25797236

fbshipit-source-id: 909e272931ca0fd1cad2d7d8a2ca7e79bd7dcd43",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1682,Rui Hou,rayhou@fb.com,2021-01-06 21:41:58-08:00,2e649232d4455826f7110f7c321b1d5e193b891e,https://github.com/pytorch/fairseq/commit/2e649232d4455826f7110f7c321b1d5e193b891e,"Make StreamingEpochBatchIterator work with batch size > 1

Summary: Previously, StreamingEpochBatchIterator only support batch_size = 1 for each GPU. This diff makes it possible to collate samples into a mini-batch such that batch_size can be greater than 1.

Reviewed By: spencerp

Differential Revision: D25295572

fbshipit-source-id: 398a4247701aa1ec71c1d16bba87430decf61ee6",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1683,Changhan Wang,changhan@fb.com,2021-01-08 15:25:59-08:00,a9f5741f58c05c581686b73465d7e3f9df5528f3,https://github.com/pytorch/fairseq/commit/a9f5741f58c05c581686b73465d7e3f9df5528f3,"update S2T examples and small fixes for S2T

Summary:
- Update S2T examples: documentation (rendered version: https://github.com/fairinternal/fairseq-py/tree/v2_fairseq_s2t/examples/speech_to_text), bug fixes and pre-trained models
- Revert `--share-decoder-input-output-embed`'s default value to `False` (for s2t_transformer)

Reviewed By: yuntang

Differential Revision: D25821616

fbshipit-source-id: 3dba2eb5566bff39305d0056daf1b9f5adf1a926",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1684,Myle Ott,myleott@fb.com,2021-01-11 12:30:55-08:00,f32de63e69aceb966b84c7c515a016ec96439125,https://github.com/pytorch/fairseq/commit/f32de63e69aceb966b84c7c515a016ec96439125,"Fix IWSLT'14 link (fixes #2984) (#3113)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3113

Reviewed By: pritamdamania87

Differential Revision: D25836423

Pulled By: myleott

fbshipit-source-id: 0fe9cafcfd0f3edab2db1025d2fcc8dbb8af570a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1685,Alex Xiao,axiao@fb.com,2021-01-11 14:01:45-08:00,60d2da7055ad696ef037c98dacc931f79d4ce117,https://github.com/pytorch/fairseq/commit/60d2da7055ad696ef037c98dacc931f79d4ce117,"cast scale window to int for memory efficient fp16

Summary:
In some of my runs I found loss scale to never increase

{F358780650}

This was because scale window when using 48 GPUs by default will not be a whole integer. Lets cast it like the default fp16 optimizer.

Reviewed By: myleott

Differential Revision: D25872832

fbshipit-source-id: 7ab5c01c555dca07bda72cae3633bb4b17709a77",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1686,Yuqing Tang,yuqtang@fb.com,2021-01-12 11:26:11-08:00,6f6f704d10fc00b89fb4a01e0a6857624132573e,https://github.com/pytorch/fairseq/commit/6f6f704d10fc00b89fb4a01e0a6857624132573e,"added data scripts and model download links (#1554)

Summary:
# Before submitting

## What does this PR do?
added data scripts and model download links for mBART 50

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1554

Reviewed By: pipibjc

Differential Revision: D25871769

Pulled By: tangyuq

fbshipit-source-id: 2c1b2f8c70e9cc4a083c87f76758fc472d42eaf0",27,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1687,Yuqing Tang,yuqtang@fb.com,2021-01-12 21:37:26-08:00,8c7793b9d9ad272a4bee080839357539743a99d4,https://github.com/pytorch/fairseq/commit/8c7793b9d9ad272a4bee080839357539743a99d4,"Enable translation_multi_simple_epoch to load only two dictionaries for source and target only

Summary: In the default settings, the translation_multi_simple_epoch task load a dictionary per language which can result in huge amount of memory consumption if all languages share the same dictionary.

Reviewed By: shruti-bh

Differential Revision: D25265741

fbshipit-source-id: c5bc3664efd800b120f015b2525c9fba2b1be3c5",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1688,Frank Seide,seide@fb.com,2021-01-13 00:02:05-08:00,4a6f89d373dafc50b416092b41c070c304b31698,https://github.com/pytorch/fairseq/commit/4a6f89d373dafc50b416092b41c070c304b31698,"Make Fairseq trainer multiply_grads resilient to sample_size 0

Summary: The Fairseq `Trainer` class does not always guard its `multiply_grads` step to the special case of `sample_size` of 0, which may happen in edge cases. This diff now guards in all conditions.

Reviewed By: myleott

Differential Revision: D25814612

fbshipit-source-id: 4974ee0148ab2a86f60980f3bf248878b2ebbb36",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1689,alexeib,alexei.b@gmail.com,2021-01-16 19:08:50-08:00,cb84694c195afced474d17318b5e746d1a9d20a3,https://github.com/pytorch/fairseq/commit/cb84694c195afced474d17318b5e746d1a9d20a3,"fixes regression in lm decoding with flashlight (#1557)

Summary:
before:
```
PYTHONPATH=. python examples/speech_recognition/infer.py /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw --task audio_pretraining --nbest 1 --path /private/home/abaevski/models/wav2vec2/960h_scratch.pt --gen-subset test_clean --w2l-decoder kenlm --lm-model /checkpoint/abaevski/data/speech/libri/4-gram.bin --lexicon /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw/lexicon_ltr.lst --lm-weight 2.601664188829183 --word-score -1.4825337752451184 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --remove-bpe letter
INFO:__main__:Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=True, suppress_crashes=False, criterion='ctc', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='audio_pretraining', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4000000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000000, batch_size_valid=None, curriculum=0, gen_subset='test_clean', num_shards=1, shard_id=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', path='/private/home/abaevski/models/wav2vec2/960h_scratch.pt', post_process='letter', quiet=False, model_overrides='{}', results_path=None, beam=5, nbest=1, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=2.601664188829183, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, kspmodel=None, wfstlm=None, rnnt_decoding_type='greedy', rnnt_len_penalty=-0.5, w2l_decoder='kenlm', lexicon='/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw/lexicon_ltr.lst', unit_lm=False, kenlm_model='/checkpoint/abaevski/data/speech/libri/4-gram.bin', beam_threshold=25.0, beam_size_token=100, word_score=-1.4825337752451184, unk_weight=-inf, sil_weight=0.0, dump_emissions=None, dump_features=None, load_emissions=None, data='/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw', labels='ltr', sample_rate=16000, normalize=False, enable_padding=False, max_sample_size=None, min_sample_size=None, eval_wer=False, eval_wer_tokenizer=None, eval_wer_post_process='letter', autoregressive=False, zero_infinity=False, wer_kenlm_model=None, wer_lexicon=None, wer_lm_weight=2.0, wer_word_score=-1.0, wer_args=None, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3)
INFO:__main__:| decoding with criterion ctc
INFO:__main__:| loading model(s) from /private/home/abaevski/models/wav2vec2/960h_scratch.pt
INFO:fairseq.data.audio.raw_audio_dataset:loaded 2620, skipped 0 samples
INFO:__main__:| /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw test_clean 2620 examples
INFO:__main__:WER: 10.415398660986002
INFO:__main__:| Processed 2620 sentences (291252 tokens) in 130.4s (20.09sentences/s, 2233.70 tokens/s)
INFO:__main__:| Generate test_clean with beam=5
```

after:
```
PYTHONPATH=. python examples/speech_recognition/infer.py /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw --task audio_pretraining --nbest 1 --path /private/home/abaevski/models/wav2vec2/960h_scratch.pt --gen-subset test_clean --w2l-decoder kenlm --lm-model /checkpoint/abaevski/data/speech/libri/4-gram.bin --lexicon /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw/lexicon_ltr.lst --lm-weight 2.601664188829183 --word-score -1.4825337752451184 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 5000000 --remove-bpe letter
INFO:__main__:Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=True, suppress_crashes=False, criterion='ctc', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='audio_pretraining', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=5000000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=5000000, batch_size_valid=None, curriculum=0, gen_subset='test_clean', num_shards=1, shard_id=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', path='/private/home/abaevski/models/wav2vec2/960h_scratch.pt', post_process='letter', quiet=False, model_overrides='{}', results_path=None, beam=5, nbest=1, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=2.601664188829183, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, kspmodel=None, wfstlm=None, rnnt_decoding_type='greedy', rnnt_len_penalty=-0.5, w2l_decoder='kenlm', lexicon='/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw/lexicon_ltr.lst', unit_lm=False, kenlm_model='/checkpoint/abaevski/data/speech/libri/4-gram.bin', beam_threshold=25.0, beam_size_token=100, word_score=-1.4825337752451184, unk_weight=-inf, sil_weight=0.0, dump_emissions=None, dump_features=None, load_emissions=None, data='/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw', labels='ltr', sample_rate=16000, normalize=False, enable_padding=False, max_sample_size=None, min_sample_size=None, eval_wer=False, eval_wer_tokenizer=None, eval_wer_post_process='letter', autoregressive=False, zero_infinity=False, wer_kenlm_model=None, wer_lexicon=None, wer_lm_weight=2.0, wer_word_score=-1.0, wer_args=None, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3)
INFO:__main__:| decoding with criterion ctc
INFO:__main__:| loading model(s) from /private/home/abaevski/models/wav2vec2/960h_scratch.pt
INFO:fairseq.data.audio.raw_audio_dataset:loaded 2620, skipped 0 samples
INFO:__main__:| /checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw test_clean 2620 examples
INFO:__main__:WER: 2.991859403530128
INFO:__main__:| Processed 2620 sentences (288370 tokens) in 129.8s (20.18sentences/s, 2220.83 tokens/s)
INFO:__main__:| Generate test_clean with beam=5
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1557

Reviewed By: wnhsu

Differential Revision: D25935711

Pulled By: alexeib

fbshipit-source-id: 36c1c9b9ba32a60b2c04275036514646d2fb33f5",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1690,alexeib,alexei.b@gmail.com,2021-01-17 10:01:00-08:00,d927d69beefd695738eff0f5c9d9b0d6dc6abcb4,https://github.com/pytorch/fairseq/commit/d927d69beefd695738eff0f5c9d9b0d6dc6abcb4,"allows overwriting nested properties with model-overrrides (#1558)

Summary:
without this, it is not possible to use --model-overrides to override properties nested within a config

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1558

Reviewed By: myleott

Differential Revision: D25935778

Pulled By: alexeib

fbshipit-source-id: 1466f04b8e67842b91299ec88f1370ca0200c6a0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1691,Lior Deutsch,sliorde@gmail.com,2021-01-17 10:03:32-08:00,9f5eda48edfad4cb33610f272cb503dedf60ab67,https://github.com/pytorch/fairseq/commit/9f5eda48edfad4cb33610f272cb503dedf60ab67,"fixed dynamic convolution wrapper function unused arguments (#3136)

Summary:
The bug that this pull request addresses was discussed [in this GitHub issue](https://github.com/pytorch/fairseq/issues/3085#issue-777177450), and myleott has [asked for the pull request](https://github.com/pytorch/fairseq/issues/3085#issuecomment-754854074).

As you can see from the diff, the pull request is very simple. However, I did not run any tests (I don't have a suitable environment, I think).

Also: I did not add any tests, I did not change any documentation, I did not check linting.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3136

Reviewed By: myleott

Differential Revision: D25935770

Pulled By: alexeib

fbshipit-source-id: b338e7cfb409fd14dac653121256276550f53b21",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1692,Changhan Wang,changhan@fb.com,2021-01-17 23:25:04-08:00,1164a7fc432a188d401895018eaa85175fb06f9d,https://github.com/pytorch/fairseq/commit/1164a7fc432a188d401895018eaa85175fb06f9d,"Fix time warping for SpecAugment

Summary:
Fix time warping for SpecAugment
Github issue: https://github.com/pytorch/fairseq/issues/3141

Reviewed By: jmp84

Differential Revision: D25941870

fbshipit-source-id: 97f9c67a49212556156b33aee0056a86ec990db4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1693,alexeib,alexei.b@gmail.com,2021-01-19 12:34:09-08:00,ecf0b60e124e1e795e30004ced00883bf8ba5192,https://github.com/pytorch/fairseq/commit/ecf0b60e124e1e795e30004ced00883bf8ba5192,"add defaults to configs (#1564)

Summary:
previously, when using the hydra_train entry point, the config object that got created would only contain things explicitly specified in config files/command line. normally this is not a problem as we load defaults when creating any object for a particular config, but in fact this config was getting stored in checkpoints. the checkpoints would then have incomplete config that would be incorrect if defaults got changed in the code.

this PR adds defaults into configs for all config objects that hydra doesn't know about

before:

```
{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'dev_other', 'validate_interval': 50, 'validate_interval_updates': 0, 'validate_after_updates': 10000, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [4], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 50, 'save_interval_updates': 10000, 'keep_interval_updates': 1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/private/home/abaevski/models/wav2vec2/wav2vec_small.pt', 'apply_mask': True, 'mask_prob': 0.65, 'mask_channel_prob': 0.5, 'mask_channel_length': 64, 'layerdrop': 0.05, 'activation_dropout': 0.1, 'feature_grad_mult': 0.0, 'freeze_finetune_updates': 10000}, 'task': {'_name': 'audio_pretraining', 'data': '/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw', 'normalize': False, 'labels': 'ltr'}, 'criterion': {'_name': 'ctc', 'zero_infinity': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08}, 'lr_scheduler': {'_name': 'tri_stage', 'phase_ratio': [0.1, 0.4, 0.5], 'final_lr_scale': 0.05}, 'scoring': None, 'bpe': None, 'tokenizer': None}
```

after:
```
{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16054', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 2}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'dev_other', 'validate_interval': 50, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [4], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 50, 'save_interval_updates': 20, 'keep_interval_updates': 1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/private/home/abaevski/models/wav2vec2/wav2vec_small.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.05, 'normalize': False, 'data': '/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw', 'w2v_args': None}, 'task': {'_name': 'audio_pretraining', 'data': '/checkpoint/abdo/old_checkpoint02/datasets/librispeech/10h/raw', 'labels': 'ltr', 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'autoregressive': False}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 20000, 'lr': [5e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None}
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1564

Reviewed By: myleott

Differential Revision: D25938221

Pulled By: alexeib

fbshipit-source-id: e088667accf974ad6d9898a63f7c33722837fcfb",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1694,Wei-Ning Hsu,wnhsu@csail.mit.edu,2021-01-19 19:10:05-08:00,1e21f7e462e53a3bf2f881632f15548ddaa43464,https://github.com/pytorch/fairseq/commit/1e21f7e462e53a3bf2f881632f15548ddaa43464,"track loaded lines in raw_audio_dataset and load corresponding labels in audio_pretraining (#1566)

Summary:
Bug: `AudioPretrainingTask` is not aware of what samples have been skipped by `FileAudioDataset`, and hence would load labels of utterances that were skipped, causing `AddTargetDataset` to misalign utterances with labels. This PR tracks line line indices loaded by the `FileAudioDataset` to filter labels correspondingly in `AudioPretrainingTask`

Before:
```
INFO:__main__:| decoding with criterion ctc
INFO:__main__:| loading model(s) from ...
INFO:fairseq.data.audio.raw_audio_dataset:loaded 284, skipped 223 samples
INFO:__main__:| /private/home/wnhsu/wav2vec2_robust/data/joint_swbd ted_dev 284 examples
INFO:__main__:WER: 152.15605749486653
INFO:__main__:| Processed 284 sentences (72405 tokens) in 20.5s (13.88sentences/s, 3538.01 tokens/s)
```

After
```
INFO:__main__:| decoding with criterion ctc
INFO:__main__:| loading model(s) from ...
INFO:fairseq.data.audio.raw_audio_dataset:loaded 284, skipped 223 samples
INFO:__main__:WER: 9.904153354632587
INFO:__main__:| Processed 284 sentences (72405 tokens) in 20.7s (13.70sentences/s, 3492.20 tokens/s)
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1566

Reviewed By: alexeib

Differential Revision: D25963317

Pulled By: wnhsu

fbshipit-source-id: c9748f5dad1ff787642ba0bc28698c4ecfbcd221",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1695,Myle Ott,myleott@fb.com,2021-01-20 05:49:22-08:00,bf54551cafa13678c0254d2c20354cc026cc0bac,https://github.com/pytorch/fairseq/commit/bf54551cafa13678c0254d2c20354cc026cc0bac,"Fix param sharing in Linformer (#1561)

Summary:
Parameter sharing (both `--untie-weights-roberta` and `--shared-layer-kv-compressed`) was broken by one of my earlier refactors (D22411012 (https://github.com/pytorch/fairseq/commit/d73e543e3853bb813d8f7955a06ce19359810707)). This fixes it.

Note: it was correct in the original version of the code for the paper.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1561

Test Plan:
- confirmed that training gives identical losses as before when not using any param sharing (including `--untie-weights-roberta`):
```
CUDA_VISIBLE_DEVICES=0 python train.py --task dummy_masked_lm --arch linformer_roberta_base --untie-weights-roberta --user-dir examples/linformer/linformer_src/ --criterion masked_lm --batch-size 8 --optimizer adam --lr 0.0001 --log-format json --log-interval 1 --max-update 5 --disable-validation --no-save

before:
2021-01-19 06:37:21 | INFO | fairseq_cli.train | num. model params: 164,465,744 (num. trained: 164,465,744)
(...)
2021-01-19 06:41:56 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""15.893"", ""ppl"": ""60870.7"", ""wps"": ""0"", ""ups"": ""0"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""1"", ""lr"": ""0.0001"", ""gnorm"": ""7.716"", ""train_wall"": ""1"", ""wall"": ""1""}
2021-01-19 06:41:56 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""13.176"", ""ppl"": ""9252.9"", ""wps"": ""11813.8"", ""ups"": ""2.88"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""2"", ""lr"": ""0.0001"", ""gnorm"": ""6.988"", ""train_wall"": ""0"", ""wall"": ""1""}
2021-01-19 06:41:57 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""11.049"", ""ppl"": ""2119.22"", ""wps"": ""12002.2"", ""ups"": ""2.93"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""3"", ""lr"": ""0.0001"", ""gnorm"": ""8.008"", ""train_wall"": ""0"", ""wall"": ""1""}
2021-01-19 06:41:57 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""9.044"", ""ppl"": ""527.7"", ""wps"": ""11894.2"", ""ups"": ""2.9"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""4"", ""lr"": ""0.0001"", ""gnorm"": ""7.893"", ""train_wall"": ""0"", ""wall"": ""2""}
2021-01-19 06:41:57 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""7.526"", ""ppl"": ""184.27"", ""wps"": ""11834.9"", ""ups"": ""2.89"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""5"", ""lr"": ""0.0001"", ""gnorm"": ""6.949"", ""train_wall"": ""0"", ""wall"": ""2""}

after:
2021-01-19 06:39:20 | INFO | fairseq_cli.train | num. model params: 164,465,744 (num. trained: 164,465,744)
(...)
2021-01-19 06:39:22 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""15.893"", ""ppl"": ""60870.7"", ""wps"": ""0"", ""ups"": ""0"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""1"", ""lr"": ""0.0001"", ""gnorm"": ""7.716"", ""train_wall"": ""1"", ""wall"": ""1""}
2021-01-19 06:39:23 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""13.176"", ""ppl"": ""9252.9"", ""wps"": ""12094.7"", ""ups"": ""2.95"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""2"", ""lr"": ""0.0001"", ""gnorm"": ""6.988"", ""train_wall"": ""0"", ""wall"": ""1""}
2021-01-19 06:39:23 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""11.049"", ""ppl"": ""2119.22"", ""wps"": ""12290"", ""ups"": ""3"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""3"", ""lr"": ""0.0001"", ""gnorm"": ""8.008"", ""train_wall"": ""0"", ""wall"": ""1""}
2021-01-19 06:39:23 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""9.044"", ""ppl"": ""527.7"", ""wps"": ""11990.4"", ""ups"": ""2.93"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""4"", ""lr"": ""0.0001"", ""gnorm"": ""7.893"", ""train_wall"": ""0"", ""wall"": ""2""}
2021-01-19 06:39:24 | INFO | train_inner | {""epoch"": 1, ""update"": 0.0, ""loss"": ""7.526"", ""ppl"": ""184.27"", ""wps"": ""12073.8"", ""ups"": ""2.95"", ""wpb"": ""4096"", ""bsz"": ""8"", ""num_updates"": ""5"", ""lr"": ""0.0001"", ""gnorm"": ""6.949"", ""train_wall"": ""0"", ""wall"": ""2""}
```
- with input embedding and output LM head param sharing, the `num. model params` now goes down (as expected), whereas before it stayed constant:
```
CUDA_VISIBLE_DEVICES=0 python train.py --task dummy_masked_lm --arch linformer_roberta_base --user-dir examples/linformer/linformer_src/ --criterion masked_lm --batch-size 8 --optimizer adam --lr 0.0001 --log-format json --log-interval 1 --max-update 5 --disable-validation --no-save

before:
2021-01-19 06:44:58 | INFO | fairseq_cli.train | num. model params: 164,465,744 (num. trained: 164,465,744)
(...)

after:
2021-01-19 06:43:03 | INFO | fairseq_cli.train | num. model params: 126,065,744 (num. trained: 126,065,744)
(...)
```
- confirmed that old checkpoints can be loaded and produce identical valid ppl:
```
python -m fairseq_cli.validate --path $MODEL --user-dir examples/linformer/linformer_src/ --task dummy_masked_lm --criterion masked_lm --max-sentences 8 --dataset-size 100

no sharing:
  before:
      2021-01-19 07:07:54 | INFO | valid |  | valid on 'valid' subset | loss 5.485 | ppl 44.8 | wps 0 | wpb 53248 | bsz 104
  after:
      2021-01-19 07:30:10 | INFO | valid |  | valid on 'valid' subset | loss 5.485 | ppl 44.8 | wps 0 | wpb 53248 | bsz 104

shared_kv_compressed:
  before:
      2021-01-19 07:08:50 | INFO | valid |  | valid on 'valid' subset | loss 5.355 | ppl 40.94 | wps 0 | wpb 53248 | bsz 104
  after:
      2021-01-19 07:30:45 | INFO | valid |  | valid on 'valid' subset | loss 5.355 | ppl 40.94 | wps 0 | wpb 53248 | bsz 104

shared_kv_compressed + shared_layer_kv_compressed:
  before:
      2021-01-19 07:09:26 | INFO | valid |  | valid on 'valid' subset | loss 5.482 | ppl 44.7 | wps 0 | wpb 53248 | bsz 104
  after:
      2021-01-19 08:09:36 | INFO | valid |  | valid on 'valid' subset | loss 5.482 | ppl 44.7 | wps 0 | wpb 53248 | bsz 104

using a really old checkpoint with sharing (trained on commit cf4219b048d31f55970356520860b2543ee97570):
  before:
       | valid on 'valid' subset | loss 5.548 | ppl 46.8 | wps 0 | wpb 53248 | bsz 104
  after:
      2021-01-19 08:34:07 | INFO | valid |  | valid on 'valid' subset | loss 5.548 | ppl 46.8 | wps 0 | wpb 53248 | bsz 104
```

Reviewed By: madian9

Differential Revision: D25938236

Pulled By: myleott

fbshipit-source-id: 4d515e5c8e0601476856ae27eb46c64c30033c88",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1696,Myle Ott,myleott@fb.com,2021-01-20 07:37:50-08:00,338aa57966b11a31120e87840d6bb68e74257182,https://github.com/pytorch/fairseq/commit/338aa57966b11a31120e87840d6bb68e74257182,"Add test for activation checkpointing (#1563)

Summary:
Forgot to merge this with the original code

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1563

Reviewed By: sshleifer

Differential Revision: D25948393

Pulled By: myleott

fbshipit-source-id: b083001015e97f7e21cfa02d4126eba79cc34bfa",1,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestActivationCheckpointing(unittest.TestCase):'],[],[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1697,Myle Ott,myleott@fb.com,2021-01-20 10:46:20-08:00,9fc53d62177b8465b1eca2dd00185540dcd2fb92,https://github.com/pytorch/fairseq/commit/9fc53d62177b8465b1eca2dd00185540dcd2fb92,"Support --post-process=""@@"" (#1571)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1571

Differential Revision: D25974834

Pulled By: myleott

fbshipit-source-id: 8cf4c4874087408f76d9d47a6b5bee46c52ac33b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1698,alexeib,alexei.b@gmail.com,2021-01-20 17:59:39-08:00,15867e12841cf16b3e6a60d5efccc169f728b70a,https://github.com/pytorch/fairseq/commit/15867e12841cf16b3e6a60d5efccc169f728b70a,"migrate translation task (#1569)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1569

Test Plan:
Imported from OSS

tests + ran

```
python fairseq_cli/train.py \                                                           18:08:56
    ~/data/iwslt14.de-en \
    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \
    --dropout 0.3 --weight-decay 0.0001 \
    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
    --max-tokens 4096 \
    --eval-bleu \
    --eval-bleu-args '{""beam"": 5, ""max_len_a"": 1.2, ""max_len_b"": 10}' \
    --eval-bleu-detok moses \
    --eval-bleu-remove-bpe \
    --eval-bleu-print-samples \
    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
```

Reviewed By: myleott

Differential Revision: D25967217

Pulled By: alexeib

fbshipit-source-id: 808f3cb0939fa13e1e05f39bfa02a7fb0b152940",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('True', '( in task.cfg.data)'), ('Equal', '(cfg.common.seed, 456)')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(task.args.seed, 123)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1699,Yun Wang,yunwang@fb.com,2021-01-20 19:14:14-08:00,fc989279647666d7ae537345954556f76ef326c1,https://github.com/pytorch/fairseq/commit/fc989279647666d7ae537345954556f76ef326c1,"TALNet Training: Log `logits` and `target` in Wav2VecCriterion

Summary: TALNet needs to log the `logits` and `target` arrays in `Wav2VecCriterion` to compute the MAP and MAUC metrics on the validation set, which are used to decide when to reduce the learning rate.

Reviewed By: alexeib

Differential Revision: D25957415

fbshipit-source-id: 9c8ccfc408dd0747611b36f430d9f0796bc5340d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1700,alexeib,alexei.b@gmail.com,2021-01-20 20:40:52-08:00,9a1c49706b35bdfc0e4de1a905298113eb236603,https://github.com/pytorch/fairseq/commit/9a1c49706b35bdfc0e4de1a905298113eb236603,"Make Hydra logging work with DDP (#1568)

Summary:
without this pr, when launching hydra based sweeps via ""hydra_train.py"", logging would only go to standard out/error files, rather than into ""hydra_train.log"" file. The problem is that the standard out/err files are placed in a different folder by submitit without a clear way to modify this behavior, while hydra_train.log is always empty (except when training on one gpu) because either a) reset_logging will remove hydra logging hooks, or b) hydra logging does not work properly with ddp based training.

to address a) we do not remove hydra logging by default (although it is optionally still possible)
to address b) we reconfigure the loggers in the train method which will be called by each spawned process

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1568

Reviewed By: myleott

Differential Revision: D25965658

Pulled By: alexeib

fbshipit-source-id: 77cbd4d310fe2d291fb1003c6a3e27e619d571aa",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1701,Myle Ott,myleott@fb.com,2021-01-21 07:32:08-08:00,cfbf0dddbc2f06b4d2975655a3959d13e5ba6667,https://github.com/pytorch/fairseq/commit/cfbf0dddbc2f06b4d2975655a3959d13e5ba6667,"Small changes to make tests more reliable (#1572)

Summary:
After this, `python setup.py test` should be more reliable (including when multiple GPUs are present)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1572

Reviewed By: alexeib

Differential Revision: D25984113

Pulled By: myleott

fbshipit-source-id: 7fef27ae90c079c07f592ed9fb350ccf8b56d23d",10,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Logs', '():')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1702,Sam Shleifer,sshleifer@gmail.com,2021-01-23 03:28:48-08:00,bfcc13e20a6cfa18fb25daaae39644f9b7872699,https://github.com/pytorch/fairseq/commit/bfcc13e20a6cfa18fb25daaae39644f9b7872699,"add return type hints for readability (#1575)

Summary:
As I was going through the dataset/preprocessing code, knowing return types would have made my life easier.

Also I don't think you need to inherit from object in python3.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1575

Reviewed By: myleott

Differential Revision: D25999257

Pulled By: sshleifer

fbshipit-source-id: 818a623c68fb7812306c760f3ae6346a14937c51",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1703,Sam Shleifer,sshleifer@fb.com,2021-01-25 09:22:08-08:00,1e6323e9346c172225f20415735c33f96dd9aad1,https://github.com/pytorch/fairseq/commit/1e6323e9346c172225f20415735c33f96dd9aad1,"Offload inputs to CPU (V2)

Reviewed By: myleott

Differential Revision: D26035523

fbshipit-source-id: 7dc08a38c10d1f26a871106f143f92fd11f6073c",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Logs', '() as logs:')]",[],[],[],[],[],[],[],[],[],[],"[('Logs', '() as logs:'), ('Logs', '():')]",[],[],[],[],[],[],[],[],[],"['len(baseline_logs) == len(offload_logs)', '(', '(']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1704,Myle Ott,myleott@fb.com,2021-01-27 10:45:38-08:00,1f23d83fcff3718805447b8d83edd666cf586850,https://github.com/pytorch/fairseq/commit/1f23d83fcff3718805447b8d83edd666cf586850,"Fix masked_lm task to be more deterministic when reloading checkpoints (#1581)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1581

Test Plan: Imported from OSS

Reviewed By: joshim5

Differential Revision: D26018994

Pulled By: myleott

fbshipit-source-id: 9013b2795936bd5877fb9e8a8397c9b0ea35ef60",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1705,Myle Ott,myleott@fb.com,2021-01-27 10:45:38-08:00,c4a4562299eb58f295426b9b81b36701390e883e,https://github.com/pytorch/fairseq/commit/c4a4562299eb58f295426b9b81b36701390e883e,"Reset meters when reloading an end-of-epoch checkpoint (#1582)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1582

Test Plan: Imported from OSS

Reviewed By: joshim5

Differential Revision: D26018993

Pulled By: myleott

fbshipit-source-id: a0e988d8a2d720443571814d8d9f51acc571f98f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1706,Myle Ott,myleott@fb.com,2021-01-27 10:45:38-08:00,6225dccb989ebfb268274bad36a794b27e4dd43f,https://github.com/pytorch/fairseq/commit/6225dccb989ebfb268274bad36a794b27e4dd43f,"Use a fixed epoch (=1) when creating validation batch iterators (#1583)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1583

Test Plan: Imported from OSS

Reviewed By: joshim5

Differential Revision: D26018992

Pulled By: myleott

fbshipit-source-id: f4908358f8db79b6034639419d47698f933ddf91",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1707,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,96da4d38eb60c3971ca88df9383e09d53493cf1e,https://github.com/pytorch/fairseq/commit/96da4d38eb60c3971ca88df9383e09d53493cf1e,"Small Hydra fix (#1543)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1543

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836852

Pulled By: myleott

fbshipit-source-id: 7fda711d21f2d1b7bac26792233997e8dea2f835",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1708,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,5e343f5f23b4a90cca2beec416b87d4dd7a4264f,https://github.com/pytorch/fairseq/commit/5e343f5f23b4a90cca2beec416b87d4dd7a4264f,"Remove --distributed-wrapper (consolidate to --ddp-backend) (#1544)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1544

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836856

Pulled By: myleott

fbshipit-source-id: eb0a6a02f4d9fe2b6b12a456ef95208dd92c97cb",32,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1709,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,922528d58feea5ada68094df117c1cdbe67aec45,https://github.com/pytorch/fairseq/commit/922528d58feea5ada68094df117c1cdbe67aec45,"Log amount of free GPU memory (#1545)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1545

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836854

Pulled By: myleott

fbshipit-source-id: 6bb5cb69a90022aa206618ee7a903a653fb1ed09",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1710,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,d68a3530dda7f8275e490864b28974ef30fe854b,https://github.com/pytorch/fairseq/commit/d68a3530dda7f8275e490864b28974ef30fe854b,"Refactor distributed code under fairseq.distributed (#1546)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1546

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836853

Pulled By: myleott

fbshipit-source-id: c5076615d49774633ecfaf0aa68b68e8b2331bd9",10,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],2,1,1,0,1,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestDistributedTimeoutWrapper(unittest.TestCase):', 'class TestModuleProxyWrapper(unittest.TestCase):']","[('Raises', '(KeyboardInterrupt):')]",['def setUp(self):'],[],['def tearDown(self):'],[],[],[],[],[],"['import unittest', 'import unittest']",[],[],[],[],[],[],[],[],[],[],[],"['module.xyz == ', 'module.get_xyz() == ', 'wrapped_module.xyz == ', 'wrapped_module.xyz == ', 'module.get_xyz() == ', 'objects_are_equal(wrapped_module.state_dict(), module.state_dict())']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1711,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,27b96eb698610d6ca8835b7bdf47528230ebfd00,https://github.com/pytorch/fairseq/commit/27b96eb698610d6ca8835b7bdf47528230ebfd00,"Move fairseq.distributed_utils -> fairseq.distributed.utils (#1547)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1547

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836855

Pulled By: myleott

fbshipit-source-id: addd8a7fe8dac43252b100d7331e04e95f555781",12,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1712,Myle Ott,myleott@fb.com,2021-01-28 14:18:48-08:00,148327d8c1e3a5f9d17a11bbb1973a7cf3f955d3,https://github.com/pytorch/fairseq/commit/148327d8c1e3a5f9d17a11bbb1973a7cf3f955d3,"Add tests for fairseq.distributed.utils.all_gather_list (#1548)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1548

Test Plan: Imported from OSS

Reviewed By: girifb

Differential Revision: D25836857

Pulled By: myleott

fbshipit-source-id: 3fb844fa21640cbda989dafa6592ef3e5c59bfa7",2,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class DistributedTest(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],['class TestDistributedUtils(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],[],"['objects_are_equal(ref_obj, obj)', 'obj.item() == i']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1713,Guillaume Wenzek,gwenzek@users.noreply.github.com,2021-02-02 09:23:27-08:00,da83e2f3568fa6c93edb528859eef7135be75c2a,https://github.com/pytorch/fairseq/commit/da83e2f3568fa6c93edb528859eef7135be75c2a,"add fast filter_indices_by_size for RoundRobinZipDatasets (#1555)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue?
    this has been extracted from https://github.com/fairinternal/fairseq-py/issues/1538
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Implements a fast RoundRobinZipDataset.filter_indices_by_size.
Instead of filtering the dataset sample by sample, the different datasets that are part of the RoundRobinZipDataset,
are now filtered before being zipped together.
This might generate slightly different datasets.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1555

Reviewed By: myleott

Differential Revision: D25924464

Pulled By: gwenzek

fbshipit-source-id: bc64d9dc35eee62da7e3e17fd75a7f9facb60452",3,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,10,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDataset(unittest.TestCase):'],"[('Equal', '(dict(dataset[0]), {: sample(1, 9)})'), ('Equal', '(dict(dataset[2]), {: sample(1, 9)})'), ('Equal', '(list(idx), [0, 1, 2, 3, 4])'), ('Equal', '(dict(dataset[0]), {: sample(2, 9)})'), ('Equal', '(dict(dataset[2]), {: sample(1, 20)})'), ('Equal', '(dict(dataset[4]), {: sample(0, 11)})'), ('Equal', '(list(idx), [0, 1, 2, 3, 4])'), ('Equal', '(dict(dataset[0]), {: sample(2, 9)})'), ('Equal', '(dict(dataset[2]), {: sample(2, 9)})'), ('Equal', '(dict(dataset[4]), {: sample(2, 9)})')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],['dataset.longest_dataset is long_dataset'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1714,Benjamin Bolte,ben@bolte.cc,2021-02-02 14:22:48-08:00,4b152cbdc029b8a4b1aa8c4189afc16bd289a0ba,https://github.com/pytorch/fairseq/commit/4b152cbdc029b8a4b1aa8c4189afc16bd289a0ba,"Speech recognition sharded infer script (#1587)

Summary:
Wrote a sharded version of `examples/speech_recognition/infer.py` (in a new `examples/speech_recognition/hydra/` folder) which uses the Hydra entry point for launching Slurm jobs.

Tested by decoding a fine-tuned HUBERT model and got a reasonable WER. Also tested using Ax sweeper to sweep WER and it seems to work fine.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1587

Reviewed By: wnhsu

Differential Revision: D26208091

Pulled By: codekansas

fbshipit-source-id: 82671514a85394036b670fe9f7b299236d6c767a",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1715,Pranav Deshpande,pranavcd@fb.com,2021-02-02 15:49:39-08:00,82ec2e722f6fe75686ab2abc872b487ca748f1ce,https://github.com/pytorch/fairseq/commit/82ec2e722f6fe75686ab2abc872b487ca748f1ce,"Fix the task data arg conversion to string.

Summary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.

Reviewed By: myleott

Differential Revision: D26205482

fbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1716,Giri Anantharaman,giriman@learnfair6000.h2.fair,2021-02-03 04:48:27-08:00,62c1bb307582a481629662cca4ce7005d8c0c236,https://github.com/pytorch/fairseq/commit/62c1bb307582a481629662cca4ce7005d8c0c236,"Adding initialization for `num_pipelines_per_node` (#1599)

Summary:
…hod` to avoid unbounded local error.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Adding initialization for `num_pipelines_per_node` in `infer_init_method` in `distributed/utils.py`

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1599

Reviewed By: myleott

Differential Revision: D26208044

Pulled By: girifb

fbshipit-source-id: 98d3c0b70b59a5e0abb027850baa3bc44d9c3c78",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1717,Yuriy Nazarov,nazarov.yuriy.pavlovich@gmail.com,2021-02-03 08:59:55-08:00,83391858c9ca951bb243d60ba3f5f23b87e99cf8,https://github.com/pytorch/fairseq/commit/83391858c9ca951bb243d60ba3f5f23b87e99cf8,"Update lang-pairs path and add fixed-dictionary for small models (#3084)

Summary:
With missing file extension in --lang-pairs option generation from 418M and 1.2B Models fails with the following error
```
ValueError: language pair en-fr contains languages that are not in the language dictionary; langs: ['language_pairs_small_models']
```

However generation still fails after restoring file extension with following error:
```
RuntimeError: Error(s) in loading state_dict for TransformerModel:
	size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).
	size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).
	size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).
```
This could be resolved by adding --fixed-dictionary model_dict.128k.txt like in Generation for the 12B model section.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3084

Reviewed By: huihuifan

Differential Revision: D26225960

Pulled By: myleott

fbshipit-source-id: 0cabe1fd074e45484264d551117704180c7ade9f",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1718,Xu Song,xusong.vip@gmail.com,2021-02-03 09:43:42-08:00,e802a30bffdad0b22ad6efc413230ca348f8f50b,https://github.com/pytorch/fairseq/commit/e802a30bffdad0b22ad6efc413230ca348f8f50b,"Fix hyperlink (#3193)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fix hyperlink

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3193

Reviewed By: ngoyal2707

Differential Revision: D26225560

Pulled By: myleott

fbshipit-source-id: a67a11cf76d1f003d8408b15edbe30f3f7b4fd5b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1719,Xu Song,xusong.vip@gmail.com,2021-02-03 11:25:52-08:00,6ec7ed9920c64ae99a787c0885c543896b525df0,https://github.com/pytorch/fairseq/commit/6ec7ed9920c64ae99a787c0885c543896b525df0,"Fix logger error (#3184)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Fix logger error

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3184

Reviewed By: ngoyal2707

Differential Revision: D26225518

Pulled By: myleott

fbshipit-source-id: eeffc5ede7de1b335148d9a2a2a9cf69fc7630ad",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1720,Xu Song,xusong.vip@gmail.com,2021-02-03 11:31:36-08:00,fd624018bf3e834c09cc03695a8fa0bcaa4a10f3,https://github.com/pytorch/fairseq/commit/fd624018bf3e834c09cc03695a8fa0bcaa4a10f3,"Fix AttributeError: 'Namespace' object has no attribute 'max_positions' (#3183)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Fix AttributeError: 'Namespace' object has no attribute 'max_positions'

https://github.com/pytorch/fairseq/blob/master/examples/bart/README.glue.md#3-fine-tuning-on-glue-task

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3183

Reviewed By: ngoyal2707

Differential Revision: D26225511

Pulled By: myleott

fbshipit-source-id: 29e219b3d9be552aa3f17963b1095c9aa610f4a1",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1721,Sugiyama,h.sugi@ieee.org,2021-02-03 11:55:02-08:00,51c312a30f33e3366b1bb61084d037a90aa1d4a0,https://github.com/pytorch/fairseq/commit/51c312a30f33e3366b1bb61084d037a90aa1d4a0,"Modify eval_bleu_args to Optional (#3175)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3158 .

## PR review
Fixes bug of loading previously trained translation-task model. It is not necessary to define eval_bleu_args and eval_bleu_detok_args when we use eval_bleu=False.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3175

Reviewed By: alexeib

Differential Revision: D26225882

Pulled By: myleott

fbshipit-source-id: ec5908179560cc44c31bac29831beb62dd81305d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1722,Hongfei XU,anoidgit@users.noreply.github.com,2021-02-03 12:03:15-08:00,791ab7c20831a76a9196aaf0db3a2cb1cf906dde,https://github.com/pytorch/fairseq/commit/791ab7c20831a76a9196aaf0db3a2cb1cf906dde,"More accurate label smoothing loss computation (#3182)

Summary:
It seems that the current implementation uses a slightly larger label smoothing value, for a large vocabulary, it is fine, but it can be more different with a small vocabulary size. By changing these 2 lines, the computation of label smoothing loss shall be consistent with the standard.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3182

Reviewed By: glample

Differential Revision: D26225506

Pulled By: myleott

fbshipit-source-id: 75447275e32336ae3b52e732e6124e15d0043b74",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1723,Tim Gates,tim.gates@iress.com,2021-02-03 12:04:25-08:00,81e38ed39da02b5104939cef941bd848b87f9e26,https://github.com/pytorch/fairseq/commit/81e38ed39da02b5104939cef941bd848b87f9e26,"docs: fix simple typo, efficieny -> efficiency (#3070)

Summary:
There is a small typo in fairseq/modules/dynamic_convolution.py, fairseq/modules/dynamicconv_layer/dynamicconv_layer.py.

Should read `efficiency` rather than `efficieny`.

Semi-automated pull request generated by
https://github.com/timgates42/meticulous/blob/master/docs/NOTE.md

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3070

Reviewed By: huihuifan

Differential Revision: D26225968

Pulled By: myleott

fbshipit-source-id: fb7479f9678bc420e80fbb72c5389a54ad4d4c1d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1724,zzxn,zzxnhackman@foxmail.com,2021-02-03 12:07:16-08:00,e996bddcd7b244b1e22d476bc6f402e4ff86167c,https://github.com/pytorch/fairseq/commit/e996bddcd7b244b1e22d476bc6f402e4ff86167c,"A small fix on a problem on Windows: AttributeError: module 'signal' has no attribute 'SIGKILL' (#3188)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3187

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3188

Reviewed By: lematt1991

Differential Revision: D26225553

Pulled By: myleott

fbshipit-source-id: 7aff636f9ba3392bee6cdf305e849aa5c8994a5b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1725,Muhammad Khalifa,moyle2010@gmail.com,2021-02-03 12:11:31-08:00,de3e0fc65158fecf7e6ffa464003839c70a7494f,https://github.com/pytorch/fairseq/commit/de3e0fc65158fecf7e6ffa464003839c70a7494f,"Added shorten dataset to denoising task (#3148)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/2318
The denoising task had no truncate option, which caused errors with longer sentences.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3148

Reviewed By: joshim5

Differential Revision: D26225934

Pulled By: myleott

fbshipit-source-id: 338194e570501293bc5d3b61b8522416d1e6cf07",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1726,Harveen Singh Chadha,30959215+harveenchadha@users.noreply.github.com,2021-02-03 12:15:10-08:00,4c197de87f92b0bd7e427aa3e094d05112b325a0,https://github.com/pytorch/fairseq/commit/4c197de87f92b0bd7e427aa3e094d05112b325a0,"Fixes #3005 (#3122)

Summary:
The normalize and encoder_embed_dim are not present in base pretraining config which leads to errors during finetuning.

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3005

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3122

Reviewed By: alexeib

Differential Revision: D26225929

Pulled By: myleott

fbshipit-source-id: 38067492b0241a30f84dc439f704324a032e054b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1727,markaa,oma654@yandex.ru,2021-02-03 12:25:02-08:00,8629245b0329a7e704ebc7ec05b94ac094468c1b,https://github.com/pytorch/fairseq/commit/8629245b0329a7e704ebc7ec05b94ac094468c1b,"Added weight initialization for ConvTBC (#3179)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3131

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3179

Reviewed By: huihuifan

Differential Revision: D26225878

Pulled By: myleott

fbshipit-source-id: 52267d10db6ec86be1c89207a768ae8b54ae1f82",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1728,Xu Song,xusong.vip@gmail.com,2021-02-03 12:27:18-08:00,b4843681b4d5af442febf8caba58ca9600b01656,https://github.com/pytorch/fairseq/commit/b4843681b4d5af442febf8caba58ca9600b01656,"Update sentence_prediction.py (#3165)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes warning
local variable `label_dict` is not used

https://github.com/pytorch/fairseq/blob/bfcc13e20a6cfa18fb25daaae39644f9b7872699/fairseq/tasks/sentence_prediction.py#L122-L132

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3165

Reviewed By: ngoyal2707

Differential Revision: D26225892

Pulled By: myleott

fbshipit-source-id: f4fe0ceb6f0959112a61c119cf437405d01179ed",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1729,Patrick von Platen,patrick.v.platen@gmail.com,2021-02-04 18:34:37-08:00,4f9831bf847b8595f5590faf30b2f0af6a03bac4,https://github.com/pytorch/fairseq/commit/4f9831bf847b8595f5590faf30b2f0af6a03bac4,"Add small section for wav2vec 2.0 HF Transformers implementation (#3216)

Summary:
# Before submitting

- [] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3216

Reviewed By: aconneau

Differential Revision: D26269645

Pulled By: alexeib

fbshipit-source-id: 239af3a16ef39b90fc7fe71b2e02e068b7727040",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1730,Changhan Wang,changhan@fb.com,2021-02-04 21:36:18-08:00,9316f13ad53fa532c7306c7261e4d76c58e38b48,https://github.com/pytorch/fairseq/commit/9316f13ad53fa532c7306c7261e4d76c58e38b48,"Add interactive.py support for S2T

Summary:
Add interactive.py support for S2T
Github issue: https://github.com/pytorch/fairseq/issues/3146

Reviewed By: jmp84

Differential Revision: D26260681

fbshipit-source-id: 7f2f6e49f8e4b7767550665a3cfe12c962469a7d",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1731,Yun Wang,yunwang@fb.com,2021-02-05 15:42:02-08:00,0f93bd1a7d451944b77804aaf25e40696510411b,https://github.com/pytorch/fairseq/commit/0f93bd1a7d451944b77804aaf25e40696510411b,"Implement Mixup as a batch transform in PySpeech

Summary:
This diff implements the Mixup data augmentation in PySpeech.
It is implemented as `MixupBatchTransform`, which acts on batches of data instead of single instances.
Such a batch transform should be called by the collater, after it collates samples into a batch and before it converts the batch from numpy arrays to PyTorch tensors.
See `TALNetTask` for an example of how to use it.

Sometimes we may want to apply SpecAugment after Mixup, when data instances have already been collated into batches.
The class `Batchify` is a wrapper that turns an instance transform into a batch transform, by applying the instance transform to every instance in a batch.

Reviewed By: nayansinghal

Differential Revision: D26228942

fbshipit-source-id: b5784d8acab840d6ae6fa636d01cd7c68955d606",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1732,Myle Ott,myleott@fb.com,2021-02-06 08:05:41-08:00,5a170841f2faba7413a2d59c792bee6a3ff38838,https://github.com/pytorch/fairseq/commit/5a170841f2faba7413a2d59c792bee6a3ff38838,"Make checkpoint wrapper pickleable (#1603)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1603

Test Plan: Imported from OSS

Reviewed By: sshleifer

Differential Revision: D26237760

Pulled By: myleott

fbshipit-source-id: 73c67bdea4b5b16e3159a5d4f0151e514e853357",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1733,Myle Ott,myleott@fb.com,2021-02-06 08:05:41-08:00,7aa999f2a8084428a9675ee9d9b782bb797fd6ce,https://github.com/pytorch/fairseq/commit/7aa999f2a8084428a9675ee9d9b782bb797fd6ce,"Add --optimizer=cpu_adam (#1604)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1604

Test Plan: Imported from OSS

Reviewed By: shruti-bh

Differential Revision: D26237761

Pulled By: myleott

fbshipit-source-id: 2deb78a93ca23c261c38370ac810c317e4ec20ee",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1734,Arthur Guo,arthurguo@fb.com,2021-02-08 12:13:39-08:00,5605d6fbb44d7ee8240048b6a172385ce3c07c6f,https://github.com/pytorch/fairseq/commit/5605d6fbb44d7ee8240048b6a172385ce3c07c6f,"Refactor Python LAS Rescoring Inference

Summary: This diff cleans up the code in D26195511

Differential Revision: D25857284

fbshipit-source-id: 6b445c3d263078bf711a429130d9f983421e6a10",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1735,Ruslan Mavlyutov,mavlyutov@fb.com,2021-02-08 14:13:34-08:00,6381aa2bb24f125d271e241c726a2fea581bc3c4,https://github.com/pytorch/fairseq/commit/6381aa2bb24f125d271e241c726a2fea581bc3c4,"Adding FBSequenceGenerator

Reviewed By: mikekgfb

Differential Revision: D26228721

fbshipit-source-id: b7a83bbc719d50d677d9b4c8a74f3c20def85357",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1736,Ning Dong,dnn@fb.com,2021-02-08 16:18:00-08:00,3aeb8fe1007f098f629bb20cc31c339dcbf5ad57,https://github.com/pytorch/fairseq/commit/3aeb8fe1007f098f629bb20cc31c339dcbf5ad57,"Explicitly annotate attn_scores as Optional[Tensor]

Summary: The behavior of ternary if's type inference changed a bit with D26278969. Need to annotate explicitly for it to work. Otherwise tests fail as in T84436394.

Reviewed By: nikithamalgifb

Differential Revision: D26320452

fbshipit-source-id: ad61f8ba5ea730150350cb839f5d9c73d476aed4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1737,Patrick von Platen,patrick.v.platen@gmail.com,2021-02-10 14:03:24-08:00,4fed0beca64a52aa718371dc3b2cf1fd979197a4,https://github.com/pytorch/fairseq/commit/4fed0beca64a52aa718371dc3b2cf1fd979197a4,"Fix padding mask for new architectures (#3228)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3227

All models that do **not** make use of group norm, such as
- Wav2Vec 2.0 Large (LV-60)*
- Wav2Vec 2.0 Large (LV-60) + Self Training *

do need this fix IMO to able to correctly run batches through the model. Before this PR, the
following code snippet failed:

```python
import fairseq
import torch

# get model
wav2vec_path = ""data/wav2vec2_vox_960h_new.pt""
model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(
    [wav2vec_path], arg_overrides={""data"": ""./data""}
)
model = model[0]
model.eval()

# create single input
input_wav_0 = torch.randn((1, 2000))
input_wav_1 = torch.randn((1, 3000))

# create batched input
batch_input_wav = torch.zeros((2, 3000))
batch_input_wav[0, :input_wav_0.shape[-1]] = input_wav_0
batch_input_wav[1, :input_wav_1.shape[-1]] = input_wav_1

# create padding mask
padding_mask = torch.zeros((2, 3000), dtype=torch.bool)
padding_mask[0, input_wav_0.shape[-1]:] = True

# run batch & single
output = model(source=input_wav_0, padding_mask=None)[""encoder_out""]
batch_output = model(source=batch_input_wav, padding_mask=padding_mask)[""encoder_out""]

# is equal?
print(""Is batched forward and simple forward equal?"", torch.allclose(output[:,0], batch_output[:output.shape[0], 0], atol=1e-3))
```
Note: It is assumed that both https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt and https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec2_vox_960h_new.pt were downloaded and stored in the folder data.

Also, see [this](https://colab.research.google.com/drive/1ASZ4lVZbKkj-dvRHDl1lo0mCcsaOERlG?usp=sharing) notebook for reproducibility.

This PR should fix the behavior and make the above code snippet / notebook run succesfully.

## PR review

Gently pinging alexeib for Wav2Vec2

Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3228

Reviewed By: aconneau

Differential Revision: D26373721

Pulled By: alexeib

fbshipit-source-id: 3d5aca2f8136d1a8c4b5b4bc9c03cd05a69a3b52",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1738,Ruslan Mavlyutov,mavlyutov@fb.com,2021-02-10 14:57:17-08:00,ac90cb3085439d15af1a33cf0a0b1a6703f07413,https://github.com/pytorch/fairseq/commit/ac90cb3085439d15af1a33cf0a0b1a6703f07413,"Extra logging to confirm OOM source

Reviewed By: myleott, chtran

Differential Revision: D26348808

fbshipit-source-id: 010ef00024e02c09ec35b624f0713ce5f1f387b4",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1739,Alex Xiao,axiao@fb.com,2021-02-10 16:25:25-08:00,7061a0ff83872ac491ba5963eb7fc04cb10d57c4,https://github.com/pytorch/fairseq/commit/7061a0ff83872ac491ba5963eb7fc04cb10d57c4,"better error handling for expired handles

Summary:
At the start of the half there were some expired handles and it was annoying to track down which datasets were responsible when sampling data among multiple datasets and which flows were running them. Lets improve the error message to address several pain points

1. Explicitly tell the user which dataset has expired handles
2. Link to a scuba query to enable the user to find all flows that have expired handles
3. Fail job if 10k handles have expired, rather than if 10k handles in a row have expired. This can detect failures from datasets that have for example 50% expired handles
4. add logging when handles fail

Reviewed By: cruvadom

Differential Revision: D26187820

fbshipit-source-id: 771a359ea01de80b38932921346e98cff812f2f7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1740,pritam,pritam.damania@fb.com,2021-02-11 09:41:54-08:00,ee48d1b95835a0e5fa2129219d205f8d9e748b76,https://github.com/pytorch/fairseq/commit/ee48d1b95835a0e5fa2129219d205f8d9e748b76,"Use torch pipe if available in fairseq. (#3149)

Summary:
fairscale.nn.Pipe has been ported to PyTorch:
https://github.com/pytorch/pytorch/blob/master/torch/distributed/pipeline/sync/pipe.py#L138.
As a result, modifying the pipeline transformer to use PyTorch pipe if available. This change depends on https://github.com/pytorch/pytorch/pull/50860.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3149

Test Plan:
```
python train.py ru_en_bin/ --arch transformer_iwslt_de_en_pipeline_parallel --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --eval-bleu --eval-bleu-args '{""beam"": 5, ""max_len_a"": 1.2, ""max_len_b"": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --pipeline-model-parallel --pipeline-balance '[1,3,5,3,3,1]' --pipeline-devices '[0,1,0,2,3,0]' --pipeline-chunks 16 --distributed-world-size 1 --distributed-no-spawn --disable-validation --max-epoch 1
```

Output with torch pipe:
```
2021-01-20 16:13:35 | INFO | train | epoch 001 | loss 12.676 | nll_loss 12.331 | ppl 5151.97 | wps 5108 | ups 1.66 | wpb 3081.6 | bsz 131.6 | num_updates 380 | lr 4.75e-05 | gnorm 2.08 | train_wall 229 | wall 233
2021-01-20 16:13:36 | INFO | fairseq_cli.train | done training in 233.1 seconds
```

Output with fairscale pipe:
```
2021-01-20 14:13:59 | INFO | train | epoch 001 | loss 12.677 | nll_loss 12.331 | ppl 5152.07 | wps 5198.9 | ups 1.69 | wpb 3081.6 | bsz 131.6 | num_updates 380 | lr 4.75e-05 | gnorm 2.08 | train_wall 224 | wall 228
2021-01-20 14:13:59 | INFO | fairseq_cli.train | done training in 228.0 seconds
```

Reviewed By: myleott

Differential Revision: D26204633

Pulled By: shruti-bh

fbshipit-source-id: 535f816e8d149b47fc6ba8385981accf67257257",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1741,Mary Williamson,marywilliamson@fb.com,2021-02-11 13:53:33-08:00,fd7c2a8b371c2abf645f558282221eba6833f35f,https://github.com/pytorch/fairseq/commit/fd7c2a8b371c2abf645f558282221eba6833f35f,"More informative exception when numpy version changes (#3231)

Summary:
More informative exception when numpy version changes to ask the user to recompile Cython files

# Before submitting

- [With myleott  ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [N/A ] Did you make sure to update the docs?
- [N/A ] Did you write any new necessary tests?

## What does this PR do?
Raises a more informative error to tell the user to recompile Cython files after an update to the numpy version.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3231

Reviewed By: myleott

Differential Revision: D26375174

Pulled By: mwillwork

fbshipit-source-id: f0a93e162bc4cf84619581110d21bea907baf7fc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1742,alexeib,alexei.b@gmail.com,2021-02-11 13:59:08-08:00,66e1803c60272602c719a5ba75acef1c530066ef,https://github.com/pytorch/fairseq/commit/66e1803c60272602c719a5ba75acef1c530066ef,"save task state in the checkpoint (#1562)

Summary:
this allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562

Test Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)

Reviewed By: myleott, gwenzek

Differential Revision: D25937974

Pulled By: alexeib

fbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1743,Kritika Singh,skritika@fb.com,2021-02-11 15:39:32-08:00,138265ce15d198e6baceae334effed8fb384a286,https://github.com/pytorch/fairseq/commit/138265ce15d198e6baceae334effed8fb384a286,"Make wav2vec_asr encoder compatible with pyspeech fst decoder

Summary:
- I don't think there is a convention for the shapes of `encoder_out` and `encoder_padding_mask` in fairseq but `fst_external_decoder.py` expects `encoder_padding_mask` to be of shape T x B. `encoder_padding_mask` also seems unused in the fairseq [CTC criterion and w2l decoder integration](https://fburl.com/diffusion/ms1zi2px) so taking the easy way out and changing its shape.
- Also checking in some changes to the pyspeech audio_pretraining task required to make decoding work

Reviewed By: alexeib

Differential Revision: D26382442

fbshipit-source-id: 87c8f9433026c0e011847f4e2e094beb2cd2182c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1744,alexeib,alexei.b@gmail.com,2021-02-11 18:12:11-08:00,1d5b075e3f30fd3f28af4c8851e8659285ded230,https://github.com/pytorch/fairseq/commit/1d5b075e3f30fd3f28af4c8851e8659285ded230,"fix fairseqlm decoder with flashlight chnages (#1617)

Summary:
fixes fairseqlm integration with flashlight (formerly wav2letter) decoder

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1617

Reviewed By: xuqiantong

Differential Revision: D26415650

Pulled By: alexeib

fbshipit-source-id: 813684ba55047e92378f508101ff1eec55754420",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1745,alexeib,alexei.b@gmail.com,2021-02-11 21:22:42-08:00,506a8e0f45c1206b1306276fed9cec92c7061dd0,https://github.com/pytorch/fairseq/commit/506a8e0f45c1206b1306276fed9cec92c7061dd0,"seq2seq autoregressive flag check (#1618)

Summary:
raise an exception if trying to use wav2vec seq2seq finetuning without autoregressive flag

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1618

Reviewed By: xuqiantong

Differential Revision: D26417249

Pulled By: alexeib

fbshipit-source-id: 777b6d170b0f8196746e03b399e4d7c21ac0b837",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1746,João Pedro Megid Carrilho,joaopedro@nindoo.ai,2021-02-12 00:26:03-08:00,7ffb40d9c8e33b272e85604892a0935d8e57bb0e,https://github.com/pytorch/fairseq/commit/7ffb40d9c8e33b272e85604892a0935d8e57bb0e,"Fix typo Wav2Vec2 README.md (#3240)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3240

Reviewed By: aconneau

Differential Revision: D26420073

Pulled By: alexeib

fbshipit-source-id: 5939535b945a64e61d655cd36dc955ae46410bfb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1747,alexeib,alexei.b@gmail.com,2021-02-12 11:32:52-08:00,f3b6f5817fbee59057ae2506f01502ea3c301b4b,https://github.com/pytorch/fairseq/commit/f3b6f5817fbee59057ae2506f01502ea3c301b4b,"Fix w2v readme (#1621)

Summary:
somehow merging previous pull request deleted the readme

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1621

Reviewed By: michaelauli

Differential Revision: D26429893

Pulled By: alexeib

fbshipit-source-id: 3e6ed1e4698e67e56e0b88d304f42907a4f6cf41",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1748,Weiyi Zheng,wyz@fb.com,2021-02-12 14:04:21-08:00,02803a1be45642b4c2f9c2970a4f4ae645a2dccf,https://github.com/pytorch/fairseq/commit/02803a1be45642b4c2f9c2970a4f4ae645a2dccf,"broadcast the whole optimizer state to each rank

Summary:
OSS removed the 'partition' key in their state dict to accommodate for changing partition size. This requires an update on the fairseq side to not look into the parameter partition, just broadcast everything, and let the optimizer on each rank decides which parameters are relevant.

This diff also needs D26419095 to function completely, and blefaudeux has made fixes upstream in https://github.com/facebookresearch/fairscale/pull/383

Reviewed By: myleott

Differential Revision: D26382917

fbshipit-source-id: 95af1022be59e88814748acaee36a1a350f7dc5b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1749,cordercorder,2205722269@qq.com,2021-02-12 14:35:55-08:00,09945b45d4e2608563b1b18c3bbe289bf9351529,https://github.com/pytorch/fairseq/commit/09945b45d4e2608563b1b18c3bbe289bf9351529,"Fixes bugs of evaluation with BLEU score when training with multi-gpus. (#3237)

Summary:
…ith BLEU scores

# Before submitting

- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [no need] Did you make sure to update the docs?
- [no need] Did you write any new necessary tests?

## What does this PR do?
Fixes bugs of evaluation with BLEU score when training with multi-gpus. But no error will happend if there is no distributed training.

when --eval-bleu is set to be `True` (default it is `False` and the best checkpoint is selected according to loss) and training with multi-gpus (when the number of gpu which participate in distributed training is greater than 1), following error will happend.

```bash
Traceback (most recent call last):
Traceback (most recent call last):
  File ""/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train"", line 33, in <module>
  File ""/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train"", line 33, in <module>
Traceback (most recent call last):
  File ""/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train"", line 33, in <module>
        sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())

  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 450, in cli_main
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 450, in cli_main
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 450, in cli_main
        distributed_utils.call_main(cfg, main)distributed_utils.call_main(cfg, main)

  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 349, in call_main
  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 349, in call_main
    distributed_utils.call_main(cfg, main)
  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 349, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 326, in distributed_main

  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 326, in distributed_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 326, in distributed_main
    main(cfg, **kwargs)

      File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 143, in main
main(cfg, **kwargs)
                                                                                                                                                                               main(cfg, **kwargs)rder/fairseq/fairseq_cli/train.py"", line 143, in main
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 143, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
                                                                                                                                                                               valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
      File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
    return func(*args, **kwds)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 259, in train
Traceback (most recent call last):
  File ""/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train"", line 33, in <module>
    return func(*args, **kwds)
return func(*args, **kwds)  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 259, in train

  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 259, in train
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 345, in validate_and_save
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 345, in validate_and_save
        cfg, trainer, task, epoch_itr, valid_subsets, end_of_epochsys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())

  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 345, in validate_and_save
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 450, in cli_main
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 413, in validate
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 413, in validate
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 413, in validate
    trainer.valid_step(sample)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
    distributed_utils.call_main(cfg, main)
  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 349, in call_main
    trainer.valid_step(sample)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
    return func(*args, **kwds)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 834, in valid_step
    trainer.valid_step(sample)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
    return func(*args, **kwds)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 834, in valid_step
        return func(*args, **kwds)distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)

  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 834, in valid_step
  File ""/data1/cordercorder/fairseq/fairseq/distributed/utils.py"", line 326, in distributed_main
    main(cfg, **kwargs)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 143, in main
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 1157, in _reduce_and_log_stats
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 1157, in _reduce_and_log_stats
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 1157, in _reduce_and_log_stats
    return func(*args, **kwds)
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 259, in train
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 345, in validate_and_save
    self.task.reduce_metrics(logging_outputs, self.get_criterion())
  File ""/data1/cordercorder/fairseq/fairseq/tasks/translation.py"", line 410, in reduce_metrics
        self.task.reduce_metrics(logging_outputs, self.get_criterion())valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)

  File ""/data1/cordercorder/fairseq/fairseq/tasks/translation.py"", line 410, in reduce_metrics
  File ""/data1/cordercorder/fairseq/fairseq_cli/train.py"", line 413, in validate
    self.task.reduce_metrics(logging_outputs, self.get_criterion())
  File ""/data1/cordercorder/fairseq/fairseq/tasks/translation.py"", line 410, in reduce_metrics
    metrics.log_scalar(""_bleu_counts"", np.array(counts))
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py"", line 480, in __array__
    trainer.valid_step(sample)
      File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py"", line 74, in inner
metrics.log_scalar(""_bleu_counts"", np.array(counts))
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py"", line 480, in __array__
        return func(*args, **kwds)metrics.log_scalar(""_bleu_counts"", np.array(counts))

  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 834, in valid_step
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py"", line 480, in __array__
    return self.numpy()
TypeError: can't convert cuda:2 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
    return self.numpy()
TypeError: can't convert cuda:3 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
    return self.numpy()
TypeError: can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)
  File ""/data1/cordercorder/fairseq/fairseq/trainer.py"", line 1157, in _reduce_and_log_stats
    self.task.reduce_metrics(logging_outputs, self.get_criterion())
  File ""/data1/cordercorder/fairseq/fairseq/tasks/translation.py"", line 410, in reduce_metrics
    metrics.log_scalar(""_bleu_counts"", np.array(counts))
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py"", line 480, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
Traceback (most recent call last):
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/distributed/launch.py"", line 261, in <module>
    main()
  File ""/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/distributed/launch.py"", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/data/cordercorder/anaconda3/envs/nmt/bin/python', '-u', '/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train', '--local_rank=3', 'tiny_data_bin', '--distributed-world-size', '4', '--arch', 'transformer', '--share-decoder-input-output-embed', '--optimizer', 'adam', '--adam-betas', '(0.9, 0.98)', '--clip-norm', '0.0', '--lr-scheduler', 'inverse_sqrt', '--warmup-init-lr', '1e-07', '--warmup-updates', '3000', '--lr', '0.0005', '--stop-min-lr', '1e-09', '--dropout', '0.25', '--weight-decay', '0.0001', '--criterion', 'label_smoothed_cross_entropy', '--label-smoothing', '0.1', '--max-tokens', '5000', '--batch-size', '64', '--update-freq', '4', '--max-epoch', '30', '--save-dir', 'checkpoint', '--skip-invalid-size-inputs-valid-test', '--eval-bleu', '--eval-bleu-args', '{""beam"": 5}', '--eval-bleu-remove-bpe', 'sentencepiece', '--eval-bleu-print-samples', '--eval-tokenized-bleu', '--best-checkpoint-metric', 'bleu', '--maximize-best-checkpoint-metric', '--validate-interval-updates', '1']' returned non-zero exit status 1.

```

The error is cased by the fact that the numpy of version 1.20.1 does't support codes like following:
```python
import torch
import numpy as np
a = torch.tensor(0, device=""cuda:0"")
b = np.array([a])
```
The above codes will lead to error: ""TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."", but the codes run well if the numpy version is 1.18.1 or 1.17.0 (when the numpy version is below 1.20.0, it is ok, I guess). However, it seems like that the latest version of fairseq need a numpy package of version 1.20.0 or higher (issue https://github.com/pytorch/fairseq/issues/3203 ).

### Reproduce the error
Download the source code of fairseq (commit ID: 7061a0ff83872ac491ba5963eb7fc04cb10d57c4) and run following code:
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
data_bin_dir=tiny_data_bin

python -m torch.distributed.launch --nproc_per_node=4 \
    --master_addr=""127.0.0.1"" \
    --master_port=12345 \
    $(which fairseq-train) ${data_bin_dir} \
    --distributed-world-size 4 \
    --arch transformer \
    --share-decoder-input-output-embed \
    --optimizer adam \
    --adam-betas '(0.9, 0.98)' \
    --clip-norm 0.0 \
    --lr-scheduler inverse_sqrt \
    --warmup-init-lr 1e-07 \
    --warmup-updates 3000 \
    --lr 0.0005 \
    --stop-min-lr 1e-09 \
    --dropout 0.25 \
    --weight-decay 0.0001 \
    --criterion label_smoothed_cross_entropy \
    --label-smoothing 0.1 \
    --max-tokens 5000 \
    --batch-size 64 \
    --update-freq 4 \
    --max-epoch 30 \
    --save-dir checkpoint \
    --skip-invalid-size-inputs-valid-test \
    --eval-bleu \
    --eval-bleu-args '{""beam"": 5}' \
    --eval-bleu-remove-bpe sentencepiece \
    --eval-bleu-print-samples \
    --eval-tokenized-bleu \
    --best-checkpoint-metric bleu \
    --maximize-best-checkpoint-metric \
    --validate-interval-updates 1
```

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3237

Reviewed By: myleott

Differential Revision: D26429732

Pulled By: alexeib

fbshipit-source-id: bc887ce952d28541cb07dbbdc7e80e99428a6b34",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1750,alexeib,alexei.b@gmail.com,2021-02-12 21:18:23-08:00,5ac5e8a20a7a914698f9970c2a384f14015ece3d,https://github.com/pytorch/fairseq/commit/5ac5e8a20a7a914698f9970c2a384f14015ece3d,"fix sharing objects between tasks (#1623)

Summary:
fixes previous change that changes state/dataset/etc to class variables instead of instance variables

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1623

Reviewed By: michaelauli

Differential Revision: D26439560

Pulled By: alexeib

fbshipit-source-id: ab9e75a425a47ac7ace006419259e254770e560e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1751,Myle Ott,myleott@fb.com,2021-02-16 15:50:46-08:00,43415b44781af6ac9c10adce0ae2a7d26d611bd1,https://github.com/pytorch/fairseq/commit/43415b44781af6ac9c10adce0ae2a7d26d611bd1,"Prepend embedding layer when return_all_hiddens=True in TransformerEncoder (#1559)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1559

This matches the behavior of RobertaEncoder.

Test Plan: Imported from OSS

Reviewed By: gwenzek

Differential Revision: D25936937

Pulled By: myleott

fbshipit-source-id: 795ec8d50298a41d9e9638101436faa01cdf1586",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1752,Myle Ott,myleott@fb.com,2021-02-16 15:50:46-08:00,54423d3b22a3e7f536e02e9e5445cef9becbd60d,https://github.com/pytorch/fairseq/commit/54423d3b22a3e7f536e02e9e5445cef9becbd60d,"refactor RobertaEncoder (#1560)

Summary:
This is long overdue, but finally deprecating the RobertaEncoder components and just using TransformerEncoder directly. This will make it easier for some upcoming online backtranslation changes, and will eventually make migrating it to dataclasses/Hydra easier too. It also fixes some longstanding inconsistencies in layernorm placement in the model parallel roberta code.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1560

Test Plan:
- confirmed that training gives identical losses as before:
https://gist.github.com/myleott/9a4d213fb88a02b00094ea074f5a2e2d
- confirmed that old roberta models can be loaded and produce identical results
- confirmed that old linformer models can be loaded and produce identical results (reran commands from D25938236 (https://github.com/pytorch/fairseq/commit/bf54551cafa13678c0254d2c20354cc026cc0bac))
- confirmed that old model parallel models can be loaded and produce identical results:
```
python -m fairseq_cli.validate --path checkpoint.mp1/checkpoint_last.pt --task dummy_masked_lm --criterion masked_lm --max-sentences 8 --dataset-size 100 --model-parallel-size 2 --distributed-world-size 2

before:
2021-01-19 19:04:14 | INFO | valid |  | valid on 'valid' subset | loss 14.62 | ppl 25174.3 | wps 0 | wpb 53248 | bsz 104

after:
2021-01-19 19:06:59 | INFO | valid |  | valid on 'valid' subset | loss 14.62 | ppl 25174.3 | wps 0 | wpb 53248 | bsz 104
```

Reviewed By: gwenzek, ngoyal2707

Differential Revision: D25937145

Pulled By: myleott

fbshipit-source-id: 1ce0bc93e28e03fb926534ea4134684a49232599",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1753,Myle Ott,myleott@fb.com,2021-02-16 15:50:46-08:00,7096ac35870aa24735bd0cc850beefa07784a668,https://github.com/pytorch/fairseq/commit/7096ac35870aa24735bd0cc850beefa07784a668,"Make validate.py work with model parallel (#1570)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1570

Test Plan: Imported from OSS

Reviewed By: gwenzek, ngoyal2707

Differential Revision: D25967675

Pulled By: myleott

fbshipit-source-id: 7c7f8d25b87ef9b4f0a85331548bb3a2886a1e92",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1754,Sam Shleifer,sshleifer@gmail.com,2021-02-17 10:54:25-08:00,e0788f7007a8473a76db573985031f3c94201e79,https://github.com/pytorch/fairseq/commit/e0788f7007a8473a76db573985031f3c94201e79,"fix bart generation bug (#1629)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1629

Reviewed By: myleott

Differential Revision: D26484942

Pulled By: sshleifer

fbshipit-source-id: 9dcbab5c404c14d8f35628d823102ad9ce59dffd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1755,Onur Çelebi,celebio@fb.com,2021-02-18 03:09:14-08:00,7040ce71f3e0e84730adc267df764f48dc483dac,https://github.com/pytorch/fairseq/commit/7040ce71f3e0e84730adc267df764f48dc483dac,"LASER training code (#1207)

Summary:
Integrating LASER (Language-Agnostic SEntence Representations) training code

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ N/A] Did you make sure to update the docs?
- [ Y] Did you write any new necessary tests?  => an additional test in `test_iterators.py`

## What does this PR do?

This diff introduces the training code for LASER.
It includes a specific `laser` task in `laser_task.py` which reads a
json configuration file describing the binarized datasets of language
pairs.

`multitask_data_utils.py` defines dataset wrappers and iterators used by
`laser` task.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Yes. �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1207

Reviewed By: myleott

Differential Revision: D26454296

Pulled By: Celebio

fbshipit-source-id: c987672aa66abf31b039ee11867b06912d3486e5",8,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1756,Myle Ott,myleott@fb.com,2021-02-18 13:10:02-08:00,3bc43c17d14c4b9f6b052a915f9589cd538bc8b6,https://github.com/pytorch/fairseq/commit/3bc43c17d14c4b9f6b052a915f9589cd538bc8b6,"Fix speed regression after RobertaEncoder refactor (#1626)

Summary:
Add back a couple speed optimizations in the original roberta code that got lost in the refactor

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1626

Reviewed By: gwenzek

Differential Revision: D26478534

Pulled By: myleott

fbshipit-source-id: b945de5e9bffd51cd63630cc3aa1f0078a41cca8",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1757,Elizabeth Salesky,elizabeth.salesky@gmail.com,2021-02-18 13:58:56-08:00,da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d,https://github.com/pytorch/fairseq/commit/da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d,"Add support for multi-channel audio and example for mTEDx data (#3253)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
- updates audio_utils to handle multi-channel audio as well as mono, with no change needed for existing recipes
- adds speech-to-text example for Multilingual TEDx (http://openslr.org/100) data

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3253

Reviewed By: yuntang

Differential Revision: D26514419

Pulled By: kahne

fbshipit-source-id: 699e428affda5b1347f96a8310691ab152dd6769",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1758,Weiyi Zheng,wyz@fb.com,2021-02-18 16:35:02-08:00,284a86a49a054dcace1e66ee4c65dfb4adb5a39f,https://github.com/pytorch/fairseq/commit/284a86a49a054dcace1e66ee4c65dfb4adb5a39f,"remove the missing _device property

Summary: after D26382917 (https://github.com/pytorch/fairseq/commit/02803a1be45642b4c2f9c2970a4f4ae645a2dccf) shipped somehow the self._device was removed in optimizer, (or maybe I didn't test it the right way in the previous diff?) fortunately OSS doesn't need it any way.

Reviewed By: myleott

Differential Revision: D26523538

fbshipit-source-id: 637c1e344670340ae40b32635ef51f5501966b0c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1759,Xutai Ma,xutai_ma@jhu.edu,2021-02-18 22:41:32-08:00,d2ee5883e774700c41b1eaddd0326e9afa6d3cd2,https://github.com/pytorch/fairseq/commit/d2ee5883e774700c41b1eaddd0326e9afa6d3cd2,"Simultaneous Speech Translation Model (#1607)

Summary:
This is the pull request for the code for the paper
[SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation](https://www.aclweb.org/anthology/2020.aacl-main.58/)

The model will also be used for [IWSLT 2021 shared task on simultaneous translation
](https://iwslt.org/2021/simultaneous)
This pull request includes

- Convtransformer offline model
- Convtransformer simultaneous translation model with fixed pre-decision module
- The agent files for inference for the convtransformer simultaneous translation model

jmp84
The README is still missing. Just curious where should I place it?

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1607

Test Plan:
Imported from GitHub, without a `Test Plan:` line.

**********
One of the failing landing integration tests
```
buck test mode/dev //multimo/fb/models/test:multimo_fb_model_test
https://fburl.com/testinfra/oxq2cn5n
```

Reviewed By: jmp84

Differential Revision: D26439663

Pulled By: sravyapopuri388

fbshipit-source-id: b127cb4962756af221b65e3ccb6598a42fc75f7f",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1760,Sravya Popuri,spopuri@fb.com,2021-02-18 22:41:32-08:00,523fe83828e6374439a6203330ed0e8c13e86b62,https://github.com/pytorch/fairseq/commit/523fe83828e6374439a6203330ed0e8c13e86b62,"Integrate Simul ST model into pyspeech

Summary:
This diff integrates simul ST training into pyspeech with very minor modifications to the open sourced code. Specific changes made are
- In fixed_pre_decision.py remove self as argument to p_choose function as it is already called with super in line 101
- In monotonic_multihead_attention.py remove pdb.set_trace()
- Move label_smoothed_cross_entropy_latency_augmented.py to fairseq/criterions folder and add missing arguments to parser
- In fairseq/data/data_utils.py type cast max_tokens to int to avoid type error.
- Update fairseq/convtransformer.py to pyspeech/convtransformer.py

# Next steps:
- Verify decoding using the model trained
- Support everstore handle based decoding in simuleval and integrate it into pyspeech.

Reviewed By: jmp84

Differential Revision: D26478861

fbshipit-source-id: 3b02b2aee757e5464b71dbdd7ebdba42659faee5",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1761,Changhan Wang,changhan@fb.com,2021-02-19 08:59:37-08:00,675f608915a216ac32777928a0b1e8210cb66df6,https://github.com/pytorch/fairseq/commit/675f608915a216ac32777928a0b1e8210cb66df6,"Fix LibriSpeech data prep script

Summary:
Fix LibriSpeech data prep script
* Lowercasing transcript to be consistent with the pre-trained models

Reviewed By: jmp84

Differential Revision: D26538845

fbshipit-source-id: 0885f99e2c85f0e722a24f3cb83f2635ce9429bc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1762,joseph.suh,joseph.suh@netmarble.com,2021-02-19 10:07:43-08:00,2909ee1852cdae7ad4115a1a04520b0522265dd2,https://github.com/pytorch/fairseq/commit/2909ee1852cdae7ad4115a1a04520b0522265dd2,"Fix bug for issue (#3211) (#3212)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes KeyError mentioned in  # (3211).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3212

Reviewed By: alexeib

Differential Revision: D26513255

Pulled By: myleott

fbshipit-source-id: 5a11cb369c9d4202fab6998d269e7da5f3d3e534",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1763,Alex Gaziev,alex.gaziev@gmail.com,2021-02-19 10:10:13-08:00,3ef18886d0a802a8c8d90b57d858df3da7e75202,https://github.com/pytorch/fairseq/commit/3ef18886d0a802a8c8d90b57d858df3da7e75202,"Remove extra arg min_length and fix min_sample_size behavior (#3249)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3178 (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding � (I did ;)

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3249

Reviewed By: alexeib

Differential Revision: D26513275

Pulled By: myleott

fbshipit-source-id: 2785098a945404c07eb72c079177654b1739a7a2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1764,Alex Xiao,axiao@fb.com,2021-02-19 10:31:08-08:00,c6b5c00312dc23f473c66ba3016cc9e3decfd317,https://github.com/pytorch/fairseq/commit/c6b5c00312dc23f473c66ba3016cc9e3decfd317,"fix criterion name check when resuming from checkpoint

Summary:
I tried resuming a run from a checkpoint in f250883864, but ran into:

AssertionError: Criterion does not match; please reset the optimizer (--reset-optimizer). DistributedTimeoutWrapper vs ContrastiveLabelsCriterion

Based on this, I believe since D25836853 (https://github.com/pytorch/fairseq/commit/d68a3530dda7f8275e490864b28974ef30fe854b) we are no longer saving the actual criterion's name, but DistributedTimeoutWrapper in the checkpoint.

This is kind of weird though, as I would expect more people to run into this issue. Not sure if I am doing something wrong, let me know if so, thanks!

Reviewed By: myleott

Differential Revision: D26478656

fbshipit-source-id: bc3c7c925f5505140d9df4438af3a73d65d4f531",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1765,Xutai Ma,xutaima@gmail.com,2021-02-19 21:52:31-08:00,ae22da652d63bd6e05a9a035f6a9dcabb1a39c73,https://github.com/pytorch/fairseq/commit/ae22da652d63bd6e05a9a035f6a9dcabb1a39c73,"Correct the estimation of cnn output lengths in convtransformer (#1636)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1636

Reviewed By: xutaima

Differential Revision: D26562816

Pulled By: jmp84

fbshipit-source-id: 4e6efd0b4236d7187bd365d790f260bd5297aed5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1766,Frankie Robertson,frankie@robertson.name,2021-02-20 06:21:45-08:00,61e46bb99758e05bc990e3687c69b507a8ebf185,https://github.com/pytorch/fairseq/commit/61e46bb99758e05bc990e3687c69b507a8ebf185,"Fix attempt to unlink directory copied into source package (Python 3.9) (#3235)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [N/A] Did you make sure to update the docs?
- [N/A] Did you write any new necessary tests?

## What does this PR do?

Currently when installing the newest source package from PyPI I get an error like so:

```
Collecting fairseq
  Using cached fairseq-0.10.2.tar.gz (938 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  ERROR: Command errored out with exit status 1:
   command: /home/frankier/sources/datasets/.venv/bin/python3 /tmp/tmp_ujftsgi_in_process.py get_requires_for_build_wheel /tmp/tmpmn0eumq2
       cwd: /tmp/pip-install-dg5d6q9y/fairseq
  Complete output (31 lines):
  Traceback (most recent call last):
    File ""setup.py"", line 214, in <module>
      do_setup(package_data)
    File ""setup.py"", line 136, in do_setup
      setup(
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/__init__.py"", line 152, in setup
      _install_setup_requires(attrs)
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/__init__.py"", line 147, in _install_setup_requires
      dist.fetch_build_eggs(dist.setup_requires)
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 60, in fetch_build_eggs
      raise SetupRequirementsError(specifier_list)
  setuptools.build_meta.SetupRequirementsError: ['cython', 'numpy', 'setuptools>=18.0']

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File ""/tmp/tmp_ujftsgi_in_process.py"", line 280, in <module>
      main()
    File ""/tmp/tmp_ujftsgi_in_process.py"", line 263, in main
      json_out['return_val'] = hook(**hook_input['kwargs'])
    File ""/tmp/tmp_ujftsgi_in_process.py"", line 114, in get_requires_for_build_wheel
      return hook(config_settings)
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 149, in get_requires_for_build_wheel
      return self._get_build_requires(
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 130, in _get_build_requires
      self.run_setup()
    File ""/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 145, in run_setup
      exec(compile(code, __file__, 'exec'), locals())
    File ""setup.py"", line 217, in <module>
      os.unlink(fairseq_examples)
  IsADirectoryError: [Errno 21] Is a directory: 'fairseq/examples'
  ----------------------------------------
ERROR: Command errored out with exit status 1: /home/frankier/sources/datasets/.venv/bin/python3 /tmp/tmp_ujftsgi_in_process.py get_requires_for_build_wheel /tmp/tmpmn0eumq2 Check the logs for full command output.
```

I believe the reason for this is that the source package contains the examples directory because it was put there during package creation (it seems the symlink because a directory). Now, when setup.py is run again, it seems the setup.py attempts to unlink the directory, which is not possible because only symlinks can be unlinked. This PR therefore only attempts to unlink it if it is a symlink. I have not thoroughly tested whether my proposed cause is the true cause, but this should fix it in any case.

Note that the source package is fetched because there is no wheel for Python 3.9, so most users will not see this because they will use the wheel.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3235

Reviewed By: alexeib

Differential Revision: D26513259

Pulled By: myleott

fbshipit-source-id: 775d6c636a5867b9983bb6419829f13ee414e2fd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1767,Pierre Andrews,mortimer@fb.com,2021-02-20 06:23:41-08:00,4cf7d76114d50008cdd98a7fde250d4ef99b66fe,https://github.com/pytorch/fairseq/commit/4cf7d76114d50008cdd98a7fde250d4ef99b66fe,"Hydra Integration doc should refer to non legacy task (#1619)

Summary:
# Before submitting

- [NO] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [YES] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [YES] Did you make sure to update the docs?
- [NO] Did you write any new necessary tests?

## What does this PR do?

This is a typo fix to the Hydra Integration doc where the example with dataclass config should user `FairseqTask` and not `LegacyFairseqTask`.

Didn't make an issue for this as it's a trivial doc change for the example to match the actual doc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1619

Reviewed By: huihuifan

Differential Revision: D26448855

Pulled By: Mortimerp9

fbshipit-source-id: 467323101b8425370f6bd7c0532e70abb319b337",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1768,Sravya Popuri,spopuri@fb.com,2021-02-22 13:55:06-08:00,38258a79a42f3ccfa596cc51bbf269cf13c3d799,https://github.com/pytorch/fairseq/commit/38258a79a42f3ccfa596cc51bbf269cf13c3d799,"Update FairseqSimulSTAgent to make it generic and reusable internally

Summary:
This diff
1. Updates FairseqSimulSTAgent to make it generic and reusable internally [Touches OSS]
2. Adds FBFairseqSimulSTAgent inheriting FairseqSimulSTAgent
3. Add TARGETS file in examples/speech_to_text
4. Update simuleval TARGETS and add a bento kernel for easy testing

Reviewed By: jmp84

Differential Revision: D26573214

fbshipit-source-id: f4b71f90693cc878cc771b46a006bcbc83a50124",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1769,Miguel Del-Agua,miguel.delagua@nuance.com,2021-02-22 14:21:36-08:00,808b751597d85c098990080d21fd450877dcb242,https://github.com/pytorch/fairseq/commit/808b751597d85c098990080d21fd450877dcb242,"Improve torchscript compatibility of transfomer and transformer pg (#3247)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?

Fixes https://github.com/pytorch/fairseq/issues/3246
Fixes https://github.com/pytorch/fairseq/issues/3248

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3247

Reviewed By: myleott

Differential Revision: D26513267

Pulled By: lematt1991

fbshipit-source-id: 958de0b3a58a0dd2a56bd6c6d7fb2644a89f6746",4,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[''],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1770,m_fomicheva,mari.fomicheva@gmail.com,2021-02-22 14:55:33-08:00,89cd70c0f0c096bdbfcfb2ab339a9c8f23540bc0,https://github.com/pytorch/fairseq/commit/89cd70c0f0c096bdbfcfb2ab339a9c8f23540bc0,"Fixed scripts and instructions for reproducing the results. (#3264)

Summary:
# Before submitting

- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [Y] Did you make sure to update the docs?
- [N] Did you write any new necessary tests?

## What does this PR do?
Small fixes in the script and documentation for correctly reproducing the results in the corresponding paper.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3264

Reviewed By: lematt1991

Differential Revision: D26587397

Pulled By: myleott

fbshipit-source-id: 3675ec4d4388cafa224d395e08b53667f142cb27",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1771,Myle Ott,myleott@fb.com,2021-02-22 15:00:15-08:00,b9778da42643f5b20fa0a555834d49537ce165c0,https://github.com/pytorch/fairseq/commit/b9778da42643f5b20fa0a555834d49537ce165c0,"Small fixes for flow-cli usage

Summary:
- Use `PathManager.ls` instead of `os.listdir`
- Add version.txt to fairseq TARGETS

Reviewed By: vishrav

Differential Revision: D26579091

fbshipit-source-id: 20d57dc19335a3006cd5fa6d1a3d5e878b105874",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1772,freewym,freewym@gmail.com,2021-02-22 15:36:56-08:00,ab560669cd9baaa4009e1fd01c970f8ffccd1ee0,https://github.com/pytorch/fairseq/commit/ab560669cd9baaa4009e1fd01c970f8ffccd1ee0,"Fixes circular import as complained by python (#3257)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
fixes circular import as complained by python

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3257

Reviewed By: jmp84

Differential Revision: D26587382

Pulled By: myleott

fbshipit-source-id: a8a6e7bee4dcfa6baf934c257958b7d7592205c8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1773,Alex Xiao,axiao@fb.com,2021-02-23 23:32:40-08:00,c3d2beec96bd609f87d8da14cc2dffdbbd843b54,https://github.com/pytorch/fairseq/commit/c3d2beec96bd609f87d8da14cc2dffdbbd843b54,"efficient batch level sampling

Summary:
Batch level sampling (each batch comes from a dataset sampled from some distribution) is useful in cases where we have a criterion that makes this assumption or a unique collator per dataset. However, the current implementation in fairseq `MultiCorpusSampledDataset` is inefficient, because it packs batches by assuming the size of item i is `max(dataset.size(i % len(dataset)) for dataset in datasets)`, which often significantly overestimates the actual sampled item's size, especially with many datasets.

We can make this more efficient by modifying `MultiCorpusDataset`, which can do efficient batch sampling by:

1. Every epoch, sampling the indices/dataset to train on.
2. When creating batches, create per-dataset batches and merge them together

Reviewed By: jay-mahadeokar

Differential Revision: D26601515

fbshipit-source-id: a3273f88d86d7922f9ba004e7324e909ecc6ecf7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1774,Sam Shleifer,sshleifer@gmail.com,2021-02-24 11:22:27-08:00,55e48f18fee765fc4d528650570b8af0133ac074,https://github.com/pytorch/fairseq/commit/55e48f18fee765fc4d528650570b8af0133ac074,"downcast indices in TokenBlockDataset (#1647)

Summary:
### Measurements
TLDR: This saves ~8% CPU RAM for training tiny model on medium sized dataset (11GB on disk)

Command below:

```
+---------------------+----------------+---------+--------+
| fname               |   cpu_mem_used |     wps |    ppl |
+=====================+================+=========+========+

+---------------------+----------------+---------+--------+
| branch_nw8_2gpu.log |          25.41 | 54721   | 429.1  |
+---------------------+----------------+---------+--------+
+---------------------+----------------+---------+--------+
| master_nw8_2gpu.log |          27.53 | 52833.1 | 429.1  |
+---------------------+----------------+---------+--------+
```

### Command

```
base_cmd () {
  dd=$1
  shift
  fairseq-train --fp16 $dd \
            --task language_modeling \
            --arch transformer_lm_gpt2_tiny \
            --sample-break-mode complete --tokens-per-sample 512 \
            --optimizer adam --clip-norm 0.0 --lr 0.0005 \
            --batch-size 1 \
            --max-update 200 --max-epoch 1 \
            --log-format simple --log-interval 100 \
            --restore-file x.pt --no-save \
            --skip-invalid-size-inputs-valid-test --disable-validation $@
}
CUDA_VISIBLE_DEVICES=0,1 base_cmd /private/home/sshleifer/data-bin/stories_mmap --num-workers 8
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1647

Reviewed By: myleott

Differential Revision: D26628861

Pulled By: sshleifer

fbshipit-source-id: 142afe0358d1c4cae448828ba811b211406509d7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1775,Sam Shleifer,sshleifer@gmail.com,2021-02-24 11:25:41-08:00,5c008e0c339ba932b551d18b0801c201e8fdf5a9,https://github.com/pytorch/fairseq/commit/5c008e0c339ba932b551d18b0801c201e8fdf5a9,"make LanguageModelingTask 1% simpler (#1641)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1641

Reviewed By: myleott

Differential Revision: D26607648

Pulled By: sshleifer

fbshipit-source-id: 9d7f9d7a0825e3124c181b651a126842e5de6109",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1776,Sam Shleifer,sshleifer@gmail.com,2021-02-24 14:21:24-08:00,52daa1b29b35c93ffb950e56507c9c1d17aa2369,https://github.com/pytorch/fairseq/commit/52daa1b29b35c93ffb950e56507c9c1d17aa2369,"move code to .py files, document usage (#1637)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1637

Test Plan:
```bash
python examples/bart/summarize.py --model-dir pytorch/fairseq --model-file bart.large.cnn --src $HOME/data-bin/cnn_dm/test.source --n 12 --out hub_hypo.txt

python examples/bart/summarize.py \
  --model-dir pytorch/fairseq \
  --model-file bart.large.cnn \
  --src cnn_dm/test.source \
  --out cnn_dm/test.hypo --xsum-kwargs
```

Reviewed By: ngoyal2707

Differential Revision: D26581703

Pulled By: sshleifer

fbshipit-source-id: 80eb28012f7770eee01ed50a1163c5a2c5cc6d37",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1777,Myle Ott,myleott@fb.com,2021-02-24 15:41:02-08:00,fb3fadbb159d8af6d83a5680674d20f7b7635766,https://github.com/pytorch/fairseq/commit/fb3fadbb159d8af6d83a5680674d20f7b7635766,"Set DynamicLossScaler class defaults to match CLI defaults (#1649)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1649

Reviewed By: stephenroller

Differential Revision: D26639303

Pulled By: myleott

fbshipit-source-id: 7def925cd7885cfe85d542464316cbc0f2ba6d2c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1778,Weiyi Zheng,wyz@fb.com,2021-02-24 15:48:38-08:00,b8651bc984413e7e45f44294dffcc85692ba89c1,https://github.com/pytorch/fairseq/commit/b8651bc984413e7e45f44294dffcc85692ba89c1,"actually checking gradnorm consistency

Summary:
D24849271 (https://github.com/pytorch/fairseq/commit/3c5647cebf454c07b52a0fb899c920789381ebda) fixed finite check, but the 'or' condition means as long as all gradients are finite, the check will pass.
This diff adds back the consistency check, the norm can't differ from each other much.

Reviewed By: myleott

Differential Revision: D26640459

fbshipit-source-id: 3e23e13841372aa04461dcde245b893715480c3c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1779,Sravya Popuri,spopuri@fb.com,2021-02-24 22:55:37-08:00,d3890e593398c485f6593ab8512ac51d37dedc9c,https://github.com/pytorch/fairseq/commit/d3890e593398c485f6593ab8512ac51d37dedc9c,"Add HiveScorer to read data from hive and EverstoreAudioInstance to load audio from everstore

Summary:
This diff
- Refactors utils/agent_finder.py to reduce the complexity of find_agent_cls function
- Refactors cli.py and server.py to remove unnecessary argument parser function calls
- Adds fb_hive_scorer.py with HiveScorer to read data from hive and process everstore handles
- Adds fb_options.py to add necessary arguments for HiveScorer
- Updates other parts of the code to include the new scorer

Reviewed By: jmp84

Differential Revision: D26575148

fbshipit-source-id: ae6e12d2adf5f393f807d5238f0d78a2f64a77a3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1780,Xutai Ma,xutaima@gmail.com,2021-02-25 22:33:48-08:00,f569c024ae6ee3e8c37c3b9dca975a3df50f7a03,https://github.com/pytorch/fairseq/commit/f569c024ae6ee3e8c37c3b9dca975a3df50f7a03,"Relocate simultaneous translation code (#1639)

Summary:
Relocate simultaneous translation code from example/simultaneous_translation to fairseq/model/simultaneous_translation, only keep the documents

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1639

Reviewed By: jmp84

Differential Revision: D26599346

Pulled By: xutaima

fbshipit-source-id: 4f708d172696a430bd4e7b14871f5c8862a20489",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1781,Sam Shleifer,sshleifer@gmail.com,2021-02-26 20:59:22-08:00,4f881a760e1cd7e11ecce2332b6ee9a435f233a5,https://github.com/pytorch/fairseq/commit/4f881a760e1cd7e11ecce2332b6ee9a435f233a5,"TokenBlockDataset np type promotion issue (#1658)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1658

Reviewed By: jxmsML

Differential Revision: D26701840

Pulled By: sshleifer

fbshipit-source-id: 90d631c3cd775ab847366fe7a05136c29d90cd63",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],"['end > 4294967295  # data must be sufficiently large to overflow uint32', 'not isinstance(']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1782,Sam Shleifer,sshleifer@fb.com,2021-02-28 12:44:23-08:00,5354aa3a6ec80092cc7bb9aecfad7077bb50b47e,https://github.com/pytorch/fairseq/commit/5354aa3a6ec80092cc7bb9aecfad7077bb50b47e,"github CI install pyarrow

Reviewed By: myleott

Differential Revision: D26643358

fbshipit-source-id: 8d7e1082c6e11f9bbab4b34de078cf05197297a5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1783,Sam Shleifer,sshleifer@gmail.com,2021-02-28 12:49:20-08:00,e5e8b3fee1e57a7abf35ad1a3ff223a2b7190c65,https://github.com/pytorch/fairseq/commit/e5e8b3fee1e57a7abf35ad1a3ff223a2b7190c65,"Fix nearly all unit-test warnings (#1652)

Summary:
2 types of warnings fixed:

```
 `np.long` is a deprecated alias for `np.compat.long`.

Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1652

Reviewed By: myleott

Differential Revision: D26643344

Pulled By: sshleifer

fbshipit-source-id: 960bccc94f299bd8a8c58a87acd80694e9d5c363",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1784,Hiromu Yakura,hiromu1996@gmail.com,2021-03-01 12:36:18-08:00,39e55139ea05da36e9ab9837c4943f660b79dcbe,https://github.com/pytorch/fairseq/commit/39e55139ea05da36e9ab9837c4943f660b79dcbe,"Fix the order of constraints in LanguagePairDataset (#3280)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3279.

This change modifies the output of `echo -e ""Ja, wer hat, wenn du willst, Götter gebildet, uns zu ihnen erhoben, sie zu uns herniedergebracht, als der Dichter?\tbard\nZu vollenden ist nicht die Sache des Schülers, es ist genug, wenn er sich übt\tstudent"" | python normalize.py | python tok.py | fairseq-interactive --constraints -s de -t en --beam 10 --batch-size 2 --buffer-size 2 --bpe fastbpe --bpe-codes ../../../models/ende30k.fastbpe.code --path ../../../models/wmt19.de-en.ffn8192.pt ../../../models/` as follows.

Before:
```
S-0	Ja , wer hat , wenn du will@@ st , Gö@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?
W-0	1.755	seconds
C-0	student
H-0	-1.1425577402114868	Yes , who , if you will , has formed go@@ ds , raised us up to them , brought them down to us , but the po@@ et student ?
D-0	-1.1425577402114868	Yes , who , if you will , has formed gods , raised us up to them , brought them down to us , but the poet student ?
P-0	-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -1.8780 -1.4195 -0.4071 -0.1347 -0.3726 -1.1306 -0.1665 -1.4588 -0.2837 -0.1722 -0.2330 -0.2840 -0.1806 -0.1432 -0.2263 -0.1395 -0.7261 -1.4593 -0.3639 -0.4030 -0.1083 -18.7577 -0.2396 -0.1837
S-1	Zu voll@@ enden ist nicht die Sache des Sch@@ ül@@ ers , es ist genug , wenn er sich übt
W-1	1.755	seconds
C-1	b@@ ard
H-1	-1.9625756740570068	It is not up to the b@@ ard to complete , it is enough if he practi@@ ses
D-1	-1.9625756740570068	It is not up to the bard to complete , it is enough if he practises
P-1	-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -17.3978 -6.0238 -0.4888 -1.7563 -0.8708 -0.6773 -0.2027 -0.2456 -1.6366 -0.2911 -2.0235 -0.1961 -0.5538
```

After:
```
S-0	Ja , wer hat , wenn du will@@ st , Gö@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?
W-0	1.740	seconds
C-0	b@@ ard
H-0	-1.2060465812683105	Yes , who , if you will , formed go@@ ds , raised us up to them , brought them down to us , but the b@@ ard ?
D-0	-1.2060465812683105	Yes , who , if you will , formed gods , raised us up to them , brought them down to us , but the bard ?
P-0	-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -2.2551 -0.5702 -0.1331 -0.3940 -1.0268 -0.1750 -1.4635 -0.2821 -0.1725 -0.2404 -0.3575 -0.1833 -0.1441 -0.2250 -0.1419 -0.7020 -1.5215 -0.3700 -16.8578 -2.7290 -0.3405 -0.2060
S-1	Zu voll@@ enden ist nicht die Sache des Sch@@ ül@@ ers , es ist genug , wenn er sich übt
W-1	1.740	seconds
C-1	student
H-1	-0.8064212203025818	It is not up to the student to complete , it is enough if he practi@@ ses
D-1	-0.8064212203025818	It is not up to the student to complete , it is enough if he practises
P-1	-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -1.5556 -0.2831 -1.3885 -0.7310 -0.6367 -0.1824 -0.2386 -1.5320 -0.2728 -2.0003 -0.2163 -0.5536
```

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3280

Reviewed By: myleott

Differential Revision: D26725013

Pulled By: lematt1991

fbshipit-source-id: 2275fcf146cb8cd9ca21f847e10a4dacdee996f9",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1785,freewym,freewym@gmail.com,2021-03-01 16:21:05-08:00,1c0439b7dabe62d39c6e7f1c8ebc86311e042b5a,https://github.com/pytorch/fairseq/commit/1c0439b7dabe62d39c6e7f1c8ebc86311e042b5a,"fixes circular imports incurred by a recent commit (#3286)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes circular imports incurred by a recent commit

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3286

Reviewed By: lematt1991

Differential Revision: D26725255

Pulled By: myleott

fbshipit-source-id: 5572f733b83bdfadcce3188c0789fc6d70a3bad3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1786,Eric Lou,ericlou@fb.com,2021-03-02 09:24:03-08:00,3100d0b8e5bb5e61b4d73b9c058389aa2c06784a,https://github.com/pytorch/fairseq/commit/3100d0b8e5bb5e61b4d73b9c058389aa2c06784a,"ioPath async - opt-in Fairseq integration (#1635)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635

**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.

**Details:**
- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.
- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.

**Usage:**
```
python train.py --save-async
```
---------
NOTE: **QUESTIONS**
1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a ""wait until a file is written and then copy/move it to another path"" feature. Is this okay for now?
2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?

**Note to Eric:** Keep this integration in check with D26375501.

Reviewed By: myleott

Differential Revision: D26467815

fbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1787,Xutai Ma,xutaima@gmail.com,2021-03-02 13:28:53-08:00,12e21b9a6e7262fa1af2090e22c301bc0b5d1399,https://github.com/pytorch/fairseq/commit/12e21b9a6e7262fa1af2090e22c301bc0b5d1399,"Add global cmvn for mustc data preparation (#1660)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1660

Reviewed By: jmp84, kahne

Differential Revision: D26708521

Pulled By: xutaima

fbshipit-source-id: c53e9052298c559706ceffeb359dadfede2f1a09",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1788,Xutai Ma,xutaima@gmail.com,2021-03-02 17:08:45-08:00,c58af189957eb15b47e507473b4da3e83dfbdf2e,https://github.com/pytorch/fairseq/commit/c58af189957eb15b47e507473b4da3e83dfbdf2e,"Several update on simultaneous translation inference. (#1655)

Summary:
Fix some issues in some corner cases.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1655

Reviewed By: jmp84

Differential Revision: D26651362

Pulled By: sravyapopuri388

fbshipit-source-id: 160d75be8d49f8263c14af225c90fe7997171a43",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1789,Xutai Ma,xutaima@gmail.com,2021-03-02 17:08:45-08:00,ddc483ff3d3a70f3abc33fc4d10bb29871c73d73,https://github.com/pytorch/fairseq/commit/ddc483ff3d3a70f3abc33fc4d10bb29871c73d73,"Streaming models for simul ST (#1552)

Summary:
`fairseq/models/speech_to_text/modules/emformer.py` mostly contains the code from Yangyang. I did a little modification to make it run on fairseq.

`fairseq/models/speech_to_text/modules/augmented_memory_attention.py` contains code for the old streaming models

`fairseq/models/speech_to_text/modules/convtransformer_simul_trans.py` contaons three convtransformer based simultaneous translation models.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1552

Reviewed By: jmp84

Differential Revision: D26563864

Pulled By: sravyapopuri388

fbshipit-source-id: a91a6247559861977cbc22db00ba9511f6b21c69",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1790,Sravya Popuri,spopuri@fb.com,2021-03-02 17:08:45-08:00,b8786dc2aadb56bb549f92ed542875096868bdd5,https://github.com/pytorch/fairseq/commit/b8786dc2aadb56bb549f92ed542875096868bdd5,"Integrate Augmented memory transformer and emformer based augmented memory transformer into fbcode

Summary:
Integrate Augmented memory transformer and emformer based augmented memory transformer into fbcode. This diff
- Modifies the way encoder_out_dict variable is accessed in transformer_monotonic_attention.py
- Fix dimension issues in augmented_memory_attention.py
- Modifies the way encoder_out is accessed in emformer.py

Reviewed By: jmp84

Differential Revision: D26567899

fbshipit-source-id: 9b298ad0bdf78de00b1182001813b0513d32a119",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1791,Xutai Ma,xutaima@gmail.com,2021-03-03 09:59:23-08:00,0c32e251e29dc6f10755addd37c5f9d963693df9,https://github.com/pytorch/fairseq/commit/0c32e251e29dc6f10755addd37c5f9d963693df9,"Update Simultaneous Translation doc (#1659)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1659

Reviewed By: jmp84, kahne

Differential Revision: D26708524

Pulled By: xutaima

fbshipit-source-id: 0f34e5e9e3bec2360e098c9c272105c793bfa7b7",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1792,Eric Lou,ericlou@fb.com,2021-03-03 10:48:42-08:00,7d2394b56f1cbdcdede9c7a8cf6de1df022e0a17,https://github.com/pytorch/fairseq/commit/7d2394b56f1cbdcdede9c7a8cf6de1df022e0a17,"ioPath async - Fairseq unittests (#1669)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1669

Unit tests for async writes integration done in D26467815 (https://github.com/pytorch/fairseq/commit/3100d0b8e5bb5e61b4d73b9c058389aa2c06784a).

Ongoing performance tests: https://fb.quip.com/kjM7Atb1kKbO

Reviewed By: myleott

Differential Revision: D26732660

fbshipit-source-id: faf8cac67b9167af4195358c1a2592804c13562c",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('IsNone', '(IOPathPathManager)'), ('IsNotNone', '(IOPathPathManager)'), ('True', '(PathManager.async_close())')]",[],[],[],[],[],[],[],[' unittest.mock '],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1793,Alex Xiao,axiao@fb.com,2021-03-03 19:29:55-08:00,1fed7a8426e8c548196add0d65d77857ab224705,https://github.com/pytorch/fairseq/commit/1fed7a8426e8c548196add0d65d77857ab224705,"add unit test for multi_corpus_dataset

Reviewed By: vimalmanohar

Differential Revision: D26220694

fbshipit-source-id: ed13f8527a1b203e1a9d004fa8a86e1ad6423d60",1,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestMultiCorpusDataset(unittest.TestCase):'],"[('Less', '(')]",['def setUp(self):'],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1794,Alex Xiao,axiao@fb.com,2021-03-03 19:29:55-08:00,fc2840de58b06f381626332153203fb32588c23d,https://github.com/pytorch/fairseq/commit/fc2840de58b06f381626332153203fb32588c23d,"optimize sampling process of multi_corpus_dataset

Summary:
The sampling process in multi_corpus_dataset is very inefficient. Turns out we can signficantly optimize it by sampling in batches rather than one by one. this allows:

1. fast local development and iteration with corpus sampling, as the turnaround time was long before
2. makes it take less time for our jobs can start training, enabling earlier signal if for example there is a configuration issue

Reviewed By: zhengwy888

Differential Revision: D26187821

fbshipit-source-id: b4f7f6b7c187b3785499308226e2af671a6c354f",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1795,alexeib,alexei.b@gmail.com,2021-03-03 21:15:01-08:00,f6d60e2fee9fe8982e3c9de1e6bb77680978e749,https://github.com/pytorch/fairseq/commit/f6d60e2fee9fe8982e3c9de1e6bb77680978e749,"minor fixes and improvements (#1671)

Summary:
there are a few changes here:
- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc
- update checkpoints that have old print_alignment saved
- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax
- fixing some edge cases for config loading

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671

Reviewed By: myleott

Differential Revision: D26791583

Pulled By: alexeib

fbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1796,Kaushik Rangadurai,krangadu@fb.com,2021-03-04 11:48:27-08:00,f1c595beb8acd2a6dc8c9fa9f7fb60ca23c61899,https://github.com/pytorch/fairseq/commit/f1c595beb8acd2a6dc8c9fa9f7fb60ca23c61899,"Ability to pass attn_mask to TransformerSentenceEncoder

Summary:
Provide an ability to pass attn_mask to TransformerSentenceEncoder. The default is None and hence this is backwards compatible.

The attention mask can either be a 2D tensor (of shape [tgt_seq_len, src_seq_len]) or a 3D tensor of shape (bcz * num_heads, tgt_seq_len, src_seq_len).

In case of self attention, tgt_seq_len = src_seq_len.

Reviewed By: myleott

Differential Revision: D26790767

fbshipit-source-id: 937d6c6cf08790c7d43d33fda97a30425f31ea06",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1797,Myle Ott,myleott@fb.com,2021-03-04 13:31:02-08:00,6d23cc7e7c32d1a6aa1d2d4a4c94abe50c980126,https://github.com/pytorch/fairseq/commit/6d23cc7e7c32d1a6aa1d2d4a4c94abe50c980126,"Move checkpoint state_dict creation into Trainer (#1666)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666

Context: the checkpoint saving call stack has become a bit convoluted:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.save_state
   + checkpoint_utils.torch_persistent_save
```

This diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.torch_persistent_save
```

This new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.

Test Plan:
- unit tests
- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results
- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/

Reviewed By: zhengwy888

Differential Revision: D26771146

Pulled By: myleott

fbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1798,Myle Ott,myleott@fb.com,2021-03-04 13:31:02-08:00,656d7e5779a9ec4ccf0ad45d86a4ce589c597588,https://github.com/pytorch/fairseq/commit/656d7e5779a9ec4ccf0ad45d86a4ce589c597588,"Add support for FullyShardedDataParallel (--ddp-backend=fully_sharded) (#1667)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1667

Add support for FullyShardedDataParallel (--ddp-backend=fully_sharded)

This enables fully parameter + optimizer state sharding by using
FullyShardedDataParallel (FSDP) from fairscale. The user just needs to provide
`--ddp-backend=fully_sharded` to enable. Other common options work
out-of-the-box (e.g., `--fp16`, `--memory-efficient-fp16`, `--update-freq`,
etc.). This should be a drop-in replacement for the ""c10d"" backend.

This yields pretty big speedups for small models and enables training ~13B
parameter models on 8 GPUs and 175B parameter models on 128 GPUs, without model
parallelism.

This also adds a new option `--cpu-offload` that offloads the optimizer state
and FP32 model copy to CPU, which is particularly useful when combined with
`--optimizer=cpu_adam`.

Note: after enabling this, each GPU will save a checkpoint file, since the
optimizer state is sharded. Each checkpoint will contain a single shard of the
optimizer state and the rank 0 checkpoint will contain the full model weights.

Note: a known limitation of the current implementation is that you cannot
resume training on a different world_size. This constraint will be relaxed in
future iterations.

Test Plan: Imported from OSS

Reviewed By: sshleifer

Differential Revision: D26771144

Pulled By: myleott

fbshipit-source-id: 74c2f46f57719e24e2dcfc9d9ee7c2fc0aeedb46",13,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,2,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Logs', '():'), ('Logs', '():')]",['def setUp(self):'],[],['def tearDown(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1799,Ning Dong,dnn@fb.com,2021-03-04 14:20:00-08:00,73886ac228f8f0368871237f7498ec8b07444322,https://github.com/pytorch/fairseq/commit/73886ac228f8f0368871237f7498ec8b07444322,"Refactor FairseqSimulSTAgent

Summary:
1. In fblearner flow we are dumping cmvn stats into json file (e.g. f253830726) Previously there's only --config option taking .npz path from a yaml file, and it's the only usage for the config. This diff adds an option --global-stats to import from json.

2. Inherit FairseqSimulSTAgent from nn.Module instead of SpeechAgent whose root class is object to prepare for scripting methods. Copy over / simplify all the necessary methods from SpeechAgent/Agent.

Reviewed By: jmp84

Differential Revision: D26800957

fbshipit-source-id: 74be527f8473c13405a60bb16ce6da5a7dc0b888",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1800,Changhan Wang,changhan@fb.com,2021-03-04 17:17:11-08:00,7c95746a7e5e4a087399d186590815e45ae775c8,https://github.com/pytorch/fairseq/commit/7c95746a7e5e4a087399d186590815e45ae775c8,"fix bug on converting stereo audio in audio_utils.py

Summary:
Fix bug on converting stereo audio in audio_utils.py
- Github issue: https://github.com/pytorch/fairseq/issues/3303

Reviewed By: jmp84

Differential Revision: D26825964

fbshipit-source-id: 26905e71540bc52e98d76996b199ac0fbe78357b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1801,sarapapi,57095209+sarapapi@users.noreply.github.com,2021-03-08 14:10:29-08:00,16c1a200f87a2adb6395e353345c19bbe990d1dd,https://github.com/pytorch/fairseq/commit/16c1a200f87a2adb6395e353345c19bbe990d1dd,"Fix Global CMVN path of MustC data preprocessing (#3307)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fix a typo in gcmv_path given for config yaml generation (actual: gcvmn_cvmn_path, correct: gcmvn_path)

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3307

Reviewed By: jmp84

Differential Revision: D26826231

Pulled By: kahne

fbshipit-source-id: 6b60f2a8a8b4ba1c0c088299a08ef04fdfe870a8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1802,Myle Ott,myleott@fb.com,2021-03-09 06:28:23-08:00,00d5b7adbeaf64e02c53a591d637efe4c8cad923,https://github.com/pytorch/fairseq/commit/00d5b7adbeaf64e02c53a591d637efe4c8cad923,"Add README/tutorial for Fully Sharded Data Parallel (#3327)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3327

Reviewed By: sshleifer

Differential Revision: D26899416

Pulled By: myleott

fbshipit-source-id: bbb493a5c4e0a51f3b26fe8f94e3962b6206d6f6",9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1803,Myle Ott,myleott@fb.com,2021-03-09 09:38:01-08:00,c6006678261bf5d52e2c744508b5ddd306cafebd,https://github.com/pytorch/fairseq/commit/c6006678261bf5d52e2c744508b5ddd306cafebd,"Update README for Fully Sharded Data Parallel (#3331)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/3331

Reviewed By: sshleifer

Differential Revision: D26912554

Pulled By: myleott

fbshipit-source-id: b45a161fbd52a12da13d7e011d562d35a5b5a1a7",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1804,Changhan Wang,changhan@fb.com,2021-03-09 16:26:05-08:00,05255f96410e5b1eaf3bf59b767d5b4b7e2c3a35,https://github.com/pytorch/fairseq/commit/05255f96410e5b1eaf3bf59b767d5b4b7e2c3a35,"update audio_utils and fix mTEDx example

Summary:
update audio_utils and fix mTEDx example
- Updated `audio_utils`
  - Added support for OGG Vorbis (the only supported lossy compressed format)
  - Added a separate `convert_to_mono()` helper function
  - Updated `get_waveform()`
    - added new arguments `frames` and `start` for reading part of audios
    - added new argument `mono` for auto conversion to mono-channel audio
    - unified returned waveform shape to channels x length (same as torchaudio default)
- Updated mTEDx and MUST-C data prep scripts
  - Replaced `torchaudio.info()` with `soundfile.info()` (the latter is faster and the former has incompatible interface between <0.8 and the latest 0.8)
  - Replaced `torchaudio.load()` with `get_waveform` for auto conversion to mono channel

Reviewed By: jmp84

Differential Revision: D26901114

fbshipit-source-id: fa9560c9714d51a91157d5141564574d4eee454d",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1805,Xutai Ma,xutaima@gmail.com,2021-03-10 20:32:49-08:00,d031611ce49cb231653cf9246667ac237cbbdaff,https://github.com/pytorch/fairseq/commit/d031611ce49cb231653cf9246667ac237cbbdaff,"Update simul trans doc (#1683)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1683

Reviewed By: jmp84

Differential Revision: D26914869

Pulled By: xutaima

fbshipit-source-id: a5d2efdcff1852e56304e77838840b3aad5124b0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1806,Sam Shleifer,sshleifer@gmail.com,2021-03-12 12:29:40-08:00,2235f86b40da5915cd801c4f2f29de4c17c9804b,https://github.com/pytorch/fairseq/commit/2235f86b40da5915cd801c4f2f29de4c17c9804b,"PlasmaView: don't materialize array in memory (#1645)

Summary:
### Changes:
- `PlasmaArray` saves the underlying data to `self.array`, `PlasmaView` never does that, instead it fetches the data from `plasma_store` shared memory when it is needed.
- `PlasmaArray` starts a new, ephemeral plasma_store and puts a new array in it when it is pickled. If `--use-plasma-view`, there is one server started before `spawn` and arrays are only put into it once, in `PlasmaArray.__init__` to accommodate this.
- user can now pass `--plasma-path` to explicitly control where server is started.
- We now make plasma keys based on `(split_path, (block_size, document_sep_len, str(break_mode), len(dataset)))`, so two jobs sharing plasma server but with different datasets, or same dataset but different clargs, will read each the other's array.

### Results [pre March 1]
This saves some CPU memory (5-15%), according to both `psutil` and `psrecord`:
here we run base_cmd (below) with num_workers=0,2,8, 2 GPUS and collect the logs. `branch` refers to `--use-plasma-view`, `master` uses `PlasmaArray`

```
+-------------------------+----------------+---------+-------+
| setting                 |   cpu_mem_used |     wps |   ppl |
+=========================+================+=========+=======+
| branch_nw0_gpu2_ddm.log |          12    | 55143.2 | 429.1 |
+-------------------------+----------------+---------+-------+
| branch_nw2_gpu2_ddm.log |          13.67 | 43377.6 | 429.1 |
+-------------------------+----------------+---------+-------+
| branch_nw8_gpu2_ddm.log |          18.36 | 53019.9 | 429.1 |
+-------------------------+----------------+---------+-------+
| master_nw0_gpu2_ddm.log |          12.26 | 56733   | 429.1 |
+-------------------------+----------------+---------+-------+
| master_nw2_gpu2_ddm.log |          14.58 | 53337.9 | 429.1 |
+-------------------------+----------------+---------+-------+
| master_nw8_gpu2_ddm.log |          21.1  | 53217.2 | 429.1 |
+-------------------------+----------------+---------+-------+
```

### Replication

1) get this branch
```bash
git fetch && git checkout share-plasma-server
```

2) Train tiny model and save logs

```bash

base_cmd () {
  fairseq-train --fp16 /private/home/sshleifer/data-bin/stories_mmap \
            --task language_modeling \
            --arch transformer_lm_gpt2_tiny \
            --sample-break-mode complete --tokens-per-sample 512 \
            --optimizer adam --clip-norm 0.0 --lr 0.0005 \
            --batch-size 1 \
            --max-update 200 --max-epoch 1 \
            --log-format simple --log-interval 100 \
            --restore-file x.pt --no-save \
            --skip-invalid-size-inputs-valid-test --disable-validation $@
}

USE_LOCK=1 CUDA_VISIBLE_DEVICES=0,1 base_cmd --num-workers 0 --use-plasma-view | tee branch_nw0_gpu2_ddm.log
```

### TODO:

- [x] test larger dataset
- [x] make it optional, cleanup
- [x] 1 GPU
- [x] unit-tests
- [x] ask hashing Q on stackoverflow https://stackoverflow.com/questions/66354598/deterministic-method-to-hash-np-array-int
- [ ] measure whether `PlasmaArray` disable for small array's logic helps
- [ x] test with fb_sweep
- [ x] measure 4 GPU savings

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1645

Test Plan: Read github PR description: https://github.com/fairinternal/fairseq-py/pull/1645

Reviewed By: myleott

Differential Revision: D26630365

Pulled By: sshleifer

fbshipit-source-id: b0c4163fbc97a7aefb116de70265fba11f6d7b42",7,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestPlasmaView(unittest.TestCase):'],"[('Raises', '(plasma.PlasmaStoreFull):')]",[],[],[],[],"['PYARROW_AVAILABLE, )']",[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['len(arr1.client.list()) == 1', '(arr1.array == data_server_1).all()', '(arr2.array == data_server_2).all()', '(arr1.array == data_server_1).all()', 'len(arr1.client.list()) == 1', 'len(arr1.client.list()) == 1', 'len(arr2.client.list()) == 1', '(arr2.array == data_server_1).all()', '(', '(', '(', 'len(self.client.list()) == 1', 'len(self.client.list()) == 1', 'len(self.client.list()) == 2', 'len(new_client.list()) == 2  # new client can access same objects', 'isinstance(arr1.object_id, plasma.ObjectID)']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1807,Xutai Ma,xutaima@gmail.com,2021-03-12 16:45:40-08:00,252d5a9ae93e68254cfb1896fb5624cf11cda15e,https://github.com/pytorch/fairseq/commit/252d5a9ae93e68254cfb1896fb5624cf11cda15e,"Fix a bug that FairseqSimulSTAgent is not an agent (#1690)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1690

Reviewed By: jmp84

Differential Revision: D27025669

Pulled By: xutaima

fbshipit-source-id: 8125365adedfdc938813d08e911e1f6ebe4f584b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1808,Alex Xiao,axiao@fb.com,2021-03-14 20:55:46-07:00,965240c784910895b05e66d7ef7e15321050b414,https://github.com/pytorch/fairseq/commit/965240c784910895b05e66d7ef7e15321050b414,"optimize memory when loading large checkpoints by deleting state dict early

Summary: I had some issues with loading checkpoints from 5B parameter models (60 GB checkpoint files) due to OOM.

Reviewed By: myleott

Differential Revision: D27027616

fbshipit-source-id: 2b816e8e46ec80f0ec721aa7a6702cee531b94eb",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1809,Xutai Ma,xutaima@gmail.com,2021-03-15 23:44:19-07:00,dd74992d0d143155998e9ed4076826bcea80fb06,https://github.com/pytorch/fairseq/commit/dd74992d0d143155998e9ed4076826bcea80fb06,"Several updates for simul speech transition example (#1703)

Summary:
Fix sever issues in simul speech transition example, including
- Load pretrained encoder with when loading model.
- Generating broken config.yaml when using gcvm.
- Fix the preprocessed databin.
- Fix some errors in the instructions.
- Add detailed instructions on evaluation a pretrained model.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1703

Reviewed By: jmp84

Differential Revision: D27071600

Pulled By: xutaima

fbshipit-source-id: bfe72005190d7936caeef4f805bd99c8d2cf9c37",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1810,Jongsoo Park,jongsoo@fb.com,2021-03-17 20:02:30-07:00,edcef1306b48e7fa9bf84dcbec25171a1e57a5dc,https://github.com/pytorch/fairseq/commit/edcef1306b48e7fa9bf84dcbec25171a1e57a5dc,"make deepspeed cpu_adam works in fbcode

Summary: To test cpu-offload + fsdp in fairseq

Differential Revision: D26873232

fbshipit-source-id: 8d4dee874713a055bb6b6541cddcbfd722eef9f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1811,Juan Miguel Pino,juancarabina@fb.com,2021-03-19 09:35:09-07:00,53b781caad3d58cd7fefed80e17faf147d15da66,https://github.com/pytorch/fairseq/commit/53b781caad3d58cd7fefed80e17faf147d15da66,"Add --update-freq 8 to simulst tutorial (#3374)

Summary:
Clarify that training is done on 1 GPU.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3374

Reviewed By: kahne

Differential Revision: D27183474

Pulled By: jmp84

fbshipit-source-id: 330ec9b6510dcbd1f38a7c1c954d7504c6de3dda",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1812,Myle Ott,myleott@fb.com,2021-03-20 15:00:56-07:00,5c87bb5ce81dbc051a37e50bca3da40633149f26,https://github.com/pytorch/fairseq/commit/5c87bb5ce81dbc051a37e50bca3da40633149f26,"Fix RoBERTa + FSDP (also minor fix for GPT-3 configs) (#1724)

Summary:
There will also be a fix on the fairscale side to fix the segfault with FSDP.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1724

Reviewed By: sshleifer

Differential Revision: D27189594

Pulled By: myleott

fbshipit-source-id: 7a0ccadf8e2104cc782faccb55756deddb2dd346",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1813,Sam Shleifer,sshleifer@gmail.com,2021-03-20 16:40:08-07:00,8f77e24cf184c9762ed48acb43e9bd5daba550b1,https://github.com/pytorch/fairseq/commit/8f77e24cf184c9762ed48acb43e9bd5daba550b1,"Deepspeed can be used outside of fbcode (#1727)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1727

Reviewed By: myleott

Differential Revision: D27213955

Pulled By: sshleifer

fbshipit-source-id: be84e7f7c1c55c407ee7445fad9b3026a79763fb",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1814,Myle Ott,myleott@fb.com,2021-03-20 19:25:54-07:00,5273bbb7c18a9b147e3f0cfc97121cc945a962bd,https://github.com/pytorch/fairseq/commit/5273bbb7c18a9b147e3f0cfc97121cc945a962bd,"Fix transformer LM arg upgrade logic (#1717)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1717

Reviewed By: alexeib

Differential Revision: D27156919

Pulled By: myleott

fbshipit-source-id: af3c2e41464c04a7808f40894e7b0106798e4822",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1815,Myle Ott,myleott@fb.com,2021-03-22 13:45:48-07:00,bd0109cdc66edbd01e7362d41e5997f85afbde7d,https://github.com/pytorch/fairseq/commit/bd0109cdc66edbd01e7362d41e5997f85afbde7d,"Revert change in defaults for LMs to learned pos embeddings (#1734)

Summary:
This was premature. Leaving it to ``True`` for GPT-3 configs, but reverting back to ``False`` in general.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1734

Reviewed By: shruti-bh

Differential Revision: D27233834

Pulled By: myleott

fbshipit-source-id: 597b36f94f28d59834f1d68ab1dd2991e82c1e32",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1816,Sam Shleifer,sshleifer@gmail.com,2021-03-22 20:51:32-07:00,8c14a8f7dfcd18abd983491fc2207ac634d25759,https://github.com/pytorch/fairseq/commit/8c14a8f7dfcd18abd983491fc2207ac634d25759,"--nval only validate for a few steps (#1735)

Summary:
Afaict, it's easy to set --max-update 4 to run training quickly, but it's hard to control validation without changing the data.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1735

Reviewed By: myleott

Differential Revision: D27246062

Pulled By: sshleifer

fbshipit-source-id: 30a210cbbb45791647a050f49e6f38fbacd0d988",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1817,Taylan Bilal,taylanbil@gmail.com,2021-03-22 21:29:42-07:00,1bba712622b8ae4efb3eb793a8a40da386fe11d0,https://github.com/pytorch/fairseq/commit/1bba712622b8ae4efb3eb793a8a40da386fe11d0,"Enable w2v2 tpu (#3328)

Summary:
This enables training wav2vec 2.0 models on TPUs courtesy of taylanbil

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3328

Reviewed By: myleott

Differential Revision: D27127542

Pulled By: alexeib

fbshipit-source-id: b402c58f812c3c36edaaa88fdbe20e37fae3d4f3",15,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1818,Liang Tan,liangtan@fb.com,2021-03-24 20:28:29-07:00,25dd9266809429c41833c151c5c09398f9f9ca9e,https://github.com/pytorch/fairseq/commit/25dd9266809429c41833c151c5c09398f9f9ca9e,"Fix the issue that manifold raises error when reading non-existing file

Summary:
1. Fairseq's expected reading behavior is ""return None when accessing non-existing file""
2. However, when reading from manifold, manifold will raises error when accessing non-existing file
3. Here I add try-except block to bypass the manifold error

Reviewed By: myleott

Differential Revision: D27300619

fbshipit-source-id: 252606c82e9810516ccfb0705e08297f646b3708",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1819,Changhan Wang,changhan@fb.com,2021-03-25 15:25:07-07:00,06c9cefed73cfd43f9453c616e1b9d3ef63f58cf,https://github.com/pytorch/fairseq/commit/06c9cefed73cfd43f9453c616e1b9d3ef63f58cf,"update CoVoST2 recipe; pretrained S2T encoder loading

Summary:
- Update CoVoST2 recipe
  - Add back label smoothing and increase dropout for En ASR
- Update pretrained S2T encoder loading
  - Omit non-existing pre-training checkpoint (this occurs when we distribute only the ST checkpoint without the ASR checkpoint it leverages for pre-training)

Related github issue: https://github.com/pytorch/fairseq/issues/3364

Reviewed By: jmp84

Differential Revision: D27159799

fbshipit-source-id: 3759926d68bd3f41f9f8e5fe5004f508eb24c2c0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1820,Weiyi Zheng,wyz@fb.com,2021-03-25 16:04:59-07:00,2d5eaa0e7b795633b7efd2645f26ab40b9b520d5,https://github.com/pytorch/fairseq/commit/2d5eaa0e7b795633b7efd2645f26ab40b9b520d5,"restore original module forward() during jit

Summary:
checkpoint_wrapper replaces m.forward with functools.partial(), but functools.partial is not compatible with jit. so we need to find a way to restore the original forward function for jit to pick it up correctly.

I decided to add one extra attribute 'precheckpoint_forward' to the module. and then it's up to the jit workflow to restore this 'precheckpoint_forward' function to the original forward location for jitting.

Also added a check that checkpoint_wrapper can't be called twice on the same module.

Reviewed By: myleott

Differential Revision: D27303905

fbshipit-source-id: 75bf3b5858ed130598dcc21d85a48b43c661818c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1821,Ning Dong,dnn@fb.com,2021-03-25 23:29:35-07:00,6e91e226441fc3c68adf91bdef9f39d9d9dc2c9c,https://github.com/pytorch/fairseq/commit/6e91e226441fc3c68adf91bdef9f39d9d9dc2c9c,"Script monotonic attention and related modules

Summary: Add types and rewrite some part of the model so JIT compiler likes it

Reviewed By: jmp84, sravyapopuri388

Differential Revision: D27194261

fbshipit-source-id: 594532212c907ed97fc711e4f6a2a211d7e2b67e",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1822,Ning Dong,dnn@fb.com,2021-03-25 23:29:35-07:00,93bb1905ac56a687d9a550537704cb339b226a9e,https://github.com/pytorch/fairseq/commit/93bb1905ac56a687d9a550537704cb339b226a9e,"Script SimulConvTransformerModel

Summary:
Copy & paste forward function from parent classes to ConvTransformerEmformerEncoder / TransformerMonotonicDecoderLayer, and modify slightly to make JIT compiler happy.

TS in general doesn't work well with polymorphism+inheritance.

Reviewed By: jmp84, sravyapopuri388

Differential Revision: D27194275

fbshipit-source-id: 5248f017ac86adcb09f398038c10a0d20bc03453",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1823,Ning Dong,dnn@fb.com,2021-03-25 23:29:35-07:00,a28511d43d2310226086c5fee82042fcdcf4cff0,https://github.com/pytorch/fairseq/commit/a28511d43d2310226086c5fee82042fcdcf4cff0,"Ad-hoc changes

Summary: Some ad-hoc changes to address errors when torchscripting simul ST models. Not sure if all of them are necessary and what the best practice is (yet) but having this diff to temporarily unblock.

Reviewed By: sravyapopuri388

Differential Revision: D27194569

fbshipit-source-id: 8936f3edb408df7e1a3fd97c0e07b2356ff0d9b4",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1824,Sam Shleifer,sshleifer@gmail.com,2021-03-26 07:18:08-07:00,be1d186fa59aa19d7a0735a32af88b5a2bacc5ae,https://github.com/pytorch/fairseq/commit/be1d186fa59aa19d7a0735a32af88b5a2bacc5ae,"FSDP uses new optimizer gathering to save optimizer state (#1744)

Summary:
- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.
- requires master version of fairscale (eventually fairscale>=0.3.3)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744

Reviewed By: myleott

Differential Revision: D27342305

Pulled By: sshleifer

fbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1825,Ning Dong,dnn@fb.com,2021-03-26 21:59:51-07:00,0975816a853eed81059db9ca02ddd3c73ea64926,https://github.com/pytorch/fairseq/commit/0975816a853eed81059db9ca02ddd3c73ea64926,"Add back try/except in data_utils

Summary: Accidentally removed try/except block in D27194569 (https://github.com/pytorch/fairseq/commit/a28511d43d2310226086c5fee82042fcdcf4cff0) and caused OSS unit test failure.

Reviewed By: myleott

Differential Revision: D27374072

fbshipit-source-id: 154c92eae106aa3bf6b406d52a8647db140dc6b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1826,Wei-Ning Hsu,31931787+wnhsu@users.noreply.github.com,2021-03-29 12:09:59-07:00,1c9738c6e9ae2e6612e3f8e4f841d22fbfbfa68c,https://github.com/pytorch/fairseq/commit/1c9738c6e9ae2e6612e3f8e4f841d22fbfbfa68c,"fix speech_recognition hydra decoder bugs (#1742)

Summary:
## What does this PR do?
Bug 1: generated hypotheses and references transcripts are incomplete when using data_parallel_world_size > 1
Reason: lack of barrier to ensure all workers completes dumping transcripts before starting merging transcripts

Bug 2: program failed when using data_parallel_world_size == 1
Reason: unnecessary reduce operation is introduced / transcripts do not need to be merged

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1742

Reviewed By: alexeib

Differential Revision: D27403362

Pulled By: wnhsu

fbshipit-source-id: b74889660c7253264b986ea35c248d80e0e32358",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1827,Michael Lewis,mikelewis@fb.com,2021-03-29 18:02:07-07:00,7dafb05754fe268bb5f76a1c97cf3a14062f44e5,https://github.com/pytorch/fairseq/commit/7dafb05754fe268bb5f76a1c97cf3a14062f44e5,"BASE layers (#1654)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654

Reviewed By: myleott

Differential Revision: D27128074

Pulled By: shruti-bh

fbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3",16,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1828,Guillaume Wenzek,guw@fb.com,2021-03-30 09:54:22-07:00,c2e8904b6072d8eddab362ac50b324e374b5951d,https://github.com/pytorch/fairseq/commit/c2e8904b6072d8eddab362ac50b324e374b5951d,"Obt 2 (#1614)

Summary:
# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.m)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?
  too many of them actually ^^

## What does this PR do?
This is a rewrite of https://github.com/fairinternal/fairseq-py/issues/1538 following the discussion there, and taking into account the proposed https://github.com/fairinternal/fairseq-py/issues/1560 from Myle.
it brings online backtranslation to fairseq.
It adds a RobertaEncDec to fairseq. RobertaEncDec can be built from a pretrained Roberta model allowing to do transfer learning. This is crucial for backtranslation.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1614

Reviewed By: myleott

Differential Revision: D27157296

Pulled By: gwenzek

fbshipit-source-id: 43020bc27743419bd4b138716165bf5764117c21",16,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],2,42,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class OnlineBacktranslationTest(unittest.TestCase):', 'class RobertaTest(unittest.TestCase):']","[('Equal', '('), ('Equal', '(sample[][0], en_bos)'), ('Equal', '(sample[][0], en_bos)'), ('Equal', '('), ('Equal', '(train[i][][0].item(), zh_bos)'), ('Less', '('), ('Equal', '(sample_0[], 0)'), ('Equal', '(sample_9[], 9)'), ('Equal', '(sample_0[][0], en_bos)'), ('Equal', '(sample_9[][0], en_bos)'), ('Equal', '(fn_x, y, f)'), ('FnMatch', '('), ('FnMatch', '('), ('FnMatch', '('), ('FnMatch', '('), ('Equal', '(t1.size(), t2.size(), )'), ('Equal', '(t1.ne(t2).long().sum(), 0)'), ('Equal', '(((t2 - t1).abs() > delta).long().sum(), 0)'), ('Equal', '(group_ids, {name: shared_id for name in group})'), ('NotIn', '(shared_id, ids)'), ('Sharing', '('), ('Sharing', '('), ('Sharing', '('), ('Sharing', '('), ('Sharing', '('), ('Equal', '(max_pos, 256)'), ('Equal', '(max_pos, model.decoder.max_positions())'), ('Equal', '(max_pos, model.encoder.max_positions())'), ('Equal', '(max_pos, model.encoder.embed_positions.max_positions)'), ('Equal', '(list(sample[]), [max_pos])'), ('Equal', '(len(sample[][0]), max_pos)'), ('Equal', '(x.shape, (1, max_pos, VOCAB_SIZE))'), ('Equal', '(logits.shape, (bs, l, VOCAB_SIZE))'), ('Equal', '(z.shape, (slen, 1, 12))'), ('Equal', '(z2.shape, (slen, 2, 12))'), ('TensorEqual', '(logits2[0], logits2[1])'), ('TensorEqual', '(logits[0], logits2[0])'), ('Equal', '(ro_dec.shape, (bs, tgt_len, VOCAB_SIZE))'), ('TensorEqual', '(ro_dec[0], ro_dec[1])'), ('Equal', '(ro.shape, (bs, 1, VOCAB_SIZE))'), ('TensorEqual', '(ro_dec_inc[l][0], ro_dec_inc[l][1])'), ('TensorEqual', '(ro_dec_inc[l][:, 0], ro_dec[:, l])')]",[],[],[],[],[],[],[],[],"['import unittest', 'import unittest']",[],[],[],[],[],[],[],[],[],[],[],"['output.exists()', 'output.with_suffix().exists()', 'len(languages) >= 2', 'obt._lang_token() in task.dictionary', 'obt._lang_token() in task.dictionary', 'obt._lang_token() in task.dictionary', ')', ')', 'task.get_bos_token_from_sample(zh_sample) == en_bos', 'target_zh == [16, 14, 12, 10]  # original zh sentence', 'generated_en[0] == en_bos', 'valid is not None', 'len(self.dictionary) == VOCAB_SIZE', 'device in ()', 'device in ()']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1829,Xutai Ma,xutaima@gmail.com,2021-03-30 10:09:30-07:00,229de0087f599e31986c85c8106456f0adf44812,https://github.com/pytorch/fairseq/commit/229de0087f599e31986c85c8106456f0adf44812,"Fix an issue for waitk when left_pad_source is set (#1752)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1752

Reviewed By: jmp84

Differential Revision: D27370799

Pulled By: xutaima

fbshipit-source-id: 1ba1ef529af5dbf6608d3029b7545392458bd827",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1830,Weiyi Zheng,wyz@fb.com,2021-03-30 10:34:40-07:00,579a48f4be3876082ea646880061a98c94357af1,https://github.com/pytorch/fairseq/commit/579a48f4be3876082ea646880061a98c94357af1,"pull unwrap_checkpoint() into library for reuse

Summary: there was not a central place that model.jit() calls into, so we had to pull the logic out of pyspeech, and make it available in fairseq library.

Reviewed By: myleott

Differential Revision: D27349919

fbshipit-source-id: f486c11fc840d4d13a4b9265ec2e7f5cc770216c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1831,Xutai Ma,xutaima@gmail.com,2021-03-31 17:08:08-07:00,14807a361202ba34dbbd3a533899db57a0ebda19,https://github.com/pytorch/fairseq/commit/14807a361202ba34dbbd3a533899db57a0ebda19,"Update simultaneous translation docs (#1767)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1767

Test Plan:
Imported from GitHub, without a `Test Plan:` line.

This pull request contains
- An example of EN_JA simul t2t model
- Reorganizing simul trans docs
- Removal of out-of-date files

Reviewed By: jmp84

Differential Revision: D27467907

Pulled By: xutaima

fbshipit-source-id: 137165b007cf5301bdc51a0a277ba91cbf733092",20,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1832,Xutai Ma,xutaima@gmail.com,2021-04-02 14:44:20-07:00,a20dc364647c94417f493ba0b0c8d1e1834e67eb,https://github.com/pytorch/fairseq/commit/a20dc364647c94417f493ba0b0c8d1e1834e67eb,"Several updates on simul st (#1774)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1774

Reviewed By: jmp84

Differential Revision: D27529935

Pulled By: xutaima

fbshipit-source-id: 35433cc1d862440ea110e084007ba187149bc193",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1833,Jongsoo Park,jongsoo@fb.com,2021-04-02 19:51:12-07:00,aa5f0119a383e013e56ae5d88e4a7aff0e67f0f9,https://github.com/pytorch/fairseq/commit/aa5f0119a383e013e56ae5d88e4a7aff0e67f0f9,"remove import logging (#1779)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1779

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3436

Remove residue of changes not meant to be landed in D26873232 (https://github.com/pytorch/fairseq/commit/edcef1306b48e7fa9bf84dcbec25171a1e57a5dc)

Reviewed By: myleott

Differential Revision: D27543742

fbshipit-source-id: ebe47baba27ec2446ef0b855d700b68373f738d8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1834,Myle Ott,myleott@fb.com,2021-04-07 16:03:55-07:00,8a42c243a7835f1f0da06efd323ddf6201eb1272,https://github.com/pytorch/fairseq/commit/8a42c243a7835f1f0da06efd323ddf6201eb1272,"Fix memory regression when using FSDP (#1788)

Summary:
FSDP overloads nn.Module.apply and inserts a step where it all-gathers params before calling apply (added in https://github.com/facebookresearch/fairscale/commit/fa1b85fbbe75f058b39f1bcf027de42e6ddbd487). This is important for the typical use case of nn.Module.apply -- weight initialization. But here we were using apply totally unnecessarily. It's easier to just loop over all the modules and call set_num_updates directly

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1788

Reviewed By: sshleifer

Differential Revision: D27622168

Pulled By: myleott

fbshipit-source-id: f5462107ad251cf7834b20a0eaccbe2f685da8f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1835,Guillaume Wenzek,guw@fb.com,2021-04-07 19:57:12-07:00,ee0d5a0f65a25e5f5372776402aac5cb9c4adbf1,https://github.com/pytorch/fairseq/commit/ee0d5a0f65a25e5f5372776402aac5cb9c4adbf1,"fixup Obt 2 (#1614) (#1791)

Summary:
restore len() checking that was lost in a merge

# Before submitting

- [x] Was this discussed/approved via a Github issue?
    This is a fix for https://github.com/fairinternal/fairseq-py/issues/1614. The regression was identified in https://github.com/pytorch/fairseq/issues/3364
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes regression in https://github.com/fairinternal/fairseq-py/pull/1614/files#diff-6e65327f729a8658d627b762ec14902e25927698f35e5495e6b8e3a1bfcfd7afR886-R943
This changes was meant to be a no-op but I forgot the len checking.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1791

Reviewed By: jmp84

Differential Revision: D27629177

Pulled By: gwenzek

fbshipit-source-id: fe6fdd486b1a61f86547d1214180d7fd3042e51b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1836,Xutai Ma,xutaima@gmail.com,2021-04-12 09:26:40-07:00,acf312418e4718996a103d67bd57516938137a7d,https://github.com/pytorch/fairseq/commit/acf312418e4718996a103d67bd57516938137a7d,"'Fix a bug when src len is smaller than wait k lagging (#1795)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1795

Reviewed By: jmp84

Differential Revision: D27701022

Pulled By: xutaima

fbshipit-source-id: 2267402077fbc7c560e260f1ca730ba102d0c7cd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1837,Alex Xiao,axiao@fb.com,2021-04-14 01:49:14-07:00,57560cb52c3a86fadbf315e24bc4ec174056a444,https://github.com/pytorch/fairseq/commit/57560cb52c3a86fadbf315e24bc4ec174056a444,"enable manifold checkpoints with --keep-interval-updates

Summary: Useful to enable --keep-interval-updates with Manifold checkpoints

Reviewed By: myleott

Differential Revision: D27577116

fbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1838,Alex Xiao,axiao@fb.com,2021-04-14 01:49:14-07:00,dbfb7103bdc41cdaa9bfea4cf5dedd10c9750647,https://github.com/pytorch/fairseq/commit/dbfb7103bdc41cdaa9bfea4cf5dedd10c9750647,"add keep_interval_updates_pattern

Summary:
Motivation:

I want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.

To save frequently, I can use a small --save-interval-updates.

To delete old checkpoints to save storage, I can use --keep-interval-updates.

However, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:

```
--save-interval-updates 1000
--keep-interval-updates 1
--keep-interval-updates-pattern 10000
```

This will:
1. checkpoint every 1000 updates so that job restarts don't impact us significantly
2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold
3. make an exception for #2 for every 10k updates so we can track WER over time

Reviewed By: myleott

Differential Revision: D27578403

fbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1839,Guillaume Wenzek,guw@fb.com,2021-04-14 04:59:04-07:00,436166a00c2ecd1215df258f022608947cca2aa8,https://github.com/pytorch/fairseq/commit/436166a00c2ecd1215df258f022608947cca2aa8,"fix MultiHeadAttention assert (#1798)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/fairinternal/fairseq-py/issues/1538.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1798

Reviewed By: myleott

Differential Revision: D27710902

Pulled By: gwenzek

fbshipit-source-id: 2efdf645bb30e4cf6653c48371bfca8df6f94eaf",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TransformerTestCase(unittest.TestCase):'],[],[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1840,Sujit Verma,sujitv@fb.com,2021-04-14 21:53:25-07:00,fc90910314ce02fe168bdbe127e08a14e944a6fd,https://github.com/pytorch/fairseq/commit/fc90910314ce02fe168bdbe127e08a14e944a6fd,"Migrating fairseq-py from fvcore to iopath.

Summary: Migrating fairseq-py from fvcore to iopath.

Reviewed By: myleott

Differential Revision: D27109864

fbshipit-source-id: 041177c1bc9b5793b2ce0ecab87692097f3f353b",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('IsNone', '(IOPathManager)')]",[],[],[],[],[],[],[],[],[],[],"[('IsNone', '(IOPathPathManager)'), ('IsNotNone', '(IOPathPathManager)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1841,Sujit Verma,sujitv@fb.com,2021-04-15 16:59:08-07:00,069763813097aa814c8c4e12d4cab4b321575b8d,https://github.com/pytorch/fairseq/commit/069763813097aa814c8c4e12d4cab4b321575b8d,"Fix bug from iopath migration.

Summary: Fix bug from iopath migration.

Reviewed By: myleott, shuliuncsu

Differential Revision: D27808039

fbshipit-source-id: 14b6c9ca3a461b00c2528d55b7269980324b3e10",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1842,Shu Liu,shuliu@fb.com,2021-04-16 15:11:19-07:00,3a90a859d4dfdbf13f15399be12a1928aa2c54ff,https://github.com/pytorch/fairseq/commit/3a90a859d4dfdbf13f15399be12a1928aa2c54ff,"Add manifold support for fairseq file_io

Summary: Recently fairseq file_io migrated to iopath in D27109864 (https://github.com/pytorch/fairseq/commit/fc90910314ce02fe168bdbe127e08a14e944a6fd). New IOPathManager doesn't have manifold support by default. Add manifold handler to fix the issue.

Reviewed By: myleott

Differential Revision: D27809504

fbshipit-source-id: 5cbf4440ed734132f865096c45cd3e47ccb6142d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1843,Yun Wang,yunwang@fb.com,2021-04-17 02:02:52-07:00,89371294e54ef8c306f19733f2e8bab8233c401e,https://github.com/pytorch/fairseq/commit/89371294e54ef8c306f19733f2e8bab8233c401e,"TALNet: Consistency loss

Summary:
During training, for each batch, apply two different data augmentations, and require the model output to be similar. The dissimilarity between the output is used as an extra loss term.

To apply two different augmentations to the same batch, we need to create batches containing two copies of the same indices, like [a, b, c, a, b, c].
I make this the responsibility of the **batch sampler**.
TALNet uses a `DataBalancingEpochBatchIterator` to generate batches. This iterator creates a `_batch_sampler` within itself; this diff adds a `dups` argument to generate duplicate indices.
If you're using the generic `EpochBatchIterator`, which accepts a batch sampler directly as input, you will need to modify the code of your own batch sampler to generate duplicate indices.
This is actually quite easy: if `batch_sampler(n)` creates a generator that returns n indices at a time, then `(batch * 2 for batch in batch_sampler(n // 2))` creates a generator that returns two copies of n/2 indices at a time.

The consistency loss is implemented by the `consistency_loss` function.
To use it, call it in your model's `forward` method and put the result in the return dict.
Then expose the loss in the `get_losses` or `get_extra_losses` method of your model, so criterion objects can read them.

Two types of consistency losses are provided:
1. KL divergence;
2. Squared difference of the probabilities.

I've observed significant gains in TALNet's MAP with both types; KL divergence is slightly better.

Reviewed By: nayansinghal

Differential Revision: D27179650

fbshipit-source-id: 76899335b029fab67bbd7941429c9bd1baf52d65",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1844,Ning Dong,dnn@fb.com,2021-04-19 03:50:13-07:00,3dd3940dc6105c8bf50f3a1be810cc035d2f105d,https://github.com/pytorch/fairseq/commit/3dd3940dc6105c8bf50f3a1be810cc035d2f105d,"Support more p_choose strategies in FixedStrideMonotonicAttention

Summary:
Earlier polymorphism worked in the class by inheritance (calling into super().p_choose) however super() is not TS friendly.

This diff moves p_choose() methods to a separate util file and uses a variable to toggle between different strategies.

Reviewed By: jmp84, sravyapopuri388

Differential Revision: D27827168

fbshipit-source-id: 3e85028796e3af0c02f1d77b93a2a3896825b9b1",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1845,Robin Jia,robinjia@fb.com,2021-04-19 15:51:59-07:00,4fc9f2be952d8f0cd476832911dc71f59b4079da,https://github.com/pytorch/fairseq/commit/4fc9f2be952d8f0cd476832911dc71f59b4079da,"Fix order issue in batched BART generation (#1785)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1785

Reviewed By: robinjia

Differential Revision: D27831673

Pulled By: sshleifer

fbshipit-source-id: 1acf142151853d24138889c956df4531dae794a2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1846,Sam Shleifer,sshleifer@gmail.com,2021-04-19 16:30:32-07:00,f6f220e917a0745bad5cc2dffdb35590f5feed8e,https://github.com/pytorch/fairseq/commit/f6f220e917a0745bad5cc2dffdb35590f5feed8e,"Delete line that breaks gh ci (#1814)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1814

Reviewed By: myleott

Differential Revision: D27867552

Pulled By: sshleifer

fbshipit-source-id: ed30e02c962b31797e003cb810c085934a53202c",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],"[('IsNone', '(IOPathManager)')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1847,Alex Xiao,axiao@fb.com,2021-04-20 12:17:11-07:00,801a64683164680562c77b688d9ca77fc3e0cea7,https://github.com/pytorch/fairseq/commit/801a64683164680562c77b688d9ca77fc3e0cea7,"allow subclasses of multi corpus dataset to not specify full_id

Summary: This diff allow subclasses of multi corpus dataset to not specify full_id, which is only a useful feature for sampling batches between datasets. This is not possible for certain cases, such as minibatch CE training.

Reviewed By: zdavid1995

Differential Revision: D27833104

fbshipit-source-id: e60f6ad200c0ca69915b9588405320ba2ecfbd0a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1848,Sam Shleifer,sshleifer@gmail.com,2021-04-21 06:38:03-07:00,da0432a3cd1ddf8f9797af83880570822265620b,https://github.com/pytorch/fairseq/commit/da0432a3cd1ddf8f9797af83880570822265620b,"MultiGPU test and --log-file workaround (#1793)

Summary:
The initial problem I set out to solve was that it's not easy to add a multigpu test. I solved that problem but it ruined log capturing, both with `self.assertLogs` and `with contextlib.redirect_stdout(StringIO())`.

After some brief digging, I gave up on trying to get those to work, and added support for `--log-file AGI_v0.log` which will write the `progress_bar.log()` statements to `log-file` as well as `stdout`. This functionality is used by the resumption test.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1793

Reviewed By: myleott

Differential Revision: D27671192

Pulled By: sshleifer

fbshipit-source-id: bcba5f9df7a965889a4cd6993f7eeb0f14b770c6",7,False,True,True,True,True,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],[],[],[],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],[],"['os.path.exists(log)', 'os.path.exists(', 'int(l2[0][', '(']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1849,Sujit Verma,sujitv@fb.com,2021-04-21 09:04:56-07:00,2af4ffe77b230a0a228af9be09e1c0e9d4731906,https://github.com/pytorch/fairseq/commit/2af4ffe77b230a0a228af9be09e1c0e9d4731906,"Supporting PathManager in cached_path util.

Summary: Supporting PathManager in cached_path util.

Reviewed By: myleott

Differential Revision: D27895813

fbshipit-source-id: 68d7345ebb50737c53a72dcd467536f86b61e1dd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1850,Michael Anderson,anderso2@fb.com,2021-04-21 09:08:06-07:00,207254bf56374831d08c20064ca7a2740a871ce5,https://github.com/pytorch/fairseq/commit/207254bf56374831d08c20064ca7a2740a871ce5,"Adding check for filler size (#3495)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/3495

Avoid creating size-0 tensor ""filler"" in case src_len is the same as key_padding_mask_size or prev_key_padding_mask_size

Reviewed By: jackm321

Differential Revision: D27897778

fbshipit-source-id: 26fd95852da2cd932717c7abcac3e1fb43deaf77",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1851,Sam Shleifer,sshleifer@gmail.com,2021-04-21 15:49:03-07:00,05b86005bcca0155319fa9b81abfd69f63c06906,https://github.com/pytorch/fairseq/commit/05b86005bcca0155319fa9b81abfd69f63c06906,"Fix FSDP optim state loading (#1819)

Summary:
### Problem:
- if we consolidate optim state dict on rank 0, rank 1+ save `optimizer.state_dict()`. When they try to load, they call get_shard(last_optim_state), which is wrong since the optim state is already shared. They should find the global consolidated optimizer state dict and load that.

### Possible Solutions:
- if world size is the same, you could just reuse the local OSD.
- [this PR] rank 1+ load optim state from the rank0 file and call get_shard
- separate file for optim_state that every rank loads. (like 'shared.pt' on `gshard-azure`). This will save some CPU Ram.

### Note:
- I don't think it's possible to pass `--use-sharded-state` from the command line. It should be I think.

### Implementation here
+ if FSDP saves -1 as state['last_optimizer_key'], it means that, on load, rank 0's optim state must be loaded.
+ regression test

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1819

Reviewed By: zhengwy888

Differential Revision: D27910281

Pulled By: sshleifer

fbshipit-source-id: d34987008f77ce7e0cb28b7224dd2aabed38a70c",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],['os.path.exists('],[],[],[],[],[],[],[],[],[],[],[],[]
1852,Jeffrey Karres,jkarres@fb.com,2021-04-22 21:32:09-07:00,40f6c758b361adc7b87fa553f6f439daa4ee9501,https://github.com/pytorch/fairseq/commit/40f6c758b361adc7b87fa553f6f439daa4ee9501,"sorting os.listdir outputs to ensure consistent import ordering

Summary:
There's a common idiom of doing imports based on the output of `os.listdir(os.path.dirname(__file__))`.  In this idiom, imports are performed based on the order of directories in that output.

This is dangerous, because the behavior of your program depends on the order of those files, but the order of those files is not guaranteed.

Reviewed By: zhengwy888

Differential Revision: D27951383

fbshipit-source-id: 97dc9a0b7d853886e19a9643c33508f146c71617",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1853,ngoyal2707,ngoyal2707@users.noreply.github.com,2021-04-26 13:14:41-07:00,b0ae834d528a4a466202107a22356aed71bb6161,https://github.com/pytorch/fairseq/commit/b0ae834d528a4a466202107a22356aed71bb6161,"Flores pretrained model release (#1825)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1825

Reviewed By: huihuifan

Differential Revision: D28002947

fbshipit-source-id: 2ba69a72431db5aa7aef890309eefb0fb31c7ad6",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1854,lematt1991,lematt1991@gmail.com,2021-04-28 05:56:31-07:00,8b861beae282ec5dd5051686440948a3f893c3ec,https://github.com/pytorch/fairseq/commit/8b861beae282ec5dd5051686440948a3f893c3ec,"Escape % in `keep_interval_updates_pattern` help description (#3514)

Summary:
This leads to the following error:

```
ValueError: unsupported format character 'k' (0x6b) at index 95
```

Resolves https://github.com/pytorch/fairseq/issues/3491

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3514

Test Plan:
`fairseq-train --help`

Produces:

```
...
  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN
                        when used with --keep-interval-updates, skips deleting any checkpoints with update X where X % keep_interval_updates_pattern == 0
...
```

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).
https://github.com/pytorch/fairseq/issues/3491
## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D28037028

Pulled By: lematt1991

fbshipit-source-id: b237a151b82e851954ad3ea51a0c4a14c572ffab",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1855,Naman Goyal,namangoyal@learnfair0732.h2.fair,2021-04-28 10:44:50-07:00,1305008e97872335d4ae8de4d015ddf8c43e87df,https://github.com/pytorch/fairseq/commit/1305008e97872335d4ae8de4d015ddf8c43e87df,"fixed typo in flores model download link (#1827)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1827

Reviewed By: ngoyal2707

Differential Revision: D28060291

Pulled By: lematt1991

fbshipit-source-id: 2540eb2a7d6a1fe37af9a3e9b4ed3df9e05a0823",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1856,Sravya Popuri,spopuri@fb.com,2021-04-28 23:22:25-07:00,c6409c029dc9b3af5308a269ec5c68dbdbafdc78,https://github.com/pytorch/fairseq/commit/c6409c029dc9b3af5308a269ec5c68dbdbafdc78,"Fix issue with encoder padding mask

Summary:
- Fix issue with encoder padding mask
- Also add lengths as a field in encoder_out of encode_src method
- Add a conditional clause in transformer_monotonic_attention.py to handle the case where encoder_padding_mask is None

Reviewed By: jmp84

Differential Revision: D28080936

fbshipit-source-id: 99f78c5e3fe5644960ade44210ea78280ef53b8c",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1857,Ruslan Mavlyutov,mavlyutov@fb.com,2021-04-29 10:24:58-07:00,9cb6fe4c93c17f1d4e327ea345cd1c653432c76c,https://github.com/pytorch/fairseq/commit/9cb6fe4c93c17f1d4e327ea345cd1c653432c76c,"Copy to local before loading checkpoint

Summary: Follow-up for ""Fix FSDP optim state loading (#1819)"". Update for remote file systems.

Reviewed By: sshleifer

Differential Revision: D28088088

fbshipit-source-id: 5d2f3ea5084fbbb21564d053317d2c07565cf2bc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1858,Yun Wang,yunwang@fb.com,2021-04-29 16:16:09-07:00,d6855baec88f99ac776962027b91d404fe917eea,https://github.com/pytorch/fairseq/commit/d6855baec88f99ac776962027b91d404fe917eea,"Simplify CountingIterator

Summary:
Simplify the implementation of `CountingIterator`, and added test cases.

The old implementation could fail on such a test case:
```
ref = list(range(10))
itr = CountingIterator(ref)
first_item = next(itr)  # consume one item
remaining_items = list(itr)  # raises exception because of ""length mismatch""
```
This happens because `list(itr)` invokes `itr.__iter__` and reiterate the underlying list from the start, but `itr.n` has been already incremented by `next(itr)`.

The new implementation is simpler and avoids such an error.

Reviewed By: myleott

Differential Revision: D27802505

fbshipit-source-id: c97fd0a27d865c0ff3b24016fa6aa0afabbf0a73",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,10,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(itr.n, 8)'), ('Equal', '(list(itr), [ref[8], ref[9]])'), ('False', '(itr.has_next())'), ('Equal', '(list(itr), ref[:8])'), ('Raises', '(IndexError, list, itr)'), ('Equal', '(len(itr), len(list(iter(itr))))'), ('Equal', '(len(itr), 5)'), ('Equal', '(next(itr), ref[0])'), ('Equal', '(next(itr), ref[1])'), ('Equal', '(next(itr), ref[4])')]",[],[],[],[],[],[],[],[],[],[],"[('Equal', '(itr.n, 9)'), ('Equal', '(next(itr), ref[9])'), ('Equal', '(len(itr), len(list(iter(itr))))'), ('Equal', '(len(itr), 5)'), ('Equal', '(next(itr), ref[0])'), ('Equal', '(next(itr), ref[1])'), ('Equal', '(next(itr), ref[4])'), ('False', '(itr.has_next())')]",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1859,alexeib,alexei.b@gmail.com,2021-05-02 19:24:59-07:00,a4e1d4a3daf4f6f5557505026fd94b8716fba7b3,https://github.com/pytorch/fairseq/commit/a4e1d4a3daf4f6f5557505026fd94b8716fba7b3,"add binarized audio dataset for large datasets (#1840)

Summary:
adds a binarized audio dataset to prevent oom when training with very large datasets

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1840

Reviewed By: arbabu123

Differential Revision: D28137756

Pulled By: alexeib

fbshipit-source-id: bfe58e9d2bc9909b38876d78cc2e8aea783b3fed",7,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1860,alexeib,alexei.b@gmail.com,2021-05-04 18:27:48-07:00,a8c8f0be177649f8178fdcbae519c17894efd4d7,https://github.com/pytorch/fairseq/commit/a8c8f0be177649f8178fdcbae519c17894efd4d7,"fix wav2vec finetuning (#1848)

Summary:
fixes a regression from previous PR that removed line_inds from the raw audio dataset

this time we dont store line indices (which can be very large for big datasets) but instead store indices of skipped examples where applicable

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1848

Reviewed By: arbabu123

Differential Revision: D28198764

Pulled By: alexeib

fbshipit-source-id: 49580e09e6c1145b45c18802f4481d6df3de8cd2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1861,alexeib,alexei.b@gmail.com,2021-05-05 17:35:54-07:00,374fdc5cd94d361bb9b1089fe2c1d30a2eb15fdd,https://github.com/pytorch/fairseq/commit/374fdc5cd94d361bb9b1089fe2c1d30a2eb15fdd,"fix eval of older checkpoints (fixes #3528) (#1851)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1851

Reviewed By: michaelauli, arbabu123

Differential Revision: D28226892

Pulled By: alexeib

fbshipit-source-id: e07641dda46be2708e1f9d0c0cbc5b8dedaa92e7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1862,Alex Xiao,axiao@fb.com,2021-05-05 23:41:04-07:00,eb228ee74c6bc9803eb7dbd398d8cda16c55ccd2,https://github.com/pytorch/fairseq/commit/eb228ee74c6bc9803eb7dbd398d8cda16c55ccd2,"do per gpu seeding for shuffling batches with batch level sampling, fix bug with not disabling iterator cache

Summary:
we want to avoid the case where each gpu has a batch from the same dataset. not sure if this is happening right now, but to avoid this we can seed a shuffle by the GPU rank.

also fixed a bug where we need to reset iterator for the new batch sampling

Differential Revision: D28085750

fbshipit-source-id: 1643738b397d850f737fcd27c6398c216342464a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1863,Weiyi Zheng,wyz@fb.com,2021-05-07 12:31:59-07:00,14439b12ad37bd8b9e2b1383209603df996bd3f5,https://github.com/pytorch/fairseq/commit/14439b12ad37bd8b9e2b1383209603df996bd3f5,"add fp16 comm hook

Summary: speed up fp32 distributed training runs.

Reviewed By: myleott

Differential Revision: D28128720

fbshipit-source-id: 7855e4ecd43e194fd79e95bf9f35d4377a98779a",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1864,Weiyi Zheng,wyz@fb.com,2021-05-10 02:24:17-07:00,a2314b4e8a2a769eca073e87cef9e52f0e01ec08,https://github.com/pytorch/fairseq/commit/a2314b4e8a2a769eca073e87cef9e52f0e01ec08,"offload state_dict to cpu

Summary:
state_dict is summoned during checkpoint to GPU0. unfortunately with large models this will exceed single GPU memory limit. Moving it to cpu.
Question for Myle: should we set this option as default? when saving checkpoint the state_dict would eventually be moved to CPU any way.

Reviewed By: myleott

Differential Revision: D28203587

fbshipit-source-id: e738f48c83e35873c46bcec3471d105f2b4f4d8e",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1865,Sam Shleifer,sshleifer@gmail.com,2021-05-10 23:42:41-07:00,97969ac5f52090fd172f508975a3e9069c57e1af,https://github.com/pytorch/fairseq/commit/97969ac5f52090fd172f508975a3e9069c57e1af,"--combine-valid-sets (#1843)

Summary:
- `--combine-valid-sets` causes valid.bin, valid1.bin, ... to be concatenated. All metrics will be reported together.
- `--valid-subsets` works the same. If you pass `--valid-subsets valid1,valid2` you get valid1_loss and valid2_loss logged separately.
- if user passes `--valid-subset valid` (the default) and we see files named valid1, valid2 we raise an error. User must pass `--ignore-unused-valid-sets` to override. This previously led to valid1, valid2 being silently ignored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1843

Reviewed By: myleott

Differential Revision: D28323815

Pulled By: sshleifer

fbshipit-source-id: dfd46076d3f684e36f8dacfadd38fd0038ce6755",5,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],2,4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestValidSubsetsErrors(unittest.TestCase):', 'class TestCombineValidSubsets(unittest.TestCase):']","[('Raises', '(ValueError):'), ('Raises', '(ValueError):'), ('Raises', '(ValueError):'), ('Logs', '() as logs:')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],"['os.path.exists(path)', 'any([ in x for x in logs])  # loaded 100 examples from valid1', 'not any([ in x for x in logs])  # metrics are combined', 'any([ in x for x in logs])  # loaded 100 examples from valid1', 'any([ in x for x in logs])  # metrics are combined']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1866,Changhan Wang,changhan@fb.com,2021-05-12 09:01:06-07:00,8f1a34af7cb6e92fa3c443e61803e0ff347a784e,https://github.com/pytorch/fairseq/commit/8f1a34af7cb6e92fa3c443e61803e0ff347a784e,"add zip file support to raw_audio_dataset

Summary:
Add zip file support to raw_audio_dataset
- Allow reading WAV/FLAC/OGG audios from stored zip file with given byte offset and length
- Path format in the manifest TSV: `[zip_path]:[byte_offset]:[byte_length]` (e.g. `en/flac.zip:33255867035:212288`)
- Packing audios in small number of zip files facilitates file management and improves loading speed (avoiding random access of many small audio files)

Reviewed By: jmp84

Differential Revision: D28343999

fbshipit-source-id: a9cd2fbeb6e318cf9787065beb3bbddac25d0aba",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1867,Changhan Wang,changhan@fb.com,2021-05-12 18:03:01-07:00,d151f2787240cca4e3c7e47640e647f8ae028c37,https://github.com/pytorch/fairseq/commit/d151f2787240cca4e3c7e47640e647f8ae028c37,"S2T example bug fixes

Summary: S2T example bug fixes

Reviewed By: jmp84

Differential Revision: D27930414

fbshipit-source-id: 14edc85a34094a4fff53646390d366b39ffd8206",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1868,Weiyi Zheng,wyz@fb.com,2021-05-14 18:52:06-07:00,425c36eafff535fe7337f8bdd5ace22ebacc78cb,https://github.com/pytorch/fairseq/commit/425c36eafff535fe7337f8bdd5ace22ebacc78cb,"support use_sharded_state on command line

Summary:
we wanted to use sharded_state because
1. to save memory
2. support sharded state loading, which allows MoE models's weight to live on their respective shard
I just added the use_sharded_state as a config option, and added unit test to make sure it runs fine.

old revision's comment:
fairseq.FSDP has a  flag use_sharded_state, but I had to address a couple problems before being able to use it.
1. fairscale FSDP (FSDP for short) calls self.state_dict/load_state_dict, which has been overwritten by fairseq.FSDP, this is not a desired behavior
2. the optimizer states shouldn't be sharded again when use_sharded_state is True
3. expose this option on the command line.

Reviewed By: sshleifer

Differential Revision: D28375035

fbshipit-source-id: c2f59a9c62163405033f34ed595ba78528aea850",4,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1869,Mandeep Singh Baines,mandeep.baines@gmail.com,2021-05-20 14:04:37-07:00,a1fea2eb0e5a68c9f91b18a344056675332181a3,https://github.com/pytorch/fairseq/commit/a1fea2eb0e5a68c9f91b18a344056675332181a3,"enable pin_memory for DataLoaders (#3560)

Summary:
To avoid the creation of a cuda:0 context, I needed to make sure that the `BackgroundConsumer` thread had its cuda device context set to the correct GPU.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3560

Reviewed By: myleott

Differential Revision: D28573071

Pulled By: msbaines

fbshipit-source-id: c2bedf67d8f356a29fa82eb4d8f15983efce3ffc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1870,Sravya Popuri,spopuri@fb.com,2021-05-20 21:39:31-07:00,f68de08a7326d1915461b84ad8e6ccb979d39578,https://github.com/pytorch/fairseq/commit/f68de08a7326d1915461b84ad8e6ccb979d39578,"Auto formatting changes

Summary: TSIA

Reviewed By: jmp84

Differential Revision: D28523558

fbshipit-source-id: 97a90050e426be071f59127596a84bf71ede476d",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1871,alexeib,alexei.b@gmail.com,2021-05-20 23:20:14-07:00,a5df5de926838c2d3b890c7b97fd68d7883cec2a,https://github.com/pytorch/fairseq/commit/a5df5de926838c2d3b890c7b97fd68d7883cec2a,"wav2vec_u_readme (#1888)

Summary:
initial wav2vec-U readme
to be updated

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1888

Reviewed By: michaelauli

Differential Revision: D28595427

Pulled By: alexeib

fbshipit-source-id: 8e1baca8a367f9b38a66e58489ad127341214f58",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1872,freewym,freewym@gmail.com,2021-05-21 00:39:54-07:00,f9edd9f9b919b5fe77255296c69e052e4a930b2b,https://github.com/pytorch/fairseq/commit/f9edd9f9b919b5fe77255296c69e052e4a930b2b,"fix the error when using pyarrow for raw_audio_dataset (#3561)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [X] Did you write any new necessary tests?

## What does this PR do?

fix the error when using pyarrow for raw_audio_dataset, Currently there is an error `TypeError: join() argument must be str, bytes, or os.PathLike object, not 'StringScalar'`. In this PR I just convert the type to string in `__getitem__()` for `os.path.join()`

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3561

Reviewed By: jmp84

Differential Revision: D28594620

Pulled By: kahne

fbshipit-source-id: fd2daa992df85ac0919ba30fa9afa67a0c89d956",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1873,alexeib,alexei.b@gmail.com,2021-05-21 07:33:12-07:00,649af635f40dfdd4ab47e26c5183e69a62f8c49c,https://github.com/pytorch/fairseq/commit/649af635f40dfdd4ab47e26c5183e69a62f8c49c,"Wav2vec u (#1889)

Summary:
Wav2vec-U implementation

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1889

Reviewed By: michaelauli

Differential Revision: D28596815

Pulled By: alexeib

fbshipit-source-id: bb09d081d167d5d10968acc6e056044bf96679ac",82,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1874,alexeib,alexei.b@gmail.com,2021-05-21 12:11:57-07:00,a8fe9434adeb6caea65b34ca968f4e5cd1d75be6,https://github.com/pytorch/fairseq/commit/a8fe9434adeb6caea65b34ca968f4e5cd1d75be6,"fix readme link (#1890)

Summary:
fix link to self training readme

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1890

Reviewed By: arbabu123, ArmenAg

Differential Revision: D28609910

Pulled By: alexeib

fbshipit-source-id: 4cbbb75cfec876938d80c8d7bc0df4ba93d4f54d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1875,Weiyi Zheng,wyz@fb.com,2021-05-21 16:17:21-07:00,78e75fa3edf4a4ce02f9aa59ded01d814036dc81,https://github.com/pytorch/fairseq/commit/78e75fa3edf4a4ce02f9aa59ded01d814036dc81,"attempt to make non-sharded FSDP checkpoint behave like regular checkpoint

Summary:
overall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.

a couple advantages
1. allows resuming from other DDP trainers.
2. allows resuming into other DDP trainers. or FSDP of a different configuration.
3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()

For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the ""-shard0"" for resuming training.

Reviewed By: sshleifer

Differential Revision: D28563032

fbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1876,Kushal Lakhotia,kushall@fb.com,2021-05-21 18:40:01-07:00,4aef9036cef814a24193dff3a678ce0f5c27309f,https://github.com/pytorch/fairseq/commit/4aef9036cef814a24193dff3a678ce0f5c27309f,"Merge Hubert to master (#1877)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ X] Did you make sure to update the docs?
- [ X] Did you write any new necessary tests?

## What does this PR do?
This PR adds relevant code for pre-training HuBERT and fine-tuning a pretrained HuBERT for ASR. It also shared trained models of different sizes.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1877

Reviewed By: wnhsu

Differential Revision: D28513359

Pulled By: hikushalhere

fbshipit-source-id: 8755862f236b7d840105b0fa8f5461ac053d79cc",34,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1877,Changhan Wang,changhan@fb.com,2021-05-22 00:21:06-07:00,49cf3e0bc389f0c704f7540c7d45100066ce7f7c,https://github.com/pytorch/fairseq/commit/49cf3e0bc389f0c704f7540c7d45100066ce7f7c,"fixing s2t transformer and N-best checkpoint saving

Summary:
- fixing the default value for `encoder_freezing_updates` in s2t transformer
- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.

Reviewed By: jmp84

Differential Revision: D28546493

fbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1878,Patrick von Platen,patrick.v.platen@gmail.com,2021-05-23 16:18:55-07:00,366974d9817138d1618693f021ea1690f9e53f33,https://github.com/pytorch/fairseq/commit/366974d9817138d1618693f021ea1690f9e53f33,"HF Wav2Vec2 Example (#3502)

Summary:
## What does this PR do?
This PR updates some outdated code from the Hugging Face Transformers library to the new, better format.

## PR review
alexeib

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3502

Reviewed By: arbabu123

Differential Revision: D28140574

Pulled By: alexeib

fbshipit-source-id: f03643e7ebba04015d942a3aa9529f7f6600c734",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1879,alexeib,alexei.b@gmail.com,2021-05-23 21:24:38-07:00,342d5daf34acaf34c93b5a0a313f46bc4104c7fc,https://github.com/pytorch/fairseq/commit/342d5daf34acaf34c93b5a0a313f46bc4104c7fc,"propagate quantizer depth and factor args through w2v (#1892)

Summary:
makes quantizer larger which helps accuracy in certain cases

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1892

Reviewed By: arbabu123

Differential Revision: D28630035

Pulled By: alexeib

fbshipit-source-id: ba5a902ff1623025e7566e901aa81cdf377a7aa0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1880,Sam Shleifer,sshleifer@gmail.com,2021-05-24 08:58:56-07:00,2be2f3c7c1ba9ec3ee6ef929f7edce13052b6844,https://github.com/pytorch/fairseq/commit/2be2f3c7c1ba9ec3ee6ef929f7edce13052b6844,"Plasma tests: ask for less disk (#1893)

Summary:
Old logs:
```
/arrow/cpp/src/plasma/store.cc:1274: Allowing the Plasma store to use up to 107.374GB of memory.
```

New logs:
```
... up to 1e-05GB of memory.
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1893

Reviewed By: myleott

Differential Revision: D28641488

Pulled By: sshleifer

fbshipit-source-id: 3373526042cdcbf434c61790be62a09f15e6ad06",1,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1881,alexeib,alexei.b@gmail.com,2021-05-24 19:09:23-07:00,30003ba4192f173d84cb0dbfee678296d8af6378,https://github.com/pytorch/fairseq/commit/30003ba4192f173d84cb0dbfee678296d8af6378,"fix serialization on python 3.6 (#1894)

Summary:
fixes serialization  errors when using python 3.6

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1894

Reviewed By: arbabu123

Differential Revision: D28655932

Pulled By: alexeib

fbshipit-source-id: df40f972966e828817a2861e6e907835fe1d9573",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1882,alexeib,alexei.b@gmail.com,2021-05-24 19:56:14-07:00,5a75b079bf8911a327940c28794608e003a9fa52,https://github.com/pytorch/fairseq/commit/5a75b079bf8911a327940c28794608e003a9fa52,"fix saving w2v args in config (#1896)

Summary:
previous changes broke saving updating w2v_args in config as the model had a copy of the config. this change makes the task copy over the field to save. not the nicest approach, but it works for now

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1896

Reviewed By: arbabu123

Differential Revision: D28658802

Pulled By: alexeib

fbshipit-source-id: a13866c42c3b88c48b8b91864c1bf1aeaeba4e8a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1883,Kushal Lakhotia,kushall@fb.com,2021-05-25 13:43:42-07:00,95cf58056dc30a9fa653dd8adcbd5b76a180a63d,https://github.com/pytorch/fairseq/commit/95cf58056dc30a9fa653dd8adcbd5b76a180a63d,"Update model table in README (#1901)

Summary:
## What does this PR do?
Updated the models' table in README to show the model sizes and groups pretrained models followed by fine tuned models.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1901

Reviewed By: wnhsu

Differential Revision: D28688952

Pulled By: hikushalhere

fbshipit-source-id: 8621398a785caa3d7bdc68367789ad7f48499d0d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1884,Weiyi Zheng,wyz@fb.com,2021-05-25 17:45:04-07:00,8df9e3a4a55bad55078967e97e8a8f31d90ec987,https://github.com/pytorch/fairseq/commit/8df9e3a4a55bad55078967e97e8a8f31d90ec987,"support FSDP sharded_state checkpoint loading during inference

Summary:
using the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.

The main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering
1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint
2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint
3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint
4. otherwise we are loading a single, plain checkpoint.

In theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.

Reviewed By: sshleifer

Differential Revision: D28563441

fbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59",3,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],"['os.path.exists(log)', 'os.path.exists(log)']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1885,Gagandeep Singh,gagandeep.singh1@nuance.com,2021-05-26 14:38:16-07:00,237184e5222b347475456f4a44f31a510c64ca35,https://github.com/pytorch/fairseq/commit/237184e5222b347475456f4a44f31a510c64ca35,"Add torch.cuda.amp support (#3460)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3282
Add support for `torch.cuda.amp`
AMP can be enabled by `--amp`, instead of using `--fp16` for the already present full fp16 support.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3460

Reviewed By: sshleifer, msbaines

Differential Revision: D27932253

Pulled By: myleott

fbshipit-source-id: 21637aefb5e788c59bf4f3c5de6c4a80f7319543",9,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,5,1,0,0,0,5,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestGradientScalingAMP(unittest.TestCase):'],"[('Equal', '(loss, torch.tensor(1.0, device=, dtype=torch.float16))'), ('AlmostEqual', '(grad_norm.item(), 2.2361, 4)'), ('Equal', '('), ('Equal', '('), ('Equal', '(self.scaler.get_scale(), 2.0)')]",['def setUp(self):'],[],[],[],"['not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )', 'not torch.cuda.is_available(), )']",[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1886,alexeib,alexei.b@gmail.com,2021-05-26 16:27:59-07:00,e6eddd805ebbc5c17bf5100c2fde6e0dfc946d2c,https://github.com/pytorch/fairseq/commit/e6eddd805ebbc5c17bf5100c2fde6e0dfc946d2c,"make hydra/infer.py work; also dont break if something is removed fro… (#1903)

Summary:
previously hydra/infer.py did not always work for several reasons which are addressed here

new example usage:

PYTHONPATH=. python examples/speech_recognition/new/infer.py --config-dir examples/speech_recognition/hydra/conf --config-name infer task=audio_pretraining task.data=/path/to/data task.labels=ltr decoding.type=kenlm decoding.lexicon=/path/to/lexicon decoding.lmpath=/path/to/lm dataset.gen_subset=dev_other common_eval.path=/path/to/model.pt decoding.beam=5 decoding.lmweight=2 decoding.wordscore=-1

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1903

Reviewed By: arbabu123

Differential Revision: D28700795

Pulled By: alexeib

fbshipit-source-id: 66fe454de49c1bf511b3529ac683f1c8cb08e579",22,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1887,Nicola De Cao,nicola.decao@uva.nl,2021-05-26 18:20:04-07:00,c8223e350cfc616bb47196151d1223683e483b6d,https://github.com/pytorch/fairseq/commit/c8223e350cfc616bb47196151d1223683e483b6d,"fixing prefix_allowed_tokens_fn (#3276)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [x] Did you make sure to update the docs?
- [x] Did you write any new necessary tests?

## What does this PR do?
Fixes the use of `prefix_allowed_tokens_fn` in generation. It was working for `fairseq==0.9.0` (see https://github.com/facebookresearch/GENRE) but with the current version is broken.

## PR review
Anyone in the community is free to review the PR once the tests have passed.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3276

Reviewed By: alexeib

Differential Revision: D26725494

Pulled By: myleott

fbshipit-source-id: ce3da725f36352687e5cb5d62a59b4c89ce0b0bc",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1888,Mandeep Singh Baines,mandeep.baines@gmail.com,2021-05-27 12:14:40-07:00,9497ae3cfb04bb6ec4735758bbe8dc767276932c,https://github.com/pytorch/fairseq/commit/9497ae3cfb04bb6ec4735758bbe8dc767276932c,"disable raise_if_valid_subsets_unintentionally_ignored check for dummy tasks (#3552)

Summary:
Fixes the following crash:
```python
Traceback (most recent call last):
  File ""/private/home/msb/.conda/envs/fairseq-20210102-pt181/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 59, in _wrap
    fn(i, *args)
  File ""/private/home/msb/code/fairseq/fairseq/distributed/utils.py"", line 328, in distributed_main
    main(cfg, **kwargs)
  File ""/private/home/msb/code/fairseq/fairseq_cli/train.py"", line 117, in main
    data_utils.raise_if_valid_subsets_unintentionally_ignored(cfg)
  File ""/private/home/msb/code/fairseq/fairseq/data/data_utils.py"", line 584, in raise_if_valid_subsets_unintentionally_ignored
    other_paths = _find_extra_valid_paths(train_cfg.task.data)
AttributeError: 'Namespace' object has no attribute 'data'
```

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3552

Reviewed By: sshleifer

Differential Revision: D28667773

Pulled By: msbaines

fbshipit-source-id: bc9a633184105dbae0cce58756bb1d379b03980a",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1889,Yun Wang,yunwang@fb.com,2021-05-27 14:28:23-07:00,19793a78e5cd9aa0de427065f125c93100a942ea,https://github.com/pytorch/fairseq/commit/19793a78e5cd9aa0de427065f125c93100a942ea,"Remove duplicate registration of ManifoldPathHandler

Summary: `ManifoldPathHandler` is automatically registered with `IOPathManager` upon importing the latter (see D27960781). Therefore it is no longer necessary to register `ManifoldPathManager` in fairseq, as introduced by D27809504 (https://github.com/pytorch/fairseq/commit/3a90a859d4dfdbf13f15399be12a1928aa2c54ff).

Reviewed By: sujitoc

Differential Revision: D28735316

fbshipit-source-id: 03e246dd17ba9f2a9a81dd4e741cce88f26feedd",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1890,alexeib,alexei.b@gmail.com,2021-05-31 01:13:41-07:00,62ccebaf70cd8d392e178b67e0af661da0431a20,https://github.com/pytorch/fairseq/commit/62ccebaf70cd8d392e178b67e0af661da0431a20,"fix '_pickle.PicklingError: Can't pickle <enum 'Choices'>: attribute … (#1915)

Summary:
for whatever reason, checkpoints are failing to save because choiceenum can't be pickled again (could be env specific). this should permanently resolve it by converting choice enum to string in the config before saving

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1915

Reviewed By: arbabu123

Differential Revision: D28784506

Pulled By: alexeib

fbshipit-source-id: 17843cfa00e8e624eb06262df8e1b71b062a237b",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1891,alexeib,alexei.b@gmail.com,2021-06-01 16:42:48-07:00,c47a9b2eef0f41b0564c8daf52cb82ea97fc6548,https://github.com/pytorch/fairseq/commit/c47a9b2eef0f41b0564c8daf52cb82ea97fc6548,"fix #3574 (#1921)

Summary:
support pre-hydra w2v models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1921

Reviewed By: arbabu123

Differential Revision: D28807630

Pulled By: alexeib

fbshipit-source-id: 0fc8bcda12cf677e909d88678f235bfdeb50e726",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1892,Henry Hu,henryhu6@fb.com,2021-06-03 16:19:27-07:00,4950c56f461db1646872159b2c470fe57ae72c69,https://github.com/pytorch/fairseq/commit/4950c56f461db1646872159b2c470fe57ae72c69,"Add export flag to transform, so LayerNorm can be TorchScripted.

Summary:
Previously on cuda, LayerNorm would always default to FusedLayerNorm, which could not be exported.

Add export flag, so torch.nn.LayerNorm would be used.

Reviewed By: myleott, mikekgfb, kpuatfb

Differential Revision: D28858633

fbshipit-source-id: 58dd4945f596b2bcc94a6b74356bd9fd3c73ca1a",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1893,Yun Wang,yunwang@fb.com,2021-06-03 17:48:17-07:00,50f3766a9d8413a5875a9b2b599b233b5690c7f2,https://github.com/pytorch/fairseq/commit/50f3766a9d8413a5875a9b2b599b233b5690c7f2,"TALNet: Use batch size as sample_size

Summary:
`Wav2VecCriterion` uses a sample_size for two purposes:
1. It weights the extra loss by multiplying it by sample_size;
2. It divides the total loss by sample_size before reporting them in the learning curves.

By default, when using the binary cross-entropy loss (`infonce = False`), `Wav2VecCriterion` uses the number of 1's in the label matrix as sample_size.
For TALNet, because each recording may have multiple labels, this sample_size is not a constant across batches.
TALNet also uses a consistency loss between the predictions on two different copies of augmented data as an extra loss, and it is undesirable for the weight of the extra loss to vary from batch to batch.

This diff adds a field ""sample_size"" to the batch in the `AcousticEventCollater`, and makes it equal to the batch size (number or recordings in a batch).
Because the extra loss is multiplied by sample_size in `Wav2VecCriterion`, this diff also divides the consistency loss by the batch size in the `forward` method of `TALNetModel`.

This diff also adds a unit test for the consistency loss.

Reviewed By: alexeib

Differential Revision: D28728699

fbshipit-source-id: dda1f2a1b02e49b894842c8990218b5fe92d0330",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1894,Yun Wang,yunwang@fb.com,2021-06-03 17:48:17-07:00,3084b812beb72a880b6ddb5d9076ead60b7232d6,https://github.com/pytorch/fairseq/commit/3084b812beb72a880b6ddb5d9076ead60b7232d6,"Teacher-student learning for TALNet

Summary:
This diff implements teacher-student learning for TALNet.

Three classes take part in the teacher-student learning:
* The task loads the teacher models;
* The model generates predictions using the teacher models, and mixes them with the original targets;
* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.

There are two types of teachers:
* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;
* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.

We actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.

The teacher models are stored in the task object, and will not be saved into checkpoints.

Reviewed By: alexeib

Differential Revision: D28728707

fbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1895,Mandeep Singh Baines,mandeep.baines@gmail.com,2021-06-04 11:23:32-07:00,45d8fefaa6871afbb747e5e65ba58b8f9fda37fe,https://github.com/pytorch/fairseq/commit/45d8fefaa6871afbb747e5e65ba58b8f9fda37fe,"fix logging when running single-process (#3592)

Summary:
In file_io.py, there is a logging message that happens in the global
scope. This logging message can be invoked before calling
logging.basicConfig() in fairseq_cli/train.py resulting in that
call becoming a no-op. This was causing the loglevel to remain at
WARNING.

Fix is to call logging.basicConfig() before import-ing any fairseq
libraries that may do logging in global scope.

Verified that I logging.info messages are now visible after applying
this PR.

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3592

Reviewed By: sujitoc

Differential Revision: D28900871

Pulled By: msbaines

fbshipit-source-id: ff5393aa7c5e4cbec168ff0b846da048de76cdbc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1896,Yun Wang,yunwang@fb.com,2021-06-04 16:19:04-07:00,fc391ff6974969649ec94dc18eb7795de66bda4f,https://github.com/pytorch/fairseq/commit/fc391ff6974969649ec94dc18eb7795de66bda4f,"Fix loading some TALNet models

Summary:
D28728718 cleaned up the ""kd_binary_cross_entropy"" criterion, but this caused loading old models trained with this criterion to fail.
This diff replaces the ""kd_binary_cross_entropy"" criterion with the ""wav2vec"" criterion when loading models, and fixes this error.

It also removes the ""log_keys"" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.

Differential Revision: D28901263

fbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1897,Naman Goyal,namangoyal@learnfair1299.h2.fair,2021-06-07 15:04:31-07:00,2fd9d8a972794ba919174baf0d1828a5a4c626f3,https://github.com/pytorch/fairseq/commit/2fd9d8a972794ba919174baf0d1828a5a4c626f3,"released xlmr xl and xxl model weights (#1944)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1944

Reviewed By: jingfeidu

Differential Revision: D28944206

fbshipit-source-id: 583837f7dd387341574d27dd9acc145455d640a8",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1898,Diana Liskovich,dianaml@devfair0471.h2.fair,2021-06-10 09:42:18-07:00,50158da3a7b293f2d2fa06a23e90c160b92f54ce,https://github.com/pytorch/fairseq/commit/50158da3a7b293f2d2fa06a23e90c160b92f54ce,"Migrate DummyMaskedLMTask to FairseqTask (#3593)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3593

Reviewed By: msbaines

Differential Revision: D28992614

Pulled By: dianaml0

fbshipit-source-id: b2dfcab472a65c41536e78600a0e6b3745dc3a08",5,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1899,alexeib,alexei.b@gmail.com,2021-06-10 21:57:48-07:00,f8a7c93440cd925f70979a6082c18f830b39e44b,https://github.com/pytorch/fairseq/commit/f8a7c93440cd925f70979a6082c18f830b39e44b,"W2v u update (#1954)

Summary:
updating the scripts and examples to be easier to follow

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1954

Reviewed By: wnhsu

Differential Revision: D29041166

Pulled By: alexeib

fbshipit-source-id: d9410c6e925337b810e92b393e226869ef9e1733",23,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1900,Valentin Andrei,vandrei@fb.com,2021-06-11 17:59:24-07:00,c36294ea4fd35eac757f417de9668b32c57d4b3d,https://github.com/pytorch/fairseq/commit/c36294ea4fd35eac757f417de9668b32c57d4b3d,"Do FP16/BF16 conversions on the host to transfer less through PCIe

Summary:
If we do the FP16/BF32 conversion on the host, we do it at DRAM speed but transfer 2X smaller buffer to the GPU through PCIe. PCIe bandwidth is an order of magnitude lower so we actually gain about 50% of execution time compared to when performing the quantization on the GPU.

Also, by transfering an already FP16 buffer, we save memory capacity.

Reviewed By: zhengwy888

Differential Revision: D24146486

fbshipit-source-id: b897e7a32835aa1b571b0fae5f3d72a131ad16a1",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1901,Henry Hu,henryhu6@fb.com,2021-06-14 09:09:25-07:00,176b2e4e766c94e64f945b9c1323e612d8887d83,https://github.com/pytorch/fairseq/commit/176b2e4e766c94e64f945b9c1323e612d8887d83,"Fix warning for empty tensor without type

Summary:
Fairseq create an empty tensor without type.
It will create warning for torchscript model.
Warning: Creating a tensor from an empty intlist will create a tensor of default floating point type  (currently Float) in python but a tensor of type int in torchscript.

This diff adds definition of the type.

Reviewed By: myleott

Differential Revision: D29081170

fbshipit-source-id: 5c32aae65c9998b245eac43bfedc820bea509338",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1902,msbaines,35972327+msbaines@users.noreply.github.com,2021-06-14 16:37:53-07:00,cd5775f30184baa414a354d9f06b747344a8ba74,https://github.com/pytorch/fairseq/commit/cd5775f30184baa414a354d9f06b747344a8ba74,"avoid freezing batches unnecessarily (#3610)

Summary:
In EpochBatchIterator, first_batch() freezes batches in order to
generate the dummy_batch. We then freeze batches again in the call
to next_epoch_itr(). We can avoid the second freeze and reduce
time to first iteration by about 50% in cases where we have a
callable batch_sampler.

Before:

![Screen Shot 2021-06-10 at 5 08 22 PM](https://user-images.githubusercontent.com/35972327/121613200-d2366600-ca10-11eb-9d1d-bafc2403766a.png)

After:

![Screen Shot 2021-06-10 at 5 07 54 PM](https://user-images.githubusercontent.com/35972327/121613224-dfebeb80-ca10-11eb-9d5a-07be9440db77.png)

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3610

Reviewed By: myleott

Differential Revision: D29105845

Pulled By: msbaines

fbshipit-source-id: 9795d46d70a99ad1218ce225092cc22ee3192bbc",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1903,Kushal Lakhotia,kushall@fb.com,2021-06-15 10:49:33-07:00,8320f6708ff27537f51a8d3a4c4a0bfbceea71d9,https://github.com/pytorch/fairseq/commit/8320f6708ff27537f51a8d3a4c4a0bfbceea71d9,"Instructions for loading HuBERT model (#1966)

Summary:
## What does this PR do?
Fixes the HuBERT README to contain instructions to load pretrained checkpoints.

## PR review
Tested in a fresh environment that doesn't have access to FAIR's dev env.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1966

Reviewed By: wnhsu

Differential Revision: D29117906

Pulled By: hikushalhere

fbshipit-source-id: 89b0407ecf8cdbeddcab80f55e6b2f1fed24c967",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1904,Kushal Lakhotia,kushall@fb.com,2021-06-15 14:09:01-07:00,128b4fc3789338d782c1ae4a27c5f0d6fa6dfed0,https://github.com/pytorch/fairseq/commit/128b4fc3789338d782c1ae4a27c5f0d6fa6dfed0,"Check attributes in trainer and checkpoint loading before using them (#1970)

Summary:
## What does this PR do?
Fixes None exception when some attributes in  don't exist in cfg.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970

Reviewed By: alexeib

Differential Revision: D29140036

Pulled By: hikushalhere

fbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1905,Neeyanth Kopparapu,neeyanth@fb.com,2021-06-15 22:01:55-07:00,afc77bdf4bb51453ce76f1572ef2ee6ddcda8eeb,https://github.com/pytorch/fairseq/commit/afc77bdf4bb51453ce76f1572ef2ee6ddcda8eeb,"Enabled storing of dictionaries (#3601)

Summary:
## What does this PR do?
For HubertPretrainingTask, added dictionaries to the task state to enable the serialization of the dictionaries (thus removing the need to load from the disk after training)

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3601

Test Plan:
To verify the success, run the Hubert Pretraining pipeline, load a checkpoint model, and verify that the ""dictionaries"" key is present in the state within the model.

Specifically,
```
PYTHONPATH=. python /path/to/fairseq/fairseq_cli/hydra_train.py -m \
        --config-dir ${fairseq_dir}/examples/hubert/config/pretrain \
        --config-name hubert_base_librispeech \
        hydra/launcher=submitit_local \
        hydra.launcher.gpus_per_node=2 \
        hydra.launcher.cpus_per_task=8 \
        hydra.launcher.mem_gb=384 \
        task.data=${tsv_dir} \
        task.label_dir=${km_dir} \
        task.labels=[""km""] \
        +data=iter1 \
        optimization.max_update=250 \
        hydra.sweep.dir=${exp_dir} \
        hydra.run.dir=${exp_dir} > ${exp_dir}/log.out 2> ${exp_dir}/log.err &
```
Then, at the location of the model, load the model using `pytorch.load`, and verifying that ""dictionaries"" is a key under the `task_state` key of the model.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: wnhsu

Differential Revision: D28995537

Pulled By: neeyanthkvk

fbshipit-source-id: e10c5163c367285518961b3ce1e719a29da06aa6",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1906,Vimal Manohar,vimalmanohar@fb.com,2021-06-17 13:19:15-07:00,67138ceb08b02c3148034e445cf5e76a616fc859,https://github.com/pytorch/fairseq/commit/67138ceb08b02c3148034e445cf5e76a616fc859,"Fix lr for reduce_lr_on_plateau when there is no warmup

Summary:
warmup_init_lr should not be used if there is no warmup i.e. warmup_updates = 0

Created from Diffusion's 'Open in Editor' feature.

Reviewed By: myleott

Differential Revision: D29174059

fbshipit-source-id: c2e4cf998aebcff090584e689f692a0abe082e65",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1907,Sravya Popuri,spopuri@fb.com,2021-06-17 13:58:56-07:00,b3491ae9d4c3eaa24292512cee7c21def713c535,https://github.com/pytorch/fairseq/commit/b3491ae9d4c3eaa24292512cee7c21def713c535,"Add latency metrics to simulate tuna inference script and some other minor updates

Summary:
- Add average lagging latency metrics for online model. Offline models by default return 0
- Pad smaller input chunks with 0.
- Enable export option in layer norm in transformer.py to avoid errors in scripted model inference.
- Warm up prediction for scripted online model
- Add additional args like force_read_cnt, data_split

Reviewed By: xutaima

Differential Revision: D28881594

fbshipit-source-id: fd4cce017539b5d8f6e39f9af9651341e47d6db0",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1908,Sravya Popuri,spopuri@fb.com,2021-06-18 13:47:20-07:00,fc77eeb550e8ae74a14b36e6b9babc129f896725,https://github.com/pytorch/fairseq/commit/fc77eeb550e8ae74a14b36e6b9babc129f896725,"Change char_inputs to export as recommended in Fairseq

Summary: TSIA

Reviewed By: jmp84, henryhu6

Differential Revision: D29232406

fbshipit-source-id: 557006705faf28d723dc9f0ed9e92b0abe68e895",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1909,alexeib,alexei.b@gmail.com,2021-06-21 11:05:15-07:00,822442e42a020bac1ddaaaf4a99124db8b0cfab0,https://github.com/pytorch/fairseq/commit/822442e42a020bac1ddaaaf4a99124db8b0cfab0,"fix task name in w2v-u generate (#1989)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1989

Reviewed By: arbabu123

Differential Revision: D29267899

Pulled By: alexeib

fbshipit-source-id: b89b804c14dbf8779b5cb56657d33bb03530f303",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1910,Neeyanth Kopparapu,neeyanth@fb.com,2021-06-21 12:32:19-07:00,e47a4c84da877382c4d37941be4e84449f99d186,https://github.com/pytorch/fairseq/commit/e47a4c84da877382c4d37941be4e84449f99d186,"hotfix to change factory creation for dictionaries (#1987)

Summary:
## What does this PR do?
Fixes issue of creating factories causing errors because the lambda function is not proper.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1987

Test Plan:
Completed a small pretraining+finetuning procedure:

Pretraining:
```
PYTHONPATH=. python /private/home/neeyanth/project/fairseq/fairseq_cli/hydra_train.py -m \
	--config-dir ${fairseq_dir}/examples/hubert/config/pretrain \
	--config-name hubert_base_librispeech \
	hydra/launcher=submitit_local \
	hydra.launcher.gpus_per_node=1 \
	hydra.launcher.cpus_per_task=8 \
	hydra.launcher.mem_gb=384 \
	task.data=${tsv_dir} \
	task.label_dir=${km_dir} \
	task.labels=[""km""] \
	+data=iter1 \
	optimization.max_update=250 \
	hydra.sweep.dir=${exp_dir} \
	hydra.run.dir=${exp_dir} > ${exp_dir}/log.out 2> ${exp_dir}/log.err &
```

Finetuning:
```
PYTHONPATH=. python /private/home/neeyanth/project/fairseq/fairseq_cli/hydra_train.py -m \
	--config-dir ${fairseq_dir}/examples/hubert/config/finetune \
	--config-name base_10h \
	hydra/launcher=submitit_local \
	hydra.launcher.gpus_per_node=1 \
	hydra.launcher.cpus_per_task=8 \
	hydra.launcher.mem_gb=384 \
	task.data=${tsv_dir} \
	task.label_dir=${tsv_dir} \
	model.w2v_path=${model_dir} \
	+data=iter1 \
	optimization.max_update=250 \
	hydra.sweep.dir=${exp_dir} \
	hydra.run.dir=${exp_dir} > ${exp_dir}/log.out 2> ${exp_dir}/log.err &
```

Reviewed By: hikushalhere

Differential Revision: D29266136

Pulled By: neeyanthkvk

fbshipit-source-id: d36c668ae38a7761b4c44f4dcb0c4cc8e15e42ce",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1911,Wei-Ning Hsu,wnhsu@csail.mit.edu,2021-06-21 19:40:02-07:00,900a607ea3226e0cfd966a894b8b1effe25faa5e,https://github.com/pytorch/fairseq/commit/900a607ea3226e0cfd966a894b8b1effe25faa5e,"add timit w2vu recipe (#1991)

Summary:
## What does this PR do?
Add TIMIT data preparation scripts for wav2vec-U

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1991

Reviewed By: alexeib

Differential Revision: D29284481

Pulled By: wnhsu

fbshipit-source-id: dccd75159a9de4f3cd95f9e4a90ce4bdf9264f2b",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1912,Nithin-Holla,nithin.holla7@gmail.com,2021-06-21 20:16:08-07:00,3c4a8e41559fa50b6c907fbefa1dab55d57bda5c,https://github.com/pytorch/fairseq/commit/3c4a8e41559fa50b6c907fbefa1dab55d57bda5c,"Enabling word-level timestamps for Wav2Vec 2.0 (#3627)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes https://github.com/pytorch/fairseq/issues/3371.

Currently, the output from Wav2Vec 2.0 decoding does not contain word-level start/end times, which can be useful for certain applications of ASR. Based on the discussion [here](https://github.com/flashlight/flashlight/issues/618), they could be computed based on the output from the Flashlight decoder. For the KenLM decoder, we could first obtain the frame number corresponding to each non-blank token. Next, the timestamp of each character could be computed as `segment_start + frame_no/total_frames * segment_duration`. Finally, the start and end time of each word could be calculated based on the timestamp of the word boundary characters. In order to enable this, the frame number of each non-blank character is returned as a result of KenLM decoding. This is similar to the `timesteps` output from the [ctcdecode](https://github.com/parlance/ctcdecode#outputs-from-the-decode-method) library.

## PR review
alexeib

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3627

Reviewed By: michaelauli

Differential Revision: D29282488

Pulled By: alexeib

fbshipit-source-id: b5fe64bf50abd7ef8e9539f4e338937c866eb0ca",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1913,Eduardo Romero,eduardoromero@fb.com,2021-06-22 09:11:13-07:00,7ca8bc12c09d91187d95117094f6b31b3342cd17,https://github.com/pytorch/fairseq/commit/7ca8bc12c09d91187d95117094f6b31b3342cd17,"KMeans Attention

Summary: KMeans attention main file

Reviewed By: yiq-liu

Differential Revision: D28478149

fbshipit-source-id: 97ef1408cfa239bdf13ee5d54d5d31b61a7f2236",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1914,Ashwyn Sharma,ashwynsharma@fb.com,2021-06-23 11:13:05-07:00,7818f6148da4ea04f0b4b3a2df780004c3580dad,https://github.com/pytorch/fairseq/commit/7818f6148da4ea04f0b4b3a2df780004c3580dad,"Tuna integration and model packaging

Reviewed By: sravyapopuri388

Differential Revision: D29118016

fbshipit-source-id: d183c821e5d8eb1b37dda48ded9e24e5efc65dc7",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1915,Alex Liu,alexliu36@gmail.com,2021-06-24 13:59:58-07:00,520d9d3ba68d06e56f0b9e1d331ed444f48755b2,https://github.com/pytorch/fairseq/commit/520d9d3ba68d06e56f0b9e1d331ed444f48755b2,"remove debug code from w2vu gen (#1997)

Summary:
see title

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1997

Reviewed By: wnhsu, Alexander-H-Liu

Differential Revision: D29371459

Pulled By: alexeib

fbshipit-source-id: 874e36462f919aa4ba698a0dd49531c89f7e27cf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1916,Shiyan Deng,dsy842974287@fb.com,2021-06-24 14:01:26-07:00,81046fc13ef05c7b9bbfb7a4cd66e59033918dc3,https://github.com/pytorch/fairseq/commit/81046fc13ef05c7b9bbfb7a4cd66e59033918dc3,"Add decoder and decoding wrapper for nmt

Summary:
Add a decoder class `FairSeqNVFasterTransformerDecoder` that could replace `TransformerDecoder` in nmt.
Add a decoding class `FairSeqNVFasterTransformerDecoding` that does `decoding + beam serach`.

We can't use `FairSeqNVFasterTransformerDecoding` right now in nmt because nmt ensembles decoders and calculate avg probabilities across those decoders.

Follow ups:
1. Currently `FairSeqNVFasterTransformerDecoder` doesn't produce ""attn"" https://fburl.com/code/pom5vhr5.
2. Move mem_cache and cache to incremental_state
2. Benchmark fairseq ft encoder decoder.
2. E2e tests stucks at somewhere.

Differential Revision: D29166310

fbshipit-source-id: 36360cfff1d22ed4f12f89068ee30dec835d2141",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1917,Kushal Lakhotia,kushall@fb.com,2021-06-26 08:59:02-07:00,f8871521f7b2496bbfce58ff72ea611c4f6ec244,https://github.com/pytorch/fairseq/commit/f8871521f7b2496bbfce58ff72ea611c4f6ec244,"Load dict from pretrained hubert model in HubertEncoder (#1999)

Summary:
## What does this PR do?
Load dict from pretrained hubert model in HubertEncoder so that the dictionary is not constructed for the labels.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1999

Test Plan:
Tested with the cmdline below. ASR training progresses as expected without any exception.

```
PYTHONPATH=. HYDRA_FULL_ERROR=1 python fairseq_cli/hydra_train.py -m \
--config-dir examples/hubert/config/finetune \
--config-name base_10h \
dataset.num_workers=0 \
task.data=/checkpoint/kushall/data/librispeech/10h/raw \
task.label_dir=/checkpoint/kushall/data/librispeech/10h/raw \
model.w2v_path=/checkpoint/kushall/final_model_checkpoints/hubert/hubert_base_ls960_updated.pt \
hydra.sweep.dir=/checkpoint/kushall/experiments/hubert_test/base_asr_10h
```

Reviewed By: Abdel-rahmanMohamed

Differential Revision: D29405491

Pulled By: hikushalhere

fbshipit-source-id: be168a0ce27f8fcfea3dc980a192ba43fdf23871",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1918,Pierre Andrews,mortimer@fb.com,2021-06-28 01:45:15-07:00,53bf2b12934aa5d38ff2d700221457ca34b55cab,https://github.com/pytorch/fairseq/commit/53bf2b12934aa5d38ff2d700221457ca34b55cab,"Extract File Chunking to its own utils (#1955)

Summary:
## What does this PR do?

there are a few places where we do file chunking for multiprocessing a single file. However, the code is partly in Binarizer and partly just duplicated here and there.

This PR extracts the file chunking/reading logic. The multiprocessing logic could probably be extracted too, but I haven't found a good abstraction yet.

# Testing

Added testing for this reading logic + maybe fixed a bug where the last part of a file might get dropped (even if it's unclear with the current stopping logic)

Tested by running the preprocessing script as follow:
```
python -m fairseq_cli.preprocess --source-lang de --target-lang en --trainpref ...train.spm.clean.de_en --srcdict ...fairseq.dict --tgtdict .../fairseq.dict --destdir ... --workers 60
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1955

Reviewed By: myleott

Differential Revision: D29065473

Pulled By: Mortimerp9

fbshipit-source-id: c60843de8cfd45a63b3dbb8290f57ef3df3bf983",6,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,7,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestFileChunker(unittest.TestCase):'],"[('Equal', '('), ('Equal', '(len(offsets), self._num_splits + 1)'), ('Equal', '(zero, 0)'), ('Equal', '('), ('Equal', '(last, self._num_bytes * self._num_lines)'), ('AlmostEqual', '('), ('ListEqual', '(')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1919,Liang Luo,liangluo@fb.com,2021-06-29 00:02:06-07:00,0972dde844e39540faf53b6d9afe76b38c7e2fd6,https://github.com/pytorch/fairseq/commit/0972dde844e39540faf53b6d9afe76b38c7e2fd6,"apply nonblocking H/D transfer optimizations

Summary:
merge D27701492 + D27701493
* make checkpoint activation cpu offloading nonblocking
* make gradient cpu offloading nonblocking
* synchronize cpu/gpu stream before applying optimizer update

Reviewed By: myleott

Differential Revision: D28047171

fbshipit-source-id: f862eca64049acc045026aa4f5e6dbe8d0f03244",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1920,Edan Tessel Sneh,edan@fb.com,2021-06-29 15:09:13-07:00,0794f9ae21bb50f940ff9b1bc3f28be08dfa7b76,https://github.com/pytorch/fairseq/commit/0794f9ae21bb50f940ff9b1bc3f28be08dfa7b76,"Back out ""Adding FBSequenceGenerator""

Summary:
Original commit changeset: b7a83bbc719d

Reverts commit D26228721 (https://github.com/pytorch/fairseq/commit/6381aa2bb24f125d271e241c726a2fea581bc3c4)

Reviewed By: theweiho

Differential Revision: D29369494

fbshipit-source-id: 9e745b11bc532ca8ced2816326aa94afbb46ba2d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1921,Omry Yadan,omry@fb.com,2021-07-01 06:37:02-07:00,9bee82e4a7b73249a88f2e2d286e991493ef13c2,https://github.com/pytorch/fairseq/commit/9bee82e4a7b73249a88f2e2d286e991493ef13c2,"Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)

Summary:
## What does this PR do?
Fixes compatibility with Hydra 1.1.
The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.

At this point I am not yet removing the restriction on the Hydra version from setup.py:
1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).
2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.

There will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3659

Reviewed By: omry

Differential Revision: D29498036

Pulled By: lematt1991

fbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1922,alexeib,alexei.b@gmail.com,2021-07-01 08:36:02-07:00,096f492a224e14ed6628d96700f1ae8b534d86a8,https://github.com/pytorch/fairseq/commit/096f492a224e14ed6628d96700f1ae8b534d86a8,"fix xlsr checkpoint finetuning saving issues (#2013)

Summary:
fixes an issue with some old checkpoints that had deep nested namespaces containing choices enum - most prominently xlsr 53 checkpoint

fixes #3634

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2013

Reviewed By: xuqiantong

Differential Revision: D29511325

Pulled By: alexeib

fbshipit-source-id: 79df978afa7482b4ce3aaf7396e193626181aa17",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1923,Wei-Ning Hsu,wnhsu@csail.mit.edu,2021-07-01 13:11:40-07:00,cdc1a553eb2af4fac720880aff4ee2566a28ad21,https://github.com/pytorch/fairseq/commit/cdc1a553eb2af4fac720880aff4ee2566a28ad21,"query tgt_dict after loading task_state (#2019)

Summary:
# Before submitting
`self.task.target_dictionary` is queried before `task_state` is loaded (in `self.load_model_ensemble()`).

## What does this PR do?
Fix the bug above

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2019

Reviewed By: alexeib

Differential Revision: D29523921

Pulled By: wnhsu

fbshipit-source-id: 763b504dc1b4899e623eaa5c19972cec9d0a8985",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1924,Omry Yadan,omry@fb.com,2021-07-06 15:06:07-07:00,dd106d9534b22e7db859a6b87ffd7780c38341f8,https://github.com/pytorch/fairseq/commit/dd106d9534b22e7db859a6b87ffd7780c38341f8,"fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)

Summary:
## What does this PR do?
Some downstream users reported that errors when passing Namespace to load_checkpoint().

A recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.
This changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3675

Reviewed By: omry

Differential Revision: D29564805

Pulled By: lematt1991

fbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1925,Henry Hu,henryhu6@fb.com,2021-07-07 15:41:37-07:00,01576be513488601e936df67e65e2817d18f576e,https://github.com/pytorch/fairseq/commit/01576be513488601e936df67e65e2817d18f576e,"Add tracing annotations

Summary:
Add profile record function at several locations in decoderlib and fariseq to annotate tracing.

Most of the code changes are due to indentation and auto format.

Reviewed By: mikekgfb

Differential Revision: D29531358

fbshipit-source-id: 59934079c0ddc75b5b97922f585f4863680f1041",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1926,alexeib,alexei.b@gmail.com,2021-07-08 15:14:45-07:00,7b710acc9e0106e0359b809f285efc45be6fbfd1,https://github.com/pytorch/fairseq/commit/7b710acc9e0106e0359b809f285efc45be6fbfd1,"Fix static container (#2036)

Summary:
fixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container

also print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036

Reviewed By: arbabu123

Differential Revision: D29608430

Pulled By: alexeib

fbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1927,alexeib,alexei.b@gmail.com,2021-07-08 16:56:58-07:00,58201a15cceccd9b8f8c6463d7338e4252963b33,https://github.com/pytorch/fairseq/commit/58201a15cceccd9b8f8c6463d7338e4252963b33,"migrate roberta glue finetuning to hydra (#2035)

Summary:
this allows roberta finetuning on different tasks using yaml config files + hydra entry point

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2035

Reviewed By: Mortimerp9

Differential Revision: D29601732

Pulled By: alexeib

fbshipit-source-id: 774ef974b4b40ad0ced76874c62047d0c46520e7",14,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1928,Pierce Chuang,pichuang@fb.com,2021-07-08 17:07:51-07:00,605e1ceaa8be442726b0df9351dd28b979761d6e,https://github.com/pytorch/fairseq/commit/605e1ceaa8be442726b0df9351dd28b979761d6e,"change denoising setup_task so that it can read from multiple shards

Summary: Follow Roberta data handling to support | based data separation

Reviewed By: myleott

Differential Revision: D29619263

fbshipit-source-id: 6912df178965c1b0f859604c8f5ad8aff443198a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1929,alexeib,alexei.b@gmail.com,2021-07-08 19:09:58-07:00,d18e44a28994a1558e9f8dc988a23bd6f77a55d5,https://github.com/pytorch/fairseq/commit/d18e44a28994a1558e9f8dc988a23bd6f77a55d5,"add robust w2v model (#2046)

Summary:
add robust wav2vec model

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2046

Reviewed By: wnhsu

Differential Revision: D29628639

Pulled By: alexeib

fbshipit-source-id: 296cd2da579a969a71a0f9ffe1062002b73a8d86",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1930,Wei Ho,weiho@fb.com,2021-07-09 16:13:51-07:00,cffc057b58d9d4b36260d30861f86b2ab8817ac1,https://github.com/pytorch/fairseq/commit/cffc057b58d9d4b36260d30861f86b2ab8817ac1,"Roll back os.path.abspath change

Reviewed By: donhusa

Differential Revision: D29641968

fbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1931,Ann Lee,an918tw@users.noreply.github.com,2021-07-09 16:45:40-07:00,7f2fb5caa872ce06f7cc5d95956f4eca2a6211fe,https://github.com/pytorch/fairseq/commit/7f2fb5caa872ce06f7cc5d95956f4eca2a6211fe,"Release code for the paper ""Discriminative Reranking for Neural Machine Translation"" (#2044)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Release the code for the paper ""Discriminative Reranking for Neural Machine Translation""

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2044

Reviewed By: michaelauli

Differential Revision: D29628590

Pulled By: an918tw

fbshipit-source-id: 7a52602d495b736573187cc721829aa545d24770",12,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1932,alexeib,alexei.b@gmail.com,2021-07-11 15:21:06-07:00,4497897fa506187db96cd9f00614f6dd7080b550,https://github.com/pytorch/fairseq/commit/4497897fa506187db96cd9f00614f6dd7080b550,"respect roberta model name to init arch (#2049)

Summary:
previous PR was always applying base architecture when finetuning roberta

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2049

Reviewed By: arbabu123

Differential Revision: D29642089

Pulled By: alexeib

fbshipit-source-id: e8c6ec6e1fa68bef233f7221e0e02787d7ad2c06",10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1933,alexeib,alexei.b@gmail.com,2021-07-11 21:25:45-07:00,1a1380e5a8b0cce49090676d95044626f208c48c,https://github.com/pytorch/fairseq/commit/1a1380e5a8b0cce49090676d95044626f208c48c,"fix hydra upgrade (#2051)

Summary:
commit 72c0e4f36d150b8244260fba6228b94ff95914bf added ""config_schema"" to config and initialize.py which did not exist, and caused crashes for any interpreted keys (e.g. ""common.tpu"" in task)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2051

Reviewed By: arbabu123

Differential Revision: D29654730

Pulled By: alexeib

fbshipit-source-id: bac18e3d2011de7c822ffcd7e6d4d4364a8edc52",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1934,Jialu Joann Li,jialuli3@fb.com,2021-07-12 13:55:29-07:00,d26dcedaf97e556da8dcab10b5de08cea72c3459,https://github.com/pytorch/fairseq/commit/d26dcedaf97e556da8dcab10b5de08cea72c3459,"pyspeech embedding extractor

Summary: Transformer to extract embeddings from pyspeech encoder models

Reviewed By: vimalmanohar

Differential Revision: D29492082

fbshipit-source-id: abf2432f43c75b93dcccb1a389039e5628f95e92",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1935,alexeib,alexei.b@gmail.com,2021-07-12 17:58:00-07:00,313ff0581561c7725ea9430321d6af2901573dfb,https://github.com/pytorch/fairseq/commit/313ff0581561c7725ea9430321d6af2901573dfb,"migrate masked lm task and criterion (#2050)

Summary:
migrates masked lm task and criterion. old command line flags still continue to work as before

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2050

Reviewed By: arbabu123

Differential Revision: D29648056

Pulled By: alexeib

fbshipit-source-id: 33079a461826ab6d1736d6016ee5e3522bcc5427",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1936,Wei-Ning Hsu,wnhsu@csail.mit.edu,2021-07-12 20:28:51-07:00,f2146bdc7abf293186de9449bfa2272775e39e1d,https://github.com/pytorch/fairseq/commit/f2146bdc7abf293186de9449bfa2272775e39e1d,"refactor clustering in hubert example (#2018)

Summary:
## What does this PR do?
- Fix edge cases of sharded feature extraction in example/hubert/simple_kmeans, where some shard has 0 samples.
- Refactor `example/hubert/simple_kmeans/dump_*_features.py`

## Test
Tested on 2 iterations of clustering from MFCC and HUBERT features

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2018

Reviewed By: hikushalhere

Differential Revision: D29523927

Pulled By: wnhsu

fbshipit-source-id: b089f3d38bd464a940954d358cd5ef091e4892a2",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1937,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,597a8fc5e36110450c6a7251506d17b42b635643,https://github.com/pytorch/fairseq/commit/597a8fc5e36110450c6a7251506d17b42b635643,"Only import MultiheadAttention when doing typechecking

Summary: This might create dependency loops in some cases and we don't use it otherwise.

Reviewed By: myleott, dianaml0

Differential Revision: D29521387

fbshipit-source-id: b4c27426a5965cd864a0c7b353082756099fead5",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1938,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,aa15dc9a1b4cd586dc832a971a5e31ad668f9e22,https://github.com/pytorch/fairseq/commit/aa15dc9a1b4cd586dc832a971a5e31ad668f9e22,"Transformer File Split

Summary: transformer.py in fairseq is enormous. This doesn't change anything but just splits that file in the model file, the encoder file and the decoder file. We then adjust a few imports that were importing from the wrong modules or creating dependency loops.

Reviewed By: myleott

Differential Revision: D29533995

fbshipit-source-id: 776f6bbdadb08d729b0b521fd767b7ac6116a723",6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1939,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,7ebdc24909eaa1478e3423a942f5e41c35b9614c,https://github.com/pytorch/fairseq/commit/7ebdc24909eaa1478e3423a942f5e41c35b9614c,"delegate namespace conversion to DC

Summary: `populate_dataclass` is very basic in how it populates the dataclass. We might want more specific behaviour for some config dataclasses (like the hierarchical behaviour in TransformerConfig, see rest of stack). This diff move the populate logic to a `from_namespace` method in `FairseqDataclass` so that the a specific Dataclass can reimplement it.

Reviewed By: myleott

Differential Revision: D29521388

fbshipit-source-id: f3a6dc80e4ddfc9563c6e85c37c563173f193f4d",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1940,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,bc1504d4d709fd2157b6ba15f754c0307eb734f3,https://github.com/pytorch/fairseq/commit/bc1504d4d709fd2157b6ba15f754c0307eb734f3,"Hierarchical Configs

Summary:
This is a precursor to D29232595

The current behaviour to convert a dataclass to a namespace is that all the fields from all DCs in the field hierarchy are flattened at the top. This is also the legacy behaviour with `add_args`.

This is kind of cumbersome to build reusable Dataclasses as we need to make sure that each field has a unique  name. In the case of Transformer for instance, we have a Decoder and Encoder config that share a large part of their fields (embed_dim, layers, etc.). We can build a single dataclass for this that can be reused and extended in other implementations. To be then able to have  a flat namespace, instead of adding all subfields as is to the root namespace, we introduce the name of the field as prefix to the arg in the namespace.

So:
`model.decoder.embed_dim` becomes `decoder_embed_dim` and `model.encoder.embed_dim` becomes `encoder_embed_dim`.

Reviewed By: myleott, dianaml0

Differential Revision: D29521386

fbshipit-source-id: f4bef036f0eeb620c6d8709ce97f96ae288848ef",2,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],1,12,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,['class TestDataclassUtils(unittest.TestCase):'],"[('Equal', '(args.num_layers, 10)'), ('Equal', '(args.data, )'), ('Equal', '(args.num_layers, 10)'), ('Equal', '(args.foo, 10)'), ('Equal', '(args.data, )'), ('Equal', '(args.encoder_arch_data, )'), ('Equal', '(args.encoder_arch_num_layers, 10)'), ('Equal', '(args.encoder_foo, 10)'), ('Equal', '(args.decoder_data, )'), ('Equal', '(args.decoder_num_layers, 10)'), ('Equal', '(args.lr, 10)'), ('Equal', '(args.data, )')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1941,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,059187f5abbbe8a8118f01daaa4a1feb3a827249,https://github.com/pytorch/fairseq/commit/059187f5abbbe8a8118f01daaa4a1feb3a827249,"ArgumentError when double declaration

Summary: A few subclasses of TransformerModel try to add the same params that have already been added. A workaround is to catch the argparse exception and just ignore it as we know the field was already added if it's thrown.

Reviewed By: myleott, dianaml0

Differential Revision: D29521389

fbshipit-source-id: 7912a260c4f55fbfe486794c9726d6289370400e",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1942,Pierre Andrews,mortimer@fb.com,2021-07-16 04:54:46-07:00,129d8594ccdc6644be84dc249e16489e049f4bfd,https://github.com/pytorch/fairseq/commit/129d8594ccdc6644be84dc249e16489e049f4bfd,"Transformer Hydration pt1. (#1984)

Summary:
## What does this PR do?

In https://github.com/fairinternal/fairseq-py/tree/hydra-transformer I tried to convert TransformerModel to hydra directly, but then had to deal with upgrading a lot of downstream classes and this got out of hand. I am trying a different approach here, this is my strategy in this PR:
0- make the argparse backward converter support hierarchical configs. This way, I can clean up the config and split it in ""sub-configs"" for Encoder/Decoder. This simplifies the config object and allows for code reusability. In the future, should simplify creating more specific configs for Enc/Dec.
1- Have a base classe that is hydrated (but not registered as model). This also mean hydrating the Encoder/Decoder/Layer classes.
2- first hide this behind a legacy model that is still called TransformerModel (to not break imports, etc. etc.). This legacy model transforms the argparse namespace in the dataclass config when it needs to.
3- make the dataclass look like the argparse namespace so that it can be used without subclassing/hydrating all the downstream classes under TransformerModel (see other branch to see what this involves)

test_binaries seems to run fine but for one state loading issue. I am still digging into that but wanted to make sure this was a good approach.

(I also decided to split the `transformer.py` file as it was getting a bit too large and I like making merges with master miserable)

## Next Steps

- Separate PR to create a registered Hydra model, ideally renaming TransformerModel to TransformerModelLegacy and codemoding everything to inherit from TransformerModelLegacy.
- add equivalent test_binaries test that run with hydra main (already done in other branch)
- start converting some downstream models to hydra where needed.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1984

Reviewed By: dianaml0

Differential Revision: D29232595

Pulled By: Mortimerp9

fbshipit-source-id: f12eadfa9ccff29f28c67b527f9996311f791359",8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1943,Pierre Andrews,mortimer@fb.com,2021-07-16 05:45:38-07:00,c1624b273b206cc7c0a1529be4d2f35b38607ec5,https://github.com/pytorch/fairseq/commit/c1624b273b206cc7c0a1529be4d2f35b38607ec5,"Criterions to Hydra

Summary: convert a couple of criterions to hydra

Reviewed By: dianaml0

Differential Revision: D29585608

fbshipit-source-id: 7790b767ed55f58bbcc0c237cfa689684b9bf5e2",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1944,Myle Ott,myleott@fb.com,2021-07-20 11:30:28-07:00,72323586aeae75e2b704c1c936784471bfa75019,https://github.com/pytorch/fairseq/commit/72323586aeae75e2b704c1c936784471bfa75019,"Add warning when combining --ddp-backend=fully_sharded and --update-freq (#2076)

Summary:
Add warning when combining `--ddp-backend=fully_sharded` and `--update-freq`, since that will result in increased memory usage.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2076

Reviewed By: xianxl

Differential Revision: D29791364

Pulled By: myleott

fbshipit-source-id: 5748f20484840f61a16448f1287b0f2e3b3ce9d8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1945,Xutai Ma,xutaima@gmail.com,2021-07-21 13:16:15-07:00,804b49397606328221dd7296026c9bcc04a967d1,https://github.com/pytorch/fairseq/commit/804b49397606328221dd7296026c9bcc04a967d1,"Several updates on simultaneous translation (#1831)

Summary:
This pull request includes several updates and refactoring related to simultaneous translation
1. Add mixed precision training for simultaneous translation decoder (avoiding nan errors)
2. Add unit test for simultaneous decoders `cd examples/simultaneous_translation; python -m unittest test_text_models.py`
3. Simplify the inference code (simuleval only)
4. Reorganize code structure
5. Remove duplicated / deprecated code
6. Fixed a bug for waitk p_choose generation
7. Fixed a issue when using fixed_pre_decision + mma

The update won't affect the current training. The old checkpoint can still be loaded and infered.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1831

Test Plan:
Imported from GitHub, without a `Test Plan:` line.
f286829279
f286829286

Reviewed By: sravyapopuri388

Differential Revision: D28082398

Pulled By: xutaima

fbshipit-source-id: 882d077e7f1b94870f8328dba89660a4f3bd5d9c",9,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],0,10,4,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('True', '(p_choose.le(1.0).all())'), ('True', '(p_choose.ge(0.0).all())'), ('True', '(p_choose.size() == alpha_system.size())'), ('True', '('), ('False', '(torch.isnan(item[key]).any())'), ('True', '(beta_system.size() == alpha_system.size())'), ('True', '(p_choose.size() == alpha_system.size())'), ('True', '('), ('True', '(p_choose[bsz_i, i, j] == 1)'), ('True', '(p_choose[bsz_i, i, j] == 0)')]","['def setUp(self):', 'def setUp(self):', 'def setUp(self):', 'def setUp(self):']",[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],['('],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1946,Changhan Wang,changhan@fb.com,2021-07-21 13:29:09-07:00,95a9cb798dc8d6375428c6a9502234de8b4c3ec8,https://github.com/pytorch/fairseq/commit/95a9cb798dc8d6375428c6a9502234de8b4c3ec8,"add text compression to FileAudioDataset + speed up AddTargetDataset

Summary:
# Add text compression to FileAudioDataset

The FileAudioDatasetstores the full manifest (mainly raw texts --- audio filenames + target texts for finetuning) in memory. This leads to large memory usage as data is duplicated in multiple workers. And it limits the scale of training data that we can use before having to switch to `BinarizedAudioDataset`. This diff aims at alleviating this limitation by in-memory text compression.

This technique can be applied to any other dataset classes that store raw texts (e.g. `speech_to_text_dataset`).

### Implementation
- `zlib` for low-level compression: built-in, relatively fast (as shown in the benchmarking below)
- `unishox2` for high-level compression: optimized for short texts but relatively slower

### Benchmarking

Tested with single process/thread.

**On 3.6G TSV manifest (ASCII filenames) data**

| Compression Level | CPU Mem Usage | Loading + Encoding | Decoding |
|---|---|---|---|
| No | 6782.18MB | 00:32 | - |
| Low | 6450.04MB (95.10%) | 04:39 | 01:03 |
| High | 4742.91MB (69.93%) | 08:49 | 01:31|

**On 7.8G label (Arabic text) data**
| Compression Level | CPU Mem Usage | Loading + Encoding | Decoding |
|---|---|---|---|
| No | 14623.57MB | 00:58 | - |
| Low | 10773.65MB (73.67%) | 05:38 | 01:45 |
| High | 7352.67MB (50.28%) | 25:03 | 04:31 |

The difference on CPU memory usage will be enlarged when data is duplicated in multiple dataloading workers.

# Speed up AddTargetDataset
AddTargetDataset gets label length from tensorized data which is very slow --- leading to 6+hrs for batching ~8G text. Replaced it with a helper function that gets length from untokenized string data and reduced the time from 6+hrs to 10min.

Reviewed By: cndn

Differential Revision: D29093876

fbshipit-source-id: b6b9a8da61944771bb7c8cb57b207ed4d79d0764",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1947,Myle Ott,myleott@fb.com,2021-07-21 15:05:35-07:00,698961dc0d2eb17d15e9665f6f31fd4c1c4b58c7,https://github.com/pytorch/fairseq/commit/698961dc0d2eb17d15e9665f6f31fd4c1c4b58c7,"Make FSDP and --update-freq play nice (#3727)

Summary:
Previously combining FSDP with `--update-freq` would result in significant memory usage because full-size gradients would be accumulated on each GPU. We can instead skip the `no_sync` context manager in this case. The tradeoff is more communication (we do reduce-scatter on each backward), but the memory savings are likely to be worth it in most cases.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3727

Reviewed By: sshleifer

Differential Revision: D29824021

Pulled By: myleott

fbshipit-source-id: 2d942586cc1a9ac33fd34b8709df91dda870dd49",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1948,Ishani Karmarkar,ikarmarkar@fb.com,2021-07-21 23:11:59-07:00,5826a6855f05cab919f567a68b66d6d8c1817551,https://github.com/pytorch/fairseq/commit/5826a6855f05cab919f567a68b66d6d8c1817551,"Allow Moving Observer to Cuda

Summary: Allow for moving observer to cuda in emulate_int8_{method} functions in order to make it compatible with pytext trainers.

Reviewed By: huihuifan

Differential Revision: D29686263

fbshipit-source-id: 7eddaadb17163bd4be33de1ae729621d043520d2",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1949,Ishani Karmarkar,ikarmarkar@fb.com,2021-07-22 05:00:38-07:00,7feb8747b2f88752a5a5c5a5c85b0b3e428ca549,https://github.com/pytorch/fairseq/commit/7feb8747b2f88752a5a5c5a5c85b0b3e428ca549,"Support Quantization to variable number of bits

Summary: allow quantization using quant noise with variable number of bits

Reviewed By: huihuifan

Differential Revision: D29686262

fbshipit-source-id: 661e7d684f006af891191b32639651214df79bc4",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1950,Ishani Karmarkar,ikarmarkar@fb.com,2021-07-22 05:16:11-07:00,eff39d5d453497a5a6e5e998e2a920fb5f0618e1,https://github.com/pytorch/fairseq/commit/eff39d5d453497a5a6e5e998e2a920fb5f0618e1,"quantize_layers_ using variable methods

Summary: allow for moving average per channel and per channel observer to be customized when calling quantize_model_

Reviewed By: huihuifan

Differential Revision: D29686457

fbshipit-source-id: 37b833d27d643e6963c9ab7e0af9b8f889e1a2fc",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1951,Kenneth Heafield,kheafield@fb.com,2021-07-22 10:38:47-07:00,bc3bd55ec98c39af45ff7323ae49bcbdf93acc36,https://github.com/pytorch/fairseq/commit/bc3bd55ec98c39af45ff7323ae49bcbdf93acc36,"Fix anchor link for inference & evaluation

Summary: The link in the documentation was wrong due to & in the anchor.

Reviewed By: xutaima

Differential Revision: D29850940

fbshipit-source-id: 5024802e868f4a2f5440a35f1792bf5337fd3abf",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1952,Edan Tessel Sneh,edan@fb.com,2021-07-23 10:43:24-07:00,e1adc9d388c1af907366c5f328bb7672f618a73c,https://github.com/pytorch/fairseq/commit/e1adc9d388c1af907366c5f328bb7672f618a73c,"hacky fix for gen_parser bug

Summary: hacky fix for gen_parser bug to unblock fbtranslate

Reviewed By: theweiho

Differential Revision: D29865385

fbshipit-source-id: 0d92c8c67c465cec6eb309185087aec469ade713",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1953,Shashank Chaudhry,gandalf@fb.com,2021-07-23 11:07:47-07:00,67ff6baa42c1208d0da85f5af2f01689034d1dfd,https://github.com/pytorch/fairseq/commit/67ff6baa42c1208d0da85f5af2f01689034d1dfd,"Apply the CLANGFORMAT linter to fbcode/deeplearning/projects/fairseq-py/**

Summary:
Try to lint the folders in deeplearning/projects/**.

Context: FBCode doesn't enable CLANGFORMAT linter over the entire repo because too many files currently don't conform to it. This is an attempt to migrate folder paths to use CLANGFORMAT conventions.

Doc: https://fburl.com/clangformat-for-fbcode

Actions: Please check the formatting recommendations and accept if they seem ok, or reply if there is a reason for concern.

Command run:
arc lint --take CLANGFORMAT -a --paths-cmd 'hg files deeplearning/projects/fairseq-py/**'

Reviewed By: dianaml0

Differential Revision: D29838082

fbshipit-source-id: 3afbff42e239a6376543c4a849da4221dbf7eb32",18,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1954,Omry Yadan,omry@fb.com,2021-07-26 16:35:40-07:00,53802e781291b63e656c89818c38bfc49ff0f108,https://github.com/pytorch/fairseq/commit/53802e781291b63e656c89818c38bfc49ff0f108,"Compatibility fix with Hydra 1.1 (#3722)

Summary:
One of the changes in Hydra 1.1 is that the default composition order is changing.
This is documented [here](https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order).
In Hydra 1.1, a config is overriding values introduced by the defaults list while in Hydra 1.0 - the defaults list was overriding the values in the config.

fairseq is currently depending on the previous behavior:
The class `FairseqConfig` defines config values, and it's expecting them to be overridden by the defaults list.
This result in a different config being created when running `fairseq_cli/hydra_train.py` with Hydra 1.0 and with 1.1.

Hydra 1.1 introduced the `_self_` keyword in the defaults list to control the composition order. In order to achieve the behavior of Hydra 1.0, `_self_` should be added as the first item in the defaults list.

To allow for a smoother migration, Hydra 1.0 is ignoring `_self_` starting from 1.0.7 (previous versions will issue an error).

This diff adds `_self_` as the first item in the defaults list the fairseq config, and introduce a dependency a Hydra 1.0 version that is equal or newer to 1.0.7.

### Testing:
I ensured that the following yield the same composed config:
Default config with Hydra 1.0.6, 1.0.7 and 1.1.0

`examples/wav2vec/config/finetuning/base_10h.yaml` with Hydra 1.0.6, 1.0.7 and 1.1.0.

This can be achieved by outputing the generated config using `--cfg job` and compating the outputs.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3722

Reviewed By: dianaml0

Differential Revision: D29917677

Pulled By: jieru-hu

fbshipit-source-id: 7e645b83cccb03fc80a6702e302c4643d2b14a78",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1955,Vaibhav Singh,sivaibhav@google.com,2021-07-27 12:59:08-07:00,0769cfe2e9ecd8c2dd15cb2491474ef0b4b3d0e2,https://github.com/pytorch/fairseq/commit/0769cfe2e9ecd8c2dd15cb2491474ef0b4b3d0e2,"Fixed the reference to mask_channel_prob in task cfg (#3742)

Summary:
Updated example config file for tpu to include mask parameters.
Noted currently cli bug in README

## What does this PR do?
Fixes # (3741) B.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3742

Reviewed By: arbabu123

Differential Revision: D29938257

Pulled By: alexeib

fbshipit-source-id: 6ab5cd2974949806621fb37cb13d918bea733a73",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1956,Yun Tang,yuntang@fb.com,2021-07-27 13:23:07-07:00,7ca95a66d64411deea49cb8710195ed2e0699f0a,https://github.com/pytorch/fairseq/commit/7ca95a66d64411deea49cb8710195ed2e0699f0a,"Add speech/text joint training for speech to text task (step 1)

Summary:
1. adding feature to generate raw audio with target sampling rate
2. fix bugs a)  empty sample in transform_eos_lang_pair_dataset.py b) S2T
decoding with language model

Reviewed By: kahne

Differential Revision: D29699692

fbshipit-source-id: cc4b76618ef3b43dbba53a422f24597b9866d17f",5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1957,Changhan Wang,changhan@fb.com,2021-07-27 13:24:45-07:00,75051ecf26239be1b082101b9d1fe8886e734b45,https://github.com/pytorch/fairseq/commit/75051ecf26239be1b082101b9d1fe8886e734b45,"wav2vec2 speech translation OSS

Summary:
wav2vec2 speech translation OSS
- Based on https://github.com/fairinternal/fairseq-py/pull/1829
- Updated `Wav2VecEncoder` API to make it consistent for `Wav2VecCTC` (for ASR) and `Wav2Vec2Seq2Seq` (for ST)
- Small fixes in `Wav2Vec2Seq2Seq`
- Refactored `audio_pretraining` into `audio_pretraining` and `audio_finetuning`

Reviewed By: sravyapopuri388, cndn

Differential Revision: D29285182

fbshipit-source-id: 89f93b42caa88079940a4b2cac0f8952547d3ff0",17,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1958,Edan Tessel Sneh,edan@fb.com,2021-07-28 14:58:47-07:00,440def26c167af762ae8e8fc64716e1b88f3968b,https://github.com/pytorch/fairseq/commit/440def26c167af762ae8e8fc64716e1b88f3968b,"added more descriptive comment to urgent change

Summary: Added description of problem and task associated with it in code

Reviewed By: dianaml0

Differential Revision: D29942196

fbshipit-source-id: 037d92af720f52bdd51f4efc5b9d49a6465cdd31",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1959,Henry Hu,henryhu6@fb.com,2021-07-29 12:03:01-07:00,1df3e50a2f3d93cd815d2a8730d6a3c8271af891,https://github.com/pytorch/fairseq/commit/1df3e50a2f3d93cd815d2a8730d6a3c8271af891,"Minor performance optimization for sequence generator

Summary: Minor performance optimization for sequence generator finalize_hypos function.

Reviewed By: myleott

Differential Revision: D29784509

fbshipit-source-id: ac42dda75995cea3750c8d4ef07d8950a891c5f8",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1960,Alex Xiao,axiao@fb.com,2021-07-29 15:59:22-07:00,f1b447075844e7cb912879786c787e30d413ebda,https://github.com/pytorch/fairseq/commit/f1b447075844e7cb912879786c787e30d413ebda,"use suffix when saving best checkpoints with metric

Summary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`

Reviewed By: sshleifer

Differential Revision: D29947728

fbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1961,Alex Xiao,axiao@fb.com,2021-07-29 15:59:22-07:00,3cf9053ea260c0de31beaa8948ee4acae2221ba1,https://github.com/pytorch/fairseq/commit/3cf9053ea260c0de31beaa8948ee4acae2221ba1,"use pathmanager to delete old checkpoints

Summary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.

Reviewed By: myleott, sshleifer

Differential Revision: D29947965

fbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1962,Alex Xiao,axiao@fb.com,2021-07-29 15:59:22-07:00,777bba3a41bb92aecc611037e49b1e42fee24836,https://github.com/pytorch/fairseq/commit/777bba3a41bb92aecc611037e49b1e42fee24836,"seed random suffix in checkpoint to be consistent across shards

Summary:
Currently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.

This diff seeds the random suffix to be consistent across shards

Reviewed By: zhengwy888

Differential Revision: D29951167

fbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1963,Yun Tang,yuntang@fb.com,2021-07-29 17:43:20-07:00,7a6706f5a3cd9995c9370ec7adcd2da454aecd97,https://github.com/pytorch/fairseq/commit/7a6706f5a3cd9995c9370ec7adcd2da454aecd97,"Add speech/text joint training for speech to text task (step 2)

Summary:
Add scripts for speech/text joint training for the speech to text task. It
includes scripts/recipes from the following papers
""A General Multi-Task Learning Framework to Leverage Text Data for Speech to
Text Tasks"", ICASSP 2021
""Improving Speech Translation by Understanding and Learning from the Auxiliary
Text Translation Task"", ACL 2021
""FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared
Task"", IWSLT 2021

Reviewed By: kahne

Differential Revision: D29820444

fbshipit-source-id: 925eaedb69233e0a6f4c110045db63a6007a2b60",18,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1964,Wei-Ning Hsu,31931787+wnhsu@users.noreply.github.com,2021-07-30 10:15:09-07:00,20fbc348215e558d23da9461a5daaec85b97d114,https://github.com/pytorch/fairseq/commit/20fbc348215e558d23da9461a5daaec85b97d114,"update hubert decode config (#2106)

Summary:
Update HuBERT decode config yaml to make compatible with the new decoder config

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2106

Reviewed By: alexeib

Differential Revision: D29967631

Pulled By: wnhsu

fbshipit-source-id: fe39c5126f50c3024022f8333e2f3aa97065cbfc",3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1965,Ann Lee,annl@fb.com,2021-07-30 14:33:53-07:00,972401937b9aa44e45ee3380fa497c8eb30005c4,https://github.com/pytorch/fairseq/commit/972401937b9aa44e45ee3380fa497c8eb30005c4,"add paper link (#2116)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2116

Reviewed By: michaelauli

Differential Revision: D30019908

Pulled By: an918tw

fbshipit-source-id: ca8d7a6e97ed81e7df9a15e778c68fad8fb0a308",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1966,Ishani Karmarkar,ikarmarkar@fb.com,2021-07-30 17:41:47-07:00,9d70f9ca6eb0dc49065a2691f302df2e68c1cac4,https://github.com/pytorch/fairseq/commit/9d70f9ca6eb0dc49065a2691f302df2e68c1cac4,"iPQ

Summary: Implemented iterative product quantization (iPQ trainer) and unit tests

Reviewed By: AkshatSh, AdithyaSagar007

Differential Revision: D29662949

fbshipit-source-id: fdc1f124decc722b54225a7fe0031695823e1c69",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1967,Wei-Ning Hsu,wnhsu@csail.mit.edu,2021-08-02 12:39:30-07:00,82e8a24bddc1cb5aecb267bb529f76f5b3901432,https://github.com/pytorch/fairseq/commit/82e8a24bddc1cb5aecb267bb529f76f5b3901432,"add max_keep_size (#2124)

Summary:
Set max_keep_size to filter long utterances. Needed when trained on labeled datasets with long utterances.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2124

Reviewed By: Abdel-rahmanMohamed

Differential Revision: D30046509

Pulled By: wnhsu

fbshipit-source-id: ec52ae0997284a05295dff35626927a71c78cf52",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1968,Jingfei Du,jingfeidu@fb.com,2021-08-02 14:35:22-07:00,db4f96b09295ffae2e534adbc6eefcd9f2d2089f,https://github.com/pytorch/fairseq/commit/db4f96b09295ffae2e534adbc6eefcd9f2d2089f,"fixing checkpoint config upgrade for generation print_alignment (#2125)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes config upgrade conditions for upgrading generation. print_alignment

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125

Reviewed By: myleott

Differential Revision: D30049140

Pulled By: jingfeidu

fbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1969,Edan Tessel Sneh,edan@fb.com,2021-08-02 18:39:08-07:00,fe15926d48167c5028d2d7e2ed7d0a66642d0700,https://github.com/pytorch/fairseq/commit/fe15926d48167c5028d2d7e2ed7d0a66642d0700,"Adding Hydra based trainer target to fairseq in fbcode

Summary: Adding fairseq entrypoint section of e2e pipeline so FairseqConfig to hydra_main, runs smoothly

Reviewed By: jieru-hu

Differential Revision: D29714729

fbshipit-source-id: e3694e0037bb4c4f69208c1d6ec7df91d42fb588",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1970,Ishani Karmarkar,ikarmarkar@fb.com,2021-08-02 21:57:40-07:00,3d90df4a1fd3e7734622f7bd6cfda5dd6b4d3aef,https://github.com/pytorch/fairseq/commit/3d90df4a1fd3e7734622f7bd6cfda5dd6b4d3aef,"Quant Noise

Summary: Implemented fix bit scalar quantization with quant noise for pytext models

Reviewed By: AkshatSh

Differential Revision: D29662977

fbshipit-source-id: ebab68a4a5ff1583a0c6dfadcf2671663e232c18",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1971,Sam Shleifer,sshleifer@gmail.com,2021-08-04 16:30:23-07:00,9825786fbe8a32053f21bec988d953d175f7262a,https://github.com/pytorch/fairseq/commit/9825786fbe8a32053f21bec988d953d175f7262a,"--fp16-adam-stats (#2139)

Summary:
- stores exp_avg and exp_sq_avg in fp16, with `scale` variables to avoid overflow.
- myleott added this to gshard, following github.com/openai/jukebox/blob/master/jukebox/utils/fp16.py

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2139

Reviewed By: myleott

Differential Revision: D30113175

Pulled By: sshleifer

fbshipit-source-id: 03995c8eb096629675eadec4e7b8e7f18fc2730e",4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1972,Kushal Lakhotia,kushall@fb.com,2021-08-11 16:07:06-07:00,741fd138c69eebc1fac09b590047da85cdeafe2d,https://github.com/pytorch/fairseq/commit/741fd138c69eebc1fac09b590047da85cdeafe2d,"Commit README for GSLM (#2151)

Summary:
## What does this PR do?
Adds GSLM directory with README.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2151

Reviewed By: wnhsu

Differential Revision: D30147672

Pulled By: hikushalhere

fbshipit-source-id: bcc7cbbde3626ea3d91917707a91aff85d715baa",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1973,alexeib,alexei.b@gmail.com,2021-08-17 06:55:26-07:00,2513524a1604dbafcc4ea9cc5a99ae0aa4f19694,https://github.com/pytorch/fairseq/commit/2513524a1604dbafcc4ea9cc5a99ae0aa4f19694,"add finetuned robust w2v models and update readme (#2196)

Summary:
adds finetuned robust w2v models and updates readme

fixes #3721

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2196

Reviewed By: wnhsu

Differential Revision: D30367999

Pulled By: alexeib

fbshipit-source-id: 616b373bf31265c89f694fba7dccce2961d394f3",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1974,Vaibhav Singh,sivaibhav@google.com,2021-08-17 07:02:41-07:00,cb747010c47a017e71285556afa9acce0ec62786,https://github.com/pytorch/fairseq/commit/cb747010c47a017e71285556afa9acce0ec62786,"Set batch size to 4 to prevent OOM due dynamic batch sizing (#3781)

Summary:
## What does this PR do?
Fixes OOM which happens from TPUs due to dynamic batching exceed the max a single core can work with.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3781

Reviewed By: wnhsu

Differential Revision: D30327091

Pulled By: alexeib

fbshipit-source-id: 0ebe6b18329fa05d359083fa8ac54aba7b48bc53",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1975,Pierre Andrews,mortimer@fb.com,2021-08-19 12:49:06-07:00,1f7ef9ed1e1061f8c7f88f8b94c7186834398690,https://github.com/pytorch/fairseq/commit/1f7ef9ed1e1061f8c7f88f8b94c7186834398690,"(fix #2177) Erase the encoder_embed_dim default (#2213)

Summary:
Fix https://github.com/fairinternal/fairseq-py/issues/2177 for the transformer conversion to Hydra.

The way the defaults are dealt with now is different so when you use the legacy Namespace configuration, you end up with a default encoder_embed_dim, which in the VGG case sets up a encoder attention in the TransformerDecoderLayer with the wrong dimentions.
The easiest solution is to erase the default value for encoder_embed_dim (by forcing it to None) when converting the VGG config to the raw Namespace for the decoder layer.

Tested with:
`pytest tests/speech_recognition/test_vggtransformer.py -k Transformer`

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2213

Test Plan: pytest tests/speech_recognition/test_vggtransformer.py -k Transformer

Reviewed By: sshleifer

Differential Revision: D30425143

Pulled By: Mortimerp9

fbshipit-source-id: 92f6dea2ffbb68e441700bcc55274b3167a587b3",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1976,Kushal Lakhotia,kushall@fb.com,2021-08-26 05:46:38-07:00,6f847c8654d56b4d1b1fbacec027f47419426ddb,https://github.com/pytorch/fairseq/commit/6f847c8654d56b4d1b1fbacec027f47419426ddb,"Release GSLM (#2201)

Summary:
## What does this PR do?
Open sourcing code for Generative Spoken Language Modeling

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2201

Reviewed By: wnhsu, eugene-kharitonov

Differential Revision: D30563114

Pulled By: hikushalhere

fbshipit-source-id: 6c1ee3b29038fd2c9fb5939bddcc70af0794dab4",47,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1977,Myle Ott,myleott@fb.com,2021-08-27 05:54:21-07:00,db0175a882e8ae0f30d89b5a610373dbe032d528,https://github.com/pytorch/fairseq/commit/db0175a882e8ae0f30d89b5a610373dbe032d528,"Require fairscale >= 0.4.0 to combine FSDP and --update-freq (#2239)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2239

Reviewed By: sshleifer, ngoyal2707

Differential Revision: D30574791

Pulled By: myleott

fbshipit-source-id: 0f83e6ffe53d608292545884df269a604a57448d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1978,Jingfei Du,jingfeidu@fb.com,2021-08-30 18:05:50-07:00,932a3d4aad6cae3ef05aad59e257eba1c765a36c,https://github.com/pytorch/fairseq/commit/932a3d4aad6cae3ef05aad59e257eba1c765a36c,"fix beam search with prefix tokens (#2227)

Summary:
1. added test for genereting pad tokens during beam search with prefix
tokens
2. modified lprobs for pad token and prefix tokens to avoid generating
pad

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2227

Reviewed By: xianxl

Differential Revision: D30649356

Pulled By: jingfeidu

fbshipit-source-id: d94903a912e767391c8fca61f98f65b5cea3b56e",2,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,[],0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],"[('Equal', '(d.pad(), 1)'), ('Equal', '(d.eos(), 2)'), ('Equal', '(d.unk(), 3)')]",['def setUp(self):'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1979,Rengan Xu,renganxu@fb.com,2021-08-30 21:47:52-07:00,5277ec47bdb51165592596e2a2c4c7ee650d9958,https://github.com/pytorch/fairseq/commit/5277ec47bdb51165592596e2a2c4c7ee650d9958,"Fix test_eval_bleu unittest (#2236)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2236

The test_eval_bleu unittest in TestTranslation in tests/test_binaries.py failed after the scarebleu version is updated to 2.0.0 in OSS testing tool. Added the fix so that the test can pass when scarebleu version is both 1.x and 2.0.0.

Reviewed By: myleott, sravyapopuri388

Differential Revision: D30525920

fbshipit-source-id: 8ef27509cec45422a8d22003c87c2a7acb55225d",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1980,Pierre Andrews,mortimer@fb.com,2021-08-31 01:11:34-07:00,68a81202a371574b3acb5f8a8c36bfac7ab255ed,https://github.com/pytorch/fairseq/commit/68a81202a371574b3acb5f8a8c36bfac7ab255ed,"Indexed Huffman Coded dataset (#2029)

Summary:
## What does this PR do?

Currently, binarized dataset are stored as a bin representation of int tensors. At best, each int is coded as uint16 on disk.

When coding a fixed size vocabulary dataset where we know the frequency of each symbol and where some symbols are more common than other, we can do better. This happens in particular when binarizing a dataset split in subword units as the most common ""tokenizers"" like bpe and spm will choose subwords with high frequencies over subwords with low frequencies.

In practice, if we know the frequency of all symbols (or a good estimate), we can use entropy encoding methods to compress the data. The idea is to assign a compressed representation where frequent symbols will have shorter representations than unfrequent symbols.

In this PR, we build a Huffman code from a frequency table and use this code to encode a dataset. The PR provides the huffman coder implementation (using the single queue approach as we usually start with a sorted set of symbols) as well as a memory map implementation of a dataset that stores the data compressed with a huffman code and can return indexed tensors from it.

Over a whole dataset, depending on how many symbols we sample to evaluate the frequency, we can save between 25% and 30% of storage space.

## Follow Ups

currently the binarizer/preprocess script make too many assumptions about the dataset writers so the huffman dataset writer cannot be used straight out of the box with it. I will make follow ups PRs to provide easy to use scripts to build such datasets. But it's as simple as doing:
```
code_builder = HuffmanCodeBuilder()
with open(sample_file, 'r', encoding=""utf-8"") as input:
    for line in input:
        code_builder.add(*line.strip().split("" ""))

coder = code_builder.build_code()

with HuffmanMMapIndexedDatasetBuilder('/tmp/testing_huffman', coder) as builder:
    with open(dataset_file, 'r', encoding=""utf-8"") as input:
        for line in input:
            builder.add_item(line.strip().split(' '))
```

a lot of the `HuffmanMMapIndexedDataset` code comes from the normal `MMapIndexedDataset` and we could probably extract commonalities in a base class

the `HuffmanCoder` is also really a special kind of `Dictionary` and again, a common base class could be abstracted out of them.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2029

Reviewed By: dianaml0

Differential Revision: D29557468

Pulled By: Mortimerp9

fbshipit-source-id: a01b6d98f38f937934cadebb3786133e257adefe",9,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],3,15,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestCodeBuilder(unittest.TestCase):', 'class TestCoder(unittest.TestCase):', 'class TestHuffmanDataset(unittest.TestCase):']","[('Equal', '(builder.symbols, counts)'), ('Equal', '(new_builder.symbols, counts + counts)'), ('Equal', '(builder.symbols, other_builder.symbols)'), ('Equal', '(coder, other_coder)'), ('Equal', '(decoded, data)'), ('Equal', '(unseen_decoded, unseen_data)'), ('Equal', '(len(dataset), len(data))'), ('Equal', '(decoded, data)'), ('Equal', '(data_sizes, sizes(data))'), ('Less', '(huff_size, mmap_size)'), ('Equal', '(len(dataset), len(data1) + len(data2))'), ('Equal', '(decoded1, data1)'), ('Equal', '(decoded2, data2)'), ('Equal', '(data_sizes[: len(data1)], sizes(data1))'), ('Equal', '(data_sizes[len(data1) : len(dataset)], sizes(data2))')]",[],[],[],[],[],[],[],[],['import unittest'],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1981,Vimal Manohar,vimalmanohar@fb.com,2021-09-01 11:43:25-07:00,8feccf94412424a4683b01090de36fa77cb4951d,https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d,"EMA

Summary:
Adds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759

1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.
2. `ema.ema_start_update` to control when the EMA starts accumulating
3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)
4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.

```
This module has the EMA class used to store a copy of the exponentially decayed
model params.

Typical usage of EMA class involves initializing an object using an existing
model (random or from a seed model) and setting the config like ema_decay,
ema_start_update which determine how the EMA model is updated. After every
update of the model i.e. at the end of the train_step, the EMA should be updated
by passing the new model to the EMA.step function. The EMA model state dict
can be stored in the extra state under the key of ""ema"" and dumped
into a checkpoint and loaded. The EMA object can be passed to tasks
by setting task.uses_ema property.
EMA is a smoothed/ensemble model which might have better performance
when used for inference or further fine-tuning. EMA class has a
reverse function to load the EMA params into a model and use it
like a regular model.
```

Reviewed By: cruvadom

Differential Revision: D24238379

fbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2",8,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,[],2,28,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"['class TestEMAGPU(unittest.TestCase):', 'class TestEMAGPU(unittest.TestCase):']","[('LessEqual', '('), ('Equal', '(ema.get_decay(), config.ema_decay)'), ('Equal', '(ema.get_model(), ema.model)'), ('Equal', '(len(ema.fp32_params), 0)'), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)'), ('True', '('), ('In', '(key, ema.fp32_params)'), ('LessEqual', '('), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)'), ('LessEqual', '('), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)'), ('LessEqual', '('), ('Equal', '(ema.get_decay(), config.ema_decay)'), ('Equal', '(ema.get_model(), ema.model)'), ('Equal', '(len(ema.fp32_params), 0)'), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)'), ('True', '('), ('In', '(key, ema.fp32_params)'), ('LessEqual', '('), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)'), ('LessEqual', '('), ('TorchAllClose', '('), ('Equal', '(len(ema.fp32_params), 0)')]",[],[],[],[],"['not torch.cuda.is_available(), )']",[],[],[],"['import unittest', 'import unittest']",[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1982,Koustuv Sinha,koustuvsinha@hotmail.com,2021-09-01 13:13:08-07:00,14c5bd027f04aae9dbb32f1bd7b34591b61af97f,https://github.com/pytorch/fairseq/commit/14c5bd027f04aae9dbb32f1bd7b34591b61af97f,"Releasing models for our paper ""Masked Language Modeling and the Distributional Hypothesis"" (#1930)

Summary:
Paper submitted to EMNLP: https://arxiv.org/abs/2104.06644

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1930

Reviewed By: lematt1991

Differential Revision: D28885634

Pulled By: shruti-bh

fbshipit-source-id: d433c87cff3603b3e676a129029a827c510a72c7",2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
1983,Jingfei Du,jingfeidu@fb.com,2021-09-07 13:18:00-07:00,5cfd373876ad374139b2de15735a870d4797c606,https://github.com/pytorch/fairseq/commit/5cfd373876ad374139b2de15735a870d4797c606,"fix default lprob score of beam search with prefix tokens (#2267)

Summary:
# Before submitting
the default score was set as min score of all lprobs, which would let us select tokens other than prefix tokens during beam search. having a pretty hacky way to make it smaller than any lprobs.
- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2267

Reviewed By: myleott

Differential Revision: D30730475

Pulled By: jingfeidu

fbshipit-source-id: 7dab4e9ed2fc094910467bad776155230987e21a",1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,[],0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]
